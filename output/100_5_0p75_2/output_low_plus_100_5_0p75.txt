nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [21:42<35:49:18, 1302.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [43:29<35:31:53, 1305.24s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [1:05:07<35:04:35, 1301.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:27:00<34:49:52, 1306.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [1:44:17<31:54:22, 1209.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [2:03:24<31:01:30, 1188.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [2:25:03<31:37:43, 1224.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [2:46:50<31:57:24, 1250.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [3:08:27<31:59:02, 1265.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [3:30:18<31:59:06, 1279.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [3:48:23<30:09:16, 1219.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [4:10:01<30:23:59, 1243.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [4:31:40<30:27:29, 1260.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [4:46:41<27:31:07, 1151.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [5:08:21<28:14:59, 1196.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [5:23:15<25:47:28, 1105.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [5:45:00<26:52:06, 1165.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [6:06:55<27:34:13, 1210.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [6:28:29<27:47:53, 1235.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [6:50:07<27:52:21, 1254.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [7:11:45<27:48:37, 1267.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [7:33:27<27:41:02, 1277.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [7:55:06<27:27:58, 1284.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [8:16:48<27:13:17, 1289.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [8:38:23<26:53:51, 1291.09s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [9:00:01<26:35:13, 1293.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [9:21:36<26:13:54, 1293.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [9:43:10<25:52:30, 1293.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [10:04:48<25:32:28, 1295.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [10:26:37<25:15:55, 1299.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [10:48:12<24:52:49, 1298.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [11:09:20<24:21:01, 1289.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [11:30:57<24:02:05, 1291.43s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [11:52:38<23:43:46, 1294.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [12:14:19<23:24:16, 1296.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [12:35:51<23:01:20, 1295.00s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [12:57:27<22:39:55, 1295.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [13:19:10<22:20:53, 1297.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [13:40:43<21:57:51, 1296.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 40%|████      | 40/100 [13:59:01<20:36:39, 1236.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11032.6240221895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20671.57588677004
Iteration 100: Loss = -11184.285511375776
Iteration 200: Loss = -11183.671253591836
Iteration 300: Loss = -11183.441550197587
Iteration 400: Loss = -11181.603517273341
Iteration 500: Loss = -11180.329367404014
Iteration 600: Loss = -11178.627920446443
Iteration 700: Loss = -11176.610080350174
Iteration 800: Loss = -11172.916386142984
Iteration 900: Loss = -11165.63689322862
Iteration 1000: Loss = -11140.654833589604
Iteration 1100: Loss = -11099.977061890238
Iteration 1200: Loss = -11023.214345481949
Iteration 1300: Loss = -11010.919941105585
Iteration 1400: Loss = -11004.3470414657
Iteration 1500: Loss = -11004.179707215408
Iteration 1600: Loss = -11002.637024954543
Iteration 1700: Loss = -11002.552921869941
Iteration 1800: Loss = -11002.102256709446
Iteration 1900: Loss = -11002.08982261508
Iteration 2000: Loss = -11002.071629456488
Iteration 2100: Loss = -11002.061475000508
Iteration 2200: Loss = -11002.047850002291
Iteration 2300: Loss = -11002.035330361616
Iteration 2400: Loss = -11002.031482169168
Iteration 2500: Loss = -11002.027522011453
Iteration 2600: Loss = -11002.024162575224
Iteration 2700: Loss = -11002.027607564514
1
Iteration 2800: Loss = -11002.015498422137
Iteration 2900: Loss = -11002.003630292342
Iteration 3000: Loss = -11001.997496443866
Iteration 3100: Loss = -11001.994668222456
Iteration 3200: Loss = -11001.985602110059
Iteration 3300: Loss = -11001.917105108263
Iteration 3400: Loss = -11001.91488986118
Iteration 3500: Loss = -11001.913678958677
Iteration 3600: Loss = -11001.912795544053
Iteration 3700: Loss = -11001.91022029166
Iteration 3800: Loss = -11001.904210719185
Iteration 3900: Loss = -11001.901345448729
Iteration 4000: Loss = -11001.90153797709
1
Iteration 4100: Loss = -11001.90005099312
Iteration 4200: Loss = -11001.903242416487
1
Iteration 4300: Loss = -11001.912123050364
2
Iteration 4400: Loss = -11001.89953078719
Iteration 4500: Loss = -11001.899464273276
Iteration 4600: Loss = -11001.91091707475
1
Iteration 4700: Loss = -11001.898633632687
Iteration 4800: Loss = -11001.898217029824
Iteration 4900: Loss = -11001.89831658362
Iteration 5000: Loss = -11001.897006873636
Iteration 5100: Loss = -11001.896682541535
Iteration 5200: Loss = -11001.896089973528
Iteration 5300: Loss = -11001.89599557736
Iteration 5400: Loss = -11001.894173277838
Iteration 5500: Loss = -11001.893269055727
Iteration 5600: Loss = -11001.893160357064
Iteration 5700: Loss = -11001.889266815782
Iteration 5800: Loss = -11001.889069196512
Iteration 5900: Loss = -11001.890400475177
1
Iteration 6000: Loss = -11001.889795803509
2
Iteration 6100: Loss = -11001.888207980674
Iteration 6200: Loss = -11001.888152466568
Iteration 6300: Loss = -11001.91162523001
1
Iteration 6400: Loss = -11001.894663306559
2
Iteration 6500: Loss = -11001.892468708578
3
Iteration 6600: Loss = -11001.889990944417
4
Iteration 6700: Loss = -11001.889381864125
5
Iteration 6800: Loss = -11001.88794454597
Iteration 6900: Loss = -11001.887864849248
Iteration 7000: Loss = -11001.887991562364
1
Iteration 7100: Loss = -11001.925098160635
2
Iteration 7200: Loss = -11001.887409193187
Iteration 7300: Loss = -11001.915623262053
1
Iteration 7400: Loss = -11001.893446200684
2
Iteration 7500: Loss = -11001.897587751706
3
Iteration 7600: Loss = -11001.886320078407
Iteration 7700: Loss = -11001.886055282737
Iteration 7800: Loss = -11001.885123588214
Iteration 7900: Loss = -11001.88508323452
Iteration 8000: Loss = -11001.896867381969
1
Iteration 8100: Loss = -11001.885323230532
2
Iteration 8200: Loss = -11001.889331246826
3
Iteration 8300: Loss = -11001.922523005187
4
Iteration 8400: Loss = -11001.884501371998
Iteration 8500: Loss = -11001.883954021852
Iteration 8600: Loss = -11001.884910877878
1
Iteration 8700: Loss = -11001.88424494218
2
Iteration 8800: Loss = -11001.88441874695
3
Iteration 8900: Loss = -11001.884581616247
4
Iteration 9000: Loss = -11001.943303513945
5
Iteration 9100: Loss = -11001.883776229219
Iteration 9200: Loss = -11001.883995204566
1
Iteration 9300: Loss = -11001.88510748809
2
Iteration 9400: Loss = -11001.919666783844
3
Iteration 9500: Loss = -11001.88374958248
Iteration 9600: Loss = -11001.885101103393
1
Iteration 9700: Loss = -11001.883594716857
Iteration 9800: Loss = -11001.884169351879
1
Iteration 9900: Loss = -11001.883479242293
Iteration 10000: Loss = -11001.88366528358
1
Iteration 10100: Loss = -11001.883461908103
Iteration 10200: Loss = -11001.88388489031
1
Iteration 10300: Loss = -11001.911866947668
2
Iteration 10400: Loss = -11001.883420989856
Iteration 10500: Loss = -11001.88459145675
1
Iteration 10600: Loss = -11001.883371302574
Iteration 10700: Loss = -11001.884019262598
1
Iteration 10800: Loss = -11001.88335448045
Iteration 10900: Loss = -11001.887963772617
1
Iteration 11000: Loss = -11001.883353562962
Iteration 11100: Loss = -11001.887259177594
1
Iteration 11200: Loss = -11001.88330661669
Iteration 11300: Loss = -11001.883848208656
1
Iteration 11400: Loss = -11001.882848498495
Iteration 11500: Loss = -11001.893569480577
1
Iteration 11600: Loss = -11001.88275658949
Iteration 11700: Loss = -11001.95362891795
1
Iteration 11800: Loss = -11001.882197211546
Iteration 11900: Loss = -11001.890251588637
1
Iteration 12000: Loss = -11001.882155946096
Iteration 12100: Loss = -11001.894402531498
1
Iteration 12200: Loss = -11001.910394501607
2
Iteration 12300: Loss = -11001.88219287006
Iteration 12400: Loss = -11001.883343367437
1
Iteration 12500: Loss = -11001.882225198277
Iteration 12600: Loss = -11001.882203515552
Iteration 12700: Loss = -11001.973016236781
1
Iteration 12800: Loss = -11001.882090107849
Iteration 12900: Loss = -11001.889044597969
1
Iteration 13000: Loss = -11001.882038437063
Iteration 13100: Loss = -11001.882364914036
1
Iteration 13200: Loss = -11001.864786578331
Iteration 13300: Loss = -11001.865795734855
1
Iteration 13400: Loss = -11001.864766283585
Iteration 13500: Loss = -11001.875256845982
1
Iteration 13600: Loss = -11001.86468159475
Iteration 13700: Loss = -11001.88006873999
1
Iteration 13800: Loss = -11001.865118486781
2
Iteration 13900: Loss = -11001.89376020064
3
Iteration 14000: Loss = -11001.9058264335
4
Iteration 14100: Loss = -11001.864723790333
Iteration 14200: Loss = -11001.865149354719
1
Iteration 14300: Loss = -11001.865386608493
2
Iteration 14400: Loss = -11001.864712163138
Iteration 14500: Loss = -11001.867146905926
1
Iteration 14600: Loss = -11001.863915322841
Iteration 14700: Loss = -11001.864514993445
1
Iteration 14800: Loss = -11001.863860725272
Iteration 14900: Loss = -11001.867895087946
1
Iteration 15000: Loss = -11001.86382697198
Iteration 15100: Loss = -11001.863786330032
Iteration 15200: Loss = -11001.868119446099
1
Iteration 15300: Loss = -11001.863713311153
Iteration 15400: Loss = -11001.863739006674
Iteration 15500: Loss = -11001.864609077016
1
Iteration 15600: Loss = -11001.863748383603
Iteration 15700: Loss = -11001.863738586484
Iteration 15800: Loss = -11001.87741934614
1
Iteration 15900: Loss = -11001.863716738491
Iteration 16000: Loss = -11001.863716026084
Iteration 16100: Loss = -11001.868415184228
1
Iteration 16200: Loss = -11001.863712654673
Iteration 16300: Loss = -11001.863424154075
Iteration 16400: Loss = -11001.863544576474
1
Iteration 16500: Loss = -11001.863285042358
Iteration 16600: Loss = -11001.863828104708
1
Iteration 16700: Loss = -11001.862154678398
Iteration 16800: Loss = -11001.862922979388
1
Iteration 16900: Loss = -11001.862137058602
Iteration 17000: Loss = -11001.862383134767
1
Iteration 17100: Loss = -11001.86216107591
Iteration 17200: Loss = -11001.862777193171
1
Iteration 17300: Loss = -11001.862166675823
Iteration 17400: Loss = -11001.901157981103
1
Iteration 17500: Loss = -11001.861455610497
Iteration 17600: Loss = -11001.861455308306
Iteration 17700: Loss = -11001.863116871886
1
Iteration 17800: Loss = -11001.861448341162
Iteration 17900: Loss = -11001.860528358804
Iteration 18000: Loss = -11001.861462338804
1
Iteration 18100: Loss = -11001.926660598467
2
Iteration 18200: Loss = -11001.860218593092
Iteration 18300: Loss = -11001.941412130485
1
Iteration 18400: Loss = -11001.860206415937
Iteration 18500: Loss = -11001.861270868507
1
Iteration 18600: Loss = -11001.860235330503
Iteration 18700: Loss = -11001.860177105562
Iteration 18800: Loss = -11001.871073019567
1
Iteration 18900: Loss = -11001.860194524743
Iteration 19000: Loss = -11002.187590625077
1
Iteration 19100: Loss = -11001.860165621161
Iteration 19200: Loss = -11001.860161179191
Iteration 19300: Loss = -11001.860495807345
1
Iteration 19400: Loss = -11001.860052868147
Iteration 19500: Loss = -11001.866011778715
1
Iteration 19600: Loss = -11001.860087655741
Iteration 19700: Loss = -11001.861419343246
1
Iteration 19800: Loss = -11001.859808618548
Iteration 19900: Loss = -11001.860154052478
1
pi: tensor([[0.7303, 0.2697],
        [0.2001, 0.7999]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4586, 0.5414], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2070, 0.0959],
         [0.7258, 0.2517]],

        [[0.5516, 0.1027],
         [0.6597, 0.6979]],

        [[0.5101, 0.0964],
         [0.5254, 0.6574]],

        [[0.5724, 0.1005],
         [0.6700, 0.5656]],

        [[0.7057, 0.1011],
         [0.5938, 0.6859]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.9137444183588902
Average Adjusted Rand Index: 0.9136079161358606
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21880.976948672735
Iteration 100: Loss = -11183.193946122175
Iteration 200: Loss = -11181.805977792419
Iteration 300: Loss = -11181.324926883643
Iteration 400: Loss = -11180.868238522884
Iteration 500: Loss = -11180.275825770485
Iteration 600: Loss = -11179.090583206722
Iteration 700: Loss = -11177.899170380257
Iteration 800: Loss = -11176.672296975355
Iteration 900: Loss = -11173.835102582187
Iteration 1000: Loss = -11159.843835973867
Iteration 1100: Loss = -11117.560878958846
Iteration 1200: Loss = -11064.824570878593
Iteration 1300: Loss = -11056.63815010841
Iteration 1400: Loss = -11056.307447193973
Iteration 1500: Loss = -11055.806979768886
Iteration 1600: Loss = -11055.720763098707
Iteration 1700: Loss = -11055.654115292808
Iteration 1800: Loss = -11055.568803970998
Iteration 1900: Loss = -11055.403270172239
Iteration 2000: Loss = -11055.221434430336
Iteration 2100: Loss = -11054.753245795348
Iteration 2200: Loss = -11054.511549924908
Iteration 2300: Loss = -11054.386678899142
Iteration 2400: Loss = -11054.30506511057
Iteration 2500: Loss = -11054.237422197757
Iteration 2600: Loss = -11054.114372066675
Iteration 2700: Loss = -11054.036188473847
Iteration 2800: Loss = -11053.833545009054
Iteration 2900: Loss = -11044.544208957002
Iteration 3000: Loss = -11043.067889135767
Iteration 3100: Loss = -11038.199842235103
Iteration 3200: Loss = -11038.120504427996
Iteration 3300: Loss = -11038.08795063796
Iteration 3400: Loss = -11038.071039769577
Iteration 3500: Loss = -11037.974211001145
Iteration 3600: Loss = -11037.898839333446
Iteration 3700: Loss = -11037.888328294717
Iteration 3800: Loss = -11037.884504956035
Iteration 3900: Loss = -11037.88119569628
Iteration 4000: Loss = -11037.878233285299
Iteration 4100: Loss = -11037.87376099126
Iteration 4200: Loss = -11037.859637015736
Iteration 4300: Loss = -11037.854874102675
Iteration 4400: Loss = -11037.853549244799
Iteration 4500: Loss = -11037.853645683812
Iteration 4600: Loss = -11037.851820338348
Iteration 4700: Loss = -11037.85111860532
Iteration 4800: Loss = -11037.850407043798
Iteration 4900: Loss = -11037.857854047277
1
Iteration 5000: Loss = -11037.849072259223
Iteration 5100: Loss = -11037.84835614525
Iteration 5200: Loss = -11037.847269901995
Iteration 5300: Loss = -11037.845546994828
Iteration 5400: Loss = -11037.840999651702
Iteration 5500: Loss = -11037.836959418846
Iteration 5600: Loss = -11037.836311542796
Iteration 5700: Loss = -11037.835887607314
Iteration 5800: Loss = -11037.840471846011
1
Iteration 5900: Loss = -11037.83530705315
Iteration 6000: Loss = -11037.835208171999
Iteration 6100: Loss = -11037.835080313096
Iteration 6200: Loss = -11037.83587225181
1
Iteration 6300: Loss = -11037.841221302908
2
Iteration 6400: Loss = -11037.834441331823
Iteration 6500: Loss = -11037.833990275358
Iteration 6600: Loss = -11037.839511133983
1
Iteration 6700: Loss = -11037.832768046948
Iteration 6800: Loss = -11037.831422749921
Iteration 6900: Loss = -11037.831127722855
Iteration 7000: Loss = -11037.830988351921
Iteration 7100: Loss = -11037.830854260561
Iteration 7200: Loss = -11037.830807051005
Iteration 7300: Loss = -11037.830670714295
Iteration 7400: Loss = -11037.830913342561
1
Iteration 7500: Loss = -11037.830497432653
Iteration 7600: Loss = -11037.83052753379
Iteration 7700: Loss = -11037.831642999972
1
Iteration 7800: Loss = -11037.830861784078
2
Iteration 7900: Loss = -11037.837040728295
3
Iteration 8000: Loss = -11037.83140594701
4
Iteration 8100: Loss = -11037.82769418023
Iteration 8200: Loss = -11037.826301122846
Iteration 8300: Loss = -11037.824581200715
Iteration 8400: Loss = -11037.824426750127
Iteration 8500: Loss = -11037.824294063934
Iteration 8600: Loss = -11037.98532778383
1
Iteration 8700: Loss = -11037.824127274149
Iteration 8800: Loss = -11037.823675285723
Iteration 8900: Loss = -11037.81460375888
Iteration 9000: Loss = -11037.814254563953
Iteration 9100: Loss = -11037.832295263077
1
Iteration 9200: Loss = -11037.814107706536
Iteration 9300: Loss = -11037.813984146034
Iteration 9400: Loss = -11037.813903463892
Iteration 9500: Loss = -11037.813725856993
Iteration 9600: Loss = -11037.815870111013
1
Iteration 9700: Loss = -11037.813651548184
Iteration 9800: Loss = -11037.841881532871
1
Iteration 9900: Loss = -11037.813600297597
Iteration 10000: Loss = -11037.814026816668
1
Iteration 10100: Loss = -11037.813528233746
Iteration 10200: Loss = -11037.813427508392
Iteration 10300: Loss = -11037.826739570715
1
Iteration 10400: Loss = -11037.81335358295
Iteration 10500: Loss = -11037.813352291281
Iteration 10600: Loss = -11037.813443902693
Iteration 10700: Loss = -11037.813344024786
Iteration 10800: Loss = -11037.814085599026
1
Iteration 10900: Loss = -11037.813302884731
Iteration 11000: Loss = -11037.816267650662
1
Iteration 11100: Loss = -11037.81327356747
Iteration 11200: Loss = -11037.813268867174
Iteration 11300: Loss = -11037.813451332193
1
Iteration 11400: Loss = -11037.81324729187
Iteration 11500: Loss = -11037.975753951658
1
Iteration 11600: Loss = -11037.813244467374
Iteration 11700: Loss = -11037.813219269254
Iteration 11800: Loss = -11037.813857488836
1
Iteration 11900: Loss = -11037.81314824035
Iteration 12000: Loss = -11037.817093931017
1
Iteration 12100: Loss = -11037.813145225124
Iteration 12200: Loss = -11037.840802718043
1
Iteration 12300: Loss = -11037.813118775084
Iteration 12400: Loss = -11037.81312757662
Iteration 12500: Loss = -11037.813278138605
1
Iteration 12600: Loss = -11037.813110472005
Iteration 12700: Loss = -11037.81802413465
1
Iteration 12800: Loss = -11037.81312376124
Iteration 12900: Loss = -11037.82415461062
1
Iteration 13000: Loss = -11037.813080658561
Iteration 13100: Loss = -11037.813676444863
1
Iteration 13200: Loss = -11037.813098296923
Iteration 13300: Loss = -11037.813078145
Iteration 13400: Loss = -11037.814211029621
1
Iteration 13500: Loss = -11037.813066567362
Iteration 13600: Loss = -11037.87894946547
1
Iteration 13700: Loss = -11037.812920035372
Iteration 13800: Loss = -11037.812688487196
Iteration 13900: Loss = -11037.812822677643
1
Iteration 14000: Loss = -11037.812708732721
Iteration 14100: Loss = -11037.857394661409
1
Iteration 14200: Loss = -11037.812706203267
Iteration 14300: Loss = -11037.812662834005
Iteration 14400: Loss = -11037.813025610336
1
Iteration 14500: Loss = -11037.812478467493
Iteration 14600: Loss = -11037.835205391004
1
Iteration 14700: Loss = -11037.81250749039
Iteration 14800: Loss = -11037.812497452987
Iteration 14900: Loss = -11037.812721913491
1
Iteration 15000: Loss = -11037.812514409332
Iteration 15100: Loss = -11038.280743499954
1
Iteration 15200: Loss = -11037.809907661922
Iteration 15300: Loss = -11037.809835844235
Iteration 15400: Loss = -11037.813687621127
1
Iteration 15500: Loss = -11037.809857076536
Iteration 15600: Loss = -11037.809832313596
Iteration 15700: Loss = -11037.809935979401
1
Iteration 15800: Loss = -11037.80984491756
Iteration 15900: Loss = -11037.814787899733
1
Iteration 16000: Loss = -11037.809825392695
Iteration 16100: Loss = -11037.80983077848
Iteration 16200: Loss = -11037.809900079694
Iteration 16300: Loss = -11037.809834319638
Iteration 16400: Loss = -11037.833140552517
1
Iteration 16500: Loss = -11037.809805575043
Iteration 16600: Loss = -11037.808530298615
Iteration 16700: Loss = -11037.808500676621
Iteration 16800: Loss = -11037.808458605765
Iteration 16900: Loss = -11037.817816169281
1
Iteration 17000: Loss = -11037.808433573093
Iteration 17100: Loss = -11038.175631069407
1
Iteration 17200: Loss = -11037.808438116124
Iteration 17300: Loss = -11037.808424945768
Iteration 17400: Loss = -11037.808590761058
1
Iteration 17500: Loss = -11037.808398706924
Iteration 17600: Loss = -11037.811477413909
1
Iteration 17700: Loss = -11037.808416118456
Iteration 17800: Loss = -11038.239801381262
1
Iteration 17900: Loss = -11037.808420245838
Iteration 18000: Loss = -11037.80840463187
Iteration 18100: Loss = -11037.80899304326
1
Iteration 18200: Loss = -11037.807935254174
Iteration 18300: Loss = -11037.811733100703
1
Iteration 18400: Loss = -11037.807942350062
Iteration 18500: Loss = -11037.852110502687
1
Iteration 18600: Loss = -11037.807913795026
Iteration 18700: Loss = -11037.807897352262
Iteration 18800: Loss = -11037.808189731135
1
Iteration 18900: Loss = -11037.8079051687
Iteration 19000: Loss = -11037.823682165592
1
Iteration 19100: Loss = -11037.807916526857
Iteration 19200: Loss = -11037.807904460791
Iteration 19300: Loss = -11037.80855089858
1
Iteration 19400: Loss = -11037.807901046242
Iteration 19500: Loss = -11037.807908237424
Iteration 19600: Loss = -11037.809095376973
1
Iteration 19700: Loss = -11037.80789996265
Iteration 19800: Loss = -11037.80829643907
1
Iteration 19900: Loss = -11037.807910057485
pi: tensor([[0.7794, 0.2206],
        [0.3690, 0.6310]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2017, 0.7983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2534, 0.0907],
         [0.6038, 0.2014]],

        [[0.6964, 0.1025],
         [0.6950, 0.6277]],

        [[0.6106, 0.0964],
         [0.5247, 0.5660]],

        [[0.6151, 0.1004],
         [0.6317, 0.5121]],

        [[0.5442, 0.1012],
         [0.6289, 0.6631]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 76
Adjusted Rand Index: 0.26187050359712233
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.4239696106423208
Average Adjusted Rand Index: 0.7680792261918967
11032.6240221895
[0.9137444183588902, 0.4239696106423208] [0.9136079161358606, 0.7680792261918967] [11001.8598134741, 11037.808696120497]
-------------------------------------
This iteration is 1
True Objective function: Loss = -10895.320003394521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21985.24667315748
Iteration 100: Loss = -11039.05043004337
Iteration 200: Loss = -11038.42957876241
Iteration 300: Loss = -11038.223732745539
Iteration 400: Loss = -11038.081898864364
Iteration 500: Loss = -11037.95297777288
Iteration 600: Loss = -11037.80446713242
Iteration 700: Loss = -11037.60372797301
Iteration 800: Loss = -11037.341917146847
Iteration 900: Loss = -11037.02018350239
Iteration 1000: Loss = -11036.700012876194
Iteration 1100: Loss = -11036.478939143022
Iteration 1200: Loss = -11036.315257990982
Iteration 1300: Loss = -11036.187732019684
Iteration 1400: Loss = -11036.083540281586
Iteration 1500: Loss = -11035.993649766493
Iteration 1600: Loss = -11035.912725724305
Iteration 1700: Loss = -11035.840232139079
Iteration 1800: Loss = -11035.775438758745
Iteration 1900: Loss = -11035.71915326361
Iteration 2000: Loss = -11035.673878499429
Iteration 2100: Loss = -11035.640105158189
Iteration 2200: Loss = -11035.615006201717
Iteration 2300: Loss = -11035.598073637035
Iteration 2400: Loss = -11035.585880783397
Iteration 2500: Loss = -11035.575717129686
Iteration 2600: Loss = -11035.56622416388
Iteration 2700: Loss = -11035.556598907608
Iteration 2800: Loss = -11035.5462185232
Iteration 2900: Loss = -11035.533952379858
Iteration 3000: Loss = -11035.517829902994
Iteration 3100: Loss = -11035.484696078103
Iteration 3200: Loss = -11034.963314754606
Iteration 3300: Loss = -10892.221928801484
Iteration 3400: Loss = -10873.0265000468
Iteration 3500: Loss = -10872.483993334146
Iteration 3600: Loss = -10872.166374521139
Iteration 3700: Loss = -10872.024954246195
Iteration 3800: Loss = -10871.354512359188
Iteration 3900: Loss = -10871.318403121364
Iteration 4000: Loss = -10871.30002450188
Iteration 4100: Loss = -10871.23157740117
Iteration 4200: Loss = -10870.971337888555
Iteration 4300: Loss = -10870.817755776881
Iteration 4400: Loss = -10870.81172148074
Iteration 4500: Loss = -10870.807573207556
Iteration 4600: Loss = -10870.802715901367
Iteration 4700: Loss = -10870.796456033466
Iteration 4800: Loss = -10870.789352679943
Iteration 4900: Loss = -10870.785324901144
Iteration 5000: Loss = -10870.781664742939
Iteration 5100: Loss = -10870.776987064237
Iteration 5200: Loss = -10870.762470984919
Iteration 5300: Loss = -10870.741169622162
Iteration 5400: Loss = -10870.7398907171
Iteration 5500: Loss = -10870.737397520124
Iteration 5600: Loss = -10870.72839096451
Iteration 5700: Loss = -10870.727104946722
Iteration 5800: Loss = -10870.726177645125
Iteration 5900: Loss = -10870.727248204712
1
Iteration 6000: Loss = -10870.723774112921
Iteration 6100: Loss = -10870.721806396572
Iteration 6200: Loss = -10870.721601242753
Iteration 6300: Loss = -10870.720311288056
Iteration 6400: Loss = -10870.720108649915
Iteration 6500: Loss = -10870.718687158591
Iteration 6600: Loss = -10870.72539294639
1
Iteration 6700: Loss = -10870.712128288511
Iteration 6800: Loss = -10870.712112777122
Iteration 6900: Loss = -10870.707995485134
Iteration 7000: Loss = -10870.67490354851
Iteration 7100: Loss = -10870.668343364328
Iteration 7200: Loss = -10870.614809281717
Iteration 7300: Loss = -10870.613842952878
Iteration 7400: Loss = -10870.457580821363
Iteration 7500: Loss = -10870.438282165449
Iteration 7600: Loss = -10870.435498294644
Iteration 7700: Loss = -10870.43473072532
Iteration 7800: Loss = -10870.434606794812
Iteration 7900: Loss = -10869.717947305995
Iteration 8000: Loss = -10869.65903014867
Iteration 8100: Loss = -10869.657640813428
Iteration 8200: Loss = -10869.651313979353
Iteration 8300: Loss = -10869.967154324995
1
Iteration 8400: Loss = -10869.599105571804
Iteration 8500: Loss = -10869.598490510427
Iteration 8600: Loss = -10869.598366472334
Iteration 8700: Loss = -10869.598375727433
Iteration 8800: Loss = -10869.59810450472
Iteration 8900: Loss = -10869.597948213008
Iteration 9000: Loss = -10869.597556302693
Iteration 9100: Loss = -10869.22328943709
Iteration 9200: Loss = -10869.263227229201
1
Iteration 9300: Loss = -10869.220249559574
Iteration 9400: Loss = -10869.221490979873
1
Iteration 9500: Loss = -10869.217672627576
Iteration 9600: Loss = -10869.217985271394
1
Iteration 9700: Loss = -10869.217425914048
Iteration 9800: Loss = -10869.216039411203
Iteration 9900: Loss = -10869.208610755913
Iteration 10000: Loss = -10869.194743862437
Iteration 10100: Loss = -10869.194899227316
1
Iteration 10200: Loss = -10869.194584468856
Iteration 10300: Loss = -10869.1948234358
1
Iteration 10400: Loss = -10869.194569213336
Iteration 10500: Loss = -10869.205749172066
1
Iteration 10600: Loss = -10869.33748404157
2
Iteration 10700: Loss = -10869.193254893697
Iteration 10800: Loss = -10869.187925928542
Iteration 10900: Loss = -10867.525445191966
Iteration 11000: Loss = -10867.525958629712
1
Iteration 11100: Loss = -10867.525373821178
Iteration 11200: Loss = -10867.532331476243
1
Iteration 11300: Loss = -10867.543019553987
2
Iteration 11400: Loss = -10867.52130013113
Iteration 11500: Loss = -10867.522167599005
1
Iteration 11600: Loss = -10867.525356793212
2
Iteration 11700: Loss = -10867.519826654294
Iteration 11800: Loss = -10867.445707641164
Iteration 11900: Loss = -10867.44258239918
Iteration 12000: Loss = -10867.441939064665
Iteration 12100: Loss = -10867.441169729309
Iteration 12200: Loss = -10867.441043900066
Iteration 12300: Loss = -10867.466210125209
1
Iteration 12400: Loss = -10867.440989573175
Iteration 12500: Loss = -10867.443168720554
1
Iteration 12600: Loss = -10867.435472163843
Iteration 12700: Loss = -10867.425721817035
Iteration 12800: Loss = -10867.296995166113
Iteration 12900: Loss = -10867.41426683863
1
Iteration 13000: Loss = -10867.299203879154
2
Iteration 13100: Loss = -10867.28739463122
Iteration 13200: Loss = -10867.286707047584
Iteration 13300: Loss = -10867.286442057497
Iteration 13400: Loss = -10867.287862144716
1
Iteration 13500: Loss = -10867.292817939066
2
Iteration 13600: Loss = -10867.285564792885
Iteration 13700: Loss = -10867.287377387389
1
Iteration 13800: Loss = -10867.285170550627
Iteration 13900: Loss = -10867.28421900167
Iteration 14000: Loss = -10867.285885700545
1
Iteration 14100: Loss = -10867.327607487601
2
Iteration 14200: Loss = -10867.285453185386
3
Iteration 14300: Loss = -10867.296066082414
4
Iteration 14400: Loss = -10867.283672019072
Iteration 14500: Loss = -10867.283381037167
Iteration 14600: Loss = -10867.28787821886
1
Iteration 14700: Loss = -10867.283434067645
Iteration 14800: Loss = -10867.285029476876
1
Iteration 14900: Loss = -10867.420879510044
2
Iteration 15000: Loss = -10867.282367705717
Iteration 15100: Loss = -10867.288960407892
1
Iteration 15200: Loss = -10867.28124873368
Iteration 15300: Loss = -10867.282445479197
1
Iteration 15400: Loss = -10867.281243549556
Iteration 15500: Loss = -10867.290086620933
1
Iteration 15600: Loss = -10867.422927516513
2
Iteration 15700: Loss = -10867.280756808095
Iteration 15800: Loss = -10867.281785886496
1
Iteration 15900: Loss = -10867.298270759085
2
Iteration 16000: Loss = -10867.280310121254
Iteration 16100: Loss = -10867.28037939515
Iteration 16200: Loss = -10867.292792154463
1
Iteration 16300: Loss = -10867.279599753707
Iteration 16400: Loss = -10867.277080408045
Iteration 16500: Loss = -10867.296200903027
1
Iteration 16600: Loss = -10867.275449940813
Iteration 16700: Loss = -10867.290980336975
1
Iteration 16800: Loss = -10867.275227187296
Iteration 16900: Loss = -10867.324503407523
1
Iteration 17000: Loss = -10867.265359675424
Iteration 17100: Loss = -10867.265301754162
Iteration 17200: Loss = -10867.270041071253
1
Iteration 17300: Loss = -10867.264989761254
Iteration 17400: Loss = -10867.35848096047
1
Iteration 17500: Loss = -10867.26513731021
2
Iteration 17600: Loss = -10867.26558428594
3
Iteration 17700: Loss = -10867.255987437971
Iteration 17800: Loss = -10867.28045967007
1
Iteration 17900: Loss = -10867.255790605639
Iteration 18000: Loss = -10867.255934511077
1
Iteration 18100: Loss = -10867.272608888043
2
Iteration 18200: Loss = -10867.255760129481
Iteration 18300: Loss = -10867.255761008206
Iteration 18400: Loss = -10867.255824272539
Iteration 18500: Loss = -10867.25570529648
Iteration 18600: Loss = -10867.32494703916
1
Iteration 18700: Loss = -10867.25566056617
Iteration 18800: Loss = -10867.256272645012
1
Iteration 18900: Loss = -10867.256149655654
2
Iteration 19000: Loss = -10867.255852740549
3
Iteration 19100: Loss = -10867.257766836963
4
Iteration 19200: Loss = -10867.255631235023
Iteration 19300: Loss = -10867.30823964793
1
Iteration 19400: Loss = -10867.255643707309
Iteration 19500: Loss = -10867.256360701955
1
Iteration 19600: Loss = -10867.255766168651
2
Iteration 19700: Loss = -10867.25576897388
3
Iteration 19800: Loss = -10867.256563682053
4
Iteration 19900: Loss = -10867.486462543875
5
pi: tensor([[0.7296, 0.2704],
        [0.2735, 0.7265]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4075, 0.5925], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2048, 0.0941],
         [0.5381, 0.2510]],

        [[0.5533, 0.0994],
         [0.6216, 0.6181]],

        [[0.6926, 0.0932],
         [0.6440, 0.5151]],

        [[0.6935, 0.0965],
         [0.5425, 0.6751]],

        [[0.6109, 0.0986],
         [0.5765, 0.5742]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
Global Adjusted Rand Index: 0.9137620885145475
Average Adjusted Rand Index: 0.9145833519782884
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21897.619861141306
Iteration 100: Loss = -11039.407049323057
Iteration 200: Loss = -11038.488360051411
Iteration 300: Loss = -11038.133170366686
Iteration 400: Loss = -11037.856311707757
Iteration 500: Loss = -11037.541079781678
Iteration 600: Loss = -11037.293940131063
Iteration 700: Loss = -11037.027278420459
Iteration 800: Loss = -11036.503576911655
Iteration 900: Loss = -11036.04821913099
Iteration 1000: Loss = -11035.861948789892
Iteration 1100: Loss = -11035.728275299865
Iteration 1200: Loss = -11035.634899615046
Iteration 1300: Loss = -11035.580918884325
Iteration 1400: Loss = -11035.550844667518
Iteration 1500: Loss = -11035.530315587068
Iteration 1600: Loss = -11035.50807897764
Iteration 1700: Loss = -11035.462051200207
Iteration 1800: Loss = -11033.478881555864
Iteration 1900: Loss = -10881.608776402654
Iteration 2000: Loss = -10875.206760390147
Iteration 2100: Loss = -10871.84923618083
Iteration 2200: Loss = -10871.396479232744
Iteration 2300: Loss = -10870.744486188416
Iteration 2400: Loss = -10870.47778279637
Iteration 2500: Loss = -10869.718195106496
Iteration 2600: Loss = -10869.627376916553
Iteration 2700: Loss = -10869.616939009065
Iteration 2800: Loss = -10869.593314578759
Iteration 2900: Loss = -10869.426207094257
Iteration 3000: Loss = -10869.420362701429
Iteration 3100: Loss = -10869.415116882168
Iteration 3200: Loss = -10869.409436930135
Iteration 3300: Loss = -10869.402162194703
Iteration 3400: Loss = -10869.396667488283
Iteration 3500: Loss = -10869.363228492652
Iteration 3600: Loss = -10869.342658958994
Iteration 3700: Loss = -10869.338844494881
Iteration 3800: Loss = -10869.335103512401
Iteration 3900: Loss = -10869.332193657714
Iteration 4000: Loss = -10869.33285325192
1
Iteration 4100: Loss = -10869.326350488316
Iteration 4200: Loss = -10867.738976796794
Iteration 4300: Loss = -10867.649124353182
Iteration 4400: Loss = -10867.646463570154
Iteration 4500: Loss = -10867.629412883616
Iteration 4600: Loss = -10867.628503287773
Iteration 4700: Loss = -10867.62722714449
Iteration 4800: Loss = -10867.626670856465
Iteration 4900: Loss = -10867.625771749721
Iteration 5000: Loss = -10867.625505715076
Iteration 5100: Loss = -10867.623792227587
Iteration 5200: Loss = -10867.62316139599
Iteration 5300: Loss = -10867.62357461431
1
Iteration 5400: Loss = -10867.6216890908
Iteration 5500: Loss = -10867.621383608534
Iteration 5600: Loss = -10867.634119064782
1
Iteration 5700: Loss = -10867.620581550107
Iteration 5800: Loss = -10867.62028422123
Iteration 5900: Loss = -10867.619529664931
Iteration 6000: Loss = -10867.616921722776
Iteration 6100: Loss = -10867.558906382486
Iteration 6200: Loss = -10867.557324101992
Iteration 6300: Loss = -10867.543564574526
Iteration 6400: Loss = -10867.543070011572
Iteration 6500: Loss = -10867.543057524628
Iteration 6600: Loss = -10867.542587058892
Iteration 6700: Loss = -10867.545732869436
1
Iteration 6800: Loss = -10867.523897151546
Iteration 6900: Loss = -10867.524024449282
1
Iteration 7000: Loss = -10867.530598975887
2
Iteration 7100: Loss = -10867.51754932397
Iteration 7200: Loss = -10867.517098659066
Iteration 7300: Loss = -10867.516656181346
Iteration 7400: Loss = -10867.518088767967
1
Iteration 7500: Loss = -10867.516934601603
2
Iteration 7600: Loss = -10867.515463170417
Iteration 7700: Loss = -10867.511776124129
Iteration 7800: Loss = -10867.487285800338
Iteration 7900: Loss = -10867.487022039966
Iteration 8000: Loss = -10867.486557363054
Iteration 8100: Loss = -10867.486356653015
Iteration 8200: Loss = -10867.46820878331
Iteration 8300: Loss = -10867.473877391645
1
Iteration 8400: Loss = -10867.467675819007
Iteration 8500: Loss = -10867.354905644972
Iteration 8600: Loss = -10867.355820714785
1
Iteration 8700: Loss = -10867.354393461203
Iteration 8800: Loss = -10867.367515596809
1
Iteration 8900: Loss = -10867.348839999606
Iteration 9000: Loss = -10867.291245324457
Iteration 9100: Loss = -10867.296627948728
1
Iteration 9200: Loss = -10867.279085896345
Iteration 9300: Loss = -10867.283780802802
1
Iteration 9400: Loss = -10867.295410364939
2
Iteration 9500: Loss = -10867.318901003478
3
Iteration 9600: Loss = -10867.245740743148
Iteration 9700: Loss = -10867.249126760114
1
Iteration 9800: Loss = -10867.32237375103
2
Iteration 9900: Loss = -10867.245522946043
Iteration 10000: Loss = -10867.2461363993
1
Iteration 10100: Loss = -10867.23876580154
Iteration 10200: Loss = -10867.34057112175
1
Iteration 10300: Loss = -10867.234919636503
Iteration 10400: Loss = -10867.231100810612
Iteration 10500: Loss = -10867.296314081557
1
Iteration 10600: Loss = -10867.22102833496
Iteration 10700: Loss = -10867.221458509375
1
Iteration 10800: Loss = -10867.221919329077
2
Iteration 10900: Loss = -10867.220469262897
Iteration 11000: Loss = -10867.220490941665
Iteration 11100: Loss = -10867.220461306231
Iteration 11200: Loss = -10867.242795359121
1
Iteration 11300: Loss = -10867.220247558662
Iteration 11400: Loss = -10867.221186922705
1
Iteration 11500: Loss = -10867.220173395672
Iteration 11600: Loss = -10867.218300809418
Iteration 11700: Loss = -10867.218567698841
1
Iteration 11800: Loss = -10867.228293524018
2
Iteration 11900: Loss = -10867.218133793745
Iteration 12000: Loss = -10867.248895878418
1
Iteration 12100: Loss = -10867.218128640674
Iteration 12200: Loss = -10867.218747193525
1
Iteration 12300: Loss = -10867.230018726339
2
Iteration 12400: Loss = -10867.218083041838
Iteration 12500: Loss = -10867.224036670901
1
Iteration 12600: Loss = -10867.218089655951
Iteration 12700: Loss = -10867.32354243593
1
Iteration 12800: Loss = -10867.218097149122
Iteration 12900: Loss = -10867.219598857888
1
Iteration 13000: Loss = -10867.218455556156
2
Iteration 13100: Loss = -10867.217902242946
Iteration 13200: Loss = -10867.21777901442
Iteration 13300: Loss = -10867.251872659326
1
Iteration 13400: Loss = -10867.220010019666
2
Iteration 13500: Loss = -10867.223539284436
3
Iteration 13600: Loss = -10867.322761487047
4
Iteration 13700: Loss = -10867.215409248733
Iteration 13800: Loss = -10867.216105513888
1
Iteration 13900: Loss = -10867.225628402384
2
Iteration 14000: Loss = -10867.215402917922
Iteration 14100: Loss = -10867.230370336418
1
Iteration 14200: Loss = -10867.214480187171
Iteration 14300: Loss = -10867.214513233328
Iteration 14400: Loss = -10867.218303349184
1
Iteration 14500: Loss = -10867.216988315522
2
Iteration 14600: Loss = -10867.217323386825
3
Iteration 14700: Loss = -10867.214449076535
Iteration 14800: Loss = -10867.216587849574
1
Iteration 14900: Loss = -10867.214925226779
2
Iteration 15000: Loss = -10867.32123092443
3
Iteration 15100: Loss = -10867.214827996368
4
Iteration 15200: Loss = -10867.218608767827
5
Iteration 15300: Loss = -10867.213651586066
Iteration 15400: Loss = -10867.213789706067
1
Iteration 15500: Loss = -10867.215184334054
2
Iteration 15600: Loss = -10867.213685323793
Iteration 15700: Loss = -10867.410949004096
1
Iteration 15800: Loss = -10867.21365650636
Iteration 15900: Loss = -10867.312552595758
1
Iteration 16000: Loss = -10867.213628982663
Iteration 16100: Loss = -10867.27403930708
1
Iteration 16200: Loss = -10867.21355611573
Iteration 16300: Loss = -10867.2471856241
1
Iteration 16400: Loss = -10867.21345561276
Iteration 16500: Loss = -10867.214408707638
1
Iteration 16600: Loss = -10867.213304693116
Iteration 16700: Loss = -10867.214003863937
1
Iteration 16800: Loss = -10867.213316827008
Iteration 16900: Loss = -10867.22113567681
1
Iteration 17000: Loss = -10867.219114746353
2
Iteration 17100: Loss = -10867.232395280387
3
Iteration 17200: Loss = -10867.213656877613
4
Iteration 17300: Loss = -10867.21372393328
5
Iteration 17400: Loss = -10867.23862200903
6
Iteration 17500: Loss = -10867.277602482012
7
Iteration 17600: Loss = -10867.24204441227
8
Iteration 17700: Loss = -10867.213405320177
Iteration 17800: Loss = -10867.21339051435
Iteration 17900: Loss = -10867.215202200903
1
Iteration 18000: Loss = -10867.214832490781
2
Iteration 18100: Loss = -10867.221022565725
3
Iteration 18200: Loss = -10867.213219862648
Iteration 18300: Loss = -10867.499626476372
1
Iteration 18400: Loss = -10867.21310874305
Iteration 18500: Loss = -10867.282028316424
1
Iteration 18600: Loss = -10867.213112637384
Iteration 18700: Loss = -10867.213895774825
1
Iteration 18800: Loss = -10867.213095825964
Iteration 18900: Loss = -10867.213264826973
1
Iteration 19000: Loss = -10867.424009213302
2
Iteration 19100: Loss = -10867.213083793818
Iteration 19200: Loss = -10867.214465664823
1
Iteration 19300: Loss = -10867.21307305508
Iteration 19400: Loss = -10867.213227321703
1
Iteration 19500: Loss = -10867.21301022054
Iteration 19600: Loss = -10867.213108129221
Iteration 19700: Loss = -10867.213002048991
Iteration 19800: Loss = -10867.23752306034
1
Iteration 19900: Loss = -10867.21297883789
pi: tensor([[0.7260, 0.2740],
        [0.2708, 0.7292]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5921, 0.4079], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2510, 0.0941],
         [0.6080, 0.2049]],

        [[0.6481, 0.0993],
         [0.6255, 0.5593]],

        [[0.6877, 0.0933],
         [0.5605, 0.5021]],

        [[0.7148, 0.0963],
         [0.6066, 0.6374]],

        [[0.6402, 0.0986],
         [0.5100, 0.5263]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
Global Adjusted Rand Index: 0.9137620885145475
Average Adjusted Rand Index: 0.9145833519782884
10895.320003394521
[0.9137620885145475, 0.9137620885145475] [0.9145833519782884, 0.9145833519782884] [10867.255648279986, 10867.21426206348]
-------------------------------------
This iteration is 2
True Objective function: Loss = -10785.689976915199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23451.642177950955
Iteration 100: Loss = -10912.539889083735
Iteration 200: Loss = -10910.281187737437
Iteration 300: Loss = -10908.76570338614
Iteration 400: Loss = -10907.678429461237
Iteration 500: Loss = -10906.655479893401
Iteration 600: Loss = -10906.062979078
Iteration 700: Loss = -10905.755860905756
Iteration 800: Loss = -10905.5767435901
Iteration 900: Loss = -10905.45234225123
Iteration 1000: Loss = -10905.365552491461
Iteration 1100: Loss = -10905.30012397201
Iteration 1200: Loss = -10905.249574046587
Iteration 1300: Loss = -10905.210581391279
Iteration 1400: Loss = -10905.180376754213
Iteration 1500: Loss = -10905.156742830592
Iteration 1600: Loss = -10905.137875061413
Iteration 1700: Loss = -10905.122786322392
Iteration 1800: Loss = -10905.110728817304
Iteration 1900: Loss = -10905.101120610221
Iteration 2000: Loss = -10905.09335377171
Iteration 2100: Loss = -10905.087127886107
Iteration 2200: Loss = -10905.082152795148
Iteration 2300: Loss = -10905.078216854421
Iteration 2400: Loss = -10905.0750921912
Iteration 2500: Loss = -10905.072663222629
Iteration 2600: Loss = -10905.070763029957
Iteration 2700: Loss = -10905.069270882761
Iteration 2800: Loss = -10905.068036288603
Iteration 2900: Loss = -10905.067092015299
Iteration 3000: Loss = -10905.066354290282
Iteration 3100: Loss = -10905.065753750392
Iteration 3200: Loss = -10905.06528145343
Iteration 3300: Loss = -10905.064881499282
Iteration 3400: Loss = -10905.064560021727
Iteration 3500: Loss = -10905.064356708337
Iteration 3600: Loss = -10905.064172977769
Iteration 3700: Loss = -10905.064015428974
Iteration 3800: Loss = -10905.063881380394
Iteration 3900: Loss = -10905.06401522986
1
Iteration 4000: Loss = -10905.063767395708
Iteration 4100: Loss = -10905.063704185644
Iteration 4200: Loss = -10905.063649050917
Iteration 4300: Loss = -10905.06361775985
Iteration 4400: Loss = -10905.063613956074
Iteration 4500: Loss = -10905.063541865828
Iteration 4600: Loss = -10905.063543889062
Iteration 4700: Loss = -10905.064171591464
1
Iteration 4800: Loss = -10905.063504954878
Iteration 4900: Loss = -10905.063498599418
Iteration 5000: Loss = -10905.06353983619
Iteration 5100: Loss = -10905.063481012603
Iteration 5200: Loss = -10905.063538667533
Iteration 5300: Loss = -10905.063481343394
Iteration 5400: Loss = -10905.063490622982
Iteration 5500: Loss = -10905.063789230922
1
Iteration 5600: Loss = -10905.06345353147
Iteration 5700: Loss = -10905.063462723016
Iteration 5800: Loss = -10905.06341282429
Iteration 5900: Loss = -10905.063486454701
Iteration 6000: Loss = -10905.063449474539
Iteration 6100: Loss = -10905.063458923365
Iteration 6200: Loss = -10905.063446014223
Iteration 6300: Loss = -10905.063440351081
Iteration 6400: Loss = -10905.06402306654
1
Iteration 6500: Loss = -10905.063626480442
2
Iteration 6600: Loss = -10905.064171282684
3
Iteration 6700: Loss = -10905.063419697752
Iteration 6800: Loss = -10905.065057766167
1
Iteration 6900: Loss = -10905.063417426247
Iteration 7000: Loss = -10905.063519697236
1
Iteration 7100: Loss = -10905.063494364906
Iteration 7200: Loss = -10905.065148947808
1
Iteration 7300: Loss = -10905.063704706467
2
Iteration 7400: Loss = -10905.065463011757
3
Iteration 7500: Loss = -10905.06507999978
4
Iteration 7600: Loss = -10905.269000159306
5
Iteration 7700: Loss = -10905.06345225524
Iteration 7800: Loss = -10905.125211665216
1
Iteration 7900: Loss = -10905.063441167718
Iteration 8000: Loss = -10905.06455195217
1
Iteration 8100: Loss = -10905.063470274645
Iteration 8200: Loss = -10905.063418782604
Iteration 8300: Loss = -10905.065486233021
1
Iteration 8400: Loss = -10905.063431630604
Iteration 8500: Loss = -10905.063440511676
Iteration 8600: Loss = -10905.098847773656
1
Iteration 8700: Loss = -10905.063415811204
Iteration 8800: Loss = -10905.063440059948
Iteration 8900: Loss = -10905.067268337367
1
Iteration 9000: Loss = -10905.063445912348
Iteration 9100: Loss = -10905.06343066339
Iteration 9200: Loss = -10905.064896726228
1
Iteration 9300: Loss = -10905.063414943173
Iteration 9400: Loss = -10905.063446993328
Iteration 9500: Loss = -10905.066151549227
1
Iteration 9600: Loss = -10905.063443382174
Iteration 9700: Loss = -10905.063410679963
Iteration 9800: Loss = -10905.063510514974
Iteration 9900: Loss = -10905.063423075928
Iteration 10000: Loss = -10905.073329520874
1
Iteration 10100: Loss = -10905.063478091613
Iteration 10200: Loss = -10905.06342541689
Iteration 10300: Loss = -10905.273513151487
1
Iteration 10400: Loss = -10905.063453228147
Iteration 10500: Loss = -10905.063408333963
Iteration 10600: Loss = -10905.088965126184
1
Iteration 10700: Loss = -10905.063435975648
Iteration 10800: Loss = -10905.063422509786
Iteration 10900: Loss = -10905.068470005013
1
Iteration 11000: Loss = -10905.063428893236
Iteration 11100: Loss = -10905.063399362394
Iteration 11200: Loss = -10905.063858154028
1
Iteration 11300: Loss = -10905.06344550443
Iteration 11400: Loss = -10905.063454540417
Iteration 11500: Loss = -10905.064172481734
1
Iteration 11600: Loss = -10905.063426843904
Iteration 11700: Loss = -10905.11698172686
1
Iteration 11800: Loss = -10905.063459159268
Iteration 11900: Loss = -10905.063415325696
Iteration 12000: Loss = -10905.063640206967
1
Iteration 12100: Loss = -10905.063451184216
Iteration 12200: Loss = -10905.243858557
1
Iteration 12300: Loss = -10905.063430377611
Iteration 12400: Loss = -10905.063442876404
Iteration 12500: Loss = -10905.06373070158
1
Iteration 12600: Loss = -10905.063401826872
Iteration 12700: Loss = -10905.071339459726
1
Iteration 12800: Loss = -10905.063453163784
Iteration 12900: Loss = -10905.063520057485
Iteration 13000: Loss = -10905.063455077341
Iteration 13100: Loss = -10905.063430826069
Iteration 13200: Loss = -10905.063809272637
1
Iteration 13300: Loss = -10905.063435358748
Iteration 13400: Loss = -10905.142004666326
1
Iteration 13500: Loss = -10905.063437733234
Iteration 13600: Loss = -10905.063427627252
Iteration 13700: Loss = -10905.064076255923
1
Iteration 13800: Loss = -10905.063436737708
Iteration 13900: Loss = -10905.064299458289
1
Iteration 14000: Loss = -10905.0634656335
Iteration 14100: Loss = -10905.06344040968
Iteration 14200: Loss = -10905.063669695255
1
Iteration 14300: Loss = -10905.063455899926
Iteration 14400: Loss = -10905.121328414665
1
Iteration 14500: Loss = -10905.063421709301
Iteration 14600: Loss = -10905.0634327882
Iteration 14700: Loss = -10905.06375149783
1
Iteration 14800: Loss = -10905.063419063794
Iteration 14900: Loss = -10905.063839255972
1
Iteration 15000: Loss = -10905.063463125669
Iteration 15100: Loss = -10905.06587832287
1
Iteration 15200: Loss = -10905.063443826
Iteration 15300: Loss = -10905.182988692148
1
Iteration 15400: Loss = -10905.06344926084
Iteration 15500: Loss = -10905.063588635558
1
Iteration 15600: Loss = -10905.06349084525
Iteration 15700: Loss = -10905.063427103709
Iteration 15800: Loss = -10905.064533875438
1
Iteration 15900: Loss = -10905.063436807894
Iteration 16000: Loss = -10905.07612664541
1
Iteration 16100: Loss = -10905.063442409319
Iteration 16200: Loss = -10905.063493761327
Iteration 16300: Loss = -10905.063527453707
Iteration 16400: Loss = -10905.063457258202
Iteration 16500: Loss = -10905.06995669751
1
Iteration 16600: Loss = -10905.063380523818
Iteration 16700: Loss = -10905.156242068519
1
Iteration 16800: Loss = -10905.063437548613
Iteration 16900: Loss = -10905.0634245717
Iteration 17000: Loss = -10905.073763146685
1
Iteration 17100: Loss = -10905.063423139998
Iteration 17200: Loss = -10905.064603046036
1
Iteration 17300: Loss = -10905.063455688907
Iteration 17400: Loss = -10905.063422346466
Iteration 17500: Loss = -10905.063989356273
1
Iteration 17600: Loss = -10905.063421125667
Iteration 17700: Loss = -10905.074642130423
1
Iteration 17800: Loss = -10905.063452809816
Iteration 17900: Loss = -10905.111585927281
1
Iteration 18000: Loss = -10905.063493383394
Iteration 18100: Loss = -10905.063418450189
Iteration 18200: Loss = -10905.06417455609
1
Iteration 18300: Loss = -10905.063439997037
Iteration 18400: Loss = -10905.071166591246
1
Iteration 18500: Loss = -10905.0634651798
Iteration 18600: Loss = -10905.077913849478
1
Iteration 18700: Loss = -10905.063488809346
Iteration 18800: Loss = -10905.063448127836
Iteration 18900: Loss = -10905.063819151736
1
Iteration 19000: Loss = -10905.06340170692
Iteration 19100: Loss = -10905.085002773836
1
Iteration 19200: Loss = -10905.063441733384
Iteration 19300: Loss = -10905.108608266893
1
Iteration 19400: Loss = -10905.063455549518
Iteration 19500: Loss = -10905.063451850463
Iteration 19600: Loss = -10905.064197867934
1
Iteration 19700: Loss = -10905.063460536487
Iteration 19800: Loss = -10905.121701236687
1
Iteration 19900: Loss = -10905.063425450044
pi: tensor([[0.3245, 0.6755],
        [0.0693, 0.9307]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0067, 0.9933], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3169, 0.1734],
         [0.6131, 0.1532]],

        [[0.5950, 0.1985],
         [0.5157, 0.6095]],

        [[0.6118, 0.2035],
         [0.7182, 0.5153]],

        [[0.7288, 0.1902],
         [0.5620, 0.5576]],

        [[0.6359, 0.2254],
         [0.7005, 0.6807]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.012285862605987194
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
Global Adjusted Rand Index: 4.194535550841894e-05
Average Adjusted Rand Index: -0.005292103465843387
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24395.318939720124
Iteration 100: Loss = -10911.482442444378
Iteration 200: Loss = -10906.392039366236
Iteration 300: Loss = -10905.561941579259
Iteration 400: Loss = -10905.279513320447
Iteration 500: Loss = -10905.161171648866
Iteration 600: Loss = -10905.110377285377
Iteration 700: Loss = -10905.087014462533
Iteration 800: Loss = -10905.07524044522
Iteration 900: Loss = -10905.069102628731
Iteration 1000: Loss = -10905.066261779939
Iteration 1100: Loss = -10905.064955073036
Iteration 1200: Loss = -10905.06437922255
Iteration 1300: Loss = -10905.064020407814
Iteration 1400: Loss = -10905.063826874868
Iteration 1500: Loss = -10905.063741489708
Iteration 1600: Loss = -10905.063624965105
Iteration 1700: Loss = -10905.063540464977
Iteration 1800: Loss = -10905.063520863168
Iteration 1900: Loss = -10905.0634978157
Iteration 2000: Loss = -10905.063445290121
Iteration 2100: Loss = -10905.063447099383
Iteration 2200: Loss = -10905.063445901307
Iteration 2300: Loss = -10905.063428271518
Iteration 2400: Loss = -10905.063436526147
Iteration 2500: Loss = -10905.06340308038
Iteration 2600: Loss = -10905.063454690282
Iteration 2700: Loss = -10905.063461207397
Iteration 2800: Loss = -10905.063453867064
Iteration 2900: Loss = -10905.063476425463
Iteration 3000: Loss = -10905.063426288203
Iteration 3100: Loss = -10905.063393549797
Iteration 3200: Loss = -10905.063443367375
Iteration 3300: Loss = -10905.063432987172
Iteration 3400: Loss = -10905.063420891147
Iteration 3500: Loss = -10905.063410185083
Iteration 3600: Loss = -10905.063406119449
Iteration 3700: Loss = -10905.063415820314
Iteration 3800: Loss = -10905.063441784489
Iteration 3900: Loss = -10905.063399284008
Iteration 4000: Loss = -10905.063424499776
Iteration 4100: Loss = -10905.063439111622
Iteration 4200: Loss = -10905.063454448384
Iteration 4300: Loss = -10905.063424999471
Iteration 4400: Loss = -10905.06697207794
1
Iteration 4500: Loss = -10905.063422798785
Iteration 4600: Loss = -10905.06340928119
Iteration 4700: Loss = -10905.063437779656
Iteration 4800: Loss = -10905.063418011414
Iteration 4900: Loss = -10905.064717646244
1
Iteration 5000: Loss = -10905.063433205307
Iteration 5100: Loss = -10905.065051182819
1
Iteration 5200: Loss = -10905.06343270247
Iteration 5300: Loss = -10905.063427240528
Iteration 5400: Loss = -10905.063471174151
Iteration 5500: Loss = -10905.06342651181
Iteration 5600: Loss = -10905.063579269548
1
Iteration 5700: Loss = -10905.073486621888
2
Iteration 5800: Loss = -10905.063471387368
Iteration 5900: Loss = -10905.063878026787
1
Iteration 6000: Loss = -10905.064724768747
2
Iteration 6100: Loss = -10905.064340621475
3
Iteration 6200: Loss = -10905.063451603603
Iteration 6300: Loss = -10905.06350785316
Iteration 6400: Loss = -10905.06343151843
Iteration 6500: Loss = -10905.063791967821
1
Iteration 6600: Loss = -10905.063407548485
Iteration 6700: Loss = -10905.074434152644
1
Iteration 6800: Loss = -10905.063427438065
Iteration 6900: Loss = -10905.063458364099
Iteration 7000: Loss = -10905.06347248913
Iteration 7100: Loss = -10905.063474505789
Iteration 7200: Loss = -10905.063475682282
Iteration 7300: Loss = -10905.064746403743
1
Iteration 7400: Loss = -10905.063541064053
Iteration 7500: Loss = -10905.063423659609
Iteration 7600: Loss = -10905.06347012635
Iteration 7700: Loss = -10905.063609701605
1
Iteration 7800: Loss = -10905.063449508885
Iteration 7900: Loss = -10905.063488887687
Iteration 8000: Loss = -10905.063476846843
Iteration 8100: Loss = -10905.063447083689
Iteration 8200: Loss = -10905.076612915756
1
Iteration 8300: Loss = -10905.063421069199
Iteration 8400: Loss = -10905.064216185276
1
Iteration 8500: Loss = -10905.076186204362
2
Iteration 8600: Loss = -10905.06346551335
Iteration 8700: Loss = -10905.069580895391
1
Iteration 8800: Loss = -10905.063449812556
Iteration 8900: Loss = -10905.071882364631
1
Iteration 9000: Loss = -10905.063444502124
Iteration 9100: Loss = -10905.069635848855
1
Iteration 9200: Loss = -10905.06341265942
Iteration 9300: Loss = -10905.063436330043
Iteration 9400: Loss = -10905.065088877898
1
Iteration 9500: Loss = -10905.063417169007
Iteration 9600: Loss = -10905.063438589696
Iteration 9700: Loss = -10905.063996689327
1
Iteration 9800: Loss = -10905.063481052382
Iteration 9900: Loss = -10905.06343592689
Iteration 10000: Loss = -10905.067969684947
1
Iteration 10100: Loss = -10905.063428792575
Iteration 10200: Loss = -10905.063452140335
Iteration 10300: Loss = -10905.063665269869
1
Iteration 10400: Loss = -10905.063485331604
Iteration 10500: Loss = -10905.066228016283
1
Iteration 10600: Loss = -10905.063419319293
Iteration 10700: Loss = -10905.06346022512
Iteration 10800: Loss = -10905.157551752323
1
Iteration 10900: Loss = -10905.063465306248
Iteration 11000: Loss = -10905.063423509
Iteration 11100: Loss = -10905.06573974233
1
Iteration 11200: Loss = -10905.063446450646
Iteration 11300: Loss = -10905.063448376706
Iteration 11400: Loss = -10905.063549356168
1
Iteration 11500: Loss = -10905.063415173507
Iteration 11600: Loss = -10905.065116982729
1
Iteration 11700: Loss = -10905.063416721638
Iteration 11800: Loss = -10905.075959192682
1
Iteration 11900: Loss = -10905.06344194822
Iteration 12000: Loss = -10905.063421308
Iteration 12100: Loss = -10905.064175723439
1
Iteration 12200: Loss = -10905.0634473665
Iteration 12300: Loss = -10905.073938475054
1
Iteration 12400: Loss = -10905.063435484422
Iteration 12500: Loss = -10905.06343461098
Iteration 12600: Loss = -10905.064036268763
1
Iteration 12700: Loss = -10905.063410364246
Iteration 12800: Loss = -10905.472958630788
1
Iteration 12900: Loss = -10905.063451151691
Iteration 13000: Loss = -10905.0634243714
Iteration 13100: Loss = -10905.156617651839
1
Iteration 13200: Loss = -10905.063468020144
Iteration 13300: Loss = -10905.072009252435
1
Iteration 13400: Loss = -10905.063405334326
Iteration 13500: Loss = -10905.067744091199
1
Iteration 13600: Loss = -10905.063428772606
Iteration 13700: Loss = -10905.074962598037
1
Iteration 13800: Loss = -10905.063453992925
Iteration 13900: Loss = -10905.063425779093
Iteration 14000: Loss = -10905.065008618045
1
Iteration 14100: Loss = -10905.063410394387
Iteration 14200: Loss = -10905.064848295135
1
Iteration 14300: Loss = -10905.063442192966
Iteration 14400: Loss = -10905.072008279147
1
Iteration 14500: Loss = -10905.06344716731
Iteration 14600: Loss = -10905.472742296979
1
Iteration 14700: Loss = -10905.063459992085
Iteration 14800: Loss = -10905.063431370932
Iteration 14900: Loss = -10905.0646916937
1
Iteration 15000: Loss = -10905.063441322236
Iteration 15100: Loss = -10905.063476417406
Iteration 15200: Loss = -10905.063478726996
Iteration 15300: Loss = -10905.06339962771
Iteration 15400: Loss = -10905.06379291627
1
Iteration 15500: Loss = -10905.063421918725
Iteration 15600: Loss = -10905.237260197451
1
Iteration 15700: Loss = -10905.06344623324
Iteration 15800: Loss = -10905.06344706884
Iteration 15900: Loss = -10905.06410189719
1
Iteration 16000: Loss = -10905.063442741208
Iteration 16100: Loss = -10905.064620201512
1
Iteration 16200: Loss = -10905.063455293031
Iteration 16300: Loss = -10905.073900037816
1
Iteration 16400: Loss = -10905.063461556842
Iteration 16500: Loss = -10905.063462147024
Iteration 16600: Loss = -10905.06421144462
1
Iteration 16700: Loss = -10905.063423386224
Iteration 16800: Loss = -10905.083311490434
1
Iteration 16900: Loss = -10905.063426374545
Iteration 17000: Loss = -10905.063432718713
Iteration 17100: Loss = -10905.063581951981
1
Iteration 17200: Loss = -10905.063434417872
Iteration 17300: Loss = -10905.068949962897
1
Iteration 17400: Loss = -10905.063420809302
Iteration 17500: Loss = -10905.452581957174
1
Iteration 17600: Loss = -10905.0634461397
Iteration 17700: Loss = -10905.06343400534
Iteration 17800: Loss = -10905.063586422206
1
Iteration 17900: Loss = -10905.063440300657
Iteration 18000: Loss = -10905.172494067394
1
Iteration 18100: Loss = -10905.063422663568
Iteration 18200: Loss = -10905.079658426277
1
Iteration 18300: Loss = -10905.063430006474
Iteration 18400: Loss = -10905.063456856675
Iteration 18500: Loss = -10905.136248661574
1
Iteration 18600: Loss = -10905.063434195808
Iteration 18700: Loss = -10905.064601148844
1
Iteration 18800: Loss = -10905.063458543993
Iteration 18900: Loss = -10905.063431846564
Iteration 19000: Loss = -10905.063763811395
1
Iteration 19100: Loss = -10905.063448268995
Iteration 19200: Loss = -10905.084237745767
1
Iteration 19300: Loss = -10905.063436210297
Iteration 19400: Loss = -10905.11501026472
1
Iteration 19500: Loss = -10905.063476235808
Iteration 19600: Loss = -10905.063450437226
Iteration 19700: Loss = -10905.063575138734
1
Iteration 19800: Loss = -10905.063446608729
Iteration 19900: Loss = -10905.06438529402
1
pi: tensor([[0.9307, 0.0693],
        [0.6755, 0.3245]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9933, 0.0067], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1532, 0.1734],
         [0.5351, 0.3169]],

        [[0.5666, 0.1985],
         [0.6145, 0.6574]],

        [[0.7297, 0.2035],
         [0.6257, 0.7077]],

        [[0.5506, 0.1902],
         [0.6915, 0.5977]],

        [[0.6798, 0.2254],
         [0.6468, 0.7049]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0007748402262652058
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
Global Adjusted Rand Index: 4.194535550841894e-05
Average Adjusted Rand Index: -0.005292103465843387
10785.689976915199
[4.194535550841894e-05, 4.194535550841894e-05] [-0.005292103465843387, -0.005292103465843387] [10905.063432265915, 10905.06342797325]
-------------------------------------
This iteration is 3
True Objective function: Loss = -10943.21656986485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22578.955885855394
Iteration 100: Loss = -10997.73370081216
Iteration 200: Loss = -10997.14702245269
Iteration 300: Loss = -10996.992910422023
Iteration 400: Loss = -10996.903222747722
Iteration 500: Loss = -10996.83351766457
Iteration 600: Loss = -10996.769198599166
Iteration 700: Loss = -10996.701762966817
Iteration 800: Loss = -10996.6236938888
Iteration 900: Loss = -10996.528823597633
Iteration 1000: Loss = -10996.415963291423
Iteration 1100: Loss = -10996.296028238365
Iteration 1200: Loss = -10996.19027850728
Iteration 1300: Loss = -10996.10130928424
Iteration 1400: Loss = -10995.974491597623
Iteration 1500: Loss = -10995.77049232433
Iteration 1600: Loss = -10995.700015520033
Iteration 1700: Loss = -10995.662648654077
Iteration 1800: Loss = -10995.637066834877
Iteration 1900: Loss = -10995.620077968773
Iteration 2000: Loss = -10995.609940074282
Iteration 2100: Loss = -10995.604432173322
Iteration 2200: Loss = -10995.601814387459
Iteration 2300: Loss = -10995.600625848492
Iteration 2400: Loss = -10995.600121565647
Iteration 2500: Loss = -10995.599903953462
Iteration 2600: Loss = -10995.599864050775
Iteration 2700: Loss = -10995.599728229457
Iteration 2800: Loss = -10995.599649671825
Iteration 2900: Loss = -10995.599540875117
Iteration 3000: Loss = -10995.599439354832
Iteration 3100: Loss = -10995.599200584404
Iteration 3200: Loss = -10995.59934894662
1
Iteration 3300: Loss = -10995.598575109112
Iteration 3400: Loss = -10995.59810187597
Iteration 3500: Loss = -10995.598130781502
Iteration 3600: Loss = -10995.596646945804
Iteration 3700: Loss = -10995.595584814779
Iteration 3800: Loss = -10995.594237322768
Iteration 3900: Loss = -10995.592536989036
Iteration 4000: Loss = -10995.590430322203
Iteration 4100: Loss = -10995.58778412525
Iteration 4200: Loss = -10995.584523395615
Iteration 4300: Loss = -10995.580343305759
Iteration 4400: Loss = -10995.575426572363
Iteration 4500: Loss = -10995.569966305347
Iteration 4600: Loss = -10995.56452447596
Iteration 4700: Loss = -10995.561629993594
Iteration 4800: Loss = -10995.556244236239
Iteration 4900: Loss = -10995.5537097846
Iteration 5000: Loss = -10995.55201803657
Iteration 5100: Loss = -10995.550866954842
Iteration 5200: Loss = -10995.550124666153
Iteration 5300: Loss = -10995.549562592267
Iteration 5400: Loss = -10995.549165356928
Iteration 5500: Loss = -10995.548966593762
Iteration 5600: Loss = -10995.548730820125
Iteration 5700: Loss = -10995.548638070837
Iteration 5800: Loss = -10995.548562455106
Iteration 5900: Loss = -10995.548467949033
Iteration 6000: Loss = -10995.548378365656
Iteration 6100: Loss = -10995.548536001766
1
Iteration 6200: Loss = -10995.54833262364
Iteration 6300: Loss = -10995.548300631008
Iteration 6400: Loss = -10995.548258792165
Iteration 6500: Loss = -10995.548300941324
Iteration 6600: Loss = -10995.54830412805
Iteration 6700: Loss = -10995.548253080633
Iteration 6800: Loss = -10995.548299994693
Iteration 6900: Loss = -10995.548278594755
Iteration 7000: Loss = -10995.548957700716
1
Iteration 7100: Loss = -10995.548251571592
Iteration 7200: Loss = -10995.548502137892
1
Iteration 7300: Loss = -10995.548285730261
Iteration 7400: Loss = -10995.548306313302
Iteration 7500: Loss = -10995.548254723059
Iteration 7600: Loss = -10995.548942042507
1
Iteration 7700: Loss = -10995.548250916892
Iteration 7800: Loss = -10995.54827521362
Iteration 7900: Loss = -10995.548272262891
Iteration 8000: Loss = -10995.548245909991
Iteration 8100: Loss = -10995.548911963791
1
Iteration 8200: Loss = -10995.548422614282
2
Iteration 8300: Loss = -10995.548800042285
3
Iteration 8400: Loss = -10995.548335628177
Iteration 8500: Loss = -10995.548750714272
1
Iteration 8600: Loss = -10995.548246834138
Iteration 8700: Loss = -10995.552530578161
1
Iteration 8800: Loss = -10995.54828079486
Iteration 8900: Loss = -10995.548260531139
Iteration 9000: Loss = -10995.548343995664
Iteration 9100: Loss = -10995.548270491612
Iteration 9200: Loss = -10995.54828864981
Iteration 9300: Loss = -10995.548535787293
1
Iteration 9400: Loss = -10995.548270945288
Iteration 9500: Loss = -10995.548237218054
Iteration 9600: Loss = -10995.549044995467
1
Iteration 9700: Loss = -10995.548291361003
Iteration 9800: Loss = -10995.564747664006
1
Iteration 9900: Loss = -10995.54824650581
Iteration 10000: Loss = -10995.548286930305
Iteration 10100: Loss = -10995.548697728347
1
Iteration 10200: Loss = -10995.548263976843
Iteration 10300: Loss = -10995.548290841887
Iteration 10400: Loss = -10995.570333979755
1
Iteration 10500: Loss = -10995.548292916534
Iteration 10600: Loss = -10995.54825018181
Iteration 10700: Loss = -10995.552886873511
1
Iteration 10800: Loss = -10995.54826642795
Iteration 10900: Loss = -10995.548234182002
Iteration 11000: Loss = -10995.548558352104
1
Iteration 11100: Loss = -10995.54825782794
Iteration 11200: Loss = -10995.560841564562
1
Iteration 11300: Loss = -10995.548286747955
Iteration 11400: Loss = -10995.584324851232
1
Iteration 11500: Loss = -10995.548280198145
Iteration 11600: Loss = -10995.557694068722
1
Iteration 11700: Loss = -10995.548350553689
Iteration 11800: Loss = -10995.548305786599
Iteration 11900: Loss = -10995.55061139923
1
Iteration 12000: Loss = -10995.548293787077
Iteration 12100: Loss = -10995.720151500775
1
Iteration 12200: Loss = -10995.548282308393
Iteration 12300: Loss = -10995.548275380195
Iteration 12400: Loss = -10995.548466497898
1
Iteration 12500: Loss = -10995.770542541402
2
Iteration 12600: Loss = -10995.548259677415
Iteration 12700: Loss = -10995.608157710498
1
Iteration 12800: Loss = -10995.548318790206
Iteration 12900: Loss = -10995.54838721943
Iteration 13000: Loss = -10995.55575416331
1
Iteration 13100: Loss = -10995.54827097787
Iteration 13200: Loss = -10995.549242256093
1
Iteration 13300: Loss = -10995.556535977224
2
Iteration 13400: Loss = -10995.548338686627
Iteration 13500: Loss = -10995.548296468594
Iteration 13600: Loss = -10995.550994394323
1
Iteration 13700: Loss = -10995.548911443551
2
Iteration 13800: Loss = -10995.556969531808
3
Iteration 13900: Loss = -10995.578603230868
4
Iteration 14000: Loss = -10995.54906322592
5
Iteration 14100: Loss = -10995.54848921509
6
Iteration 14200: Loss = -10995.550497873166
7
Iteration 14300: Loss = -10995.567995213054
8
Iteration 14400: Loss = -10995.548315849503
Iteration 14500: Loss = -10995.551543164434
1
Iteration 14600: Loss = -10995.548941535682
2
Iteration 14700: Loss = -10995.548356889663
Iteration 14800: Loss = -10995.549385852606
1
Iteration 14900: Loss = -10995.548281045369
Iteration 15000: Loss = -10995.548648928916
1
Iteration 15100: Loss = -10995.548231077202
Iteration 15200: Loss = -10995.548827488388
1
Iteration 15300: Loss = -10995.623647790828
2
Iteration 15400: Loss = -10995.548298851762
Iteration 15500: Loss = -10995.548708259037
1
Iteration 15600: Loss = -10995.549338325445
2
Iteration 15700: Loss = -10995.552469556837
3
Iteration 15800: Loss = -10995.548317481363
Iteration 15900: Loss = -10995.548705537809
1
Iteration 16000: Loss = -10995.553368516486
2
Iteration 16100: Loss = -10995.603600371393
3
Iteration 16200: Loss = -10995.548327289078
Iteration 16300: Loss = -10995.548399393823
Iteration 16400: Loss = -10995.54909937587
1
Iteration 16500: Loss = -10995.551746175786
2
Iteration 16600: Loss = -10995.557755317432
3
Iteration 16700: Loss = -10995.578161422565
4
Iteration 16800: Loss = -10995.54831781596
Iteration 16900: Loss = -10995.549776110622
1
Iteration 17000: Loss = -10995.548297523106
Iteration 17100: Loss = -10995.548486437046
1
Iteration 17200: Loss = -10995.568992820969
2
Iteration 17300: Loss = -10995.550113830403
3
Iteration 17400: Loss = -10995.563366387398
4
Iteration 17500: Loss = -10995.548332695118
Iteration 17600: Loss = -10995.548437968675
1
Iteration 17700: Loss = -10995.54912212159
2
Iteration 17800: Loss = -10995.665475597178
3
Iteration 17900: Loss = -10995.548303957004
Iteration 18000: Loss = -10995.548668083466
1
Iteration 18100: Loss = -10995.582041720152
2
Iteration 18200: Loss = -10995.554750095966
3
Iteration 18300: Loss = -10995.549633727156
4
Iteration 18400: Loss = -10995.554531610942
5
Iteration 18500: Loss = -10995.548458913692
6
Iteration 18600: Loss = -10995.548378147454
Iteration 18700: Loss = -10995.557815922406
1
Iteration 18800: Loss = -10995.548279203744
Iteration 18900: Loss = -10995.549575204524
1
Iteration 19000: Loss = -10995.54891185271
2
Iteration 19100: Loss = -10995.610346958423
3
Iteration 19200: Loss = -10995.59086945953
4
Iteration 19300: Loss = -10995.551520928617
5
Iteration 19400: Loss = -10995.548555996244
6
Iteration 19500: Loss = -10995.548520020327
7
Iteration 19600: Loss = -10995.552742178097
8
Iteration 19700: Loss = -10995.571096369105
9
Iteration 19800: Loss = -10995.548314093594
Iteration 19900: Loss = -10995.595100574194
1
pi: tensor([[0.9525, 0.0475],
        [0.0138, 0.9862]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0588, 0.9412], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1354, 0.1282],
         [0.6982, 0.1651]],

        [[0.5943, 0.1743],
         [0.5518, 0.6592]],

        [[0.5623, 0.1738],
         [0.6677, 0.5134]],

        [[0.5106, 0.1222],
         [0.6536, 0.5823]],

        [[0.6845, 0.1522],
         [0.7113, 0.6887]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
Global Adjusted Rand Index: -0.003939592085003663
Average Adjusted Rand Index: -0.004072443522857265
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20098.135234173544
Iteration 100: Loss = -10998.040198368413
Iteration 200: Loss = -10997.287016780068
Iteration 300: Loss = -10997.160573807127
Iteration 400: Loss = -10997.084422658263
Iteration 500: Loss = -10997.024134144578
Iteration 600: Loss = -10996.969742668183
Iteration 700: Loss = -10996.918753385688
Iteration 800: Loss = -10996.872641324018
Iteration 900: Loss = -10996.834390274504
Iteration 1000: Loss = -10996.80446642525
Iteration 1100: Loss = -10996.7810220438
Iteration 1200: Loss = -10996.762143299555
Iteration 1300: Loss = -10996.746964484713
Iteration 1400: Loss = -10996.734813317456
Iteration 1500: Loss = -10996.725294785558
Iteration 1600: Loss = -10996.718083320835
Iteration 1700: Loss = -10996.712725474243
Iteration 1800: Loss = -10996.708871473596
Iteration 1900: Loss = -10996.706161003587
Iteration 2000: Loss = -10996.704254316946
Iteration 2100: Loss = -10996.702986963977
Iteration 2200: Loss = -10996.702049995982
Iteration 2300: Loss = -10996.701409117113
Iteration 2400: Loss = -10996.70094737352
Iteration 2500: Loss = -10996.700558939927
Iteration 2600: Loss = -10996.700306971656
Iteration 2700: Loss = -10996.700107747034
Iteration 2800: Loss = -10996.699929318977
Iteration 2900: Loss = -10996.699731692368
Iteration 3000: Loss = -10996.699638634724
Iteration 3100: Loss = -10996.699492055372
Iteration 3200: Loss = -10996.69936803819
Iteration 3300: Loss = -10996.699250219335
Iteration 3400: Loss = -10996.699138271095
Iteration 3500: Loss = -10996.699040111107
Iteration 3600: Loss = -10996.698941879627
Iteration 3700: Loss = -10996.698826297703
Iteration 3800: Loss = -10996.698755595375
Iteration 3900: Loss = -10996.69860310422
Iteration 4000: Loss = -10996.698536099912
Iteration 4100: Loss = -10996.698363585967
Iteration 4200: Loss = -10996.698278944972
Iteration 4300: Loss = -10996.698140602397
Iteration 4400: Loss = -10996.698012630837
Iteration 4500: Loss = -10996.697896627453
Iteration 4600: Loss = -10996.697697831774
Iteration 4700: Loss = -10996.697533231547
Iteration 4800: Loss = -10996.697343567837
Iteration 4900: Loss = -10996.69699762367
Iteration 5000: Loss = -10996.697454334377
1
Iteration 5100: Loss = -10996.696140886916
Iteration 5200: Loss = -10996.695346164295
Iteration 5300: Loss = -10996.694005708221
Iteration 5400: Loss = -10996.691224433238
Iteration 5500: Loss = -10996.683914909094
Iteration 5600: Loss = -10996.658465482726
Iteration 5700: Loss = -10996.587825025192
Iteration 5800: Loss = -10996.564205548239
Iteration 5900: Loss = -10996.560320014287
Iteration 6000: Loss = -10996.114424089175
Iteration 6100: Loss = -10995.416053018538
Iteration 6200: Loss = -10995.307718700384
Iteration 6300: Loss = -10995.271767685834
Iteration 6400: Loss = -10995.249508804394
Iteration 6500: Loss = -10995.226144392167
Iteration 6600: Loss = -10995.196199214719
Iteration 6700: Loss = -10995.171621647447
Iteration 6800: Loss = -10995.157021541725
Iteration 6900: Loss = -10995.148533270232
Iteration 7000: Loss = -10995.143472370943
Iteration 7100: Loss = -10995.140242894573
Iteration 7200: Loss = -10995.138124714511
Iteration 7300: Loss = -10995.13667012988
Iteration 7400: Loss = -10995.13565085597
Iteration 7500: Loss = -10995.134907357085
Iteration 7600: Loss = -10995.134363899393
Iteration 7700: Loss = -10995.134300138436
Iteration 7800: Loss = -10995.133549257083
Iteration 7900: Loss = -10995.133213719793
Iteration 8000: Loss = -10995.135219893278
1
Iteration 8100: Loss = -10995.132700933886
Iteration 8200: Loss = -10995.132449822855
Iteration 8300: Loss = -10995.132266276256
Iteration 8400: Loss = -10995.133068654159
1
Iteration 8500: Loss = -10995.131934808649
Iteration 8600: Loss = -10995.131778503646
Iteration 8700: Loss = -10995.131684440217
Iteration 8800: Loss = -10995.131627925222
Iteration 8900: Loss = -10995.131512562468
Iteration 9000: Loss = -10995.13141500049
Iteration 9100: Loss = -10995.13173894489
1
Iteration 9200: Loss = -10995.13128165591
Iteration 9300: Loss = -10995.131194011814
Iteration 9400: Loss = -10995.131150442934
Iteration 9500: Loss = -10995.140503018614
1
Iteration 9600: Loss = -10995.131058904306
Iteration 9700: Loss = -10995.13096365356
Iteration 9800: Loss = -10995.130919303665
Iteration 9900: Loss = -10995.137065629162
1
Iteration 10000: Loss = -10995.130867446682
Iteration 10100: Loss = -10995.130810865241
Iteration 10200: Loss = -10995.130788197808
Iteration 10300: Loss = -10995.134067635221
1
Iteration 10400: Loss = -10995.130715258183
Iteration 10500: Loss = -10995.13070312504
Iteration 10600: Loss = -10995.130635713915
Iteration 10700: Loss = -10995.22419215187
1
Iteration 10800: Loss = -10995.130588242302
Iteration 10900: Loss = -10995.130583468814
Iteration 11000: Loss = -10995.130545461268
Iteration 11100: Loss = -10995.1307437047
1
Iteration 11200: Loss = -10995.130488130288
Iteration 11300: Loss = -10995.130479840731
Iteration 11400: Loss = -10995.258031252608
1
Iteration 11500: Loss = -10995.13046414247
Iteration 11600: Loss = -10995.130426862574
Iteration 11700: Loss = -10995.130379432623
Iteration 11800: Loss = -10995.130372434032
Iteration 11900: Loss = -10995.130379235688
Iteration 12000: Loss = -10995.13035058992
Iteration 12100: Loss = -10995.132275463431
1
Iteration 12200: Loss = -10995.130299920864
Iteration 12300: Loss = -10995.13024435825
Iteration 12400: Loss = -10995.133047214838
1
Iteration 12500: Loss = -10995.130090358365
Iteration 12600: Loss = -10995.130062275963
Iteration 12700: Loss = -10995.135595197726
1
Iteration 12800: Loss = -10995.130017344316
Iteration 12900: Loss = -10995.130037151324
Iteration 13000: Loss = -10995.130006606883
Iteration 13100: Loss = -10995.129946216943
Iteration 13200: Loss = -10995.129910495109
Iteration 13300: Loss = -10995.12988200554
Iteration 13400: Loss = -10995.177648499599
1
Iteration 13500: Loss = -10995.129910438582
Iteration 13600: Loss = -10995.129875086335
Iteration 13700: Loss = -10995.129862517402
Iteration 13800: Loss = -10995.129986128291
1
Iteration 13900: Loss = -10995.129862900916
Iteration 14000: Loss = -10995.129841735214
Iteration 14100: Loss = -10995.132551778122
1
Iteration 14200: Loss = -10995.399734737233
2
Iteration 14300: Loss = -10995.129833024577
Iteration 14400: Loss = -10995.130343622226
1
Iteration 14500: Loss = -10995.129865980602
Iteration 14600: Loss = -10995.129805074652
Iteration 14700: Loss = -10995.141655277743
1
Iteration 14800: Loss = -10995.129817083187
Iteration 14900: Loss = -10995.129808412732
Iteration 15000: Loss = -10995.131991123118
1
Iteration 15100: Loss = -10995.129799640239
Iteration 15200: Loss = -10995.129780869494
Iteration 15300: Loss = -10995.133306370306
1
Iteration 15400: Loss = -10995.129865510162
Iteration 15500: Loss = -10995.12976053004
Iteration 15600: Loss = -10995.129872721158
1
Iteration 15700: Loss = -10995.129860602465
2
Iteration 15800: Loss = -10995.129764280098
Iteration 15900: Loss = -10995.130488339912
1
Iteration 16000: Loss = -10995.129749287791
Iteration 16100: Loss = -10995.19231668935
1
Iteration 16200: Loss = -10995.12977294844
Iteration 16300: Loss = -10995.129727488096
Iteration 16400: Loss = -10995.245911621269
1
Iteration 16500: Loss = -10995.129746120545
Iteration 16600: Loss = -10995.129715444667
Iteration 16700: Loss = -10995.131649285255
1
Iteration 16800: Loss = -10995.129742168278
Iteration 16900: Loss = -10995.129844285122
1
Iteration 17000: Loss = -10995.129768777319
Iteration 17100: Loss = -10995.129801999588
Iteration 17200: Loss = -10995.129723395912
Iteration 17300: Loss = -10995.130715615913
1
Iteration 17400: Loss = -10995.129715085208
Iteration 17500: Loss = -10995.525576653943
1
Iteration 17600: Loss = -10995.129744381904
Iteration 17700: Loss = -10995.129742536372
Iteration 17800: Loss = -10995.239346165552
1
Iteration 17900: Loss = -10995.129766192504
Iteration 18000: Loss = -10995.129742224977
Iteration 18100: Loss = -10995.182418582594
1
Iteration 18200: Loss = -10995.129729727798
Iteration 18300: Loss = -10995.134391412974
1
Iteration 18400: Loss = -10995.129716584184
Iteration 18500: Loss = -10995.13431941826
1
Iteration 18600: Loss = -10995.129730180373
Iteration 18700: Loss = -10995.365116723537
1
Iteration 18800: Loss = -10995.129734554876
Iteration 18900: Loss = -10995.12972307985
Iteration 19000: Loss = -10995.129872163463
1
Iteration 19100: Loss = -10995.129699667525
Iteration 19200: Loss = -10995.13082214267
1
Iteration 19300: Loss = -10995.129719439525
Iteration 19400: Loss = -10995.169256898764
1
Iteration 19500: Loss = -10995.129705286028
Iteration 19600: Loss = -10995.134640457145
1
Iteration 19700: Loss = -10995.129672429583
Iteration 19800: Loss = -10995.129783015094
1
Iteration 19900: Loss = -10995.129666917945
pi: tensor([[1.0000e+00, 9.3758e-07],
        [2.1162e-03, 9.9788e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0097, 0.9903], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4976, 0.0926],
         [0.6981, 0.1627]],

        [[0.5973, 0.1028],
         [0.6623, 0.5594]],

        [[0.6828, 0.1842],
         [0.6111, 0.6359]],

        [[0.5173, 0.2567],
         [0.5034, 0.5005]],

        [[0.5749, 0.1456],
         [0.5775, 0.6810]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: -0.007201490658206174
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
Global Adjusted Rand Index: -0.0044651133928074556
Average Adjusted Rand Index: -0.004705330992874066
10943.21656986485
[-0.003939592085003663, -0.0044651133928074556] [-0.004072443522857265, -0.004705330992874066] [10995.555505014789, 10995.129776121674]
-------------------------------------
This iteration is 4
True Objective function: Loss = -10949.447079662514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20774.931925839166
Iteration 100: Loss = -11018.107655826967
Iteration 200: Loss = -11017.176903741936
Iteration 300: Loss = -11016.955962412745
Iteration 400: Loss = -11016.875385182519
Iteration 500: Loss = -11016.83702675685
Iteration 600: Loss = -11016.812739959709
Iteration 700: Loss = -11016.792029015121
Iteration 800: Loss = -11016.769141598534
Iteration 900: Loss = -11016.738811184274
Iteration 1000: Loss = -11016.698478601536
Iteration 1100: Loss = -11016.65715826551
Iteration 1200: Loss = -11016.626035732073
Iteration 1300: Loss = -11016.597848249177
Iteration 1400: Loss = -11016.566922488804
Iteration 1500: Loss = -11016.53153930843
Iteration 1600: Loss = -11016.489734264453
Iteration 1700: Loss = -11016.438220042399
Iteration 1800: Loss = -11016.372176016766
Iteration 1900: Loss = -11016.286939000023
Iteration 2000: Loss = -11016.180514206799
Iteration 2100: Loss = -11016.064454361453
Iteration 2200: Loss = -11015.998737896603
Iteration 2300: Loss = -11015.982117960866
Iteration 2400: Loss = -11015.976537938688
Iteration 2500: Loss = -11015.97080781639
Iteration 2600: Loss = -11015.9663264302
Iteration 2700: Loss = -11015.961510398358
Iteration 2800: Loss = -11015.958765584764
Iteration 2900: Loss = -11015.953680513958
Iteration 3000: Loss = -11015.953070930798
Iteration 3100: Loss = -11015.947502160296
Iteration 3200: Loss = -11015.944790212276
Iteration 3300: Loss = -11015.942598787846
Iteration 3400: Loss = -11015.940202574997
Iteration 3500: Loss = -11015.939770350999
Iteration 3600: Loss = -11015.935182480069
Iteration 3700: Loss = -11015.931234779107
Iteration 3800: Loss = -11015.923676626793
Iteration 3900: Loss = -11015.907933932036
Iteration 4000: Loss = -11015.931947060444
1
Iteration 4100: Loss = -11015.89267970981
Iteration 4200: Loss = -11015.891813248529
Iteration 4300: Loss = -11015.891410343114
Iteration 4400: Loss = -11015.891143837765
Iteration 4500: Loss = -11015.891045106442
Iteration 4600: Loss = -11015.890839208287
Iteration 4700: Loss = -11015.890743459027
Iteration 4800: Loss = -11015.890733419805
Iteration 4900: Loss = -11015.890673860771
Iteration 5000: Loss = -11015.891662947488
1
Iteration 5100: Loss = -11015.89055246004
Iteration 5200: Loss = -11015.914539114256
1
Iteration 5300: Loss = -11015.890458802878
Iteration 5400: Loss = -11015.890446983325
Iteration 5500: Loss = -11015.890365821959
Iteration 5600: Loss = -11015.890277059994
Iteration 5700: Loss = -11015.890851122258
1
Iteration 5800: Loss = -11015.890149658735
Iteration 5900: Loss = -11015.890019334582
Iteration 6000: Loss = -11015.889972503153
Iteration 6100: Loss = -11015.889755266919
Iteration 6200: Loss = -11015.89017447466
1
Iteration 6300: Loss = -11015.889387767005
Iteration 6400: Loss = -11015.889103946598
Iteration 6500: Loss = -11015.892618268794
1
Iteration 6600: Loss = -11015.888410313108
Iteration 6700: Loss = -11015.887878636357
Iteration 6800: Loss = -11015.89774878332
1
Iteration 6900: Loss = -11015.967976283013
2
Iteration 7000: Loss = -11015.883677902424
Iteration 7100: Loss = -11015.878892356917
Iteration 7200: Loss = -11015.847410142094
Iteration 7300: Loss = -11015.728630864574
Iteration 7400: Loss = -11015.722537475032
Iteration 7500: Loss = -11015.716009303394
Iteration 7600: Loss = -11015.480033902295
Iteration 7700: Loss = -11015.365440870795
Iteration 7800: Loss = -11015.353066964386
Iteration 7900: Loss = -11015.34798305663
Iteration 8000: Loss = -11015.345206730663
Iteration 8100: Loss = -11015.343333037721
Iteration 8200: Loss = -11015.342037242868
Iteration 8300: Loss = -11015.341543362978
Iteration 8400: Loss = -11015.340323642544
Iteration 8500: Loss = -11015.339728917881
Iteration 8600: Loss = -11015.339630640618
Iteration 8700: Loss = -11015.338812213857
Iteration 8800: Loss = -11015.338518429444
Iteration 8900: Loss = -11015.33819318381
Iteration 9000: Loss = -11015.337901489913
Iteration 9100: Loss = -11015.466155641208
1
Iteration 9200: Loss = -11015.337517030941
Iteration 9300: Loss = -11015.369067547223
1
Iteration 9400: Loss = -11015.337200117476
Iteration 9500: Loss = -11015.342484378423
1
Iteration 9600: Loss = -11015.3369045997
Iteration 9700: Loss = -11015.341718551392
1
Iteration 9800: Loss = -11015.336713508581
Iteration 9900: Loss = -11015.520474842637
1
Iteration 10000: Loss = -11015.33650708721
Iteration 10100: Loss = -11015.33641015083
Iteration 10200: Loss = -11015.339002554818
1
Iteration 10300: Loss = -11015.336264287813
Iteration 10400: Loss = -11015.336245811179
Iteration 10500: Loss = -11015.336415738157
1
Iteration 10600: Loss = -11015.336120429289
Iteration 10700: Loss = -11015.33873717508
1
Iteration 10800: Loss = -11015.337905466195
2
Iteration 10900: Loss = -11015.358080340588
3
Iteration 11000: Loss = -11015.35787569302
4
Iteration 11100: Loss = -11015.345584681114
5
Iteration 11200: Loss = -11015.3403784475
6
Iteration 11300: Loss = -11015.337355552925
7
Iteration 11400: Loss = -11015.342074353983
8
Iteration 11500: Loss = -11015.335996490712
Iteration 11600: Loss = -11015.33595386735
Iteration 11700: Loss = -11015.34666147608
1
Iteration 11800: Loss = -11015.342102757075
2
Iteration 11900: Loss = -11015.377630340137
3
Iteration 12000: Loss = -11015.34584002133
4
Iteration 12100: Loss = -11015.339272243411
5
Iteration 12200: Loss = -11015.340382250899
6
Iteration 12300: Loss = -11015.338156501746
7
Iteration 12400: Loss = -11015.343667018804
8
Iteration 12500: Loss = -11015.415851574007
9
Iteration 12600: Loss = -11015.339960149797
10
Iteration 12700: Loss = -11015.36449594508
11
Iteration 12800: Loss = -11015.448202358491
12
Iteration 12900: Loss = -11015.33659760837
13
Iteration 13000: Loss = -11015.335767610626
Iteration 13100: Loss = -11015.33590278249
1
Iteration 13200: Loss = -11015.35575600527
2
Iteration 13300: Loss = -11015.339216600525
3
Iteration 13400: Loss = -11015.3369989401
4
Iteration 13500: Loss = -11015.337452880747
5
Iteration 13600: Loss = -11015.375471952406
6
Iteration 13700: Loss = -11015.360813341716
7
Iteration 13800: Loss = -11015.337831182193
8
Iteration 13900: Loss = -11015.335552219314
Iteration 14000: Loss = -11015.335835218713
1
Iteration 14100: Loss = -11015.342122295822
2
Iteration 14200: Loss = -11015.354751374276
3
Iteration 14300: Loss = -11015.335528548578
Iteration 14400: Loss = -11015.335430193347
Iteration 14500: Loss = -11015.336581980924
1
Iteration 14600: Loss = -11015.340029421008
2
Iteration 14700: Loss = -11015.35281866074
3
Iteration 14800: Loss = -11015.335482391762
Iteration 14900: Loss = -11015.335767062603
1
Iteration 15000: Loss = -11015.335570662879
Iteration 15100: Loss = -11015.339812363858
1
Iteration 15200: Loss = -11015.335478664923
Iteration 15300: Loss = -11015.335396284436
Iteration 15400: Loss = -11015.357443267552
1
Iteration 15500: Loss = -11015.335348146857
Iteration 15600: Loss = -11015.33590882815
1
Iteration 15700: Loss = -11015.335512340613
2
Iteration 15800: Loss = -11015.335620767086
3
Iteration 15900: Loss = -11015.335879579436
4
Iteration 16000: Loss = -11015.337974404287
5
Iteration 16100: Loss = -11015.341498399599
6
Iteration 16200: Loss = -11015.360431989362
7
Iteration 16300: Loss = -11015.336431772097
8
Iteration 16400: Loss = -11015.342187167746
9
Iteration 16500: Loss = -11015.335463182717
10
Iteration 16600: Loss = -11015.337009170029
11
Iteration 16700: Loss = -11015.3369563696
12
Iteration 16800: Loss = -11015.342457831379
13
Iteration 16900: Loss = -11015.362275276786
14
Iteration 17000: Loss = -11015.337645705033
15
Stopping early at iteration 17000 due to no improvement.
pi: tensor([[1.0097e-06, 1.0000e+00],
        [9.9999e-01, 7.4240e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9139, 0.0861], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1599, 0.1568],
         [0.5967, 0.1735]],

        [[0.7085, 0.1115],
         [0.6651, 0.6234]],

        [[0.6004, 0.1803],
         [0.7005, 0.6781]],

        [[0.5978, 0.1351],
         [0.5666, 0.5411]],

        [[0.5501, 0.1779],
         [0.5750, 0.5144]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0035158395898187145
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.020872495483242322
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.003418276053557105
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: -0.0019379225681602195
Average Adjusted Rand Index: 0.0021541423337755545
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21506.130479738218
Iteration 100: Loss = -11018.276450501377
Iteration 200: Loss = -11017.418033338674
Iteration 300: Loss = -11017.100043625991
Iteration 400: Loss = -11016.95820130726
Iteration 500: Loss = -11016.88523079198
Iteration 600: Loss = -11016.843768407558
Iteration 700: Loss = -11016.817891170844
Iteration 800: Loss = -11016.800347309252
Iteration 900: Loss = -11016.787685217541
Iteration 1000: Loss = -11016.778028920926
Iteration 1100: Loss = -11016.770334364715
Iteration 1200: Loss = -11016.764010116527
Iteration 1300: Loss = -11016.758607526532
Iteration 1400: Loss = -11016.753891492195
Iteration 1500: Loss = -11016.74953843994
Iteration 1600: Loss = -11016.745531893559
Iteration 1700: Loss = -11016.741572092345
Iteration 1800: Loss = -11016.737665302895
Iteration 1900: Loss = -11016.73368612534
Iteration 2000: Loss = -11016.729475433716
Iteration 2100: Loss = -11016.724853457878
Iteration 2200: Loss = -11016.719744205388
Iteration 2300: Loss = -11016.713927278883
Iteration 2400: Loss = -11016.706990036526
Iteration 2500: Loss = -11016.698468399525
Iteration 2600: Loss = -11016.687514000256
Iteration 2700: Loss = -11016.67248523528
Iteration 2800: Loss = -11016.649983135449
Iteration 2900: Loss = -11016.61192739093
Iteration 3000: Loss = -11016.535529327652
Iteration 3100: Loss = -11016.381847565664
Iteration 3200: Loss = -11016.282314239825
Iteration 3300: Loss = -11016.231099990628
Iteration 3400: Loss = -11016.20047319006
Iteration 3500: Loss = -11016.181694398752
Iteration 3600: Loss = -11016.169506635568
Iteration 3700: Loss = -11016.161185514848
Iteration 3800: Loss = -11016.155171283766
Iteration 3900: Loss = -11016.150639280524
Iteration 4000: Loss = -11016.147100287206
Iteration 4100: Loss = -11016.14424487423
Iteration 4200: Loss = -11016.141887267977
Iteration 4300: Loss = -11016.13991844216
Iteration 4400: Loss = -11016.138170113427
Iteration 4500: Loss = -11016.136648576974
Iteration 4600: Loss = -11016.13521023783
Iteration 4700: Loss = -11016.133920288428
Iteration 4800: Loss = -11016.132699986596
Iteration 4900: Loss = -11016.131437479504
Iteration 5000: Loss = -11016.130181459435
Iteration 5100: Loss = -11016.128891503899
Iteration 5200: Loss = -11016.127554367757
Iteration 5300: Loss = -11016.126081611188
Iteration 5400: Loss = -11016.124398604914
Iteration 5500: Loss = -11016.122430805997
Iteration 5600: Loss = -11016.12012083075
Iteration 5700: Loss = -11016.117199067303
Iteration 5800: Loss = -11016.113439582043
Iteration 5900: Loss = -11016.108433438962
Iteration 6000: Loss = -11016.101423129514
Iteration 6100: Loss = -11016.091185383224
Iteration 6200: Loss = -11016.075946660236
Iteration 6300: Loss = -11016.055700822908
Iteration 6400: Loss = -11016.03095112445
Iteration 6500: Loss = -11016.006836470291
Iteration 6600: Loss = -11015.98791426912
Iteration 6700: Loss = -11015.976695713722
Iteration 6800: Loss = -11015.960709783421
Iteration 6900: Loss = -11015.950171768392
Iteration 7000: Loss = -11015.93991789616
Iteration 7100: Loss = -11015.882403165797
Iteration 7200: Loss = -11015.863006560496
Iteration 7300: Loss = -11015.746121337388
Iteration 7400: Loss = -11015.73328867359
Iteration 7500: Loss = -11015.727884117634
Iteration 7600: Loss = -11015.721998454748
Iteration 7700: Loss = -11015.651867995093
Iteration 7800: Loss = -11015.3952118576
Iteration 7900: Loss = -11015.36640362548
Iteration 8000: Loss = -11015.356976240486
Iteration 8100: Loss = -11015.35209675276
Iteration 8200: Loss = -11015.348960946249
Iteration 8300: Loss = -11015.34676332957
Iteration 8400: Loss = -11015.349021788135
1
Iteration 8500: Loss = -11015.343888058038
Iteration 8600: Loss = -11015.342862353098
Iteration 8700: Loss = -11015.343560601828
1
Iteration 8800: Loss = -11015.341339156921
Iteration 8900: Loss = -11015.34075625803
Iteration 9000: Loss = -11015.341562473279
1
Iteration 9100: Loss = -11015.339851437631
Iteration 9200: Loss = -11015.33946915339
Iteration 9300: Loss = -11015.339304720628
Iteration 9400: Loss = -11015.338837588106
Iteration 9500: Loss = -11015.338605239021
Iteration 9600: Loss = -11015.338412729821
Iteration 9700: Loss = -11015.338134922897
Iteration 9800: Loss = -11015.337925161073
Iteration 9900: Loss = -11015.338498067826
1
Iteration 10000: Loss = -11015.33755052301
Iteration 10100: Loss = -11015.342390393163
1
Iteration 10200: Loss = -11015.337316415984
Iteration 10300: Loss = -11015.337143502273
Iteration 10400: Loss = -11015.365610380755
1
Iteration 10500: Loss = -11015.336963559352
Iteration 10600: Loss = -11015.341392269735
1
Iteration 10700: Loss = -11015.424700245443
2
Iteration 10800: Loss = -11015.353715374364
3
Iteration 10900: Loss = -11015.339208811843
4
Iteration 11000: Loss = -11015.341962939563
5
Iteration 11100: Loss = -11015.395630772806
6
Iteration 11200: Loss = -11015.336891218562
Iteration 11300: Loss = -11015.368165218608
1
Iteration 11400: Loss = -11015.347488546686
2
Iteration 11500: Loss = -11015.347315741285
3
Iteration 11600: Loss = -11015.365958269787
4
Iteration 11700: Loss = -11015.352323344907
5
Iteration 11800: Loss = -11015.373065108332
6
Iteration 11900: Loss = -11015.338383316033
7
Iteration 12000: Loss = -11015.395394797359
8
Iteration 12100: Loss = -11015.342805975559
9
Iteration 12200: Loss = -11015.33898742663
10
Iteration 12300: Loss = -11015.33610611671
Iteration 12400: Loss = -11015.335903298823
Iteration 12500: Loss = -11015.336067680653
1
Iteration 12600: Loss = -11015.337189581625
2
Iteration 12700: Loss = -11015.335811418117
Iteration 12800: Loss = -11015.336487402368
1
Iteration 12900: Loss = -11015.355490003632
2
Iteration 13000: Loss = -11015.344572481707
3
Iteration 13100: Loss = -11015.33587724379
Iteration 13200: Loss = -11015.336267787428
1
Iteration 13300: Loss = -11015.370329235979
2
Iteration 13400: Loss = -11015.33691099768
3
Iteration 13500: Loss = -11015.340885287733
4
Iteration 13600: Loss = -11015.33616656761
5
Iteration 13700: Loss = -11015.339479575861
6
Iteration 13800: Loss = -11015.350786619772
7
Iteration 13900: Loss = -11015.34845071501
8
Iteration 14000: Loss = -11015.33910614017
9
Iteration 14100: Loss = -11015.394352630996
10
Iteration 14200: Loss = -11015.33603347271
11
Iteration 14300: Loss = -11015.357590119022
12
Iteration 14400: Loss = -11015.336494454667
13
Iteration 14500: Loss = -11015.33847575343
14
Iteration 14600: Loss = -11015.343273933815
15
Stopping early at iteration 14600 due to no improvement.
pi: tensor([[1.0846e-05, 9.9999e-01],
        [9.9996e-01, 3.5505e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9139, 0.0861], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1599, 0.1573],
         [0.7193, 0.1735]],

        [[0.6225, 0.1115],
         [0.6467, 0.5350]],

        [[0.5913, 0.1801],
         [0.7259, 0.6778]],

        [[0.6557, 0.1356],
         [0.5990, 0.7055]],

        [[0.6101, 0.1774],
         [0.7154, 0.6558]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0035158395898187145
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.020872495483242322
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.003418276053557105
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: -0.0019379225681602195
Average Adjusted Rand Index: 0.0021541423337755545
10949.447079662514
[-0.0019379225681602195, -0.0019379225681602195] [0.0021541423337755545, 0.0021541423337755545] [11015.337645705033, 11015.343273933815]
-------------------------------------
This iteration is 5
True Objective function: Loss = -10919.809076492535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21573.823520623115
Iteration 100: Loss = -11027.396479178826
Iteration 200: Loss = -11026.633227875616
Iteration 300: Loss = -11026.447098993549
Iteration 400: Loss = -11026.305875193275
Iteration 500: Loss = -11026.16019732809
Iteration 600: Loss = -11026.021958991394
Iteration 700: Loss = -11025.896279777728
Iteration 800: Loss = -11025.792329547328
Iteration 900: Loss = -11025.714107703536
Iteration 1000: Loss = -11025.6531072312
Iteration 1100: Loss = -11025.601559318795
Iteration 1200: Loss = -11025.556008501459
Iteration 1300: Loss = -11025.515291705155
Iteration 1400: Loss = -11025.478730154142
Iteration 1500: Loss = -11025.445991490442
Iteration 1600: Loss = -11025.416746108363
Iteration 1700: Loss = -11025.390716971464
Iteration 1800: Loss = -11025.367546289028
Iteration 1900: Loss = -11025.346748698532
Iteration 2000: Loss = -11025.327620231687
Iteration 2100: Loss = -11025.309527300435
Iteration 2200: Loss = -11025.29231210416
Iteration 2300: Loss = -11025.27680914239
Iteration 2400: Loss = -11025.26433381327
Iteration 2500: Loss = -11025.254939896005
Iteration 2600: Loss = -11025.247910910432
Iteration 2700: Loss = -11025.242488719492
Iteration 2800: Loss = -11025.23830227968
Iteration 2900: Loss = -11025.23497144225
Iteration 3000: Loss = -11025.232308271521
Iteration 3100: Loss = -11025.230241014226
Iteration 3200: Loss = -11025.228499611647
Iteration 3300: Loss = -11025.227095540442
Iteration 3400: Loss = -11025.225874927379
Iteration 3500: Loss = -11025.224897706828
Iteration 3600: Loss = -11025.224007122122
Iteration 3700: Loss = -11025.223371028225
Iteration 3800: Loss = -11025.222512041759
Iteration 3900: Loss = -11025.221903206424
Iteration 4000: Loss = -11025.22133883723
Iteration 4100: Loss = -11025.220740425668
Iteration 4200: Loss = -11025.220182118552
Iteration 4300: Loss = -11025.21968218512
Iteration 4400: Loss = -11025.219181559225
Iteration 4500: Loss = -11025.219767566665
1
Iteration 4600: Loss = -11025.218240828674
Iteration 4700: Loss = -11025.217820911388
Iteration 4800: Loss = -11025.217395639293
Iteration 4900: Loss = -11025.216999118229
Iteration 5000: Loss = -11025.216646891588
Iteration 5100: Loss = -11025.216365302616
Iteration 5200: Loss = -11025.215900601443
Iteration 5300: Loss = -11025.215530412981
Iteration 5400: Loss = -11025.218627808841
1
Iteration 5500: Loss = -11025.214911561947
Iteration 5600: Loss = -11025.214597006405
Iteration 5700: Loss = -11025.214391002011
Iteration 5800: Loss = -11025.21404361741
Iteration 5900: Loss = -11025.213754952785
Iteration 6000: Loss = -11025.213510037314
Iteration 6100: Loss = -11025.213241357053
Iteration 6200: Loss = -11025.2174634973
1
Iteration 6300: Loss = -11025.212775831858
Iteration 6400: Loss = -11025.212601378953
Iteration 6500: Loss = -11025.213526159869
1
Iteration 6600: Loss = -11025.21219205842
Iteration 6700: Loss = -11025.212018097673
Iteration 6800: Loss = -11025.211816274115
Iteration 6900: Loss = -11025.21162391432
Iteration 7000: Loss = -11025.211496061755
Iteration 7100: Loss = -11025.211331836847
Iteration 7200: Loss = -11025.211260883727
Iteration 7300: Loss = -11025.211050981261
Iteration 7400: Loss = -11025.217129025556
1
Iteration 7500: Loss = -11025.210832303059
Iteration 7600: Loss = -11025.210691318112
Iteration 7700: Loss = -11025.21061446505
Iteration 7800: Loss = -11025.210481110682
Iteration 7900: Loss = -11025.210396321132
Iteration 8000: Loss = -11025.540241189357
1
Iteration 8100: Loss = -11025.210158119706
Iteration 8200: Loss = -11025.210074828481
Iteration 8300: Loss = -11025.210159635071
Iteration 8400: Loss = -11025.209915980993
Iteration 8500: Loss = -11025.209873536362
Iteration 8600: Loss = -11025.209879275506
Iteration 8700: Loss = -11025.209706195572
Iteration 8800: Loss = -11025.233686787999
1
Iteration 8900: Loss = -11025.209614744961
Iteration 9000: Loss = -11025.209572055719
Iteration 9100: Loss = -11025.209499519657
Iteration 9200: Loss = -11025.209539827341
Iteration 9300: Loss = -11025.209374153115
Iteration 9400: Loss = -11025.209458066585
Iteration 9500: Loss = -11025.20928510842
Iteration 9600: Loss = -11025.209271395453
Iteration 9700: Loss = -11025.209193349689
Iteration 9800: Loss = -11025.209311524088
1
Iteration 9900: Loss = -11025.209135908111
Iteration 10000: Loss = -11025.209116684427
Iteration 10100: Loss = -11025.20998744872
1
Iteration 10200: Loss = -11025.209066157287
Iteration 10300: Loss = -11025.210187521625
1
Iteration 10400: Loss = -11025.209077898568
Iteration 10500: Loss = -11025.26123968208
1
Iteration 10600: Loss = -11025.208946360055
Iteration 10700: Loss = -11025.209372434074
1
Iteration 10800: Loss = -11025.209061081423
2
Iteration 10900: Loss = -11025.208882062425
Iteration 11000: Loss = -11025.217736270959
1
Iteration 11100: Loss = -11025.23096919496
2
Iteration 11200: Loss = -11025.212943256245
3
Iteration 11300: Loss = -11025.20879230854
Iteration 11400: Loss = -11025.209629334358
1
Iteration 11500: Loss = -11025.211200467913
2
Iteration 11600: Loss = -11025.208757530932
Iteration 11700: Loss = -11025.209098126934
1
Iteration 11800: Loss = -11025.217316459673
2
Iteration 11900: Loss = -11025.213941357553
3
Iteration 12000: Loss = -11025.208722635422
Iteration 12100: Loss = -11025.208873231382
1
Iteration 12200: Loss = -11025.232832511654
2
Iteration 12300: Loss = -11025.208676605971
Iteration 12400: Loss = -11025.221883873302
1
Iteration 12500: Loss = -11025.22758719422
2
Iteration 12600: Loss = -11025.208650549854
Iteration 12700: Loss = -11025.212464394333
1
Iteration 12800: Loss = -11025.249385321846
2
Iteration 12900: Loss = -11025.208765560008
3
Iteration 13000: Loss = -11025.224445085789
4
Iteration 13100: Loss = -11025.20859543026
Iteration 13200: Loss = -11025.209165707049
1
Iteration 13300: Loss = -11025.208628152863
Iteration 13400: Loss = -11025.208760614207
1
Iteration 13500: Loss = -11025.208737781482
2
Iteration 13600: Loss = -11025.227012679923
3
Iteration 13700: Loss = -11025.212580106527
4
Iteration 13800: Loss = -11025.2101923019
5
Iteration 13900: Loss = -11025.213131499582
6
Iteration 14000: Loss = -11025.228730168994
7
Iteration 14100: Loss = -11025.22655697028
8
Iteration 14200: Loss = -11025.209917610304
9
Iteration 14300: Loss = -11025.20899230075
10
Iteration 14400: Loss = -11025.208980478214
11
Iteration 14500: Loss = -11025.219213410577
12
Iteration 14600: Loss = -11025.333901827262
13
Iteration 14700: Loss = -11025.208502609647
Iteration 14800: Loss = -11025.208622294991
1
Iteration 14900: Loss = -11025.208514576881
Iteration 15000: Loss = -11025.292361627115
1
Iteration 15100: Loss = -11025.208558313734
Iteration 15200: Loss = -11025.208638614236
Iteration 15300: Loss = -11025.2126539825
1
Iteration 15400: Loss = -11025.248537763327
2
Iteration 15500: Loss = -11025.208771350444
3
Iteration 15600: Loss = -11025.250483788604
4
Iteration 15700: Loss = -11025.354996700182
5
Iteration 15800: Loss = -11025.210538903748
6
Iteration 15900: Loss = -11025.311234686338
7
Iteration 16000: Loss = -11025.209773331753
8
Iteration 16100: Loss = -11025.283382066233
9
Iteration 16200: Loss = -11025.230098849961
10
Iteration 16300: Loss = -11025.26647164424
11
Iteration 16400: Loss = -11025.216609829948
12
Iteration 16500: Loss = -11025.21989016756
13
Iteration 16600: Loss = -11025.270529037181
14
Iteration 16700: Loss = -11025.210124843668
15
Stopping early at iteration 16700 due to no improvement.
pi: tensor([[3.9907e-05, 9.9996e-01],
        [9.1926e-02, 9.0807e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0197, 0.9803], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2190, 0.2102],
         [0.5006, 0.1599]],

        [[0.5225, 0.1901],
         [0.5225, 0.6398]],

        [[0.6341, 0.1824],
         [0.5885, 0.5815]],

        [[0.7157, 0.1539],
         [0.6019, 0.7044]],

        [[0.6048, 0.1988],
         [0.5122, 0.6739]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0004528243715857524
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22232.31596464959
Iteration 100: Loss = -11027.470724612665
Iteration 200: Loss = -11026.836656081467
Iteration 300: Loss = -11026.66601854374
Iteration 400: Loss = -11026.584148967842
Iteration 500: Loss = -11026.531729989843
Iteration 600: Loss = -11026.486182512583
Iteration 700: Loss = -11026.439345834948
Iteration 800: Loss = -11026.386205704457
Iteration 900: Loss = -11026.324330579868
Iteration 1000: Loss = -11026.257807761667
Iteration 1100: Loss = -11026.180865791734
Iteration 1200: Loss = -11026.081129193262
Iteration 1300: Loss = -11025.946303323444
Iteration 1400: Loss = -11025.77101524261
Iteration 1500: Loss = -11025.595713485302
Iteration 1600: Loss = -11025.499539592805
Iteration 1700: Loss = -11025.44451166836
Iteration 1800: Loss = -11025.402187362797
Iteration 1900: Loss = -11025.366935323234
Iteration 2000: Loss = -11025.337304325576
Iteration 2100: Loss = -11025.31160079313
Iteration 2200: Loss = -11025.284370650788
Iteration 2300: Loss = -11025.259427361876
Iteration 2400: Loss = -11025.245125654616
Iteration 2500: Loss = -11025.23589042394
Iteration 2600: Loss = -11025.230070868407
Iteration 2700: Loss = -11025.226284125269
Iteration 2800: Loss = -11025.22371345768
Iteration 2900: Loss = -11025.22199992296
Iteration 3000: Loss = -11025.220738494078
Iteration 3100: Loss = -11025.219784188354
Iteration 3200: Loss = -11025.21904803995
Iteration 3300: Loss = -11025.221204488806
1
Iteration 3400: Loss = -11025.217929841947
Iteration 3500: Loss = -11025.217466672806
Iteration 3600: Loss = -11025.217061154419
Iteration 3700: Loss = -11025.21664470948
Iteration 3800: Loss = -11025.225112450453
1
Iteration 3900: Loss = -11025.215937354767
Iteration 4000: Loss = -11025.21561933297
Iteration 4100: Loss = -11025.215337396528
Iteration 4200: Loss = -11025.214974410193
Iteration 4300: Loss = -11025.217730049519
1
Iteration 4400: Loss = -11025.214387980188
Iteration 4500: Loss = -11025.214128768008
Iteration 4600: Loss = -11025.21425838903
1
Iteration 4700: Loss = -11025.213631307297
Iteration 4800: Loss = -11025.213413868083
Iteration 4900: Loss = -11025.215520906095
1
Iteration 5000: Loss = -11025.212954893794
Iteration 5100: Loss = -11025.212709625575
Iteration 5200: Loss = -11025.212713467194
Iteration 5300: Loss = -11025.21232968353
Iteration 5400: Loss = -11025.212285235553
Iteration 5500: Loss = -11025.211969103719
Iteration 5600: Loss = -11025.211777192668
Iteration 5700: Loss = -11025.21177016877
Iteration 5800: Loss = -11025.21145593453
Iteration 5900: Loss = -11025.215578721836
1
Iteration 6000: Loss = -11025.211191999146
Iteration 6100: Loss = -11025.211019377542
Iteration 6200: Loss = -11025.211143272678
1
Iteration 6300: Loss = -11025.210777286473
Iteration 6400: Loss = -11025.213711452507
1
Iteration 6500: Loss = -11025.21054256712
Iteration 6600: Loss = -11025.210474700352
Iteration 6700: Loss = -11025.210356114889
Iteration 6800: Loss = -11025.210282441189
Iteration 6900: Loss = -11025.2101778197
Iteration 7000: Loss = -11025.210093013919
Iteration 7100: Loss = -11025.210039599457
Iteration 7200: Loss = -11025.209950552517
Iteration 7300: Loss = -11025.209985913109
Iteration 7400: Loss = -11025.209782845248
Iteration 7500: Loss = -11025.222770638604
1
Iteration 7600: Loss = -11025.20964416368
Iteration 7700: Loss = -11025.220981933993
1
Iteration 7800: Loss = -11025.209524511214
Iteration 7900: Loss = -11025.211535189414
1
Iteration 8000: Loss = -11025.209419164441
Iteration 8100: Loss = -11025.214659073197
1
Iteration 8200: Loss = -11025.209352150516
Iteration 8300: Loss = -11025.20930746974
Iteration 8400: Loss = -11025.209458257761
1
Iteration 8500: Loss = -11025.209231198463
Iteration 8600: Loss = -11025.209173863386
Iteration 8700: Loss = -11025.209657617901
1
Iteration 8800: Loss = -11025.209104512409
Iteration 8900: Loss = -11025.20998190872
1
Iteration 9000: Loss = -11025.209050154863
Iteration 9100: Loss = -11025.209011618168
Iteration 9200: Loss = -11025.209083911115
Iteration 9300: Loss = -11025.209002890891
Iteration 9400: Loss = -11025.208901323032
Iteration 9500: Loss = -11025.210709071764
1
Iteration 9600: Loss = -11025.213394007085
2
Iteration 9700: Loss = -11025.2624669001
3
Iteration 9800: Loss = -11025.208879078977
Iteration 9900: Loss = -11025.210023497935
1
Iteration 10000: Loss = -11025.208804801763
Iteration 10100: Loss = -11025.208962645343
1
Iteration 10200: Loss = -11025.212826259027
2
Iteration 10300: Loss = -11025.208753529303
Iteration 10400: Loss = -11025.226589056201
1
Iteration 10500: Loss = -11025.20877783282
Iteration 10600: Loss = -11025.309194343037
1
Iteration 10700: Loss = -11025.208745335505
Iteration 10800: Loss = -11025.208862179787
1
Iteration 10900: Loss = -11025.254751135379
2
Iteration 11000: Loss = -11025.208639642025
Iteration 11100: Loss = -11025.208965395996
1
Iteration 11200: Loss = -11025.21699285752
2
Iteration 11300: Loss = -11025.246144211102
3
Iteration 11400: Loss = -11025.208665590122
Iteration 11500: Loss = -11025.214808007166
1
Iteration 11600: Loss = -11025.209519140686
2
Iteration 11700: Loss = -11025.20876150871
Iteration 11800: Loss = -11025.208772661019
Iteration 11900: Loss = -11025.212630967799
1
Iteration 12000: Loss = -11025.20866245654
Iteration 12100: Loss = -11025.20975827413
1
Iteration 12200: Loss = -11025.210137537439
2
Iteration 12300: Loss = -11025.208733593478
Iteration 12400: Loss = -11025.208730718536
Iteration 12500: Loss = -11025.208838188339
1
Iteration 12600: Loss = -11025.215165443025
2
Iteration 12700: Loss = -11025.209891894163
3
Iteration 12800: Loss = -11025.226218893795
4
Iteration 12900: Loss = -11025.208520214073
Iteration 13000: Loss = -11025.269583143327
1
Iteration 13100: Loss = -11025.20853987845
Iteration 13200: Loss = -11025.209141783227
1
Iteration 13300: Loss = -11025.210699180267
2
Iteration 13400: Loss = -11025.265009651275
3
Iteration 13500: Loss = -11025.208541737627
Iteration 13600: Loss = -11025.208787808251
1
Iteration 13700: Loss = -11025.20894770496
2
Iteration 13800: Loss = -11025.208526436007
Iteration 13900: Loss = -11025.240390740173
1
Iteration 14000: Loss = -11025.208497793878
Iteration 14100: Loss = -11025.210373477097
1
Iteration 14200: Loss = -11025.208507361769
Iteration 14300: Loss = -11025.211298963142
1
Iteration 14400: Loss = -11025.249126278664
2
Iteration 14500: Loss = -11025.211036295364
3
Iteration 14600: Loss = -11025.32968883858
4
Iteration 14700: Loss = -11025.372102008563
5
Iteration 14800: Loss = -11025.216747449953
6
Iteration 14900: Loss = -11025.208540118678
Iteration 15000: Loss = -11025.209009557226
1
Iteration 15100: Loss = -11025.22211296535
2
Iteration 15200: Loss = -11025.221322838555
3
Iteration 15300: Loss = -11025.212900623856
4
Iteration 15400: Loss = -11025.208756762131
5
Iteration 15500: Loss = -11025.209063028293
6
Iteration 15600: Loss = -11025.208696357226
7
Iteration 15700: Loss = -11025.210700659078
8
Iteration 15800: Loss = -11025.213440495158
9
Iteration 15900: Loss = -11025.20854260624
Iteration 16000: Loss = -11025.212062002973
1
Iteration 16100: Loss = -11025.35870332506
2
Iteration 16200: Loss = -11025.235989091107
3
Iteration 16300: Loss = -11025.209786410856
4
Iteration 16400: Loss = -11025.214641527726
5
Iteration 16500: Loss = -11025.215763535778
6
Iteration 16600: Loss = -11025.208564120236
Iteration 16700: Loss = -11025.20852827378
Iteration 16800: Loss = -11025.303469602439
1
Iteration 16900: Loss = -11025.22323982543
2
Iteration 17000: Loss = -11025.224088109933
3
Iteration 17100: Loss = -11025.221101185012
4
Iteration 17200: Loss = -11025.20989682921
5
Iteration 17300: Loss = -11025.213656033206
6
Iteration 17400: Loss = -11025.285487883593
7
Iteration 17500: Loss = -11025.211971309383
8
Iteration 17600: Loss = -11025.208844686678
9
Iteration 17700: Loss = -11025.21506676908
10
Iteration 17800: Loss = -11025.227898286228
11
Iteration 17900: Loss = -11025.21073634884
12
Iteration 18000: Loss = -11025.20895668811
13
Iteration 18100: Loss = -11025.208758742896
14
Iteration 18200: Loss = -11025.215112214162
15
Stopping early at iteration 18200 due to no improvement.
pi: tensor([[9.1121e-01, 8.8795e-02],
        [9.9999e-01, 1.0694e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9812, 0.0188], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1603, 0.2107],
         [0.5228, 0.2194]],

        [[0.6595, 0.1904],
         [0.5148, 0.6061]],

        [[0.6763, 0.1824],
         [0.7228, 0.5331]],

        [[0.6465, 0.1530],
         [0.5457, 0.6221]],

        [[0.6609, 0.1996],
         [0.5874, 0.6703]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0004528243715857524
Average Adjusted Rand Index: -0.0008569898232458489
10919.809076492535
[-0.0004528243715857524, -0.0004528243715857524] [-0.0008569898232458489, -0.0008569898232458489] [11025.210124843668, 11025.215112214162]
-------------------------------------
This iteration is 6
True Objective function: Loss = -10880.97307742104
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20177.365787662122
Iteration 100: Loss = -10868.964072465638
Iteration 200: Loss = -10868.629618613617
Iteration 300: Loss = -10868.562658634126
Iteration 400: Loss = -10868.528137673808
Iteration 500: Loss = -10868.505066578955
Iteration 600: Loss = -10868.486653739646
Iteration 700: Loss = -10868.47036317757
Iteration 800: Loss = -10868.454964196215
Iteration 900: Loss = -10868.439120213054
Iteration 1000: Loss = -10868.419792816845
Iteration 1100: Loss = -10868.386337899614
Iteration 1200: Loss = -10868.26235738738
Iteration 1300: Loss = -10867.030846839241
Iteration 1400: Loss = -10866.301059615234
Iteration 1500: Loss = -10865.997025706776
Iteration 1600: Loss = -10865.71400744423
Iteration 1700: Loss = -10865.616466794607
Iteration 1800: Loss = -10865.547830046266
Iteration 1900: Loss = -10865.435769932465
Iteration 2000: Loss = -10865.050231417683
Iteration 2100: Loss = -10864.659794758592
Iteration 2200: Loss = -10864.626848085278
Iteration 2300: Loss = -10864.610268630688
Iteration 2400: Loss = -10864.599699684219
Iteration 2500: Loss = -10864.592295370372
Iteration 2600: Loss = -10864.586867320353
Iteration 2700: Loss = -10864.582697230633
Iteration 2800: Loss = -10864.579356720666
Iteration 2900: Loss = -10864.576646276057
Iteration 3000: Loss = -10864.574461326802
Iteration 3100: Loss = -10864.572620689187
Iteration 3200: Loss = -10864.571028287512
Iteration 3300: Loss = -10864.570164358003
Iteration 3400: Loss = -10864.568480550553
Iteration 3500: Loss = -10864.567965118846
Iteration 3600: Loss = -10864.566550995594
Iteration 3700: Loss = -10864.567091126466
1
Iteration 3800: Loss = -10864.565093031219
Iteration 3900: Loss = -10864.564455909027
Iteration 4000: Loss = -10864.563873171797
Iteration 4100: Loss = -10864.563349298896
Iteration 4200: Loss = -10864.562902018552
Iteration 4300: Loss = -10864.562463604583
Iteration 4400: Loss = -10864.56209471626
Iteration 4500: Loss = -10864.561725806609
Iteration 4600: Loss = -10864.571108512055
1
Iteration 4700: Loss = -10864.561117925727
Iteration 4800: Loss = -10864.57719349855
1
Iteration 4900: Loss = -10864.560573730922
Iteration 5000: Loss = -10864.56035565945
Iteration 5100: Loss = -10864.560132518533
Iteration 5200: Loss = -10864.559930910444
Iteration 5300: Loss = -10864.560218551958
1
Iteration 5400: Loss = -10864.55957300103
Iteration 5500: Loss = -10864.56013242453
1
Iteration 5600: Loss = -10864.55924913372
Iteration 5700: Loss = -10864.560868261226
1
Iteration 5800: Loss = -10864.559008642987
Iteration 5900: Loss = -10864.563005138241
1
Iteration 6000: Loss = -10864.558741739971
Iteration 6100: Loss = -10864.571425300901
1
Iteration 6200: Loss = -10864.558538483137
Iteration 6300: Loss = -10864.558573376586
Iteration 6400: Loss = -10864.558332442024
Iteration 6500: Loss = -10864.558283888173
Iteration 6600: Loss = -10864.558176741131
Iteration 6700: Loss = -10864.558353646536
1
Iteration 6800: Loss = -10864.558065458135
Iteration 6900: Loss = -10864.557994994952
Iteration 7000: Loss = -10864.557934572775
Iteration 7100: Loss = -10864.557888333622
Iteration 7200: Loss = -10864.560645283722
1
Iteration 7300: Loss = -10864.55778783091
Iteration 7400: Loss = -10864.55805629237
1
Iteration 7500: Loss = -10864.557658491283
Iteration 7600: Loss = -10864.55766126864
Iteration 7700: Loss = -10864.55866648936
1
Iteration 7800: Loss = -10864.567533060517
2
Iteration 7900: Loss = -10864.55752358882
Iteration 8000: Loss = -10864.557466512719
Iteration 8100: Loss = -10864.558713542912
1
Iteration 8200: Loss = -10864.55828540015
2
Iteration 8300: Loss = -10864.573585158214
3
Iteration 8400: Loss = -10864.557343109362
Iteration 8500: Loss = -10864.558233383243
1
Iteration 8600: Loss = -10864.557326552049
Iteration 8700: Loss = -10864.562005973818
1
Iteration 8800: Loss = -10864.557832911392
2
Iteration 8900: Loss = -10864.557813159703
3
Iteration 9000: Loss = -10864.55720431374
Iteration 9100: Loss = -10864.557256402219
Iteration 9200: Loss = -10864.557207963273
Iteration 9300: Loss = -10864.566320709198
1
Iteration 9400: Loss = -10864.566462690438
2
Iteration 9500: Loss = -10864.55716461612
Iteration 9600: Loss = -10864.557145668881
Iteration 9700: Loss = -10864.55732914542
1
Iteration 9800: Loss = -10864.557369387468
2
Iteration 9900: Loss = -10864.557358434322
3
Iteration 10000: Loss = -10864.557030919268
Iteration 10100: Loss = -10864.56235524951
1
Iteration 10200: Loss = -10864.557008703632
Iteration 10300: Loss = -10864.558412173317
1
Iteration 10400: Loss = -10864.557043776167
Iteration 10500: Loss = -10864.612150551377
1
Iteration 10600: Loss = -10864.556976828864
Iteration 10700: Loss = -10864.557526928496
1
Iteration 10800: Loss = -10864.64735850237
2
Iteration 10900: Loss = -10864.651648763143
3
Iteration 11000: Loss = -10864.562001895723
4
Iteration 11100: Loss = -10864.568106499548
5
Iteration 11200: Loss = -10864.558591902422
6
Iteration 11300: Loss = -10864.556961038006
Iteration 11400: Loss = -10864.557899388456
1
Iteration 11500: Loss = -10864.556936811301
Iteration 11600: Loss = -10864.556992826532
Iteration 11700: Loss = -10864.556951742894
Iteration 11800: Loss = -10864.587077539942
1
Iteration 11900: Loss = -10864.571149002872
2
Iteration 12000: Loss = -10864.586529203361
3
Iteration 12100: Loss = -10864.593698738483
4
Iteration 12200: Loss = -10864.570746913982
5
Iteration 12300: Loss = -10864.557057191745
6
Iteration 12400: Loss = -10864.559849988029
7
Iteration 12500: Loss = -10864.557421090884
8
Iteration 12600: Loss = -10864.557808542102
9
Iteration 12700: Loss = -10864.561821326548
10
Iteration 12800: Loss = -10864.557488755414
11
Iteration 12900: Loss = -10864.558181721672
12
Iteration 13000: Loss = -10864.55690931835
Iteration 13100: Loss = -10864.556954442789
Iteration 13200: Loss = -10864.557583672564
1
Iteration 13300: Loss = -10864.556881582936
Iteration 13400: Loss = -10864.568403158466
1
Iteration 13500: Loss = -10864.556896825621
Iteration 13600: Loss = -10864.556912928216
Iteration 13700: Loss = -10864.558921018981
1
Iteration 13800: Loss = -10864.557195135525
2
Iteration 13900: Loss = -10864.557166140048
3
Iteration 14000: Loss = -10864.567660647579
4
Iteration 14100: Loss = -10864.556950265083
Iteration 14200: Loss = -10864.557013515116
Iteration 14300: Loss = -10864.57139094924
1
Iteration 14400: Loss = -10864.556878823496
Iteration 14500: Loss = -10864.557669449534
1
Iteration 14600: Loss = -10864.556870903001
Iteration 14700: Loss = -10864.556963581588
Iteration 14800: Loss = -10864.556856973502
Iteration 14900: Loss = -10864.556948885765
Iteration 15000: Loss = -10864.556889736046
Iteration 15100: Loss = -10864.55684991266
Iteration 15200: Loss = -10864.558280046982
1
Iteration 15300: Loss = -10864.556888554489
Iteration 15400: Loss = -10864.832314452731
1
Iteration 15500: Loss = -10864.556881330394
Iteration 15600: Loss = -10864.55685608024
Iteration 15700: Loss = -10864.558719759041
1
Iteration 15800: Loss = -10864.556879855138
Iteration 15900: Loss = -10864.564394931727
1
Iteration 16000: Loss = -10864.557602286452
2
Iteration 16100: Loss = -10864.560427429333
3
Iteration 16200: Loss = -10864.557712458885
4
Iteration 16300: Loss = -10864.56198625162
5
Iteration 16400: Loss = -10864.556880343305
Iteration 16500: Loss = -10864.557914439103
1
Iteration 16600: Loss = -10864.565545499034
2
Iteration 16700: Loss = -10864.564856268868
3
Iteration 16800: Loss = -10864.564141044779
4
Iteration 16900: Loss = -10864.57471160567
5
Iteration 17000: Loss = -10864.556912907306
Iteration 17100: Loss = -10864.559216309623
1
Iteration 17200: Loss = -10864.563440457852
2
Iteration 17300: Loss = -10864.563834042288
3
Iteration 17400: Loss = -10864.560352609267
4
Iteration 17500: Loss = -10864.557990230192
5
Iteration 17600: Loss = -10864.556991975447
Iteration 17700: Loss = -10864.5733395103
1
Iteration 17800: Loss = -10864.55695198374
Iteration 17900: Loss = -10864.557058251965
1
Iteration 18000: Loss = -10864.556860639474
Iteration 18100: Loss = -10864.556843318578
Iteration 18200: Loss = -10864.55700543399
1
Iteration 18300: Loss = -10864.556824598241
Iteration 18400: Loss = -10864.564530144653
1
Iteration 18500: Loss = -10864.557494583727
2
Iteration 18600: Loss = -10864.558394032474
3
Iteration 18700: Loss = -10864.556823069712
Iteration 18800: Loss = -10864.557421414762
1
Iteration 18900: Loss = -10864.571481442325
2
Iteration 19000: Loss = -10864.556808655028
Iteration 19100: Loss = -10864.573353757349
1
Iteration 19200: Loss = -10864.558615299198
2
Iteration 19300: Loss = -10864.556893077173
Iteration 19400: Loss = -10864.567770519245
1
Iteration 19500: Loss = -10864.556854441336
Iteration 19600: Loss = -10864.564509569407
1
Iteration 19700: Loss = -10864.556804173013
Iteration 19800: Loss = -10864.58878745296
1
Iteration 19900: Loss = -10864.55680938388
pi: tensor([[4.6360e-01, 5.3640e-01],
        [1.7401e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2177, 0.7823], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1451, 0.1504],
         [0.6018, 0.1604]],

        [[0.6595, 0.1807],
         [0.6286, 0.5331]],

        [[0.6353, 0.1556],
         [0.6940, 0.5002]],

        [[0.5010, 0.0620],
         [0.6018, 0.6665]],

        [[0.7224, 0.2453],
         [0.7045, 0.6856]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.01126387000386094
Global Adjusted Rand Index: -0.0001651605939884664
Average Adjusted Rand Index: 0.001419847625925239
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19601.144208477996
Iteration 100: Loss = -10869.513368493863
Iteration 200: Loss = -10868.810188889383
Iteration 300: Loss = -10868.642564858537
Iteration 400: Loss = -10868.569865141193
Iteration 500: Loss = -10868.519895996717
Iteration 600: Loss = -10868.47614706497
Iteration 700: Loss = -10868.432062919208
Iteration 800: Loss = -10868.382813868157
Iteration 900: Loss = -10868.32351405428
Iteration 1000: Loss = -10868.250598758355
Iteration 1100: Loss = -10868.167914167536
Iteration 1200: Loss = -10868.087804018154
Iteration 1300: Loss = -10868.013889933029
Iteration 1400: Loss = -10867.935829158245
Iteration 1500: Loss = -10867.835283217715
Iteration 1600: Loss = -10867.592161247587
Iteration 1700: Loss = -10866.735792809188
Iteration 1800: Loss = -10866.318137235015
Iteration 1900: Loss = -10866.160292173303
Iteration 2000: Loss = -10866.065377667486
Iteration 2100: Loss = -10865.994239462725
Iteration 2200: Loss = -10865.92481855656
Iteration 2300: Loss = -10865.83838556071
Iteration 2400: Loss = -10865.750940691914
Iteration 2500: Loss = -10865.68321897558
Iteration 2600: Loss = -10865.641425734859
Iteration 2700: Loss = -10865.616823964872
Iteration 2800: Loss = -10865.602786898273
Iteration 2900: Loss = -10865.594763744873
Iteration 3000: Loss = -10865.589991688557
Iteration 3100: Loss = -10865.587015822963
Iteration 3200: Loss = -10865.584999897888
Iteration 3300: Loss = -10865.583521998675
Iteration 3400: Loss = -10865.582403759146
Iteration 3500: Loss = -10865.58145363007
Iteration 3600: Loss = -10865.580609751765
Iteration 3700: Loss = -10865.57989605307
Iteration 3800: Loss = -10865.579301418493
Iteration 3900: Loss = -10865.578634437834
Iteration 4000: Loss = -10865.57807818062
Iteration 4100: Loss = -10865.577558727506
Iteration 4200: Loss = -10865.577015332146
Iteration 4300: Loss = -10865.576568909633
Iteration 4400: Loss = -10865.576110751977
Iteration 4500: Loss = -10865.57574874432
Iteration 4600: Loss = -10865.576572071583
1
Iteration 4700: Loss = -10865.574953170688
Iteration 4800: Loss = -10865.574550741196
Iteration 4900: Loss = -10865.574589328375
Iteration 5000: Loss = -10865.573936285373
Iteration 5100: Loss = -10865.573660702556
Iteration 5200: Loss = -10865.57344651432
Iteration 5300: Loss = -10865.573208591268
Iteration 5400: Loss = -10865.572868942145
Iteration 5500: Loss = -10865.57265206831
Iteration 5600: Loss = -10865.572634760229
Iteration 5700: Loss = -10865.574747882181
1
Iteration 5800: Loss = -10865.571995112537
Iteration 5900: Loss = -10865.571856926148
Iteration 6000: Loss = -10865.571663226609
Iteration 6100: Loss = -10865.57239693725
1
Iteration 6200: Loss = -10865.571281420936
Iteration 6300: Loss = -10865.574700177513
1
Iteration 6400: Loss = -10865.570974736847
Iteration 6500: Loss = -10865.570960957777
Iteration 6600: Loss = -10865.570745854038
Iteration 6700: Loss = -10865.574372384892
1
Iteration 6800: Loss = -10865.570487743209
Iteration 6900: Loss = -10865.572228470968
1
Iteration 7000: Loss = -10865.570357870552
Iteration 7100: Loss = -10865.570348210169
Iteration 7200: Loss = -10865.570127491585
Iteration 7300: Loss = -10865.569999550511
Iteration 7400: Loss = -10865.56999191398
Iteration 7500: Loss = -10865.572283774365
1
Iteration 7600: Loss = -10865.579099220391
2
Iteration 7700: Loss = -10865.569678136846
Iteration 7800: Loss = -10865.570532189298
1
Iteration 7900: Loss = -10865.569528003653
Iteration 8000: Loss = -10865.569430369975
Iteration 8100: Loss = -10865.56942630985
Iteration 8200: Loss = -10865.569453407299
Iteration 8300: Loss = -10865.569300373954
Iteration 8400: Loss = -10865.61855886074
1
Iteration 8500: Loss = -10865.569212421653
Iteration 8600: Loss = -10865.569146111797
Iteration 8700: Loss = -10865.57138885417
1
Iteration 8800: Loss = -10865.569078031
Iteration 8900: Loss = -10865.569024111219
Iteration 9000: Loss = -10865.569344768273
1
Iteration 9100: Loss = -10865.568968234402
Iteration 9200: Loss = -10865.56948666347
1
Iteration 9300: Loss = -10865.589799566986
2
Iteration 9400: Loss = -10865.574695105708
3
Iteration 9500: Loss = -10865.569685887926
4
Iteration 9600: Loss = -10865.569123461057
5
Iteration 9700: Loss = -10865.652489023509
6
Iteration 9800: Loss = -10865.568753311432
Iteration 9900: Loss = -10865.605788161189
1
Iteration 10000: Loss = -10865.56872812873
Iteration 10100: Loss = -10865.568659708906
Iteration 10200: Loss = -10865.56876961282
1
Iteration 10300: Loss = -10865.5686522345
Iteration 10400: Loss = -10865.581252844997
1
Iteration 10500: Loss = -10865.669494231393
2
Iteration 10600: Loss = -10865.568622099829
Iteration 10700: Loss = -10865.570289929034
1
Iteration 10800: Loss = -10865.568545515238
Iteration 10900: Loss = -10865.568704053132
1
Iteration 11000: Loss = -10865.568538510828
Iteration 11100: Loss = -10865.568507859953
Iteration 11200: Loss = -10865.568712197548
1
Iteration 11300: Loss = -10865.568586644335
Iteration 11400: Loss = -10865.568469202268
Iteration 11500: Loss = -10865.568617164234
1
Iteration 11600: Loss = -10865.568448790009
Iteration 11700: Loss = -10865.568495611189
Iteration 11800: Loss = -10865.568432458884
Iteration 11900: Loss = -10865.569574482215
1
Iteration 12000: Loss = -10865.568413786928
Iteration 12100: Loss = -10865.56845562632
Iteration 12200: Loss = -10865.568448711998
Iteration 12300: Loss = -10865.568466396047
Iteration 12400: Loss = -10865.570507769122
1
Iteration 12500: Loss = -10865.568372811584
Iteration 12600: Loss = -10865.571721749802
1
Iteration 12700: Loss = -10865.568392290454
Iteration 12800: Loss = -10865.569083008224
1
Iteration 12900: Loss = -10865.568359561361
Iteration 13000: Loss = -10865.570232441045
1
Iteration 13100: Loss = -10865.568536083954
2
Iteration 13200: Loss = -10865.568343229299
Iteration 13300: Loss = -10865.572418544858
1
Iteration 13400: Loss = -10865.57027432413
2
Iteration 13500: Loss = -10865.570941150341
3
Iteration 13600: Loss = -10865.56838384538
Iteration 13700: Loss = -10865.568370315332
Iteration 13800: Loss = -10865.56857521895
1
Iteration 13900: Loss = -10865.57511052664
2
Iteration 14000: Loss = -10865.573120287538
3
Iteration 14100: Loss = -10865.572525551723
4
Iteration 14200: Loss = -10865.568435141011
Iteration 14300: Loss = -10865.571250485415
1
Iteration 14400: Loss = -10865.56862591142
2
Iteration 14500: Loss = -10865.568336720626
Iteration 14600: Loss = -10865.661386152226
1
Iteration 14700: Loss = -10865.568298736867
Iteration 14800: Loss = -10865.570087465807
1
Iteration 14900: Loss = -10865.568307353185
Iteration 15000: Loss = -10865.569354615898
1
Iteration 15100: Loss = -10865.737701455999
2
Iteration 15200: Loss = -10865.573195011215
3
Iteration 15300: Loss = -10865.568835690194
4
Iteration 15400: Loss = -10865.602238371
5
Iteration 15500: Loss = -10865.569566443059
6
Iteration 15600: Loss = -10865.6451910079
7
Iteration 15700: Loss = -10865.625882438837
8
Iteration 15800: Loss = -10865.56834366198
Iteration 15900: Loss = -10865.568502669554
1
Iteration 16000: Loss = -10865.573149939984
2
Iteration 16100: Loss = -10865.56831785189
Iteration 16200: Loss = -10865.574910047379
1
Iteration 16300: Loss = -10865.568721884772
2
Iteration 16400: Loss = -10865.58138555832
3
Iteration 16500: Loss = -10865.568303719288
Iteration 16600: Loss = -10865.56831050696
Iteration 16700: Loss = -10865.590116979922
1
Iteration 16800: Loss = -10865.568870444942
2
Iteration 16900: Loss = -10865.577142585136
3
Iteration 17000: Loss = -10865.568395132557
Iteration 17100: Loss = -10865.568323182268
Iteration 17200: Loss = -10865.569313378266
1
Iteration 17300: Loss = -10865.603318714477
2
Iteration 17400: Loss = -10865.568355135363
Iteration 17500: Loss = -10865.56826612749
Iteration 17600: Loss = -10865.573665809987
1
Iteration 17700: Loss = -10865.569506828851
2
Iteration 17800: Loss = -10865.568276059697
Iteration 17900: Loss = -10865.572852487381
1
Iteration 18000: Loss = -10865.572354725797
2
Iteration 18100: Loss = -10865.570019379946
3
Iteration 18200: Loss = -10865.577545252723
4
Iteration 18300: Loss = -10865.6127608618
5
Iteration 18400: Loss = -10865.57168223911
6
Iteration 18500: Loss = -10865.568277191842
Iteration 18600: Loss = -10865.56936119546
1
Iteration 18700: Loss = -10865.569330343806
2
Iteration 18800: Loss = -10865.60744677219
3
Iteration 18900: Loss = -10865.568274452722
Iteration 19000: Loss = -10865.573307304348
1
Iteration 19100: Loss = -10865.568269670983
Iteration 19200: Loss = -10865.57040689715
1
Iteration 19300: Loss = -10865.597547591864
2
Iteration 19400: Loss = -10865.568634253537
3
Iteration 19500: Loss = -10865.568485367021
4
Iteration 19600: Loss = -10865.571598437582
5
Iteration 19700: Loss = -10865.568442727063
6
Iteration 19800: Loss = -10865.568448428805
7
Iteration 19900: Loss = -10865.568895224389
8
pi: tensor([[2.0023e-06, 1.0000e+00],
        [3.7578e-02, 9.6242e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2344, 0.7656], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1340, 0.1481],
         [0.6243, 0.1632]],

        [[0.5769, 0.1825],
         [0.6305, 0.6316]],

        [[0.5624, 0.1599],
         [0.5343, 0.5323]],

        [[0.6070, 0.0747],
         [0.6839, 0.5939]],

        [[0.5732, 0.1124],
         [0.7273, 0.6863]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006241292691481576
Average Adjusted Rand Index: -0.0007874654537391088
10880.97307742104
[-0.0001651605939884664, -0.0006241292691481576] [0.001419847625925239, -0.0007874654537391088] [10864.562005417065, 10865.706590902808]
-------------------------------------
This iteration is 7
True Objective function: Loss = -10777.188434266836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20864.353541271255
Iteration 100: Loss = -10891.494278506885
Iteration 200: Loss = -10890.741852583458
Iteration 300: Loss = -10890.414929093804
Iteration 400: Loss = -10890.26150532365
Iteration 500: Loss = -10890.18245174858
Iteration 600: Loss = -10890.12752947571
Iteration 700: Loss = -10889.995519015813
Iteration 800: Loss = -10887.348363107565
Iteration 900: Loss = -10886.976352734919
Iteration 1000: Loss = -10886.86454014335
Iteration 1100: Loss = -10886.799548053003
Iteration 1200: Loss = -10886.753789490349
Iteration 1300: Loss = -10886.717782360063
Iteration 1400: Loss = -10886.686929724448
Iteration 1500: Loss = -10886.6585857429
Iteration 1600: Loss = -10886.630862543218
Iteration 1700: Loss = -10886.602017809993
Iteration 1800: Loss = -10886.570788492467
Iteration 1900: Loss = -10886.536366597582
Iteration 2000: Loss = -10886.498921446078
Iteration 2100: Loss = -10886.462612232302
Iteration 2200: Loss = -10886.433500933217
Iteration 2300: Loss = -10886.413269279346
Iteration 2400: Loss = -10886.399766957897
Iteration 2500: Loss = -10886.39040832677
Iteration 2600: Loss = -10886.383502802455
Iteration 2700: Loss = -10886.3781205791
Iteration 2800: Loss = -10886.373752576717
Iteration 2900: Loss = -10886.370184943413
Iteration 3000: Loss = -10886.367184108409
Iteration 3100: Loss = -10886.364686637968
Iteration 3200: Loss = -10886.362630348649
Iteration 3300: Loss = -10886.360877786949
Iteration 3400: Loss = -10886.359429066602
Iteration 3500: Loss = -10886.358164049529
Iteration 3600: Loss = -10886.3570922133
Iteration 3700: Loss = -10886.356223871644
Iteration 3800: Loss = -10886.355361981667
Iteration 3900: Loss = -10886.354706473161
Iteration 4000: Loss = -10886.354083371582
Iteration 4100: Loss = -10886.353502027916
Iteration 4200: Loss = -10886.35297759178
Iteration 4300: Loss = -10886.352461935921
Iteration 4400: Loss = -10886.352039908244
Iteration 4500: Loss = -10886.351586777198
Iteration 4600: Loss = -10886.351171230855
Iteration 4700: Loss = -10886.350774989065
Iteration 4800: Loss = -10886.350433617583
Iteration 4900: Loss = -10886.350105815527
Iteration 5000: Loss = -10886.349839516999
Iteration 5100: Loss = -10886.349611823787
Iteration 5200: Loss = -10886.349321573041
Iteration 5300: Loss = -10886.34910765172
Iteration 5400: Loss = -10886.34890141041
Iteration 5500: Loss = -10886.348699605503
Iteration 5600: Loss = -10886.34852733191
Iteration 5700: Loss = -10886.348306385516
Iteration 5800: Loss = -10886.348187565796
Iteration 5900: Loss = -10886.348006264245
Iteration 6000: Loss = -10886.347880126441
Iteration 6100: Loss = -10886.34768511587
Iteration 6200: Loss = -10886.347616422396
Iteration 6300: Loss = -10886.347467572603
Iteration 6400: Loss = -10886.347341601273
Iteration 6500: Loss = -10886.347250078541
Iteration 6600: Loss = -10886.347128433901
Iteration 6700: Loss = -10886.347027132799
Iteration 6800: Loss = -10886.346899331584
Iteration 6900: Loss = -10886.346835664075
Iteration 7000: Loss = -10886.346686056895
Iteration 7100: Loss = -10886.346591529751
Iteration 7200: Loss = -10886.3464672048
Iteration 7300: Loss = -10886.346364626492
Iteration 7400: Loss = -10886.346504602598
1
Iteration 7500: Loss = -10886.346370186331
Iteration 7600: Loss = -10886.351992738137
1
Iteration 7700: Loss = -10886.346029935214
Iteration 7800: Loss = -10886.382060810873
1
Iteration 7900: Loss = -10886.345913874064
Iteration 8000: Loss = -10886.345820010565
Iteration 8100: Loss = -10886.34580361126
Iteration 8200: Loss = -10886.34570295167
Iteration 8300: Loss = -10886.444223871054
1
Iteration 8400: Loss = -10886.34561338256
Iteration 8500: Loss = -10886.345488159883
Iteration 8600: Loss = -10886.413660046466
1
Iteration 8700: Loss = -10886.34542636592
Iteration 8800: Loss = -10886.345393429472
Iteration 8900: Loss = -10886.345343273302
Iteration 9000: Loss = -10886.345601020985
1
Iteration 9100: Loss = -10886.345243511601
Iteration 9200: Loss = -10886.345172284622
Iteration 9300: Loss = -10886.362041692968
1
Iteration 9400: Loss = -10886.345119098585
Iteration 9500: Loss = -10886.345085280103
Iteration 9600: Loss = -10886.34507656663
Iteration 9700: Loss = -10886.345223764733
1
Iteration 9800: Loss = -10886.344999490142
Iteration 9900: Loss = -10886.344964721608
Iteration 10000: Loss = -10886.346398157082
1
Iteration 10100: Loss = -10886.344953210515
Iteration 10200: Loss = -10886.344889982889
Iteration 10300: Loss = -10886.391819782544
1
Iteration 10400: Loss = -10886.344871075362
Iteration 10500: Loss = -10886.344848298148
Iteration 10600: Loss = -10886.344909406296
Iteration 10700: Loss = -10886.344838747287
Iteration 10800: Loss = -10886.344785028674
Iteration 10900: Loss = -10886.344755340373
Iteration 11000: Loss = -10886.345925284835
1
Iteration 11100: Loss = -10886.34476855318
Iteration 11200: Loss = -10886.344743711132
Iteration 11300: Loss = -10886.344729890956
Iteration 11400: Loss = -10886.3452092063
1
Iteration 11500: Loss = -10886.344725142075
Iteration 11600: Loss = -10886.344676881996
Iteration 11700: Loss = -10886.347130020473
1
Iteration 11800: Loss = -10886.344663101168
Iteration 11900: Loss = -10886.344645538587
Iteration 12000: Loss = -10886.346361464859
1
Iteration 12100: Loss = -10886.344626082517
Iteration 12200: Loss = -10886.34461980842
Iteration 12300: Loss = -10886.34463821404
Iteration 12400: Loss = -10886.3448249965
1
Iteration 12500: Loss = -10886.344605670378
Iteration 12600: Loss = -10886.344593188784
Iteration 12700: Loss = -10886.357971715659
1
Iteration 12800: Loss = -10886.344596660152
Iteration 12900: Loss = -10886.346480294002
1
Iteration 13000: Loss = -10886.344593331261
Iteration 13100: Loss = -10886.344558872492
Iteration 13200: Loss = -10886.344735172934
1
Iteration 13300: Loss = -10886.344568054286
Iteration 13400: Loss = -10886.519200391807
1
Iteration 13500: Loss = -10886.344583690146
Iteration 13600: Loss = -10886.344555772312
Iteration 13700: Loss = -10886.346202593279
1
Iteration 13800: Loss = -10886.344563190067
Iteration 13900: Loss = -10886.34484730562
1
Iteration 14000: Loss = -10886.344549591926
Iteration 14100: Loss = -10886.344707805316
1
Iteration 14200: Loss = -10886.344530032055
Iteration 14300: Loss = -10886.3572336286
1
Iteration 14400: Loss = -10886.344502416347
Iteration 14500: Loss = -10886.344535926086
Iteration 14600: Loss = -10886.403203655585
1
Iteration 14700: Loss = -10886.344518220696
Iteration 14800: Loss = -10886.344517811476
Iteration 14900: Loss = -10886.361516423482
1
Iteration 15000: Loss = -10886.344551831373
Iteration 15100: Loss = -10886.344534604783
Iteration 15200: Loss = -10886.344513386915
Iteration 15300: Loss = -10886.344865107898
1
Iteration 15400: Loss = -10886.347546667299
2
Iteration 15500: Loss = -10886.344486668755
Iteration 15600: Loss = -10886.344522437565
Iteration 15700: Loss = -10886.453476885463
1
Iteration 15800: Loss = -10886.34451873791
Iteration 15900: Loss = -10886.348310227273
1
Iteration 16000: Loss = -10886.344502187978
Iteration 16100: Loss = -10886.344752718836
1
Iteration 16200: Loss = -10886.34454440341
Iteration 16300: Loss = -10886.346574403371
1
Iteration 16400: Loss = -10886.344576821444
Iteration 16500: Loss = -10886.344558050963
Iteration 16600: Loss = -10886.34455256484
Iteration 16700: Loss = -10886.344597053447
Iteration 16800: Loss = -10886.344476463008
Iteration 16900: Loss = -10886.346290744368
1
Iteration 17000: Loss = -10886.351580934072
2
Iteration 17100: Loss = -10886.34592457285
3
Iteration 17200: Loss = -10886.34468697374
4
Iteration 17300: Loss = -10886.344547895373
Iteration 17400: Loss = -10886.344569986262
Iteration 17500: Loss = -10886.349870936976
1
Iteration 17600: Loss = -10886.344529930604
Iteration 17700: Loss = -10886.42274129898
1
Iteration 17800: Loss = -10886.344499210758
Iteration 17900: Loss = -10886.34546173582
1
Iteration 18000: Loss = -10886.344504990035
Iteration 18100: Loss = -10886.344741333001
1
Iteration 18200: Loss = -10886.344465636956
Iteration 18300: Loss = -10886.344585969386
1
Iteration 18400: Loss = -10886.344505910807
Iteration 18500: Loss = -10886.396401035281
1
Iteration 18600: Loss = -10886.344497329696
Iteration 18700: Loss = -10886.345810765575
1
Iteration 18800: Loss = -10886.364918652456
2
Iteration 18900: Loss = -10886.344489606903
Iteration 19000: Loss = -10886.349980653331
1
Iteration 19100: Loss = -10886.344603266445
2
Iteration 19200: Loss = -10886.344488605127
Iteration 19300: Loss = -10886.352559426581
1
Iteration 19400: Loss = -10886.34450615518
Iteration 19500: Loss = -10886.344484286068
Iteration 19600: Loss = -10886.34465641185
1
Iteration 19700: Loss = -10886.344556731798
Iteration 19800: Loss = -10886.344507646016
Iteration 19900: Loss = -10886.34482389421
1
pi: tensor([[9.8615e-01, 1.3845e-02],
        [9.9999e-01, 7.0098e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9959, 0.0041], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.2061],
         [0.5004, 0.1449]],

        [[0.7261, 0.3186],
         [0.5640, 0.6816]],

        [[0.5306, 0.1392],
         [0.5387, 0.5462]],

        [[0.7159, 0.1175],
         [0.5512, 0.5116]],

        [[0.6067, 0.1874],
         [0.5866, 0.5105]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005093525538803316
Average Adjusted Rand Index: -0.0005926784880256398
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20472.795833033113
Iteration 100: Loss = -10889.593969349899
Iteration 200: Loss = -10887.897481309745
Iteration 300: Loss = -10887.260708952314
Iteration 400: Loss = -10886.97742331269
Iteration 500: Loss = -10886.814599307489
Iteration 600: Loss = -10886.707705415309
Iteration 700: Loss = -10886.633341219267
Iteration 800: Loss = -10886.579700823826
Iteration 900: Loss = -10886.539856189143
Iteration 1000: Loss = -10886.509371108228
Iteration 1100: Loss = -10886.485232793508
Iteration 1200: Loss = -10886.465275078284
Iteration 1300: Loss = -10886.448133822209
Iteration 1400: Loss = -10886.432656333172
Iteration 1500: Loss = -10886.418158498707
Iteration 1600: Loss = -10886.403801593118
Iteration 1700: Loss = -10886.388462482344
Iteration 1800: Loss = -10886.36965438924
Iteration 1900: Loss = -10886.34073436432
Iteration 2000: Loss = -10886.282657431455
Iteration 2100: Loss = -10886.11163380834
Iteration 2200: Loss = -10885.065451822893
Iteration 2300: Loss = -10884.543360574182
Iteration 2400: Loss = -10884.395762589575
Iteration 2500: Loss = -10884.324297770663
Iteration 2600: Loss = -10884.281038750842
Iteration 2700: Loss = -10884.25261801596
Iteration 2800: Loss = -10884.231170273933
Iteration 2900: Loss = -10884.213735171297
Iteration 3000: Loss = -10884.198835062636
Iteration 3100: Loss = -10884.18547605988
Iteration 3200: Loss = -10884.172991597332
Iteration 3300: Loss = -10884.160999316067
Iteration 3400: Loss = -10884.149279672532
Iteration 3500: Loss = -10884.137950393395
Iteration 3600: Loss = -10884.12728079346
Iteration 3700: Loss = -10884.117573526644
Iteration 3800: Loss = -10884.109066069932
Iteration 3900: Loss = -10884.10178327834
Iteration 4000: Loss = -10884.095627215293
Iteration 4100: Loss = -10884.090399679424
Iteration 4200: Loss = -10884.085938438786
Iteration 4300: Loss = -10884.082094938272
Iteration 4400: Loss = -10884.078732785856
Iteration 4500: Loss = -10884.075833958488
Iteration 4600: Loss = -10884.073270711693
Iteration 4700: Loss = -10884.070959948936
Iteration 4800: Loss = -10884.068794208006
Iteration 4900: Loss = -10884.066801886
Iteration 5000: Loss = -10884.064918208966
Iteration 5100: Loss = -10884.062945788466
Iteration 5200: Loss = -10884.061133238063
Iteration 5300: Loss = -10884.059778012273
Iteration 5400: Loss = -10884.05855236416
Iteration 5500: Loss = -10884.057079153603
Iteration 5600: Loss = -10884.055724393846
Iteration 5700: Loss = -10884.05411741867
Iteration 5800: Loss = -10884.05321969791
Iteration 5900: Loss = -10884.052622298277
Iteration 6000: Loss = -10884.052118525406
Iteration 6100: Loss = -10884.051634508758
Iteration 6200: Loss = -10884.051241397377
Iteration 6300: Loss = -10884.050808252168
Iteration 6400: Loss = -10884.050336084238
Iteration 6500: Loss = -10884.049719529554
Iteration 6600: Loss = -10884.048605257762
Iteration 6700: Loss = -10884.048114862315
Iteration 6800: Loss = -10884.047764204035
Iteration 6900: Loss = -10884.04743120704
Iteration 7000: Loss = -10884.047156750106
Iteration 7100: Loss = -10884.046928889913
Iteration 7200: Loss = -10884.046790095863
Iteration 7300: Loss = -10884.046618706852
Iteration 7400: Loss = -10884.046505767354
Iteration 7500: Loss = -10884.04638679137
Iteration 7600: Loss = -10884.046278501199
Iteration 7700: Loss = -10884.04614968494
Iteration 7800: Loss = -10884.046054219694
Iteration 7900: Loss = -10884.047664739966
1
Iteration 8000: Loss = -10884.045876495562
Iteration 8100: Loss = -10884.045932868297
Iteration 8200: Loss = -10884.045751627547
Iteration 8300: Loss = -10884.045503312358
Iteration 8400: Loss = -10884.241570702663
1
Iteration 8500: Loss = -10884.044383483759
Iteration 8600: Loss = -10884.044275571787
Iteration 8700: Loss = -10884.044161075879
Iteration 8800: Loss = -10884.043899022327
Iteration 8900: Loss = -10884.043629115178
Iteration 9000: Loss = -10884.0434981403
Iteration 9100: Loss = -10884.047165837177
1
Iteration 9200: Loss = -10884.042696795617
Iteration 9300: Loss = -10884.042362187107
Iteration 9400: Loss = -10884.04225780246
Iteration 9500: Loss = -10884.042140035614
Iteration 9600: Loss = -10884.042036316112
Iteration 9700: Loss = -10884.041806105748
Iteration 9800: Loss = -10884.044332419884
1
Iteration 9900: Loss = -10884.04129666645
Iteration 10000: Loss = -10884.041229814076
Iteration 10100: Loss = -10884.599427012232
1
Iteration 10200: Loss = -10884.0411151103
Iteration 10300: Loss = -10884.041002051212
Iteration 10400: Loss = -10884.040930417626
Iteration 10500: Loss = -10884.040936061985
Iteration 10600: Loss = -10884.040802743788
Iteration 10700: Loss = -10884.040643482602
Iteration 10800: Loss = -10884.053914481552
1
Iteration 10900: Loss = -10884.040518040714
Iteration 11000: Loss = -10884.040416619966
Iteration 11100: Loss = -10884.04042920481
Iteration 11200: Loss = -10884.042349798177
1
Iteration 11300: Loss = -10884.04038956326
Iteration 11400: Loss = -10884.04033545877
Iteration 11500: Loss = -10884.041233242062
1
Iteration 11600: Loss = -10884.04017513852
Iteration 11700: Loss = -10884.04007809735
Iteration 11800: Loss = -10884.040055106221
Iteration 11900: Loss = -10884.052020754712
1
Iteration 12000: Loss = -10884.040044451363
Iteration 12100: Loss = -10884.040020169909
Iteration 12200: Loss = -10884.23586026389
1
Iteration 12300: Loss = -10884.039984056277
Iteration 12400: Loss = -10884.039924720724
Iteration 12500: Loss = -10884.039814938697
Iteration 12600: Loss = -10884.039068736205
Iteration 12700: Loss = -10884.039068453203
Iteration 12800: Loss = -10884.039024002803
Iteration 12900: Loss = -10884.043725531295
1
Iteration 13000: Loss = -10884.039015586792
Iteration 13100: Loss = -10884.038992613448
Iteration 13200: Loss = -10884.038958340945
Iteration 13300: Loss = -10884.040560471512
1
Iteration 13400: Loss = -10884.038102439232
Iteration 13500: Loss = -10884.038039331757
Iteration 13600: Loss = -10884.045373989567
1
Iteration 13700: Loss = -10884.038034170715
Iteration 13800: Loss = -10884.03801666349
Iteration 13900: Loss = -10884.483197539883
1
Iteration 14000: Loss = -10884.038028720308
Iteration 14100: Loss = -10884.038018961379
Iteration 14200: Loss = -10884.038396797034
1
Iteration 14300: Loss = -10884.037891933689
Iteration 14400: Loss = -10884.037917975043
Iteration 14500: Loss = -10884.037881030355
Iteration 14600: Loss = -10884.03866016511
1
Iteration 14700: Loss = -10884.037884025787
Iteration 14800: Loss = -10884.037908767594
Iteration 14900: Loss = -10884.04312377285
1
Iteration 15000: Loss = -10884.03788902592
Iteration 15100: Loss = -10884.072460318363
1
Iteration 15200: Loss = -10884.037714407541
Iteration 15300: Loss = -10884.037706535373
Iteration 15400: Loss = -10884.038758289626
1
Iteration 15500: Loss = -10884.037728511044
Iteration 15600: Loss = -10884.037740337168
Iteration 15700: Loss = -10884.03783109228
Iteration 15800: Loss = -10884.039026094953
1
Iteration 15900: Loss = -10884.03755658919
Iteration 16000: Loss = -10884.03730983753
Iteration 16100: Loss = -10884.037209739308
Iteration 16200: Loss = -10884.037155932992
Iteration 16300: Loss = -10884.038673396675
1
Iteration 16400: Loss = -10884.037158483501
Iteration 16500: Loss = -10884.042502002108
1
Iteration 16600: Loss = -10884.037104627221
Iteration 16700: Loss = -10884.03707236273
Iteration 16800: Loss = -10884.040767317209
1
Iteration 16900: Loss = -10884.037017787186
Iteration 17000: Loss = -10884.065790663139
1
Iteration 17100: Loss = -10884.037051065203
Iteration 17200: Loss = -10884.041048269804
1
Iteration 17300: Loss = -10884.037028132383
Iteration 17400: Loss = -10884.039275548743
1
Iteration 17500: Loss = -10884.03702571951
Iteration 17600: Loss = -10884.037017479215
Iteration 17700: Loss = -10884.038491748595
1
Iteration 17800: Loss = -10884.036965009402
Iteration 17900: Loss = -10884.36804344739
1
Iteration 18000: Loss = -10884.036978468941
Iteration 18100: Loss = -10884.03696917114
Iteration 18200: Loss = -10884.037288222542
1
Iteration 18300: Loss = -10884.036968856253
Iteration 18400: Loss = -10884.037887442119
1
Iteration 18500: Loss = -10884.0369439526
Iteration 18600: Loss = -10884.051095358398
1
Iteration 18700: Loss = -10884.036951966573
Iteration 18800: Loss = -10884.03701533804
Iteration 18900: Loss = -10884.037689710438
1
Iteration 19000: Loss = -10884.036862693218
Iteration 19100: Loss = -10884.036908430464
Iteration 19200: Loss = -10884.036965049452
Iteration 19300: Loss = -10884.036891444264
Iteration 19400: Loss = -10884.060863716442
1
Iteration 19500: Loss = -10884.036912930856
Iteration 19600: Loss = -10884.037745625365
1
Iteration 19700: Loss = -10884.03694324397
Iteration 19800: Loss = -10884.036925601811
Iteration 19900: Loss = -10884.037002309815
pi: tensor([[1.0000e+00, 6.2181e-07],
        [2.6804e-03, 9.9732e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0092, 0.9908], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0398, 0.1448],
         [0.7075, 0.1603]],

        [[0.5688, 0.3194],
         [0.6511, 0.6082]],

        [[0.6863, 0.1540],
         [0.7066, 0.5868]],

        [[0.6266, 0.1038],
         [0.5152, 0.5469]],

        [[0.5977, 0.1127],
         [0.5457, 0.7298]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.001390164742435905
Average Adjusted Rand Index: -0.002431739796940097
10777.188434266836
[-0.0005093525538803316, -0.001390164742435905] [-0.0005926784880256398, -0.002431739796940097] [10886.344609655414, 10884.036897611964]
-------------------------------------
This iteration is 8
True Objective function: Loss = -10793.899248623846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21409.650770903107
Iteration 100: Loss = -10880.622310534181
Iteration 200: Loss = -10879.751885817013
Iteration 300: Loss = -10876.826670159571
Iteration 400: Loss = -10875.008536206571
Iteration 500: Loss = -10873.836577620188
Iteration 600: Loss = -10873.229731396907
Iteration 700: Loss = -10872.950051939311
Iteration 800: Loss = -10872.79416832716
Iteration 900: Loss = -10872.702341319111
Iteration 1000: Loss = -10872.641054427499
Iteration 1100: Loss = -10872.592338059323
Iteration 1200: Loss = -10872.550035653961
Iteration 1300: Loss = -10872.51233732739
Iteration 1400: Loss = -10872.47834446685
Iteration 1500: Loss = -10872.44768188921
Iteration 1600: Loss = -10872.420184974466
Iteration 1700: Loss = -10872.39571492687
Iteration 1800: Loss = -10872.373261809062
Iteration 1900: Loss = -10872.352558375556
Iteration 2000: Loss = -10872.3329109252
Iteration 2100: Loss = -10872.312845188299
Iteration 2200: Loss = -10872.285985923236
Iteration 2300: Loss = -10871.044202199259
Iteration 2400: Loss = -10870.579600077726
Iteration 2500: Loss = -10870.432199607221
Iteration 2600: Loss = -10870.321462280946
Iteration 2700: Loss = -10870.262395309954
Iteration 2800: Loss = -10870.23495523199
Iteration 2900: Loss = -10870.220089439495
Iteration 3000: Loss = -10870.2106614652
Iteration 3100: Loss = -10870.204068708923
Iteration 3200: Loss = -10870.199127644566
Iteration 3300: Loss = -10870.195184100996
Iteration 3400: Loss = -10870.192044770893
Iteration 3500: Loss = -10870.189465431313
Iteration 3600: Loss = -10870.187265810951
Iteration 3700: Loss = -10870.185339055342
Iteration 3800: Loss = -10870.18374796839
Iteration 3900: Loss = -10870.182310127102
Iteration 4000: Loss = -10870.181044710585
Iteration 4100: Loss = -10870.179904436925
Iteration 4200: Loss = -10870.178869264739
Iteration 4300: Loss = -10870.177946989359
Iteration 4400: Loss = -10870.177139660507
Iteration 4500: Loss = -10870.176376194004
Iteration 4600: Loss = -10870.175670522054
Iteration 4700: Loss = -10870.175019913158
Iteration 4800: Loss = -10870.174440057268
Iteration 4900: Loss = -10870.173914131685
Iteration 5000: Loss = -10870.17341870292
Iteration 5100: Loss = -10870.172974890307
Iteration 5200: Loss = -10870.172571358537
Iteration 5300: Loss = -10870.172142272775
Iteration 5400: Loss = -10870.171795290933
Iteration 5500: Loss = -10870.17147785727
Iteration 5600: Loss = -10870.17114369801
Iteration 5700: Loss = -10870.170867791372
Iteration 5800: Loss = -10870.172554000705
1
Iteration 5900: Loss = -10870.170316571719
Iteration 6000: Loss = -10870.170127885805
Iteration 6100: Loss = -10870.169907997024
Iteration 6200: Loss = -10870.169680946381
Iteration 6300: Loss = -10870.16949904492
Iteration 6400: Loss = -10870.169310253092
Iteration 6500: Loss = -10870.169117709464
Iteration 6600: Loss = -10870.170071827268
1
Iteration 6700: Loss = -10870.168797961065
Iteration 6800: Loss = -10870.168661253807
Iteration 6900: Loss = -10870.168496555212
Iteration 7000: Loss = -10870.170197249665
1
Iteration 7100: Loss = -10870.1682533354
Iteration 7200: Loss = -10870.168143588684
Iteration 7300: Loss = -10870.168062772173
Iteration 7400: Loss = -10870.167930837215
Iteration 7500: Loss = -10870.168405618862
1
Iteration 7600: Loss = -10870.167780683933
Iteration 7700: Loss = -10870.168605372091
1
Iteration 7800: Loss = -10870.167556360653
Iteration 7900: Loss = -10870.169847917028
1
Iteration 8000: Loss = -10870.167435024674
Iteration 8100: Loss = -10870.172522214856
1
Iteration 8200: Loss = -10870.170262096239
2
Iteration 8300: Loss = -10870.169646488706
3
Iteration 8400: Loss = -10870.195652376473
4
Iteration 8500: Loss = -10870.167523278904
Iteration 8600: Loss = -10870.167344198
Iteration 8700: Loss = -10870.167046133014
Iteration 8800: Loss = -10870.16741755002
1
Iteration 8900: Loss = -10870.166934185709
Iteration 9000: Loss = -10870.172688820561
1
Iteration 9100: Loss = -10870.16694369837
Iteration 9200: Loss = -10870.166798433615
Iteration 9300: Loss = -10870.166837262463
Iteration 9400: Loss = -10870.166764185164
Iteration 9500: Loss = -10870.57801961506
1
Iteration 9600: Loss = -10870.16665455685
Iteration 9700: Loss = -10870.166639408293
Iteration 9800: Loss = -10870.62509019756
1
Iteration 9900: Loss = -10870.166579406772
Iteration 10000: Loss = -10870.166520952767
Iteration 10100: Loss = -10870.17775641875
1
Iteration 10200: Loss = -10870.16652579165
Iteration 10300: Loss = -10870.166485711963
Iteration 10400: Loss = -10870.233529736342
1
Iteration 10500: Loss = -10870.166431717127
Iteration 10600: Loss = -10870.283253837182
1
Iteration 10700: Loss = -10870.166388387483
Iteration 10800: Loss = -10870.17438967829
1
Iteration 10900: Loss = -10870.166362642483
Iteration 11000: Loss = -10870.166555619813
1
Iteration 11100: Loss = -10870.166351853226
Iteration 11200: Loss = -10870.16631127442
Iteration 11300: Loss = -10870.167036771918
1
Iteration 11400: Loss = -10870.166294138373
Iteration 11500: Loss = -10870.166273890954
Iteration 11600: Loss = -10870.166606949757
1
Iteration 11700: Loss = -10870.166222033904
Iteration 11800: Loss = -10870.166296765885
Iteration 11900: Loss = -10870.176360295363
1
Iteration 12000: Loss = -10870.218818338897
2
Iteration 12100: Loss = -10870.166289631623
Iteration 12200: Loss = -10870.181089721918
1
Iteration 12300: Loss = -10870.166168361442
Iteration 12400: Loss = -10870.167125435368
1
Iteration 12500: Loss = -10870.169300152891
2
Iteration 12600: Loss = -10870.1661574595
Iteration 12700: Loss = -10870.166625292482
1
Iteration 12800: Loss = -10870.166140128313
Iteration 12900: Loss = -10870.166163815655
Iteration 13000: Loss = -10870.2044669465
1
Iteration 13100: Loss = -10870.166108756906
Iteration 13200: Loss = -10870.169457485128
1
Iteration 13300: Loss = -10870.188993546652
2
Iteration 13400: Loss = -10870.166176247836
Iteration 13500: Loss = -10870.166567731572
1
Iteration 13600: Loss = -10870.166101030398
Iteration 13700: Loss = -10870.167302896161
1
Iteration 13800: Loss = -10870.203819449778
2
Iteration 13900: Loss = -10870.166167182726
Iteration 14000: Loss = -10870.181925285387
1
Iteration 14100: Loss = -10870.171087663657
2
Iteration 14200: Loss = -10870.166157753401
Iteration 14300: Loss = -10870.166324327487
1
Iteration 14400: Loss = -10870.167931387889
2
Iteration 14500: Loss = -10870.166570910558
3
Iteration 14600: Loss = -10870.413301718638
4
Iteration 14700: Loss = -10870.166053049916
Iteration 14800: Loss = -10870.187085927168
1
Iteration 14900: Loss = -10870.175440837995
2
Iteration 15000: Loss = -10870.168942236658
3
Iteration 15100: Loss = -10870.170654933148
4
Iteration 15200: Loss = -10870.166250407547
5
Iteration 15300: Loss = -10870.183578635793
6
Iteration 15400: Loss = -10870.166048852028
Iteration 15500: Loss = -10870.166155300136
1
Iteration 15600: Loss = -10870.166045957452
Iteration 15700: Loss = -10870.166130033136
Iteration 15800: Loss = -10870.17056434866
1
Iteration 15900: Loss = -10870.166020118342
Iteration 16000: Loss = -10870.166221514057
1
Iteration 16100: Loss = -10870.173133817101
2
Iteration 16200: Loss = -10870.166014911792
Iteration 16300: Loss = -10870.166915031896
1
Iteration 16400: Loss = -10870.166035510783
Iteration 16500: Loss = -10870.16735851692
1
Iteration 16600: Loss = -10870.16723770405
2
Iteration 16700: Loss = -10870.167109808384
3
Iteration 16800: Loss = -10870.166165223682
4
Iteration 16900: Loss = -10870.187450328773
5
Iteration 17000: Loss = -10870.16641800376
6
Iteration 17100: Loss = -10870.174457040774
7
Iteration 17200: Loss = -10870.168134496244
8
Iteration 17300: Loss = -10870.1684068375
9
Iteration 17400: Loss = -10870.166048090356
Iteration 17500: Loss = -10870.171621390831
1
Iteration 17600: Loss = -10870.166056682834
Iteration 17700: Loss = -10870.17468982244
1
Iteration 17800: Loss = -10870.166336642369
2
Iteration 17900: Loss = -10870.170216852963
3
Iteration 18000: Loss = -10870.166065578698
Iteration 18100: Loss = -10870.166042506076
Iteration 18200: Loss = -10870.166095353785
Iteration 18300: Loss = -10870.166147510457
Iteration 18400: Loss = -10870.216053637405
1
Iteration 18500: Loss = -10870.166476918783
2
Iteration 18600: Loss = -10870.166797189078
3
Iteration 18700: Loss = -10870.166072583315
Iteration 18800: Loss = -10870.166312077905
1
Iteration 18900: Loss = -10870.191553397024
2
Iteration 19000: Loss = -10870.193908563531
3
Iteration 19100: Loss = -10870.19108788321
4
Iteration 19200: Loss = -10870.16660591948
5
Iteration 19300: Loss = -10870.179722938241
6
Iteration 19400: Loss = -10870.166116226188
Iteration 19500: Loss = -10870.166242999365
1
Iteration 19600: Loss = -10870.169716885588
2
Iteration 19700: Loss = -10870.173847552594
3
Iteration 19800: Loss = -10870.166034530765
Iteration 19900: Loss = -10870.166810606397
1
pi: tensor([[1.0000e+00, 2.8971e-07],
        [1.0508e-02, 9.8949e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0395, 0.9605], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4528, 0.1080],
         [0.6617, 0.1572]],

        [[0.7054, 0.1406],
         [0.6248, 0.5183]],

        [[0.5270, 0.1660],
         [0.5109, 0.7256]],

        [[0.5753, 0.2208],
         [0.6350, 0.5124]],

        [[0.5434, 0.1854],
         [0.5085, 0.7137]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.007419103651150718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.0030755875240280274
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.02216370547261358
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0028959952356207414
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.005661239846689405
Global Adjusted Rand Index: 0.008332115573762863
Average Adjusted Rand Index: 0.0017807539372732342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20147.572931241473
Iteration 100: Loss = -10880.965617057711
Iteration 200: Loss = -10880.464081204544
Iteration 300: Loss = -10880.22027061435
Iteration 400: Loss = -10879.889092189691
Iteration 500: Loss = -10877.443264671107
Iteration 600: Loss = -10876.239861884716
Iteration 700: Loss = -10875.410838761523
Iteration 800: Loss = -10874.973309909741
Iteration 900: Loss = -10874.492551971016
Iteration 1000: Loss = -10873.962974898785
Iteration 1100: Loss = -10873.426662607568
Iteration 1200: Loss = -10873.066211064275
Iteration 1300: Loss = -10872.956839498076
Iteration 1400: Loss = -10872.839074839814
Iteration 1500: Loss = -10872.749104732897
Iteration 1600: Loss = -10872.691380546426
Iteration 1700: Loss = -10872.640940320423
Iteration 1800: Loss = -10872.59666738985
Iteration 1900: Loss = -10872.554119792003
Iteration 2000: Loss = -10872.523364601724
Iteration 2100: Loss = -10872.502732835916
Iteration 2200: Loss = -10872.485101833483
Iteration 2300: Loss = -10872.46870584388
Iteration 2400: Loss = -10872.454720011827
Iteration 2500: Loss = -10872.4422166323
Iteration 2600: Loss = -10872.43017575146
Iteration 2700: Loss = -10872.419645621309
Iteration 2800: Loss = -10872.409840333956
Iteration 2900: Loss = -10872.39823341129
Iteration 3000: Loss = -10872.387197206672
Iteration 3100: Loss = -10872.376364812671
Iteration 3200: Loss = -10872.365322287156
Iteration 3300: Loss = -10872.35490912897
Iteration 3400: Loss = -10872.342756930868
Iteration 3500: Loss = -10871.176481101224
Iteration 3600: Loss = -10870.324533896825
Iteration 3700: Loss = -10870.263680799193
Iteration 3800: Loss = -10870.23278517673
Iteration 3900: Loss = -10870.205369761867
Iteration 4000: Loss = -10870.200278773615
Iteration 4100: Loss = -10870.196784885848
Iteration 4200: Loss = -10870.194124768423
Iteration 4300: Loss = -10870.192009193755
Iteration 4400: Loss = -10870.190148056781
Iteration 4500: Loss = -10870.188443877107
Iteration 4600: Loss = -10870.186813560045
Iteration 4700: Loss = -10870.185052241704
Iteration 4800: Loss = -10870.18326426057
Iteration 4900: Loss = -10870.181870932829
Iteration 5000: Loss = -10870.181018335112
Iteration 5100: Loss = -10870.180241654805
Iteration 5200: Loss = -10870.179542063117
Iteration 5300: Loss = -10870.178890512827
Iteration 5400: Loss = -10870.178271914145
Iteration 5500: Loss = -10870.177626969453
Iteration 5600: Loss = -10870.176988112107
Iteration 5700: Loss = -10870.176392442516
Iteration 5800: Loss = -10870.175866279944
Iteration 5900: Loss = -10870.177050877499
1
Iteration 6000: Loss = -10870.17492862251
Iteration 6100: Loss = -10870.174554712068
Iteration 6200: Loss = -10870.174116937444
Iteration 6300: Loss = -10870.173515834189
Iteration 6400: Loss = -10870.172870840373
Iteration 6500: Loss = -10870.172349593155
Iteration 6600: Loss = -10870.172390774927
Iteration 6700: Loss = -10870.171655828846
Iteration 6800: Loss = -10870.171437550882
Iteration 6900: Loss = -10870.170736951699
Iteration 7000: Loss = -10870.170445916616
Iteration 7100: Loss = -10870.170153061996
Iteration 7200: Loss = -10870.169852723362
Iteration 7300: Loss = -10870.169822917032
Iteration 7400: Loss = -10870.169375882082
Iteration 7500: Loss = -10870.169215992326
Iteration 7600: Loss = -10870.169381847683
1
Iteration 7700: Loss = -10870.168970105644
Iteration 7800: Loss = -10870.215967928596
1
Iteration 7900: Loss = -10870.168506387117
Iteration 8000: Loss = -10870.175885306651
1
Iteration 8100: Loss = -10870.168174683842
Iteration 8200: Loss = -10870.174978983707
1
Iteration 8300: Loss = -10870.168024593035
Iteration 8400: Loss = -10870.167872429065
Iteration 8500: Loss = -10870.167806595507
Iteration 8600: Loss = -10870.167696879937
Iteration 8700: Loss = -10870.169213473771
1
Iteration 8800: Loss = -10870.167534522603
Iteration 8900: Loss = -10870.167521224317
Iteration 9000: Loss = -10870.167392812165
Iteration 9100: Loss = -10870.173048052025
1
Iteration 9200: Loss = -10870.167303707874
Iteration 9300: Loss = -10870.167379372864
Iteration 9400: Loss = -10870.167222415133
Iteration 9500: Loss = -10870.167154923765
Iteration 9600: Loss = -10870.16712387941
Iteration 9700: Loss = -10870.17617642838
1
Iteration 9800: Loss = -10870.167064327108
Iteration 9900: Loss = -10870.167033787586
Iteration 10000: Loss = -10870.172824813548
1
Iteration 10100: Loss = -10870.166976066543
Iteration 10200: Loss = -10870.166941468799
Iteration 10300: Loss = -10870.171884075433
1
Iteration 10400: Loss = -10870.166864670258
Iteration 10500: Loss = -10870.166848976316
Iteration 10600: Loss = -10870.16680035342
Iteration 10700: Loss = -10870.166804491573
Iteration 10800: Loss = -10870.166770861157
Iteration 10900: Loss = -10870.180292694471
1
Iteration 11000: Loss = -10870.16669516728
Iteration 11100: Loss = -10870.166660897903
Iteration 11200: Loss = -10870.16668419004
Iteration 11300: Loss = -10870.175545461987
1
Iteration 11400: Loss = -10870.166557779781
Iteration 11500: Loss = -10870.166525854967
Iteration 11600: Loss = -10870.166529696358
Iteration 11700: Loss = -10870.167346830129
1
Iteration 11800: Loss = -10870.16646842603
Iteration 11900: Loss = -10870.166467712412
Iteration 12000: Loss = -10870.166566787739
Iteration 12100: Loss = -10870.16702158375
1
Iteration 12200: Loss = -10870.287023580888
2
Iteration 12300: Loss = -10870.16635773601
Iteration 12400: Loss = -10870.166948870938
1
Iteration 12500: Loss = -10870.166375339959
Iteration 12600: Loss = -10870.16672250467
1
Iteration 12700: Loss = -10870.167800201025
2
Iteration 12800: Loss = -10870.167884653853
3
Iteration 12900: Loss = -10870.184067663084
4
Iteration 13000: Loss = -10870.180255935209
5
Iteration 13100: Loss = -10870.169786825561
6
Iteration 13200: Loss = -10870.16706074641
7
Iteration 13300: Loss = -10870.167575670324
8
Iteration 13400: Loss = -10870.204952866463
9
Iteration 13500: Loss = -10870.166348078768
Iteration 13600: Loss = -10870.221315944385
1
Iteration 13700: Loss = -10870.172803641777
2
Iteration 13800: Loss = -10870.180442785953
3
Iteration 13900: Loss = -10870.16870551106
4
Iteration 14000: Loss = -10870.16997677044
5
Iteration 14100: Loss = -10870.166392156942
Iteration 14200: Loss = -10870.166493583745
1
Iteration 14300: Loss = -10870.167757201089
2
Iteration 14400: Loss = -10870.183643386234
3
Iteration 14500: Loss = -10870.166169388698
Iteration 14600: Loss = -10870.166175419254
Iteration 14700: Loss = -10870.16616151168
Iteration 14800: Loss = -10870.171843579827
1
Iteration 14900: Loss = -10870.1661410343
Iteration 15000: Loss = -10870.166822177747
1
Iteration 15100: Loss = -10870.175833626847
2
Iteration 15200: Loss = -10870.166368208551
3
Iteration 15300: Loss = -10870.166392087118
4
Iteration 15400: Loss = -10870.270457416385
5
Iteration 15500: Loss = -10870.166198263503
Iteration 15600: Loss = -10870.18952988728
1
Iteration 15700: Loss = -10870.166598182575
2
Iteration 15800: Loss = -10870.166253661202
Iteration 15900: Loss = -10870.166167503956
Iteration 16000: Loss = -10870.166689086356
1
Iteration 16100: Loss = -10870.166257964209
Iteration 16200: Loss = -10870.167064999858
1
Iteration 16300: Loss = -10870.166066969508
Iteration 16400: Loss = -10870.172352857562
1
Iteration 16500: Loss = -10870.166092241274
Iteration 16600: Loss = -10870.169142039034
1
Iteration 16700: Loss = -10870.173903781324
2
Iteration 16800: Loss = -10870.173365104041
3
Iteration 16900: Loss = -10870.166126895245
Iteration 17000: Loss = -10870.16628117934
1
Iteration 17100: Loss = -10870.16611584077
Iteration 17200: Loss = -10870.168567583709
1
Iteration 17300: Loss = -10870.166065916748
Iteration 17400: Loss = -10870.182411487827
1
Iteration 17500: Loss = -10870.166693997175
2
Iteration 17600: Loss = -10870.168208328136
3
Iteration 17700: Loss = -10870.22205938218
4
Iteration 17800: Loss = -10870.16855558998
5
Iteration 17900: Loss = -10870.166162994057
Iteration 18000: Loss = -10870.166305739078
1
Iteration 18100: Loss = -10870.179790381922
2
Iteration 18200: Loss = -10870.195043103176
3
Iteration 18300: Loss = -10870.166083686041
Iteration 18400: Loss = -10870.166701208702
1
Iteration 18500: Loss = -10870.1663404083
2
Iteration 18600: Loss = -10870.16618650184
3
Iteration 18700: Loss = -10870.214820603142
4
Iteration 18800: Loss = -10870.193717872751
5
Iteration 18900: Loss = -10870.177099105545
6
Iteration 19000: Loss = -10870.166086013342
Iteration 19100: Loss = -10870.166242636144
1
Iteration 19200: Loss = -10870.169660179816
2
Iteration 19300: Loss = -10870.172499923649
3
Iteration 19400: Loss = -10870.181490496594
4
Iteration 19500: Loss = -10870.16604786607
Iteration 19600: Loss = -10870.167052362223
1
Iteration 19700: Loss = -10870.166057842715
Iteration 19800: Loss = -10870.166405783364
1
Iteration 19900: Loss = -10870.166041108534
pi: tensor([[1.0000e+00, 2.7862e-07],
        [1.0483e-02, 9.8952e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0396, 0.9604], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4529, 0.1080],
         [0.6935, 0.1573]],

        [[0.6002, 0.1406],
         [0.5605, 0.7052]],

        [[0.6947, 0.1678],
         [0.5632, 0.7081]],

        [[0.5282, 0.2211],
         [0.6110, 0.5771]],

        [[0.6757, 0.1855],
         [0.6508, 0.6814]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.007419103651150718
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: -0.0030755875240280274
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.02216370547261358
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0028959952356207414
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.005661239846689405
Global Adjusted Rand Index: 0.008332115573762863
Average Adjusted Rand Index: 0.0017807539372732342
10793.899248623846
[0.008332115573762863, 0.008332115573762863] [0.0017807539372732342, 0.0017807539372732342] [10870.172641157133, 10870.16623947481]
-------------------------------------
This iteration is 9
True Objective function: Loss = -10875.040932281672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23200.668815987116
Iteration 100: Loss = -10983.670626870144
Iteration 200: Loss = -10982.817064628844
Iteration 300: Loss = -10982.55764342214
Iteration 400: Loss = -10982.40131501367
Iteration 500: Loss = -10982.152278584812
Iteration 600: Loss = -10981.66178688083
Iteration 700: Loss = -10981.350092722336
Iteration 800: Loss = -10981.139032593883
Iteration 900: Loss = -10980.97104128364
Iteration 1000: Loss = -10980.8211027836
Iteration 1100: Loss = -10980.673200647852
Iteration 1200: Loss = -10980.514529146782
Iteration 1300: Loss = -10980.33489653407
Iteration 1400: Loss = -10980.137060082265
Iteration 1500: Loss = -10979.93216051207
Iteration 1600: Loss = -10979.729994039591
Iteration 1700: Loss = -10979.555660599564
Iteration 1800: Loss = -10979.424932678749
Iteration 1900: Loss = -10979.340638731377
Iteration 2000: Loss = -10979.294604459892
Iteration 2100: Loss = -10979.271243078749
Iteration 2200: Loss = -10979.256313922817
Iteration 2300: Loss = -10979.24345364025
Iteration 2400: Loss = -10979.230200178052
Iteration 2500: Loss = -10979.215324690284
Iteration 2600: Loss = -10979.1999476156
Iteration 2700: Loss = -10979.190093783245
Iteration 2800: Loss = -10979.17250852738
Iteration 2900: Loss = -10979.162974537965
Iteration 3000: Loss = -10979.164870466902
1
Iteration 3100: Loss = -10979.152459376812
Iteration 3200: Loss = -10979.149781410182
Iteration 3300: Loss = -10979.149497852102
Iteration 3400: Loss = -10979.146930683666
Iteration 3500: Loss = -10979.146136872461
Iteration 3600: Loss = -10979.145541351212
Iteration 3700: Loss = -10979.145435047538
Iteration 3800: Loss = -10979.14461207667
Iteration 3900: Loss = -10979.144315548545
Iteration 4000: Loss = -10979.143959080504
Iteration 4100: Loss = -10979.143619283786
Iteration 4200: Loss = -10979.143352714422
Iteration 4300: Loss = -10979.143003074141
Iteration 4400: Loss = -10979.142691629087
Iteration 4500: Loss = -10979.142343345422
Iteration 4600: Loss = -10979.144195735167
1
Iteration 4700: Loss = -10979.141332391413
Iteration 4800: Loss = -10979.141129275456
Iteration 4900: Loss = -10979.13992441078
Iteration 5000: Loss = -10979.139458611224
Iteration 5100: Loss = -10979.13801980792
Iteration 5200: Loss = -10979.136857437126
Iteration 5300: Loss = -10979.135815505697
Iteration 5400: Loss = -10979.133713564548
Iteration 5500: Loss = -10979.1370069528
1
Iteration 5600: Loss = -10979.139317498168
2
Iteration 5700: Loss = -10979.127199907263
Iteration 5800: Loss = -10979.124284929745
Iteration 5900: Loss = -10979.120793385344
Iteration 6000: Loss = -10979.114305196485
Iteration 6100: Loss = -10934.162713455373
Iteration 6200: Loss = -10833.820832898928
Iteration 6300: Loss = -10832.101700382584
Iteration 6400: Loss = -10831.42823857308
Iteration 6500: Loss = -10831.39284197865
Iteration 6600: Loss = -10831.37191118498
Iteration 6700: Loss = -10831.360772989388
Iteration 6800: Loss = -10831.3439633782
Iteration 6900: Loss = -10831.334988189541
Iteration 7000: Loss = -10831.326080564719
Iteration 7100: Loss = -10831.313645511282
Iteration 7200: Loss = -10831.304071692079
Iteration 7300: Loss = -10831.243561108771
Iteration 7400: Loss = -10831.23956608469
Iteration 7500: Loss = -10831.21511226727
Iteration 7600: Loss = -10831.208033340303
Iteration 7700: Loss = -10831.202625279451
Iteration 7800: Loss = -10831.121833029332
Iteration 7900: Loss = -10831.119491880667
Iteration 8000: Loss = -10831.11475610778
Iteration 8100: Loss = -10831.117579657888
1
Iteration 8200: Loss = -10831.109035209862
Iteration 8300: Loss = -10831.108088542058
Iteration 8400: Loss = -10831.106332442436
Iteration 8500: Loss = -10831.10778733436
1
Iteration 8600: Loss = -10831.111101231734
2
Iteration 8700: Loss = -10831.104477304963
Iteration 8800: Loss = -10831.099351796947
Iteration 8900: Loss = -10831.098248974733
Iteration 9000: Loss = -10831.09738162435
Iteration 9100: Loss = -10831.096224000597
Iteration 9200: Loss = -10831.095605923123
Iteration 9300: Loss = -10831.108221180697
1
Iteration 9400: Loss = -10831.095267469353
Iteration 9500: Loss = -10831.124620592396
1
Iteration 9600: Loss = -10831.102598628353
2
Iteration 9700: Loss = -10830.94554189158
Iteration 9800: Loss = -10830.951260450327
1
Iteration 9900: Loss = -10830.958900937123
2
Iteration 10000: Loss = -10830.941540089929
Iteration 10100: Loss = -10830.939545101293
Iteration 10200: Loss = -10830.934515141153
Iteration 10300: Loss = -10830.93325268067
Iteration 10400: Loss = -10830.942219370909
1
Iteration 10500: Loss = -10830.94631436206
2
Iteration 10600: Loss = -10831.04173483765
3
Iteration 10700: Loss = -10831.044563261577
4
Iteration 10800: Loss = -10830.998554472126
5
Iteration 10900: Loss = -10830.925729002083
Iteration 11000: Loss = -10830.93097276162
1
Iteration 11100: Loss = -10830.927342113613
2
Iteration 11200: Loss = -10830.930569870445
3
Iteration 11300: Loss = -10831.030970656497
4
Iteration 11400: Loss = -10831.036041539588
5
Iteration 11500: Loss = -10830.925183496876
Iteration 11600: Loss = -10830.92518034616
Iteration 11700: Loss = -10830.924912814877
Iteration 11800: Loss = -10830.943217207934
1
Iteration 11900: Loss = -10830.923919718445
Iteration 12000: Loss = -10830.924665281596
1
Iteration 12100: Loss = -10830.92315861628
Iteration 12200: Loss = -10830.92536952299
1
Iteration 12300: Loss = -10830.985768185816
2
Iteration 12400: Loss = -10830.922970825044
Iteration 12500: Loss = -10830.923201319358
1
Iteration 12600: Loss = -10830.932955463384
2
Iteration 12700: Loss = -10830.922971981343
Iteration 12800: Loss = -10830.954689049739
1
Iteration 12900: Loss = -10830.922953549369
Iteration 13000: Loss = -10830.93601740904
1
Iteration 13100: Loss = -10830.923312619308
2
Iteration 13200: Loss = -10830.92439974096
3
Iteration 13300: Loss = -10830.923312774676
4
Iteration 13400: Loss = -10830.927124944614
5
Iteration 13500: Loss = -10830.923049601697
Iteration 13600: Loss = -10830.933787437525
1
Iteration 13700: Loss = -10830.945206067729
2
Iteration 13800: Loss = -10830.932767979957
3
Iteration 13900: Loss = -10830.941964164163
4
Iteration 14000: Loss = -10831.134811017713
5
Iteration 14100: Loss = -10830.921246746664
Iteration 14200: Loss = -10830.927928840865
1
Iteration 14300: Loss = -10830.92705690592
2
Iteration 14400: Loss = -10830.922006856708
3
Iteration 14500: Loss = -10830.921901309906
4
Iteration 14600: Loss = -10830.922305937787
5
Iteration 14700: Loss = -10830.92321880545
6
Iteration 14800: Loss = -10830.935345191585
7
Iteration 14900: Loss = -10831.098355399225
8
Iteration 15000: Loss = -10830.921264387744
Iteration 15100: Loss = -10830.92151858914
1
Iteration 15200: Loss = -10830.922059861325
2
Iteration 15300: Loss = -10830.933128702958
3
Iteration 15400: Loss = -10830.922102794519
4
Iteration 15500: Loss = -10830.921181415053
Iteration 15600: Loss = -10830.927328177217
1
Iteration 15700: Loss = -10830.997883662581
2
Iteration 15800: Loss = -10830.92934311434
3
Iteration 15900: Loss = -10830.927244480576
4
Iteration 16000: Loss = -10830.962832809337
5
Iteration 16100: Loss = -10830.99145290574
6
Iteration 16200: Loss = -10830.92117761016
Iteration 16300: Loss = -10830.925350495698
1
Iteration 16400: Loss = -10830.923818243746
2
Iteration 16500: Loss = -10830.925098574351
3
Iteration 16600: Loss = -10830.952461142015
4
Iteration 16700: Loss = -10830.921542081538
5
Iteration 16800: Loss = -10830.921209664755
Iteration 16900: Loss = -10830.927266902041
1
Iteration 17000: Loss = -10830.925067539953
2
Iteration 17100: Loss = -10830.94551265514
3
Iteration 17200: Loss = -10830.942782588718
4
Iteration 17300: Loss = -10830.921437300894
5
Iteration 17400: Loss = -10830.92115279174
Iteration 17500: Loss = -10830.922226929413
1
Iteration 17600: Loss = -10830.955195979852
2
Iteration 17700: Loss = -10830.935730298715
3
Iteration 17800: Loss = -10830.921142534622
Iteration 17900: Loss = -10830.92218023874
1
Iteration 18000: Loss = -10830.944019155935
2
Iteration 18100: Loss = -10830.962138962761
3
Iteration 18200: Loss = -10830.946365187483
4
Iteration 18300: Loss = -10830.921308091854
5
Iteration 18400: Loss = -10830.921224582016
Iteration 18500: Loss = -10830.922360802637
1
Iteration 18600: Loss = -10830.966990597542
2
Iteration 18700: Loss = -10830.923152517302
3
Iteration 18800: Loss = -10830.922895977865
4
Iteration 18900: Loss = -10830.921267777005
Iteration 19000: Loss = -10830.923656762923
1
Iteration 19100: Loss = -10830.924141381907
2
Iteration 19200: Loss = -10830.950140588613
3
Iteration 19300: Loss = -10830.92373395517
4
Iteration 19400: Loss = -10830.927853632158
5
Iteration 19500: Loss = -10830.922217865327
6
Iteration 19600: Loss = -10830.928325189574
7
Iteration 19700: Loss = -10830.922230346992
8
Iteration 19800: Loss = -10830.949348260108
9
Iteration 19900: Loss = -10830.956820634745
10
pi: tensor([[0.8026, 0.1974],
        [0.2144, 0.7856]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5434, 0.4566], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1939, 0.1009],
         [0.7151, 0.2613]],

        [[0.7050, 0.0951],
         [0.5237, 0.5011]],

        [[0.5174, 0.1031],
         [0.6370, 0.6076]],

        [[0.6339, 0.1148],
         [0.7303, 0.5999]],

        [[0.5491, 0.0951],
         [0.6024, 0.5793]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.8609059311743978
Average Adjusted Rand Index: 0.8619733572632187
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21023.021659420123
Iteration 100: Loss = -10983.962253484333
Iteration 200: Loss = -10983.107479629942
Iteration 300: Loss = -10982.713841456694
Iteration 400: Loss = -10982.520598000787
Iteration 500: Loss = -10982.408755759629
Iteration 600: Loss = -10982.30146397754
Iteration 700: Loss = -10982.081185949673
Iteration 800: Loss = -10981.80780332764
Iteration 900: Loss = -10981.621006414694
Iteration 1000: Loss = -10981.448456557388
Iteration 1100: Loss = -10981.272196957354
Iteration 1200: Loss = -10981.072527644514
Iteration 1300: Loss = -10980.83556728845
Iteration 1400: Loss = -10980.582449733785
Iteration 1500: Loss = -10980.35372879025
Iteration 1600: Loss = -10980.141761602466
Iteration 1700: Loss = -10979.931243078132
Iteration 1800: Loss = -10979.742985993285
Iteration 1900: Loss = -10979.597095291478
Iteration 2000: Loss = -10979.4771441773
Iteration 2100: Loss = -10979.38983856287
Iteration 2200: Loss = -10979.329202027951
Iteration 2300: Loss = -10979.304328873857
Iteration 2400: Loss = -10979.29297566487
Iteration 2500: Loss = -10979.285652448563
Iteration 2600: Loss = -10979.27909964534
Iteration 2700: Loss = -10979.271979981357
Iteration 2800: Loss = -10979.263619541125
Iteration 2900: Loss = -10979.253475614982
Iteration 3000: Loss = -10979.241313993105
Iteration 3100: Loss = -10979.22720610767
Iteration 3200: Loss = -10979.211746574418
Iteration 3300: Loss = -10979.196305367172
Iteration 3400: Loss = -10979.184158528784
Iteration 3500: Loss = -10979.171712765816
Iteration 3600: Loss = -10979.163638828788
Iteration 3700: Loss = -10979.161835479905
Iteration 3800: Loss = -10979.154071672134
Iteration 3900: Loss = -10979.151276129967
Iteration 4000: Loss = -10979.149269830006
Iteration 4100: Loss = -10979.147949430486
Iteration 4200: Loss = -10979.14685219474
Iteration 4300: Loss = -10979.146235363993
Iteration 4400: Loss = -10979.14543025934
Iteration 4500: Loss = -10979.153403088172
1
Iteration 4600: Loss = -10979.145237660932
Iteration 4700: Loss = -10979.146803354826
1
Iteration 4800: Loss = -10979.143961303414
Iteration 4900: Loss = -10979.143685382889
Iteration 5000: Loss = -10979.143289627382
Iteration 5100: Loss = -10979.14418279193
1
Iteration 5200: Loss = -10979.142537953378
Iteration 5300: Loss = -10979.14214750694
Iteration 5400: Loss = -10979.142410882
1
Iteration 5500: Loss = -10979.141085518182
Iteration 5600: Loss = -10979.140378865928
Iteration 5700: Loss = -10979.139632974748
Iteration 5800: Loss = -10979.139307470627
Iteration 5900: Loss = -10979.139848106915
1
Iteration 6000: Loss = -10979.13651897409
Iteration 6100: Loss = -10979.134653735005
Iteration 6200: Loss = -10979.13525809118
1
Iteration 6300: Loss = -10979.134913876831
2
Iteration 6400: Loss = -10979.135795729251
3
Iteration 6500: Loss = -10979.125540065552
Iteration 6600: Loss = -10979.123068193228
Iteration 6700: Loss = -10979.116149262058
Iteration 6800: Loss = -10979.061222212415
Iteration 6900: Loss = -10832.742039381654
Iteration 7000: Loss = -10831.801384939528
Iteration 7100: Loss = -10831.658002092754
Iteration 7200: Loss = -10831.593463691743
Iteration 7300: Loss = -10831.415314080426
Iteration 7400: Loss = -10831.383894414214
Iteration 7500: Loss = -10831.361997268967
Iteration 7600: Loss = -10831.317167723566
Iteration 7700: Loss = -10831.308508199088
Iteration 7800: Loss = -10831.302200516797
Iteration 7900: Loss = -10831.298874822765
Iteration 8000: Loss = -10831.293633928257
Iteration 8100: Loss = -10831.335908072202
1
Iteration 8200: Loss = -10831.203793232768
Iteration 8300: Loss = -10831.192641545138
Iteration 8400: Loss = -10831.189842081076
Iteration 8500: Loss = -10831.172599720134
Iteration 8600: Loss = -10831.167772376544
Iteration 8700: Loss = -10831.166589233313
Iteration 8800: Loss = -10831.163147724372
Iteration 8900: Loss = -10831.162478152652
Iteration 9000: Loss = -10831.161506310142
Iteration 9100: Loss = -10831.155019922573
Iteration 9200: Loss = -10831.152082536522
Iteration 9300: Loss = -10831.14962625788
Iteration 9400: Loss = -10831.149139165142
Iteration 9500: Loss = -10831.16285387993
1
Iteration 9600: Loss = -10831.141826154837
Iteration 9700: Loss = -10831.214157562024
1
Iteration 9800: Loss = -10831.136699744311
Iteration 9900: Loss = -10831.240756286534
1
Iteration 10000: Loss = -10831.132505637179
Iteration 10100: Loss = -10831.123293278386
Iteration 10200: Loss = -10831.122109681572
Iteration 10300: Loss = -10830.976771087715
Iteration 10400: Loss = -10830.968563052067
Iteration 10500: Loss = -10830.967726618386
Iteration 10600: Loss = -10830.992590936383
1
Iteration 10700: Loss = -10830.96653706759
Iteration 10800: Loss = -10830.965555698704
Iteration 10900: Loss = -10830.986742404708
1
Iteration 11000: Loss = -10831.056075724524
2
Iteration 11100: Loss = -10830.966684750168
3
Iteration 11200: Loss = -10830.963017603726
Iteration 11300: Loss = -10830.963312407943
1
Iteration 11400: Loss = -10830.972487065701
2
Iteration 11500: Loss = -10830.93039699557
Iteration 11600: Loss = -10830.928319094386
Iteration 11700: Loss = -10830.927317565725
Iteration 11800: Loss = -10830.929936182067
1
Iteration 11900: Loss = -10830.93138077737
2
Iteration 12000: Loss = -10830.926568813493
Iteration 12100: Loss = -10830.925354129582
Iteration 12200: Loss = -10830.924556094986
Iteration 12300: Loss = -10830.924585042754
Iteration 12400: Loss = -10830.925719746094
1
Iteration 12500: Loss = -10830.93398728234
2
Iteration 12600: Loss = -10830.92533785052
3
Iteration 12700: Loss = -10830.92399524498
Iteration 12800: Loss = -10830.924381675217
1
Iteration 12900: Loss = -10830.948155856311
2
Iteration 13000: Loss = -10831.015008181896
3
Iteration 13100: Loss = -10830.928316016305
4
Iteration 13200: Loss = -10830.942937252203
5
Iteration 13300: Loss = -10830.939766284377
6
Iteration 13400: Loss = -10830.952334658201
7
Iteration 13500: Loss = -10830.977677582048
8
Iteration 13600: Loss = -10830.927692499541
9
Iteration 13700: Loss = -10830.92477439302
10
Iteration 13800: Loss = -10831.005523496751
11
Iteration 13900: Loss = -10830.92277982098
Iteration 14000: Loss = -10830.924002307382
1
Iteration 14100: Loss = -10830.92274016119
Iteration 14200: Loss = -10830.923870307806
1
Iteration 14300: Loss = -10830.929957004342
2
Iteration 14400: Loss = -10830.923446079192
3
Iteration 14500: Loss = -10830.922989854402
4
Iteration 14600: Loss = -10830.923173648483
5
Iteration 14700: Loss = -10830.924080583874
6
Iteration 14800: Loss = -10830.925087911599
7
Iteration 14900: Loss = -10830.927425002717
8
Iteration 15000: Loss = -10830.926914318186
9
Iteration 15100: Loss = -10830.946393314945
10
Iteration 15200: Loss = -10830.922151152667
Iteration 15300: Loss = -10830.921768025088
Iteration 15400: Loss = -10831.231930708938
1
Iteration 15500: Loss = -10830.921691239138
Iteration 15600: Loss = -10830.922408796256
1
Iteration 15700: Loss = -10830.921310727532
Iteration 15800: Loss = -10830.932990200265
1
Iteration 15900: Loss = -10831.064785443705
2
Iteration 16000: Loss = -10830.936181156234
3
Iteration 16100: Loss = -10830.921828299834
4
Iteration 16200: Loss = -10830.9215672158
5
Iteration 16300: Loss = -10830.921960168602
6
Iteration 16400: Loss = -10830.928565751092
7
Iteration 16500: Loss = -10830.921191203963
Iteration 16600: Loss = -10830.92170658294
1
Iteration 16700: Loss = -10830.921221997201
Iteration 16800: Loss = -10830.925391240487
1
Iteration 16900: Loss = -10830.924894841657
2
Iteration 17000: Loss = -10830.921394682146
3
Iteration 17100: Loss = -10830.921227806966
Iteration 17200: Loss = -10830.92153714591
1
Iteration 17300: Loss = -10830.948631550178
2
Iteration 17400: Loss = -10831.028318819617
3
Iteration 17500: Loss = -10830.952073333034
4
Iteration 17600: Loss = -10830.956455963422
5
Iteration 17700: Loss = -10831.045989775597
6
Iteration 17800: Loss = -10830.931831322316
7
Iteration 17900: Loss = -10831.054722348683
8
Iteration 18000: Loss = -10830.928296185759
9
Iteration 18100: Loss = -10830.92128437621
Iteration 18200: Loss = -10830.92492119722
1
Iteration 18300: Loss = -10830.956437571282
2
Iteration 18400: Loss = -10831.010066587567
3
Iteration 18500: Loss = -10830.923098008509
4
Iteration 18600: Loss = -10830.922783106962
5
Iteration 18700: Loss = -10830.925406937788
6
Iteration 18800: Loss = -10830.948360825158
7
Iteration 18900: Loss = -10831.017784133472
8
Iteration 19000: Loss = -10830.921144976599
Iteration 19100: Loss = -10830.932219132861
1
Iteration 19200: Loss = -10830.951293959453
2
Iteration 19300: Loss = -10830.922861485737
3
Iteration 19400: Loss = -10830.932884015125
4
Iteration 19500: Loss = -10830.921140243503
Iteration 19600: Loss = -10830.92881772002
1
Iteration 19700: Loss = -10830.9211274361
Iteration 19800: Loss = -10830.927952487174
1
Iteration 19900: Loss = -10830.924226317276
2
pi: tensor([[0.7842, 0.2158],
        [0.1974, 0.8026]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4560, 0.5440], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2616, 0.1009],
         [0.7099, 0.1938]],

        [[0.5305, 0.0951],
         [0.7182, 0.6457]],

        [[0.6406, 0.1033],
         [0.5147, 0.5358]],

        [[0.5645, 0.1147],
         [0.5468, 0.5786]],

        [[0.5348, 0.0948],
         [0.5519, 0.5522]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
Global Adjusted Rand Index: 0.8609059311743978
Average Adjusted Rand Index: 0.8619733572632187
10875.040932281672
[0.8609059311743978, 0.8609059311743978] [0.8619733572632187, 0.8619733572632187] [10830.92390905655, 10830.924095375287]
-------------------------------------
This iteration is 10
True Objective function: Loss = -10902.742916528518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25487.386822831802
Iteration 100: Loss = -11016.15098036282
Iteration 200: Loss = -11015.399156665242
Iteration 300: Loss = -11015.245408247818
Iteration 400: Loss = -11015.161983893639
Iteration 500: Loss = -11015.099759041099
Iteration 600: Loss = -11015.035137076879
Iteration 700: Loss = -11014.941875271024
Iteration 800: Loss = -11014.832790422422
Iteration 900: Loss = -11014.709997750011
Iteration 1000: Loss = -11014.594492725377
Iteration 1100: Loss = -11014.489832324629
Iteration 1200: Loss = -11014.35121859606
Iteration 1300: Loss = -11014.163842292191
Iteration 1400: Loss = -11014.022748309304
Iteration 1500: Loss = -11013.87876962202
Iteration 1600: Loss = -11013.11096131904
Iteration 1700: Loss = -10938.244269717929
Iteration 1800: Loss = -10908.82448340279
Iteration 1900: Loss = -10908.44850349541
Iteration 2000: Loss = -10908.369370884107
Iteration 2100: Loss = -10908.346812115595
Iteration 2200: Loss = -10908.328509540583
Iteration 2300: Loss = -10908.293548448546
Iteration 2400: Loss = -10905.368579343693
Iteration 2500: Loss = -10905.355685571401
Iteration 2600: Loss = -10905.320190740415
Iteration 2700: Loss = -10905.288873820697
Iteration 2800: Loss = -10903.377115381665
Iteration 2900: Loss = -10903.33599927607
Iteration 3000: Loss = -10903.035474667427
Iteration 3100: Loss = -10901.842520320382
Iteration 3200: Loss = -10878.627303291552
Iteration 3300: Loss = -10873.213894828084
Iteration 3400: Loss = -10860.942782883114
Iteration 3500: Loss = -10860.889606370143
Iteration 3600: Loss = -10860.887899558886
Iteration 3700: Loss = -10860.892001797649
1
Iteration 3800: Loss = -10860.88600168756
Iteration 3900: Loss = -10860.885378784116
Iteration 4000: Loss = -10860.885280122782
Iteration 4100: Loss = -10860.883438658162
Iteration 4200: Loss = -10860.882979475888
Iteration 4300: Loss = -10860.8825176987
Iteration 4400: Loss = -10860.882210813186
Iteration 4500: Loss = -10860.882044697926
Iteration 4600: Loss = -10860.880985538159
Iteration 4700: Loss = -10860.875787945732
Iteration 4800: Loss = -10860.875694449864
Iteration 4900: Loss = -10860.875603528453
Iteration 5000: Loss = -10860.881310655168
1
Iteration 5100: Loss = -10860.875471620477
Iteration 5200: Loss = -10860.875377280072
Iteration 5300: Loss = -10860.875318566415
Iteration 5400: Loss = -10860.875044314134
Iteration 5500: Loss = -10860.884505341059
1
Iteration 5600: Loss = -10860.844063794373
Iteration 5700: Loss = -10860.87889970693
1
Iteration 5800: Loss = -10860.843897077551
Iteration 5900: Loss = -10860.84390347097
Iteration 6000: Loss = -10860.843756937276
Iteration 6100: Loss = -10860.843673783862
Iteration 6200: Loss = -10860.84567186399
1
Iteration 6300: Loss = -10860.843666689743
Iteration 6400: Loss = -10860.843592376776
Iteration 6500: Loss = -10860.843522020277
Iteration 6600: Loss = -10860.843965002861
1
Iteration 6700: Loss = -10860.843398591649
Iteration 6800: Loss = -10860.855499071713
1
Iteration 6900: Loss = -10860.843267197073
Iteration 7000: Loss = -10860.877504894572
1
Iteration 7100: Loss = -10860.842800676837
Iteration 7200: Loss = -10860.842387648803
Iteration 7300: Loss = -10860.841197717295
Iteration 7400: Loss = -10860.87813971897
1
Iteration 7500: Loss = -10860.840132190833
Iteration 7600: Loss = -10860.840351960363
1
Iteration 7700: Loss = -10860.840112384743
Iteration 7800: Loss = -10860.841421436438
1
Iteration 7900: Loss = -10860.838555253331
Iteration 8000: Loss = -10860.838422745923
Iteration 8100: Loss = -10860.838368091752
Iteration 8200: Loss = -10860.839520916028
1
Iteration 8300: Loss = -10860.83953014948
2
Iteration 8400: Loss = -10860.838044979126
Iteration 8500: Loss = -10860.891358996756
1
Iteration 8600: Loss = -10860.837363967556
Iteration 8700: Loss = -10860.85866814681
1
Iteration 8800: Loss = -10860.837284921834
Iteration 8900: Loss = -10860.838453212375
1
Iteration 9000: Loss = -10860.849145486294
2
Iteration 9100: Loss = -10860.83791408279
3
Iteration 9200: Loss = -10860.83860520757
4
Iteration 9300: Loss = -10860.85826925979
5
Iteration 9400: Loss = -10860.837268250023
Iteration 9500: Loss = -10860.837486455383
1
Iteration 9600: Loss = -10860.837216046446
Iteration 9700: Loss = -10860.83829585197
1
Iteration 9800: Loss = -10860.839011945216
2
Iteration 9900: Loss = -10860.863320806504
3
Iteration 10000: Loss = -10860.848362066281
4
Iteration 10100: Loss = -10860.838041270525
5
Iteration 10200: Loss = -10860.840447063694
6
Iteration 10300: Loss = -10860.83995990013
7
Iteration 10400: Loss = -10860.837541925715
8
Iteration 10500: Loss = -10860.841074013717
9
Iteration 10600: Loss = -10860.838295620491
10
Iteration 10700: Loss = -10860.83723556723
Iteration 10800: Loss = -10860.861024571204
1
Iteration 10900: Loss = -10860.866507325107
2
Iteration 11000: Loss = -10860.848984918039
3
Iteration 11100: Loss = -10860.83919465917
4
Iteration 11200: Loss = -10860.83820629946
5
Iteration 11300: Loss = -10860.837595744704
6
Iteration 11400: Loss = -10860.840581324455
7
Iteration 11500: Loss = -10860.85289995111
8
Iteration 11600: Loss = -10860.848344253009
9
Iteration 11700: Loss = -10860.8373601975
10
Iteration 11800: Loss = -10860.838086657996
11
Iteration 11900: Loss = -10860.838706968905
12
Iteration 12000: Loss = -10860.837470364511
13
Iteration 12100: Loss = -10860.837126304386
Iteration 12200: Loss = -10860.837886567657
1
Iteration 12300: Loss = -10860.837612702218
2
Iteration 12400: Loss = -10860.838208481506
3
Iteration 12500: Loss = -10860.838377842172
4
Iteration 12600: Loss = -10860.84572270498
5
Iteration 12700: Loss = -10860.842141299787
6
Iteration 12800: Loss = -10860.91359245139
7
Iteration 12900: Loss = -10860.853969109592
8
Iteration 13000: Loss = -10860.840862430812
9
Iteration 13100: Loss = -10860.871768460102
10
Iteration 13200: Loss = -10860.849754206634
11
Iteration 13300: Loss = -10860.840530728392
12
Iteration 13400: Loss = -10860.83757184771
13
Iteration 13500: Loss = -10860.838627607447
14
Iteration 13600: Loss = -10860.866252110603
15
Stopping early at iteration 13600 due to no improvement.
pi: tensor([[0.7681, 0.2319],
        [0.2254, 0.7746]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4929, 0.5071], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1999, 0.1000],
         [0.6895, 0.2540]],

        [[0.7156, 0.0991],
         [0.5121, 0.6130]],

        [[0.7225, 0.1064],
         [0.5207, 0.5711]],

        [[0.7024, 0.1029],
         [0.5709, 0.6652]],

        [[0.7154, 0.0905],
         [0.6379, 0.5517]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6690636805379029
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 93
Adjusted Rand Index: 0.7369552685595733
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8241117949725959
Average Adjusted Rand Index: 0.8276247361987583
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20881.091311484273
Iteration 100: Loss = -11016.344019258802
Iteration 200: Loss = -11015.32989847171
Iteration 300: Loss = -11015.131948855578
Iteration 400: Loss = -11015.044454490904
Iteration 500: Loss = -11014.966734633701
Iteration 600: Loss = -11014.874312332473
Iteration 700: Loss = -11014.760928137739
Iteration 800: Loss = -11014.634144747006
Iteration 900: Loss = -11014.505605426233
Iteration 1000: Loss = -11014.384519496523
Iteration 1100: Loss = -11014.27347831979
Iteration 1200: Loss = -11014.173456435034
Iteration 1300: Loss = -11014.0855921608
Iteration 1400: Loss = -11014.00975258313
Iteration 1500: Loss = -11013.94298350643
Iteration 1600: Loss = -11013.876972942116
Iteration 1700: Loss = -11013.792104938402
Iteration 1800: Loss = -11013.623312425874
Iteration 1900: Loss = -11013.093812026404
Iteration 2000: Loss = -11005.978799151995
Iteration 2100: Loss = -10927.616787162304
Iteration 2200: Loss = -10916.659740798294
Iteration 2300: Loss = -10905.361837001743
Iteration 2400: Loss = -10903.37032149189
Iteration 2500: Loss = -10903.295705315652
Iteration 2600: Loss = -10903.242510773784
Iteration 2700: Loss = -10903.155103704474
Iteration 2800: Loss = -10902.976059718736
Iteration 2900: Loss = -10902.552883079858
Iteration 3000: Loss = -10902.139015864654
Iteration 3100: Loss = -10900.104243680817
Iteration 3200: Loss = -10873.70350015481
Iteration 3300: Loss = -10861.092138942151
Iteration 3400: Loss = -10860.974192619882
Iteration 3500: Loss = -10860.955420348351
Iteration 3600: Loss = -10860.880732793039
Iteration 3700: Loss = -10860.866611774256
Iteration 3800: Loss = -10860.862643322658
Iteration 3900: Loss = -10860.859387350249
Iteration 4000: Loss = -10860.855478068685
Iteration 4100: Loss = -10860.851274115861
Iteration 4200: Loss = -10860.854201177835
1
Iteration 4300: Loss = -10860.848515628846
Iteration 4400: Loss = -10860.846774299289
Iteration 4500: Loss = -10860.844617453822
Iteration 4600: Loss = -10860.843191479255
Iteration 4700: Loss = -10860.842502792288
Iteration 4800: Loss = -10860.842992951671
1
Iteration 4900: Loss = -10860.841855233899
Iteration 5000: Loss = -10860.843398028848
1
Iteration 5100: Loss = -10860.84544753231
2
Iteration 5200: Loss = -10860.850730140519
3
Iteration 5300: Loss = -10860.841881765524
Iteration 5400: Loss = -10860.840528694092
Iteration 5500: Loss = -10860.840222321072
Iteration 5600: Loss = -10860.840117120817
Iteration 5700: Loss = -10860.845904511623
1
Iteration 5800: Loss = -10860.839733391904
Iteration 5900: Loss = -10860.839659739095
Iteration 6000: Loss = -10860.839466451327
Iteration 6100: Loss = -10860.839345901506
Iteration 6200: Loss = -10860.83917334659
Iteration 6300: Loss = -10860.839095787918
Iteration 6400: Loss = -10860.83910064373
Iteration 6500: Loss = -10860.839746422247
1
Iteration 6600: Loss = -10860.85041835443
2
Iteration 6700: Loss = -10860.84093898507
3
Iteration 6800: Loss = -10860.83922742099
4
Iteration 6900: Loss = -10860.839367649382
5
Iteration 7000: Loss = -10860.838707575671
Iteration 7100: Loss = -10860.838645029922
Iteration 7200: Loss = -10860.838603924807
Iteration 7300: Loss = -10860.838563481328
Iteration 7400: Loss = -10860.838542405503
Iteration 7500: Loss = -10860.83860757623
Iteration 7600: Loss = -10860.838469870385
Iteration 7700: Loss = -10860.838468674776
Iteration 7800: Loss = -10860.83841488811
Iteration 7900: Loss = -10860.838398285618
Iteration 8000: Loss = -10860.838411003331
Iteration 8100: Loss = -10860.838377175302
Iteration 8200: Loss = -10860.83832171257
Iteration 8300: Loss = -10860.838271641935
Iteration 8400: Loss = -10860.838340400258
Iteration 8500: Loss = -10860.83823611798
Iteration 8600: Loss = -10860.922981276954
1
Iteration 8700: Loss = -10860.838223013012
Iteration 8800: Loss = -10860.854259319482
1
Iteration 8900: Loss = -10860.838111355286
Iteration 9000: Loss = -10860.83804233583
Iteration 9100: Loss = -10860.83853862968
1
Iteration 9200: Loss = -10860.837792959679
Iteration 9300: Loss = -10860.837527240628
Iteration 9400: Loss = -10860.837891150082
1
Iteration 9500: Loss = -10860.837229400298
Iteration 9600: Loss = -10860.840707566156
1
Iteration 9700: Loss = -10860.837247480753
Iteration 9800: Loss = -10860.839541543859
1
Iteration 9900: Loss = -10860.895740745378
2
Iteration 10000: Loss = -10860.949137657
3
Iteration 10100: Loss = -10860.880599611526
4
Iteration 10200: Loss = -10860.840305728783
5
Iteration 10300: Loss = -10860.890970940754
6
Iteration 10400: Loss = -10860.848964147688
7
Iteration 10500: Loss = -10860.8371795685
Iteration 10600: Loss = -10860.837273466263
Iteration 10700: Loss = -10860.939253324184
1
Iteration 10800: Loss = -10860.837370100615
Iteration 10900: Loss = -10860.837300885902
Iteration 11000: Loss = -10860.838529492094
1
Iteration 11100: Loss = -10860.848506258544
2
Iteration 11200: Loss = -10860.83876947559
3
Iteration 11300: Loss = -10860.837236117724
Iteration 11400: Loss = -10860.842534748554
1
Iteration 11500: Loss = -10860.837724024594
2
Iteration 11600: Loss = -10860.837124554067
Iteration 11700: Loss = -10860.83725898013
1
Iteration 11800: Loss = -10860.879266128077
2
Iteration 11900: Loss = -10860.837094085971
Iteration 12000: Loss = -10860.837193996644
Iteration 12100: Loss = -10860.985036647828
1
Iteration 12200: Loss = -10860.83889097664
2
Iteration 12300: Loss = -10860.837129091982
Iteration 12400: Loss = -10860.838838450305
1
Iteration 12500: Loss = -10860.838054724309
2
Iteration 12600: Loss = -10860.838865435922
3
Iteration 12700: Loss = -10860.84155121475
4
Iteration 12800: Loss = -10860.846112180632
5
Iteration 12900: Loss = -10860.841035197798
6
Iteration 13000: Loss = -10860.837234460721
7
Iteration 13100: Loss = -10860.837162961863
Iteration 13200: Loss = -10860.901011557155
1
Iteration 13300: Loss = -10860.83779228107
2
Iteration 13400: Loss = -10860.837095684103
Iteration 13500: Loss = -10860.842391564873
1
Iteration 13600: Loss = -10860.949923903025
2
Iteration 13700: Loss = -10860.855359742407
3
Iteration 13800: Loss = -10860.843897742547
4
Iteration 13900: Loss = -10860.83707266472
Iteration 14000: Loss = -10860.83711382661
Iteration 14100: Loss = -10860.844347772563
1
Iteration 14200: Loss = -10860.95772027736
2
Iteration 14300: Loss = -10860.861241276754
3
Iteration 14400: Loss = -10860.839417469122
4
Iteration 14500: Loss = -10860.840041736097
5
Iteration 14600: Loss = -10860.85673739705
6
Iteration 14700: Loss = -10860.858129852766
7
Iteration 14800: Loss = -10860.980615131739
8
Iteration 14900: Loss = -10860.849878403364
9
Iteration 15000: Loss = -10860.8373729017
10
Iteration 15100: Loss = -10860.837104117334
Iteration 15200: Loss = -10860.838270153738
1
Iteration 15300: Loss = -10860.866318995899
2
Iteration 15400: Loss = -10860.847818435761
3
Iteration 15500: Loss = -10860.84594975955
4
Iteration 15600: Loss = -10860.838524378543
5
Iteration 15700: Loss = -10860.842108959809
6
Iteration 15800: Loss = -10860.837087017928
Iteration 15900: Loss = -10860.837203990697
1
Iteration 16000: Loss = -10860.862074955348
2
Iteration 16100: Loss = -10860.837021686562
Iteration 16200: Loss = -10860.83833956067
1
Iteration 16300: Loss = -10860.837229383833
2
Iteration 16400: Loss = -10860.837430713065
3
Iteration 16500: Loss = -10860.869751478893
4
Iteration 16600: Loss = -10860.84773884951
5
Iteration 16700: Loss = -10861.00803540219
6
Iteration 16800: Loss = -10860.841289038683
7
Iteration 16900: Loss = -10860.841907895856
8
Iteration 17000: Loss = -10860.837168592769
9
Iteration 17100: Loss = -10860.837091778463
Iteration 17200: Loss = -10860.85210074009
1
Iteration 17300: Loss = -10860.843412332277
2
Iteration 17400: Loss = -10860.875168297958
3
Iteration 17500: Loss = -10860.960892953668
4
Iteration 17600: Loss = -10860.837631573047
5
Iteration 17700: Loss = -10860.83708503796
Iteration 17800: Loss = -10860.837499751862
1
Iteration 17900: Loss = -10860.838782171442
2
Iteration 18000: Loss = -10860.84921223017
3
Iteration 18100: Loss = -10860.943917430086
4
Iteration 18200: Loss = -10860.837293074892
5
Iteration 18300: Loss = -10860.83705288704
Iteration 18400: Loss = -10860.854320066986
1
Iteration 18500: Loss = -10860.841982609396
2
Iteration 18600: Loss = -10860.83805555738
3
Iteration 18700: Loss = -10860.844648153392
4
Iteration 18800: Loss = -10860.837346170032
5
Iteration 18900: Loss = -10860.83716362309
6
Iteration 19000: Loss = -10860.838575653324
7
Iteration 19100: Loss = -10860.845358749426
8
Iteration 19200: Loss = -10860.842605493903
9
Iteration 19300: Loss = -10861.019548711547
10
Iteration 19400: Loss = -10860.845912785564
11
Iteration 19500: Loss = -10860.838177903996
12
Iteration 19600: Loss = -10860.837239247368
13
Iteration 19700: Loss = -10860.838289021658
14
Iteration 19800: Loss = -10860.84768497066
15
Stopping early at iteration 19800 due to no improvement.
pi: tensor([[0.7757, 0.2243],
        [0.2324, 0.7676]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5085, 0.4915], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2534, 0.0999],
         [0.6072, 0.2003]],

        [[0.5996, 0.0989],
         [0.5568, 0.7006]],

        [[0.6205, 0.1068],
         [0.6454, 0.6283]],

        [[0.6150, 0.1034],
         [0.6882, 0.6785]],

        [[0.5558, 0.0900],
         [0.5446, 0.5156]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6690636805379029
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8241117949725959
Average Adjusted Rand Index: 0.8276247361987583
10902.742916528518
[0.8241117949725959, 0.8241117949725959] [0.8276247361987583, 0.8276247361987583] [10860.866252110603, 10860.84768497066]
-------------------------------------
This iteration is 11
True Objective function: Loss = -10753.453751409359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21306.913858876127
Iteration 100: Loss = -10842.391641100245
Iteration 200: Loss = -10841.9728614958
Iteration 300: Loss = -10841.697769314203
Iteration 400: Loss = -10841.513141131281
Iteration 500: Loss = -10841.475126853727
Iteration 600: Loss = -10841.447652266485
Iteration 700: Loss = -10841.423952475168
Iteration 800: Loss = -10841.401279793305
Iteration 900: Loss = -10841.378221652627
Iteration 1000: Loss = -10841.353871759218
Iteration 1100: Loss = -10841.326533373564
Iteration 1200: Loss = -10841.284639085125
Iteration 1300: Loss = -10841.035611205141
Iteration 1400: Loss = -10840.45011039101
Iteration 1500: Loss = -10839.57891968306
Iteration 1600: Loss = -10839.331551413057
Iteration 1700: Loss = -10839.253564037135
Iteration 1800: Loss = -10839.214363451154
Iteration 1900: Loss = -10839.19109547032
Iteration 2000: Loss = -10839.175182784169
Iteration 2100: Loss = -10839.15800348355
Iteration 2200: Loss = -10839.129609733924
Iteration 2300: Loss = -10839.122547217183
Iteration 2400: Loss = -10839.117076199995
Iteration 2500: Loss = -10839.112657802127
Iteration 2600: Loss = -10839.108846230884
Iteration 2700: Loss = -10839.104788273191
Iteration 2800: Loss = -10839.094480499063
Iteration 2900: Loss = -10839.091927762316
Iteration 3000: Loss = -10839.090039895782
Iteration 3100: Loss = -10839.088425198293
Iteration 3200: Loss = -10839.086988370726
Iteration 3300: Loss = -10839.085786291562
Iteration 3400: Loss = -10839.084616548076
Iteration 3500: Loss = -10839.083355709326
Iteration 3600: Loss = -10839.081440138863
Iteration 3700: Loss = -10839.080136044779
Iteration 3800: Loss = -10839.079085765698
Iteration 3900: Loss = -10839.077610145232
Iteration 4000: Loss = -10839.076694183832
Iteration 4100: Loss = -10839.075963593023
Iteration 4200: Loss = -10839.075088135034
Iteration 4300: Loss = -10839.073947931589
Iteration 4400: Loss = -10839.072874541584
Iteration 4500: Loss = -10839.060899129878
Iteration 4600: Loss = -10839.060183463065
Iteration 4700: Loss = -10839.05981875457
Iteration 4800: Loss = -10839.059471713113
Iteration 4900: Loss = -10839.059185285729
Iteration 5000: Loss = -10839.058905929987
Iteration 5100: Loss = -10839.05868131303
Iteration 5200: Loss = -10839.058389048423
Iteration 5300: Loss = -10839.058088971131
Iteration 5400: Loss = -10839.057573008762
Iteration 5500: Loss = -10839.057002638752
Iteration 5600: Loss = -10839.056583754635
Iteration 5700: Loss = -10839.054703185906
Iteration 5800: Loss = -10839.053937900726
Iteration 5900: Loss = -10839.053724114965
Iteration 6000: Loss = -10839.053583094234
Iteration 6100: Loss = -10839.053437409984
Iteration 6200: Loss = -10839.053209361175
Iteration 6300: Loss = -10839.052468140357
Iteration 6400: Loss = -10839.05228081952
Iteration 6500: Loss = -10839.052162253325
Iteration 6600: Loss = -10839.05206055015
Iteration 6700: Loss = -10839.051981706138
Iteration 6800: Loss = -10839.051914549635
Iteration 6900: Loss = -10839.051803550983
Iteration 7000: Loss = -10839.051762552546
Iteration 7100: Loss = -10839.051696557477
Iteration 7200: Loss = -10839.051599514087
Iteration 7300: Loss = -10839.051566962067
Iteration 7400: Loss = -10839.051489587753
Iteration 7500: Loss = -10839.051469412232
Iteration 7600: Loss = -10839.051378713051
Iteration 7700: Loss = -10839.051310428744
Iteration 7800: Loss = -10839.05134402962
Iteration 7900: Loss = -10839.054399166092
1
Iteration 8000: Loss = -10839.052166784204
2
Iteration 8100: Loss = -10839.275438427003
3
Iteration 8200: Loss = -10839.050275297026
Iteration 8300: Loss = -10839.084039493118
1
Iteration 8400: Loss = -10839.04840817738
Iteration 8500: Loss = -10839.045816274047
Iteration 8600: Loss = -10839.045315526451
Iteration 8700: Loss = -10839.045074090925
Iteration 8800: Loss = -10839.070759594308
1
Iteration 8900: Loss = -10839.044995785915
Iteration 9000: Loss = -10839.044974702361
Iteration 9100: Loss = -10839.07499925764
1
Iteration 9200: Loss = -10839.044916069013
Iteration 9300: Loss = -10839.044865444108
Iteration 9400: Loss = -10839.044432538663
Iteration 9500: Loss = -10839.044763295184
1
Iteration 9600: Loss = -10839.044371990052
Iteration 9700: Loss = -10839.044336724563
Iteration 9800: Loss = -10839.050200307418
1
Iteration 9900: Loss = -10839.044339621363
Iteration 10000: Loss = -10839.044271892284
Iteration 10100: Loss = -10839.260346228695
1
Iteration 10200: Loss = -10839.044293456593
Iteration 10300: Loss = -10839.041124362271
Iteration 10400: Loss = -10839.433825017415
1
Iteration 10500: Loss = -10839.033047508563
Iteration 10600: Loss = -10839.033000363426
Iteration 10700: Loss = -10839.236009961516
1
Iteration 10800: Loss = -10839.032945473218
Iteration 10900: Loss = -10839.0329784728
Iteration 11000: Loss = -10839.03687396875
1
Iteration 11100: Loss = -10839.032964409449
Iteration 11200: Loss = -10839.032944582445
Iteration 11300: Loss = -10839.032937664715
Iteration 11400: Loss = -10839.033841016386
1
Iteration 11500: Loss = -10839.032894096212
Iteration 11600: Loss = -10839.032538774532
Iteration 11700: Loss = -10839.169434351183
1
Iteration 11800: Loss = -10839.032575904614
Iteration 11900: Loss = -10839.031944764509
Iteration 12000: Loss = -10839.094506244697
1
Iteration 12100: Loss = -10839.03171806298
Iteration 12200: Loss = -10839.031639852941
Iteration 12300: Loss = -10839.421724142836
1
Iteration 12400: Loss = -10839.031643497494
Iteration 12500: Loss = -10839.031635661331
Iteration 12600: Loss = -10839.031666648068
Iteration 12700: Loss = -10839.031783466782
1
Iteration 12800: Loss = -10839.031623684512
Iteration 12900: Loss = -10839.031664120599
Iteration 13000: Loss = -10839.031658505997
Iteration 13100: Loss = -10839.031640107267
Iteration 13200: Loss = -10839.031614826352
Iteration 13300: Loss = -10839.032390314413
1
Iteration 13400: Loss = -10839.031625415475
Iteration 13500: Loss = -10839.031578125823
Iteration 13600: Loss = -10839.032134368565
1
Iteration 13700: Loss = -10839.031617406145
Iteration 13800: Loss = -10839.034064190071
1
Iteration 13900: Loss = -10839.031635472576
Iteration 14000: Loss = -10839.039570395911
1
Iteration 14100: Loss = -10839.029432972213
Iteration 14200: Loss = -10839.02961533569
1
Iteration 14300: Loss = -10839.029861539953
2
Iteration 14400: Loss = -10839.03266391506
3
Iteration 14500: Loss = -10839.030719884442
4
Iteration 14600: Loss = -10839.029337995888
Iteration 14700: Loss = -10839.03001045906
1
Iteration 14800: Loss = -10839.031756467477
2
Iteration 14900: Loss = -10839.029200727686
Iteration 15000: Loss = -10839.029323924244
1
Iteration 15100: Loss = -10839.028872536954
Iteration 15200: Loss = -10839.028831630518
Iteration 15300: Loss = -10839.03276634864
1
Iteration 15400: Loss = -10839.031186145063
2
Iteration 15500: Loss = -10839.028822581586
Iteration 15600: Loss = -10839.029395551339
1
Iteration 15700: Loss = -10839.031888145286
2
Iteration 15800: Loss = -10839.028800319802
Iteration 15900: Loss = -10839.035400135148
1
Iteration 16000: Loss = -10839.142328404208
2
Iteration 16100: Loss = -10839.028378206749
Iteration 16200: Loss = -10839.028364394988
Iteration 16300: Loss = -10839.252141365756
1
Iteration 16400: Loss = -10839.028063714413
Iteration 16500: Loss = -10839.048123866316
1
Iteration 16600: Loss = -10839.028041558413
Iteration 16700: Loss = -10839.037342763024
1
Iteration 16800: Loss = -10839.028030688112
Iteration 16900: Loss = -10839.043512802104
1
Iteration 17000: Loss = -10839.028017124412
Iteration 17100: Loss = -10839.086639844936
1
Iteration 17200: Loss = -10839.02801810999
Iteration 17300: Loss = -10839.02801424867
Iteration 17400: Loss = -10839.028146822164
1
Iteration 17500: Loss = -10839.0277492178
Iteration 17600: Loss = -10839.038276375331
1
Iteration 17700: Loss = -10839.027763625025
Iteration 17800: Loss = -10839.02773154346
Iteration 17900: Loss = -10839.092754282601
1
Iteration 18000: Loss = -10839.027373564266
Iteration 18100: Loss = -10839.041962716537
1
Iteration 18200: Loss = -10839.025509587065
Iteration 18300: Loss = -10839.071514630527
1
Iteration 18400: Loss = -10839.025513520377
Iteration 18500: Loss = -10839.1558270696
1
Iteration 18600: Loss = -10839.024456869784
Iteration 18700: Loss = -10839.058903447483
1
Iteration 18800: Loss = -10839.02433710209
Iteration 18900: Loss = -10839.024112117937
Iteration 19000: Loss = -10839.025215736794
1
Iteration 19100: Loss = -10839.024763362475
2
Iteration 19200: Loss = -10839.02403315532
Iteration 19300: Loss = -10839.024096639607
Iteration 19400: Loss = -10839.084267000853
1
Iteration 19500: Loss = -10839.025801789932
2
Iteration 19600: Loss = -10839.02402818166
Iteration 19700: Loss = -10839.024216491272
1
Iteration 19800: Loss = -10839.100134491166
2
Iteration 19900: Loss = -10839.023987070928
pi: tensor([[1.0000e+00, 2.7898e-07],
        [8.2293e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0291, 0.9709], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2807, 0.2462],
         [0.5535, 0.1574]],

        [[0.5270, 0.2034],
         [0.7266, 0.7071]],

        [[0.6063, 0.1849],
         [0.5637, 0.5124]],

        [[0.5611, 0.1693],
         [0.6336, 0.6946]],

        [[0.5565, 0.1290],
         [0.6229, 0.6333]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 0.0010524474355843583
Average Adjusted Rand Index: -0.0001307858112401553
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22913.4541760237
Iteration 100: Loss = -10844.21280042789
Iteration 200: Loss = -10842.718627939323
Iteration 300: Loss = -10842.28064020606
Iteration 400: Loss = -10842.1162783485
Iteration 500: Loss = -10842.026365819858
Iteration 600: Loss = -10841.944080336933
Iteration 700: Loss = -10841.773641459085
Iteration 800: Loss = -10841.254391893735
Iteration 900: Loss = -10841.127801508752
Iteration 1000: Loss = -10841.062615284302
Iteration 1100: Loss = -10840.997947035554
Iteration 1200: Loss = -10840.921460278849
Iteration 1300: Loss = -10840.826854033638
Iteration 1400: Loss = -10840.719276737304
Iteration 1500: Loss = -10840.607688643931
Iteration 1600: Loss = -10840.494624246803
Iteration 1700: Loss = -10840.369515573697
Iteration 1800: Loss = -10840.228577007985
Iteration 1900: Loss = -10840.09019490925
Iteration 2000: Loss = -10839.939675981894
Iteration 2100: Loss = -10839.788231859182
Iteration 2200: Loss = -10839.65240845608
Iteration 2300: Loss = -10839.540706208138
Iteration 2400: Loss = -10839.4521347445
Iteration 2500: Loss = -10839.382489467234
Iteration 2600: Loss = -10839.32828748107
Iteration 2700: Loss = -10839.284482470746
Iteration 2800: Loss = -10839.24777644724
Iteration 2900: Loss = -10839.218419222294
Iteration 3000: Loss = -10839.194821831816
Iteration 3100: Loss = -10839.175093958673
Iteration 3200: Loss = -10839.158277462187
Iteration 3300: Loss = -10839.143894880208
Iteration 3400: Loss = -10839.131722660928
Iteration 3500: Loss = -10839.12132835371
Iteration 3600: Loss = -10839.112445800356
Iteration 3700: Loss = -10839.104758583182
Iteration 3800: Loss = -10839.097985070748
Iteration 3900: Loss = -10839.092004611124
Iteration 4000: Loss = -10839.08665765194
Iteration 4100: Loss = -10839.081866583303
Iteration 4200: Loss = -10839.077526465684
Iteration 4300: Loss = -10839.073585932376
Iteration 4400: Loss = -10839.07000211043
Iteration 4500: Loss = -10839.066735699596
Iteration 4600: Loss = -10839.063705417222
Iteration 4700: Loss = -10839.060951021676
Iteration 4800: Loss = -10839.05836956334
Iteration 4900: Loss = -10839.055990449606
Iteration 5000: Loss = -10839.0537252974
Iteration 5100: Loss = -10839.051673822016
Iteration 5200: Loss = -10839.049767085808
Iteration 5300: Loss = -10839.047996534868
Iteration 5400: Loss = -10839.04642642094
Iteration 5500: Loss = -10839.044939138934
Iteration 5600: Loss = -10839.04353693779
Iteration 5700: Loss = -10839.042295183246
Iteration 5800: Loss = -10839.041109374093
Iteration 5900: Loss = -10839.039943160602
Iteration 6000: Loss = -10839.038888345032
Iteration 6100: Loss = -10839.03790741075
Iteration 6200: Loss = -10839.036961559237
Iteration 6300: Loss = -10839.036093062758
Iteration 6400: Loss = -10839.035176351841
Iteration 6500: Loss = -10839.034341139875
Iteration 6600: Loss = -10839.033589437295
Iteration 6700: Loss = -10839.032931517126
Iteration 6800: Loss = -10839.032271543456
Iteration 6900: Loss = -10839.031715242703
Iteration 7000: Loss = -10839.031154574408
Iteration 7100: Loss = -10839.030634553228
Iteration 7200: Loss = -10839.030547781435
Iteration 7300: Loss = -10839.029677197801
Iteration 7400: Loss = -10839.02920984874
Iteration 7500: Loss = -10839.028833215674
Iteration 7600: Loss = -10839.028439408537
Iteration 7700: Loss = -10839.028068567739
Iteration 7800: Loss = -10839.029061498224
1
Iteration 7900: Loss = -10839.02739515087
Iteration 8000: Loss = -10839.027562506773
1
Iteration 8100: Loss = -10839.028734647794
2
Iteration 8200: Loss = -10839.072997990277
3
Iteration 8300: Loss = -10839.026408739479
Iteration 8400: Loss = -10839.026083431214
Iteration 8500: Loss = -10839.055449476915
1
Iteration 8600: Loss = -10839.025567394712
Iteration 8700: Loss = -10839.035315004267
1
Iteration 8800: Loss = -10839.025196491368
Iteration 8900: Loss = -10839.024997151368
Iteration 9000: Loss = -10839.034646078979
1
Iteration 9100: Loss = -10839.02458838179
Iteration 9200: Loss = -10839.024438300685
Iteration 9300: Loss = -10839.02575527471
1
Iteration 9400: Loss = -10839.024055500378
Iteration 9500: Loss = -10839.0238380039
Iteration 9600: Loss = -10839.023600431388
Iteration 9700: Loss = -10839.023129615882
Iteration 9800: Loss = -10839.022966854594
Iteration 9900: Loss = -10839.04644837559
1
Iteration 10000: Loss = -10839.022674903978
Iteration 10100: Loss = -10839.022594888333
Iteration 10200: Loss = -10839.026817618353
1
Iteration 10300: Loss = -10839.02238511247
Iteration 10400: Loss = -10839.022263256451
Iteration 10500: Loss = -10839.02219011615
Iteration 10600: Loss = -10839.022102583125
Iteration 10700: Loss = -10839.02206601965
Iteration 10800: Loss = -10839.02196777786
Iteration 10900: Loss = -10839.030547342176
1
Iteration 11000: Loss = -10839.021863576208
Iteration 11100: Loss = -10839.021802932568
Iteration 11200: Loss = -10839.1776703228
1
Iteration 11300: Loss = -10839.021608901341
Iteration 11400: Loss = -10839.02158809592
Iteration 11500: Loss = -10839.02141163053
Iteration 11600: Loss = -10839.023782701202
1
Iteration 11700: Loss = -10839.020918746435
Iteration 11800: Loss = -10839.020747249113
Iteration 11900: Loss = -10839.054396203315
1
Iteration 12000: Loss = -10839.020301679531
Iteration 12100: Loss = -10839.02021563584
Iteration 12200: Loss = -10839.415655680092
1
Iteration 12300: Loss = -10839.02009682836
Iteration 12400: Loss = -10839.02005544794
Iteration 12500: Loss = -10839.02001345472
Iteration 12600: Loss = -10839.021509365784
1
Iteration 12700: Loss = -10839.019966317963
Iteration 12800: Loss = -10839.019949967458
Iteration 12900: Loss = -10839.238613421167
1
Iteration 13000: Loss = -10839.01986802038
Iteration 13100: Loss = -10839.019820942047
Iteration 13200: Loss = -10839.019940059457
1
Iteration 13300: Loss = -10839.019903733564
Iteration 13400: Loss = -10839.019804504884
Iteration 13500: Loss = -10839.105383940916
1
Iteration 13600: Loss = -10839.019746904558
Iteration 13700: Loss = -10839.030301829594
1
Iteration 13800: Loss = -10839.019695154146
Iteration 13900: Loss = -10839.029390940163
1
Iteration 14000: Loss = -10839.019709846112
Iteration 14100: Loss = -10839.027603554801
1
Iteration 14200: Loss = -10839.019699041586
Iteration 14300: Loss = -10839.146941564391
1
Iteration 14400: Loss = -10839.019691770189
Iteration 14500: Loss = -10839.020105426587
1
Iteration 14600: Loss = -10839.019652588111
Iteration 14700: Loss = -10839.01990001953
1
Iteration 14800: Loss = -10839.019617284199
Iteration 14900: Loss = -10839.01958875617
Iteration 15000: Loss = -10839.02006430035
1
Iteration 15100: Loss = -10839.019609422083
Iteration 15200: Loss = -10839.066845981031
1
Iteration 15300: Loss = -10839.019693175745
Iteration 15400: Loss = -10839.019646899116
Iteration 15500: Loss = -10839.359053264501
1
Iteration 15600: Loss = -10839.019553721171
Iteration 15700: Loss = -10839.01952076164
Iteration 15800: Loss = -10839.019715160055
1
Iteration 15900: Loss = -10839.019533884884
Iteration 16000: Loss = -10839.021579716504
1
Iteration 16100: Loss = -10839.019524631407
Iteration 16200: Loss = -10839.021842597194
1
Iteration 16300: Loss = -10839.020295741188
2
Iteration 16400: Loss = -10839.018408875576
Iteration 16500: Loss = -10839.018882536471
1
Iteration 16600: Loss = -10839.018464881114
Iteration 16700: Loss = -10839.018483586513
Iteration 16800: Loss = -10839.040522700856
1
Iteration 16900: Loss = -10839.041624819582
2
Iteration 17000: Loss = -10839.018392720456
Iteration 17100: Loss = -10839.02438618177
1
Iteration 17200: Loss = -10839.018722665869
2
Iteration 17300: Loss = -10839.053780065822
3
Iteration 17400: Loss = -10839.018431440629
Iteration 17500: Loss = -10839.01857646681
1
Iteration 17600: Loss = -10839.077551558365
2
Iteration 17700: Loss = -10839.018311467604
Iteration 17800: Loss = -10839.026498551233
1
Iteration 17900: Loss = -10839.018340777866
Iteration 18000: Loss = -10839.01828188301
Iteration 18100: Loss = -10839.018343883658
Iteration 18200: Loss = -10839.018289063024
Iteration 18300: Loss = -10839.018590582125
1
Iteration 18400: Loss = -10839.022160209575
2
Iteration 18500: Loss = -10839.056615505719
3
Iteration 18600: Loss = -10839.018267857438
Iteration 18700: Loss = -10839.028022582459
1
Iteration 18800: Loss = -10839.018244860357
Iteration 18900: Loss = -10839.021427576887
1
Iteration 19000: Loss = -10839.019522258368
2
Iteration 19100: Loss = -10839.046301042059
3
Iteration 19200: Loss = -10839.018269271213
Iteration 19300: Loss = -10839.02467271616
1
Iteration 19400: Loss = -10839.018241475087
Iteration 19500: Loss = -10839.02482787938
1
Iteration 19600: Loss = -10839.019694527815
2
Iteration 19700: Loss = -10839.055613596505
3
Iteration 19800: Loss = -10839.018178111723
Iteration 19900: Loss = -10839.018264369955
pi: tensor([[1.0000e+00, 1.9174e-06],
        [7.0676e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0293, 0.9707], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2783, 0.2461],
         [0.5944, 0.1573]],

        [[0.5796, 0.2032],
         [0.6497, 0.5102]],

        [[0.5292, 0.1850],
         [0.6502, 0.6703]],

        [[0.6704, 0.1698],
         [0.5008, 0.5354]],

        [[0.6959, 0.1293],
         [0.5332, 0.5446]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 0.0010524474355843583
Average Adjusted Rand Index: -0.0001307858112401553
10753.453751409359
[0.0010524474355843583, 0.0010524474355843583] [-0.0001307858112401553, -0.0001307858112401553] [10839.024418651858, 10839.01841827568]
-------------------------------------
This iteration is 12
True Objective function: Loss = -10687.824012263682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23244.019664668118
Iteration 100: Loss = -10767.246793083597
Iteration 200: Loss = -10765.569092710457
Iteration 300: Loss = -10765.119037145632
Iteration 400: Loss = -10764.955668347984
Iteration 500: Loss = -10764.859871693765
Iteration 600: Loss = -10764.783237072306
Iteration 700: Loss = -10764.711366645344
Iteration 800: Loss = -10764.635915962705
Iteration 900: Loss = -10764.55077816238
Iteration 1000: Loss = -10764.450637759985
Iteration 1100: Loss = -10764.330234333533
Iteration 1200: Loss = -10764.186912749712
Iteration 1300: Loss = -10764.022449817825
Iteration 1400: Loss = -10763.840281599176
Iteration 1500: Loss = -10763.656395917762
Iteration 1600: Loss = -10763.492251706772
Iteration 1700: Loss = -10763.352275255915
Iteration 1800: Loss = -10763.227634237415
Iteration 1900: Loss = -10763.101882095842
Iteration 2000: Loss = -10762.94763722828
Iteration 2100: Loss = -10762.734212834086
Iteration 2200: Loss = -10762.464558979706
Iteration 2300: Loss = -10762.194173916778
Iteration 2400: Loss = -10761.968918368042
Iteration 2500: Loss = -10761.787159718957
Iteration 2600: Loss = -10761.433045645656
Iteration 2700: Loss = -10750.634899365554
Iteration 2800: Loss = -10684.636305557795
Iteration 2900: Loss = -10683.468373416445
Iteration 3000: Loss = -10683.332444783147
Iteration 3100: Loss = -10683.288911909922
Iteration 3200: Loss = -10683.259332716278
Iteration 3300: Loss = -10683.239318779439
Iteration 3400: Loss = -10683.22156893536
Iteration 3500: Loss = -10683.205263537217
Iteration 3600: Loss = -10683.176726333351
Iteration 3700: Loss = -10683.164747579069
Iteration 3800: Loss = -10683.159880097504
Iteration 3900: Loss = -10683.154091135128
Iteration 4000: Loss = -10683.104649575542
Iteration 4100: Loss = -10683.096365018257
Iteration 4200: Loss = -10683.093221426625
Iteration 4300: Loss = -10683.090293168778
Iteration 4400: Loss = -10683.08793690954
Iteration 4500: Loss = -10683.085810304352
Iteration 4600: Loss = -10683.083614981626
Iteration 4700: Loss = -10683.080933479016
Iteration 4800: Loss = -10683.077084654291
Iteration 4900: Loss = -10683.072455694544
Iteration 5000: Loss = -10683.06856174562
Iteration 5100: Loss = -10683.095765226393
1
Iteration 5200: Loss = -10683.056881179931
Iteration 5300: Loss = -10683.048771249636
Iteration 5400: Loss = -10683.033379862938
Iteration 5500: Loss = -10682.98842147155
Iteration 5600: Loss = -10682.873296541306
Iteration 5700: Loss = -10682.352122105094
Iteration 5800: Loss = -10647.952944886227
Iteration 5900: Loss = -10647.782053850957
Iteration 6000: Loss = -10647.446099472234
Iteration 6100: Loss = -10647.44253556691
Iteration 6200: Loss = -10647.440256068498
Iteration 6300: Loss = -10647.435870189965
Iteration 6400: Loss = -10647.43486364675
Iteration 6500: Loss = -10647.434420685207
Iteration 6600: Loss = -10647.434924620895
1
Iteration 6700: Loss = -10647.433594314412
Iteration 6800: Loss = -10647.43318265274
Iteration 6900: Loss = -10647.4326336522
Iteration 7000: Loss = -10647.432100943593
Iteration 7100: Loss = -10647.431668331343
Iteration 7200: Loss = -10647.44755857853
1
Iteration 7300: Loss = -10647.42956100533
Iteration 7400: Loss = -10647.426755940753
Iteration 7500: Loss = -10647.426646729218
Iteration 7600: Loss = -10647.428605754503
1
Iteration 7700: Loss = -10647.426532394604
Iteration 7800: Loss = -10647.427279698773
1
Iteration 7900: Loss = -10647.428984267195
2
Iteration 8000: Loss = -10647.426655356663
3
Iteration 8100: Loss = -10647.429496374945
4
Iteration 8200: Loss = -10647.426858363828
5
Iteration 8300: Loss = -10647.425886714234
Iteration 8400: Loss = -10647.426190463908
1
Iteration 8500: Loss = -10647.425776101776
Iteration 8600: Loss = -10647.425995087387
1
Iteration 8700: Loss = -10647.42575385632
Iteration 8800: Loss = -10647.42673812003
1
Iteration 8900: Loss = -10647.425728512399
Iteration 9000: Loss = -10647.425685460483
Iteration 9100: Loss = -10647.425696792548
Iteration 9200: Loss = -10647.425652254484
Iteration 9300: Loss = -10647.438399423441
1
Iteration 9400: Loss = -10647.425638831492
Iteration 9500: Loss = -10647.456661053484
1
Iteration 9600: Loss = -10647.431628187762
2
Iteration 9700: Loss = -10647.5516055641
3
Iteration 9800: Loss = -10647.425605736715
Iteration 9900: Loss = -10647.425897611465
1
Iteration 10000: Loss = -10647.42840502937
2
Iteration 10100: Loss = -10647.425517493264
Iteration 10200: Loss = -10647.427132949097
1
Iteration 10300: Loss = -10647.425469023072
Iteration 10400: Loss = -10647.4257040026
1
Iteration 10500: Loss = -10647.425530976914
Iteration 10600: Loss = -10647.42549334219
Iteration 10700: Loss = -10647.42542207271
Iteration 10800: Loss = -10647.426175980101
1
Iteration 10900: Loss = -10647.425412615163
Iteration 11000: Loss = -10647.95007306741
1
Iteration 11100: Loss = -10647.42527817688
Iteration 11200: Loss = -10647.40752340112
Iteration 11300: Loss = -10647.433349082898
1
Iteration 11400: Loss = -10647.407452022118
Iteration 11500: Loss = -10647.407457034473
Iteration 11600: Loss = -10647.410743917091
1
Iteration 11700: Loss = -10647.407380281513
Iteration 11800: Loss = -10647.40740463111
Iteration 11900: Loss = -10647.407458605072
Iteration 12000: Loss = -10647.407079061462
Iteration 12100: Loss = -10647.672012399413
1
Iteration 12200: Loss = -10647.40617130422
Iteration 12300: Loss = -10647.406174833706
Iteration 12400: Loss = -10647.55497732334
1
Iteration 12500: Loss = -10647.406143058719
Iteration 12600: Loss = -10647.40615107284
Iteration 12700: Loss = -10647.698027534745
1
Iteration 12800: Loss = -10647.406121761562
Iteration 12900: Loss = -10647.406104295842
Iteration 13000: Loss = -10647.406116735407
Iteration 13100: Loss = -10647.40627703193
1
Iteration 13200: Loss = -10647.406101941595
Iteration 13300: Loss = -10647.406167468529
Iteration 13400: Loss = -10647.406037152497
Iteration 13500: Loss = -10647.412354035941
1
Iteration 13600: Loss = -10647.406005365376
Iteration 13700: Loss = -10647.418885921144
1
Iteration 13800: Loss = -10647.40626439039
2
Iteration 13900: Loss = -10647.405986245085
Iteration 14000: Loss = -10647.406824037502
1
Iteration 14100: Loss = -10647.405981725891
Iteration 14200: Loss = -10647.40654511024
1
Iteration 14300: Loss = -10647.405969306557
Iteration 14400: Loss = -10647.40734747433
1
Iteration 14500: Loss = -10647.405957859091
Iteration 14600: Loss = -10647.604371123163
1
Iteration 14700: Loss = -10647.405996555744
Iteration 14800: Loss = -10647.405988348075
Iteration 14900: Loss = -10647.43788141449
1
Iteration 15000: Loss = -10647.405988915065
Iteration 15100: Loss = -10647.406430202744
1
Iteration 15200: Loss = -10647.406021211515
Iteration 15300: Loss = -10647.418131495951
1
Iteration 15400: Loss = -10647.405987359542
Iteration 15500: Loss = -10647.40634858627
1
Iteration 15600: Loss = -10647.50413516032
2
Iteration 15700: Loss = -10647.405995879273
Iteration 15800: Loss = -10647.435615797762
1
Iteration 15900: Loss = -10647.40599819421
Iteration 16000: Loss = -10647.406074805713
Iteration 16100: Loss = -10647.406040070231
Iteration 16200: Loss = -10647.405971940734
Iteration 16300: Loss = -10647.467031428821
1
Iteration 16400: Loss = -10647.405929757108
Iteration 16500: Loss = -10647.405975690492
Iteration 16600: Loss = -10647.407435965662
1
Iteration 16700: Loss = -10647.405955285367
Iteration 16800: Loss = -10647.406213943657
1
Iteration 16900: Loss = -10647.405959788608
Iteration 17000: Loss = -10647.405955618347
Iteration 17100: Loss = -10647.550639263078
1
Iteration 17200: Loss = -10647.405982014361
Iteration 17300: Loss = -10647.405931999998
Iteration 17400: Loss = -10647.406458771531
1
Iteration 17500: Loss = -10647.40596121673
Iteration 17600: Loss = -10647.500907520996
1
Iteration 17700: Loss = -10647.405957948316
Iteration 17800: Loss = -10647.40596979921
Iteration 17900: Loss = -10647.406564819232
1
Iteration 18000: Loss = -10647.405939656779
Iteration 18100: Loss = -10647.40875850836
1
Iteration 18200: Loss = -10647.405939221064
Iteration 18300: Loss = -10647.406104187496
1
Iteration 18400: Loss = -10647.405968053556
Iteration 18500: Loss = -10647.405955924121
Iteration 18600: Loss = -10647.41400806729
1
Iteration 18700: Loss = -10647.405944279215
Iteration 18800: Loss = -10647.406034186031
Iteration 18900: Loss = -10647.405990815467
Iteration 19000: Loss = -10647.405930476443
Iteration 19100: Loss = -10647.462347175684
1
Iteration 19200: Loss = -10647.405939432829
Iteration 19300: Loss = -10647.405965040045
Iteration 19400: Loss = -10647.44186792997
1
Iteration 19500: Loss = -10647.405875183531
Iteration 19600: Loss = -10647.405854193668
Iteration 19700: Loss = -10647.407452234691
1
Iteration 19800: Loss = -10647.405846514415
Iteration 19900: Loss = -10647.405844373228
pi: tensor([[0.7419, 0.2581],
        [0.2246, 0.7754]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4936, 0.5064], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.0949],
         [0.6139, 0.2374]],

        [[0.5818, 0.1014],
         [0.5177, 0.5648]],

        [[0.5763, 0.0952],
         [0.5246, 0.5823]],

        [[0.5209, 0.0890],
         [0.5815, 0.5820]],

        [[0.5493, 0.1030],
         [0.6481, 0.6384]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363484387634443
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.8096187884879187
Average Adjusted Rand Index: 0.8116235164712968
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21744.94488341532
Iteration 100: Loss = -10766.00644139502
Iteration 200: Loss = -10764.9605383679
Iteration 300: Loss = -10764.7439040338
Iteration 400: Loss = -10764.623222337184
Iteration 500: Loss = -10764.51152049379
Iteration 600: Loss = -10764.384442636234
Iteration 700: Loss = -10764.239972980577
Iteration 800: Loss = -10764.090314740575
Iteration 900: Loss = -10763.949162087643
Iteration 1000: Loss = -10763.814315892143
Iteration 1100: Loss = -10763.68337107863
Iteration 1200: Loss = -10763.563687063524
Iteration 1300: Loss = -10763.463091512553
Iteration 1400: Loss = -10763.381335014665
Iteration 1500: Loss = -10763.314035166324
Iteration 1600: Loss = -10763.255904937481
Iteration 1700: Loss = -10763.200682864845
Iteration 1800: Loss = -10763.14120720366
Iteration 1900: Loss = -10763.069777907336
Iteration 2000: Loss = -10762.978026184088
Iteration 2100: Loss = -10762.860113266026
Iteration 2200: Loss = -10762.714917725409
Iteration 2300: Loss = -10762.526701641022
Iteration 2400: Loss = -10762.221197458657
Iteration 2500: Loss = -10761.968568947177
Iteration 2600: Loss = -10761.783857172908
Iteration 2700: Loss = -10761.377928917324
Iteration 2800: Loss = -10750.316860067429
Iteration 2900: Loss = -10684.508436723772
Iteration 3000: Loss = -10683.439865079674
Iteration 3100: Loss = -10683.333330016736
Iteration 3200: Loss = -10683.291524973572
Iteration 3300: Loss = -10683.24100503489
Iteration 3400: Loss = -10683.215633352764
Iteration 3500: Loss = -10683.192355861553
Iteration 3600: Loss = -10683.16508287074
Iteration 3700: Loss = -10683.149655161793
Iteration 3800: Loss = -10683.108645889914
Iteration 3900: Loss = -10683.104898346493
Iteration 4000: Loss = -10683.097264195361
Iteration 4100: Loss = -10683.090573938558
Iteration 4200: Loss = -10683.08148387819
Iteration 4300: Loss = -10683.071568792793
Iteration 4400: Loss = -10683.065383288535
Iteration 4500: Loss = -10683.035434161298
Iteration 4600: Loss = -10682.987337993536
Iteration 4700: Loss = -10682.684820179482
Iteration 4800: Loss = -10682.290156789239
Iteration 4900: Loss = -10647.864594700153
Iteration 5000: Loss = -10647.590827569611
Iteration 5100: Loss = -10647.4773666669
Iteration 5200: Loss = -10647.463297838554
Iteration 5300: Loss = -10647.458657650144
Iteration 5400: Loss = -10647.450656296647
Iteration 5500: Loss = -10647.448303918458
Iteration 5600: Loss = -10647.463888597424
1
Iteration 5700: Loss = -10647.443396574014
Iteration 5800: Loss = -10647.437812265754
Iteration 5900: Loss = -10647.433310936402
Iteration 6000: Loss = -10647.432502574396
Iteration 6100: Loss = -10647.432061733509
Iteration 6200: Loss = -10647.431085188473
Iteration 6300: Loss = -10647.428229846339
Iteration 6400: Loss = -10647.427946367769
Iteration 6500: Loss = -10647.427504222816
Iteration 6600: Loss = -10647.42738458823
Iteration 6700: Loss = -10647.42707767537
Iteration 6800: Loss = -10647.426941338437
Iteration 6900: Loss = -10647.426810544644
Iteration 7000: Loss = -10647.426695171222
Iteration 7100: Loss = -10647.426717578257
Iteration 7200: Loss = -10647.426546764495
Iteration 7300: Loss = -10647.429334955781
1
Iteration 7400: Loss = -10647.426380577726
Iteration 7500: Loss = -10647.438923973623
1
Iteration 7600: Loss = -10647.426100864463
Iteration 7700: Loss = -10647.427024109962
1
Iteration 7800: Loss = -10647.42532576257
Iteration 7900: Loss = -10647.425283736162
Iteration 8000: Loss = -10647.42514857582
Iteration 8100: Loss = -10647.425076360063
Iteration 8200: Loss = -10647.425354830606
1
Iteration 8300: Loss = -10647.425040534634
Iteration 8400: Loss = -10647.425345158521
1
Iteration 8500: Loss = -10647.42521255164
2
Iteration 8600: Loss = -10647.424994668754
Iteration 8700: Loss = -10647.426326100998
1
Iteration 8800: Loss = -10647.42491944457
Iteration 8900: Loss = -10647.424852191603
Iteration 9000: Loss = -10647.42494225892
Iteration 9100: Loss = -10647.424751016031
Iteration 9200: Loss = -10647.429283039755
1
Iteration 9300: Loss = -10647.4246595538
Iteration 9400: Loss = -10647.424726895018
Iteration 9500: Loss = -10647.424601917523
Iteration 9600: Loss = -10647.424959889542
1
Iteration 9700: Loss = -10647.424535185151
Iteration 9800: Loss = -10647.424536764496
Iteration 9900: Loss = -10647.42442410872
Iteration 10000: Loss = -10647.424382396548
Iteration 10100: Loss = -10647.424725273479
1
Iteration 10200: Loss = -10647.424359334544
Iteration 10300: Loss = -10647.47292441554
1
Iteration 10400: Loss = -10647.424311243429
Iteration 10500: Loss = -10647.424311369956
Iteration 10600: Loss = -10647.440075748718
1
Iteration 10700: Loss = -10647.424305311743
Iteration 10800: Loss = -10647.42426073798
Iteration 10900: Loss = -10647.425396197457
1
Iteration 11000: Loss = -10647.424156707713
Iteration 11100: Loss = -10647.424248688365
Iteration 11200: Loss = -10647.424105140106
Iteration 11300: Loss = -10647.42428457538
1
Iteration 11400: Loss = -10647.424096931463
Iteration 11500: Loss = -10647.42511938688
1
Iteration 11600: Loss = -10647.424118351208
Iteration 11700: Loss = -10647.424280453306
1
Iteration 11800: Loss = -10647.424102474215
Iteration 11900: Loss = -10647.424515909024
1
Iteration 12000: Loss = -10647.424042697556
Iteration 12100: Loss = -10647.443198417213
1
Iteration 12200: Loss = -10647.406190013364
Iteration 12300: Loss = -10647.406133218372
Iteration 12400: Loss = -10647.406309891476
1
Iteration 12500: Loss = -10647.406129118395
Iteration 12600: Loss = -10647.406110001131
Iteration 12700: Loss = -10647.406240234848
1
Iteration 12800: Loss = -10647.406067833066
Iteration 12900: Loss = -10647.450883287484
1
Iteration 13000: Loss = -10647.406034410738
Iteration 13100: Loss = -10647.406009512884
Iteration 13200: Loss = -10647.406083137093
Iteration 13300: Loss = -10647.405976321837
Iteration 13400: Loss = -10647.443433472421
1
Iteration 13500: Loss = -10647.406015509641
Iteration 13600: Loss = -10647.406014316435
Iteration 13700: Loss = -10647.407278830688
1
Iteration 13800: Loss = -10647.406004848746
Iteration 13900: Loss = -10647.405999067321
Iteration 14000: Loss = -10647.40620423963
1
Iteration 14100: Loss = -10647.405980242027
Iteration 14200: Loss = -10647.405982346238
Iteration 14300: Loss = -10647.406711512669
1
Iteration 14400: Loss = -10647.405988011318
Iteration 14500: Loss = -10647.405948985708
Iteration 14600: Loss = -10647.406022445799
Iteration 14700: Loss = -10647.405983590796
Iteration 14800: Loss = -10647.541651513591
1
Iteration 14900: Loss = -10647.405957954175
Iteration 15000: Loss = -10647.406797455264
1
Iteration 15100: Loss = -10647.405984293126
Iteration 15200: Loss = -10647.405972769224
Iteration 15300: Loss = -10647.406388153728
1
Iteration 15400: Loss = -10647.40595391416
Iteration 15500: Loss = -10647.406203728979
1
Iteration 15600: Loss = -10647.435749484906
2
Iteration 15700: Loss = -10647.405975913516
Iteration 15800: Loss = -10647.40624992634
1
Iteration 15900: Loss = -10647.503992827227
2
Iteration 16000: Loss = -10647.405974444959
Iteration 16100: Loss = -10647.408672692758
1
Iteration 16200: Loss = -10647.406015421777
Iteration 16300: Loss = -10647.406085594723
Iteration 16400: Loss = -10647.405952049105
Iteration 16500: Loss = -10647.406179676505
1
Iteration 16600: Loss = -10647.405934011107
Iteration 16700: Loss = -10647.407913661074
1
Iteration 16800: Loss = -10647.405955267359
Iteration 16900: Loss = -10647.408002602395
1
Iteration 17000: Loss = -10647.40594832339
Iteration 17100: Loss = -10647.40592634386
Iteration 17200: Loss = -10647.417413342966
1
Iteration 17300: Loss = -10647.40594870728
Iteration 17400: Loss = -10647.405939214408
Iteration 17500: Loss = -10647.408067536446
1
Iteration 17600: Loss = -10647.405942148125
Iteration 17700: Loss = -10647.405921636633
Iteration 17800: Loss = -10647.40619608176
1
Iteration 17900: Loss = -10647.405926659767
Iteration 18000: Loss = -10647.482920358509
1
Iteration 18100: Loss = -10647.405932554508
Iteration 18200: Loss = -10647.407485493079
1
Iteration 18300: Loss = -10647.429139096057
2
Iteration 18400: Loss = -10647.405948783535
Iteration 18500: Loss = -10647.40658354298
1
Iteration 18600: Loss = -10647.41254686328
2
Iteration 18700: Loss = -10647.405969287565
Iteration 18800: Loss = -10647.6983790641
1
Iteration 18900: Loss = -10647.405916792304
Iteration 19000: Loss = -10647.405993622751
Iteration 19100: Loss = -10647.405951048419
Iteration 19200: Loss = -10647.40582120337
Iteration 19300: Loss = -10647.421030220075
1
Iteration 19400: Loss = -10647.405831311373
Iteration 19500: Loss = -10647.405848228456
Iteration 19600: Loss = -10647.405929069964
Iteration 19700: Loss = -10647.483131482284
1
Iteration 19800: Loss = -10647.405829101335
Iteration 19900: Loss = -10647.442466016044
1
pi: tensor([[0.7420, 0.2580],
        [0.2245, 0.7755]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4936, 0.5064], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.0950],
         [0.5991, 0.2374]],

        [[0.6689, 0.1014],
         [0.6126, 0.6382]],

        [[0.6836, 0.0952],
         [0.6542, 0.6236]],

        [[0.7154, 0.0891],
         [0.5591, 0.7025]],

        [[0.5453, 0.1030],
         [0.5276, 0.7310]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 90
Adjusted Rand Index: 0.6363484387634443
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 4
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.8096187884879187
Average Adjusted Rand Index: 0.8116235164712968
10687.824012263682
[0.8096187884879187, 0.8096187884879187] [0.8116235164712968, 0.8116235164712968] [10647.406066338766, 10647.40584187206]
-------------------------------------
This iteration is 13
True Objective function: Loss = -10950.074845159359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23472.65289994756
Iteration 100: Loss = -10991.6705287449
Iteration 200: Loss = -10990.682174954629
Iteration 300: Loss = -10990.071875301055
Iteration 400: Loss = -10989.185520007391
Iteration 500: Loss = -10987.51665631353
Iteration 600: Loss = -10987.335427652059
Iteration 700: Loss = -10987.229995450616
Iteration 800: Loss = -10987.163733865464
Iteration 900: Loss = -10987.133951324937
Iteration 1000: Loss = -10987.117517334314
Iteration 1100: Loss = -10987.103951926862
Iteration 1200: Loss = -10987.08811951404
Iteration 1300: Loss = -10987.055183381954
Iteration 1400: Loss = -10986.623643622679
Iteration 1500: Loss = -10984.158854071478
Iteration 1600: Loss = -10983.901151087422
Iteration 1700: Loss = -10983.865759088589
Iteration 1800: Loss = -10983.857484426457
Iteration 1900: Loss = -10983.854895863831
Iteration 2000: Loss = -10983.851761191974
Iteration 2100: Loss = -10983.850956412714
Iteration 2200: Loss = -10983.870935963438
1
Iteration 2300: Loss = -10983.849859955018
Iteration 2400: Loss = -10983.849305624373
Iteration 2500: Loss = -10983.849014899075
Iteration 2600: Loss = -10983.848452049671
Iteration 2700: Loss = -10983.84802813593
Iteration 2800: Loss = -10983.847649858748
Iteration 2900: Loss = -10983.847273645133
Iteration 3000: Loss = -10983.847449437335
1
Iteration 3100: Loss = -10983.846375712177
Iteration 3200: Loss = -10983.865098967522
1
Iteration 3300: Loss = -10983.845877565946
Iteration 3400: Loss = -10983.845638367064
Iteration 3500: Loss = -10983.845409003625
Iteration 3600: Loss = -10983.845217786107
Iteration 3700: Loss = -10983.847837380925
1
Iteration 3800: Loss = -10983.844859588331
Iteration 3900: Loss = -10983.844667290095
Iteration 4000: Loss = -10983.844580524954
Iteration 4100: Loss = -10983.844349512887
Iteration 4200: Loss = -10983.844287007732
Iteration 4300: Loss = -10983.844121567867
Iteration 4400: Loss = -10983.84398594955
Iteration 4500: Loss = -10983.84388799855
Iteration 4600: Loss = -10983.843790698937
Iteration 4700: Loss = -10983.8436780807
Iteration 4800: Loss = -10983.843860494471
1
Iteration 4900: Loss = -10983.843503171514
Iteration 5000: Loss = -10983.843737847114
1
Iteration 5100: Loss = -10983.84532857176
2
Iteration 5200: Loss = -10983.84380875964
3
Iteration 5300: Loss = -10983.843172623814
Iteration 5400: Loss = -10983.846718354836
1
Iteration 5500: Loss = -10983.843027426972
Iteration 5600: Loss = -10983.843173542813
1
Iteration 5700: Loss = -10983.842899897989
Iteration 5800: Loss = -10983.869009238006
1
Iteration 5900: Loss = -10983.842824665142
Iteration 6000: Loss = -10983.842766476017
Iteration 6100: Loss = -10983.842712790809
Iteration 6200: Loss = -10983.842661533849
Iteration 6300: Loss = -10983.843933307722
1
Iteration 6400: Loss = -10983.842612244156
Iteration 6500: Loss = -10983.844959905802
1
Iteration 6600: Loss = -10983.842515625942
Iteration 6700: Loss = -10983.842438783258
Iteration 6800: Loss = -10983.842649599817
1
Iteration 6900: Loss = -10983.842413913959
Iteration 7000: Loss = -10983.843407105442
1
Iteration 7100: Loss = -10983.842366393974
Iteration 7200: Loss = -10983.932274265293
1
Iteration 7300: Loss = -10983.842305586464
Iteration 7400: Loss = -10983.858743675673
1
Iteration 7500: Loss = -10983.842274927827
Iteration 7600: Loss = -10983.84378693924
1
Iteration 7700: Loss = -10983.842231851002
Iteration 7800: Loss = -10983.842228285925
Iteration 7900: Loss = -10983.842658555977
1
Iteration 8000: Loss = -10983.842172246254
Iteration 8100: Loss = -10983.843929634704
1
Iteration 8200: Loss = -10983.8421682765
Iteration 8300: Loss = -10983.899062860914
1
Iteration 8400: Loss = -10983.842111534415
Iteration 8500: Loss = -10983.842140654575
Iteration 8600: Loss = -10983.842143328362
Iteration 8700: Loss = -10983.842063657296
Iteration 8800: Loss = -10983.842449019241
1
Iteration 8900: Loss = -10983.84454577631
2
Iteration 9000: Loss = -10983.842144742714
Iteration 9100: Loss = -10983.845334338643
1
Iteration 9200: Loss = -10983.868138115033
2
Iteration 9300: Loss = -10983.86131448287
3
Iteration 9400: Loss = -10983.88057260095
4
Iteration 9500: Loss = -10983.845793995548
5
Iteration 9600: Loss = -10983.842185059168
Iteration 9700: Loss = -10983.84208302319
Iteration 9800: Loss = -10983.842111714492
Iteration 9900: Loss = -10983.852102231749
1
Iteration 10000: Loss = -10983.887491806017
2
Iteration 10100: Loss = -10983.85933331632
3
Iteration 10200: Loss = -10983.845847899243
4
Iteration 10300: Loss = -10983.845303475717
5
Iteration 10400: Loss = -10983.84665111095
6
Iteration 10500: Loss = -10983.843847747854
7
Iteration 10600: Loss = -10983.856376957983
8
Iteration 10700: Loss = -10983.842997642816
9
Iteration 10800: Loss = -10983.845019649778
10
Iteration 10900: Loss = -10983.854165242044
11
Iteration 11000: Loss = -10983.879508181963
12
Iteration 11100: Loss = -10983.85369157533
13
Iteration 11200: Loss = -10983.859922429197
14
Iteration 11300: Loss = -10983.842096027956
Iteration 11400: Loss = -10983.849952170673
1
Iteration 11500: Loss = -10983.853481304419
2
Iteration 11600: Loss = -10983.87991620732
3
Iteration 11700: Loss = -10983.84195365952
Iteration 11800: Loss = -10983.842257760596
1
Iteration 11900: Loss = -10983.846190554275
2
Iteration 12000: Loss = -10983.93715574705
3
Iteration 12100: Loss = -10983.850562134521
4
Iteration 12200: Loss = -10983.855866072006
5
Iteration 12300: Loss = -10983.849046044064
6
Iteration 12400: Loss = -10983.84549218651
7
Iteration 12500: Loss = -10983.863469916534
8
Iteration 12600: Loss = -10983.845479315658
9
Iteration 12700: Loss = -10983.853652902399
10
Iteration 12800: Loss = -10983.842177612
11
Iteration 12900: Loss = -10983.842829029398
12
Iteration 13000: Loss = -10983.86853210832
13
Iteration 13100: Loss = -10984.01591871824
14
Iteration 13200: Loss = -10984.033002039161
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[2.5711e-06, 1.0000e+00],
        [2.6684e-01, 7.3316e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3955, 0.6045], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2296, 0.1749],
         [0.5221, 0.1435]],

        [[0.7093, 0.1960],
         [0.6608, 0.7048]],

        [[0.6230, 0.1753],
         [0.6350, 0.6047]],

        [[0.5055, 0.1736],
         [0.6440, 0.5525]],

        [[0.6319, 0.1986],
         [0.5335, 0.5288]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 35
Adjusted Rand Index: 0.08163924367441185
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.010344451541541386
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0013192612137203166
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.026938115180335877
Global Adjusted Rand Index: 0.01460373690212434
Average Adjusted Rand Index: 0.021823540139544063
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20527.287812382634
Iteration 100: Loss = -10992.95908822308
Iteration 200: Loss = -10992.32483759225
Iteration 300: Loss = -10991.644313487734
Iteration 400: Loss = -10989.925531241688
Iteration 500: Loss = -10989.66822560952
Iteration 600: Loss = -10989.530724917893
Iteration 700: Loss = -10989.389649820834
Iteration 800: Loss = -10989.190763859255
Iteration 900: Loss = -10988.865385231577
Iteration 1000: Loss = -10987.71459762276
Iteration 1100: Loss = -10985.898093462796
Iteration 1200: Loss = -10985.183980249049
Iteration 1300: Loss = -10984.669408275082
Iteration 1400: Loss = -10984.312927014269
Iteration 1500: Loss = -10984.11328753746
Iteration 1600: Loss = -10984.020689645356
Iteration 1700: Loss = -10983.98215631431
Iteration 1800: Loss = -10983.962413881949
Iteration 1900: Loss = -10983.949160088116
Iteration 2000: Loss = -10983.938545860325
Iteration 2100: Loss = -10983.929463243721
Iteration 2200: Loss = -10983.921812773688
Iteration 2300: Loss = -10983.914471198761
Iteration 2400: Loss = -10983.908174749264
Iteration 2500: Loss = -10983.908685101233
1
Iteration 2600: Loss = -10983.897677467165
Iteration 2700: Loss = -10983.893119315024
Iteration 2800: Loss = -10983.897341718377
1
Iteration 2900: Loss = -10983.885476025995
Iteration 3000: Loss = -10983.882185172908
Iteration 3100: Loss = -10983.880381653042
Iteration 3200: Loss = -10983.876483566357
Iteration 3300: Loss = -10983.874037525911
Iteration 3400: Loss = -10983.871774903604
Iteration 3500: Loss = -10983.869696650168
Iteration 3600: Loss = -10983.867810616563
Iteration 3700: Loss = -10983.86607356507
Iteration 3800: Loss = -10983.864512248829
Iteration 3900: Loss = -10983.863870180423
Iteration 4000: Loss = -10983.86168320675
Iteration 4100: Loss = -10983.860416528363
Iteration 4200: Loss = -10983.862502805041
1
Iteration 4300: Loss = -10983.858164856403
Iteration 4400: Loss = -10983.857139205515
Iteration 4500: Loss = -10983.861710874198
1
Iteration 4600: Loss = -10983.855395310371
Iteration 4700: Loss = -10983.854598172515
Iteration 4800: Loss = -10983.854733270497
1
Iteration 4900: Loss = -10983.853154846764
Iteration 5000: Loss = -10983.852492928676
Iteration 5100: Loss = -10983.852664363
1
Iteration 5200: Loss = -10983.851331879087
Iteration 5300: Loss = -10983.850771455387
Iteration 5400: Loss = -10983.89684619627
1
Iteration 5500: Loss = -10983.849830883104
Iteration 5600: Loss = -10983.84934617908
Iteration 5700: Loss = -10983.85004748302
1
Iteration 5800: Loss = -10983.848597262531
Iteration 5900: Loss = -10983.848257428193
Iteration 6000: Loss = -10983.847833085616
Iteration 6100: Loss = -10983.847581409213
Iteration 6200: Loss = -10983.850451076623
1
Iteration 6300: Loss = -10983.846953224765
Iteration 6400: Loss = -10983.847973626542
1
Iteration 6500: Loss = -10983.846360039342
Iteration 6600: Loss = -10983.846394654907
Iteration 6700: Loss = -10983.846360438292
Iteration 6800: Loss = -10983.845746455776
Iteration 6900: Loss = -10983.845507473983
Iteration 7000: Loss = -10983.86323777379
1
Iteration 7100: Loss = -10983.84513825044
Iteration 7200: Loss = -10983.845818893813
1
Iteration 7300: Loss = -10983.844813727252
Iteration 7400: Loss = -10983.845610351735
1
Iteration 7500: Loss = -10983.84452289887
Iteration 7600: Loss = -10983.84473153887
1
Iteration 7700: Loss = -10983.844242020521
Iteration 7800: Loss = -10983.860277830576
1
Iteration 7900: Loss = -10983.843982196651
Iteration 8000: Loss = -10983.848640491386
1
Iteration 8100: Loss = -10983.843785522993
Iteration 8200: Loss = -10983.968334519139
1
Iteration 8300: Loss = -10983.843576971823
Iteration 8400: Loss = -10983.843513768717
Iteration 8500: Loss = -10983.844771894957
1
Iteration 8600: Loss = -10983.843336541835
Iteration 8700: Loss = -10983.850630919245
1
Iteration 8800: Loss = -10983.86150726117
2
Iteration 8900: Loss = -10983.86488084536
3
Iteration 9000: Loss = -10983.847455840329
4
Iteration 9100: Loss = -10983.847794244823
5
Iteration 9200: Loss = -10983.874832256384
6
Iteration 9300: Loss = -10983.845739529947
7
Iteration 9400: Loss = -10983.842877010617
Iteration 9500: Loss = -10983.848142030702
1
Iteration 9600: Loss = -10983.90841987689
2
Iteration 9700: Loss = -10983.859262403646
3
Iteration 9800: Loss = -10983.848006128177
4
Iteration 9900: Loss = -10983.851890491338
5
Iteration 10000: Loss = -10983.89454314662
6
Iteration 10100: Loss = -10983.842520331245
Iteration 10200: Loss = -10983.843275214467
1
Iteration 10300: Loss = -10983.885973249831
2
Iteration 10400: Loss = -10983.931627529533
3
Iteration 10500: Loss = -10983.843479839301
4
Iteration 10600: Loss = -10983.852591709167
5
Iteration 10700: Loss = -10983.844404528172
6
Iteration 10800: Loss = -10983.862390765742
7
Iteration 10900: Loss = -10983.842490180452
Iteration 11000: Loss = -10983.848360754371
1
Iteration 11100: Loss = -10983.844283488344
2
Iteration 11200: Loss = -10983.842489076724
Iteration 11300: Loss = -10983.843740077811
1
Iteration 11400: Loss = -10983.849017163751
2
Iteration 11500: Loss = -10983.846604957314
3
Iteration 11600: Loss = -10983.842223598193
Iteration 11700: Loss = -10983.843187081975
1
Iteration 11800: Loss = -10983.842740949985
2
Iteration 11900: Loss = -10983.859235304553
3
Iteration 12000: Loss = -10983.852274650912
4
Iteration 12100: Loss = -10983.85531032333
5
Iteration 12200: Loss = -10983.871951656478
6
Iteration 12300: Loss = -10983.84897669059
7
Iteration 12400: Loss = -10983.842539850437
8
Iteration 12500: Loss = -10983.849526654667
9
Iteration 12600: Loss = -10983.842098151892
Iteration 12700: Loss = -10983.844002935446
1
Iteration 12800: Loss = -10983.864817207095
2
Iteration 12900: Loss = -10983.842974237386
3
Iteration 13000: Loss = -10983.871023040072
4
Iteration 13100: Loss = -10983.842055529116
Iteration 13200: Loss = -10983.843231835044
1
Iteration 13300: Loss = -10983.853539257398
2
Iteration 13400: Loss = -10983.842181442491
3
Iteration 13500: Loss = -10983.846387380778
4
Iteration 13600: Loss = -10983.844747924484
5
Iteration 13700: Loss = -10983.843446420284
6
Iteration 13800: Loss = -10983.845396366894
7
Iteration 13900: Loss = -10983.846216565766
8
Iteration 14000: Loss = -10983.859570645223
9
Iteration 14100: Loss = -10983.852325449645
10
Iteration 14200: Loss = -10983.849616051551
11
Iteration 14300: Loss = -10983.886169974801
12
Iteration 14400: Loss = -10983.843653517742
13
Iteration 14500: Loss = -10983.842487208947
14
Iteration 14600: Loss = -10983.844909737436
15
Stopping early at iteration 14600 due to no improvement.
pi: tensor([[7.2939e-01, 2.7061e-01],
        [9.9999e-01, 7.7526e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5964, 0.4036], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1436, 0.1729],
         [0.6128, 0.2270]],

        [[0.7122, 0.1978],
         [0.5702, 0.5772]],

        [[0.6472, 0.1733],
         [0.7247, 0.6623]],

        [[0.5846, 0.1722],
         [0.7199, 0.5095]],

        [[0.5376, 0.1983],
         [0.7304, 0.6987]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 65
Adjusted Rand Index: 0.08186566364694332
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.010344451541541386
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0013192612137203166
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.026938115180335877
Global Adjusted Rand Index: 0.01460816831032459
Average Adjusted Rand Index: 0.021868824134050356
10950.074845159359
[0.01460373690212434, 0.01460816831032459] [0.021823540139544063, 0.021868824134050356] [10984.033002039161, 10983.844909737436]
-------------------------------------
This iteration is 14
True Objective function: Loss = -10946.041375224544
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22128.84637225667
Iteration 100: Loss = -11035.065147360245
Iteration 200: Loss = -11033.521388303898
Iteration 300: Loss = -11033.108784614025
Iteration 400: Loss = -11032.894547143644
Iteration 500: Loss = -11032.49606616984
Iteration 600: Loss = -11031.206199675742
Iteration 700: Loss = -11030.338135181557
Iteration 800: Loss = -11029.591371065791
Iteration 900: Loss = -11029.087830472516
Iteration 1000: Loss = -11028.777109282693
Iteration 1100: Loss = -11028.498663895338
Iteration 1200: Loss = -11028.212201525635
Iteration 1300: Loss = -11027.929351295383
Iteration 1400: Loss = -11027.65352794866
Iteration 1500: Loss = -11027.38159942274
Iteration 1600: Loss = -11027.131671041598
Iteration 1700: Loss = -11026.90040480348
Iteration 1800: Loss = -11026.700253228368
Iteration 1900: Loss = -11026.545230231524
Iteration 2000: Loss = -11026.443218675486
Iteration 2100: Loss = -11026.385975152834
Iteration 2200: Loss = -11026.356829011498
Iteration 2300: Loss = -11026.341484660044
Iteration 2400: Loss = -11026.33315604329
Iteration 2500: Loss = -11026.32816661155
Iteration 2600: Loss = -11026.324584735108
Iteration 2700: Loss = -11026.32179016397
Iteration 2800: Loss = -11026.31948852791
Iteration 2900: Loss = -11026.31756436926
Iteration 3000: Loss = -11026.316650555675
Iteration 3100: Loss = -11026.314570349663
Iteration 3200: Loss = -11026.313397757338
Iteration 3300: Loss = -11026.312698837017
Iteration 3400: Loss = -11026.31141103287
Iteration 3500: Loss = -11026.310622876324
Iteration 3600: Loss = -11026.310008320292
Iteration 3700: Loss = -11026.309888095528
Iteration 3800: Loss = -11026.308750641254
Iteration 3900: Loss = -11026.308188261406
Iteration 4000: Loss = -11026.307777579494
Iteration 4100: Loss = -11026.30742710524
Iteration 4200: Loss = -11026.3069622527
Iteration 4300: Loss = -11026.314109209858
1
Iteration 4400: Loss = -11026.306360981114
Iteration 4500: Loss = -11026.306088231933
Iteration 4600: Loss = -11026.305988771757
Iteration 4700: Loss = -11026.305662349587
Iteration 4800: Loss = -11026.306428572248
1
Iteration 4900: Loss = -11026.305294810076
Iteration 5000: Loss = -11026.313481707311
1
Iteration 5100: Loss = -11026.304961740196
Iteration 5200: Loss = -11026.310356696187
1
Iteration 5300: Loss = -11026.306371258523
2
Iteration 5400: Loss = -11026.304790355456
Iteration 5500: Loss = -11026.306627083017
1
Iteration 5600: Loss = -11026.304595431298
Iteration 5700: Loss = -11026.304345777282
Iteration 5800: Loss = -11026.304268643133
Iteration 5900: Loss = -11026.304188518547
Iteration 6000: Loss = -11026.304319234374
1
Iteration 6100: Loss = -11026.304057197993
Iteration 6200: Loss = -11026.304029520232
Iteration 6300: Loss = -11026.303970311299
Iteration 6400: Loss = -11026.304104367828
1
Iteration 6500: Loss = -11026.30386182633
Iteration 6600: Loss = -11026.304915909672
1
Iteration 6700: Loss = -11026.303773758542
Iteration 6800: Loss = -11026.303780048493
Iteration 6900: Loss = -11026.304953336252
1
Iteration 7000: Loss = -11026.303834955223
Iteration 7100: Loss = -11026.306240013037
1
Iteration 7200: Loss = -11026.30389219836
Iteration 7300: Loss = -11026.304048312873
1
Iteration 7400: Loss = -11026.316856017685
2
Iteration 7500: Loss = -11026.303613973025
Iteration 7600: Loss = -11026.305119100789
1
Iteration 7700: Loss = -11026.303558206473
Iteration 7800: Loss = -11026.303606061783
Iteration 7900: Loss = -11026.30354094481
Iteration 8000: Loss = -11026.30361068383
Iteration 8100: Loss = -11026.35042444548
1
Iteration 8200: Loss = -11026.303489449356
Iteration 8300: Loss = -11026.303528301414
Iteration 8400: Loss = -11026.303472672842
Iteration 8500: Loss = -11026.306037746976
1
Iteration 8600: Loss = -11026.303431914062
Iteration 8700: Loss = -11026.322397664819
1
Iteration 8800: Loss = -11026.30341821831
Iteration 8900: Loss = -11026.303730525096
1
Iteration 9000: Loss = -11026.30352439481
2
Iteration 9100: Loss = -11026.303422201732
Iteration 9200: Loss = -11026.317692286393
1
Iteration 9300: Loss = -11026.30343485005
Iteration 9400: Loss = -11026.303417893156
Iteration 9500: Loss = -11026.303464819192
Iteration 9600: Loss = -11026.303411209852
Iteration 9700: Loss = -11026.353890289787
1
Iteration 9800: Loss = -11026.303414296799
Iteration 9900: Loss = -11026.303405483795
Iteration 10000: Loss = -11026.307127945172
1
Iteration 10100: Loss = -11026.303386169115
Iteration 10200: Loss = -11026.322668153456
1
Iteration 10300: Loss = -11026.3033515188
Iteration 10400: Loss = -11026.321150225473
1
Iteration 10500: Loss = -11026.303399793827
Iteration 10600: Loss = -11026.325155820394
1
Iteration 10700: Loss = -11026.44563787842
2
Iteration 10800: Loss = -11026.304635827519
3
Iteration 10900: Loss = -11026.303411929806
Iteration 11000: Loss = -11026.314603625417
1
Iteration 11100: Loss = -11026.303402881495
Iteration 11200: Loss = -11026.30363572556
1
Iteration 11300: Loss = -11026.397872303092
2
Iteration 11400: Loss = -11026.303514183486
3
Iteration 11500: Loss = -11026.303472712363
Iteration 11600: Loss = -11026.305994766279
1
Iteration 11700: Loss = -11026.30345087136
Iteration 11800: Loss = -11026.303367573846
Iteration 11900: Loss = -11026.309240365455
1
Iteration 12000: Loss = -11026.303373315812
Iteration 12100: Loss = -11026.303463297281
Iteration 12200: Loss = -11026.517766358449
1
Iteration 12300: Loss = -11026.303363809733
Iteration 12400: Loss = -11026.329656022175
1
Iteration 12500: Loss = -11026.303368606525
Iteration 12600: Loss = -11026.306872851754
1
Iteration 12700: Loss = -11026.303379898956
Iteration 12800: Loss = -11026.304735601345
1
Iteration 12900: Loss = -11026.303350462758
Iteration 13000: Loss = -11026.31190306958
1
Iteration 13100: Loss = -11026.303379835803
Iteration 13200: Loss = -11026.49111257455
1
Iteration 13300: Loss = -11026.303338778345
Iteration 13400: Loss = -11026.303378959663
Iteration 13500: Loss = -11026.307779218461
1
Iteration 13600: Loss = -11026.303360622287
Iteration 13700: Loss = -11026.320827999274
1
Iteration 13800: Loss = -11026.303394454379
Iteration 13900: Loss = -11026.305177084692
1
Iteration 14000: Loss = -11026.303393513632
Iteration 14100: Loss = -11026.30335906053
Iteration 14200: Loss = -11026.303526151636
1
Iteration 14300: Loss = -11026.303376086957
Iteration 14400: Loss = -11026.316404088831
1
Iteration 14500: Loss = -11026.303372660701
Iteration 14600: Loss = -11026.534521589589
1
Iteration 14700: Loss = -11026.303393048862
Iteration 14800: Loss = -11026.303465043911
Iteration 14900: Loss = -11026.30823692545
1
Iteration 15000: Loss = -11026.304303985475
2
Iteration 15100: Loss = -11026.353704404904
3
Iteration 15200: Loss = -11026.303474172502
Iteration 15300: Loss = -11026.303405319917
Iteration 15400: Loss = -11026.303672427353
1
Iteration 15500: Loss = -11026.303383612689
Iteration 15600: Loss = -11026.303412508827
Iteration 15700: Loss = -11026.307920303097
1
Iteration 15800: Loss = -11026.303387917209
Iteration 15900: Loss = -11026.30491650285
1
Iteration 16000: Loss = -11026.483105083054
2
Iteration 16100: Loss = -11026.303452702034
Iteration 16200: Loss = -11026.357847478735
1
Iteration 16300: Loss = -11026.31071201865
2
Iteration 16400: Loss = -11026.317943714112
3
Iteration 16500: Loss = -11026.303951294754
4
Iteration 16600: Loss = -11026.304411359872
5
Iteration 16700: Loss = -11026.305951687535
6
Iteration 16800: Loss = -11026.304317674676
7
Iteration 16900: Loss = -11026.3034793602
Iteration 17000: Loss = -11026.303584475843
1
Iteration 17100: Loss = -11026.449690291904
2
Iteration 17200: Loss = -11026.303364505357
Iteration 17300: Loss = -11026.30358485998
1
Iteration 17400: Loss = -11026.517022614638
2
Iteration 17500: Loss = -11026.303361999357
Iteration 17600: Loss = -11026.306146732846
1
Iteration 17700: Loss = -11026.3033546662
Iteration 17800: Loss = -11026.30483234367
1
Iteration 17900: Loss = -11026.303348880901
Iteration 18000: Loss = -11026.305148197553
1
Iteration 18100: Loss = -11026.303367295155
Iteration 18200: Loss = -11026.303825470193
1
Iteration 18300: Loss = -11026.303388473134
Iteration 18400: Loss = -11026.318623221967
1
Iteration 18500: Loss = -11026.303385029254
Iteration 18600: Loss = -11026.303403820395
Iteration 18700: Loss = -11026.303581571916
1
Iteration 18800: Loss = -11026.30335596865
Iteration 18900: Loss = -11026.303368521485
Iteration 19000: Loss = -11026.305828344988
1
Iteration 19100: Loss = -11026.303348687636
Iteration 19200: Loss = -11026.31709977623
1
Iteration 19300: Loss = -11026.303340364166
Iteration 19400: Loss = -11026.404869947182
1
Iteration 19500: Loss = -11026.30367139878
2
Iteration 19600: Loss = -11026.303364368454
Iteration 19700: Loss = -11026.305477893928
1
Iteration 19800: Loss = -11026.30339319555
Iteration 19900: Loss = -11026.3046678751
1
pi: tensor([[0.8352, 0.1648],
        [0.3914, 0.6086]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9810, 0.0190], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1514, 0.2753],
         [0.6739, 0.2216]],

        [[0.5070, 0.1678],
         [0.6844, 0.6139]],

        [[0.6995, 0.1847],
         [0.7222, 0.6336]],

        [[0.6304, 0.1817],
         [0.5593, 0.6206]],

        [[0.5329, 0.1778],
         [0.6086, 0.7056]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.011345218800648298
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.005004658644184422
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.027108887462139547
Global Adjusted Rand Index: 0.0016084095299759163
Average Adjusted Rand Index: 0.002923235147585964
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23660.137418284216
Iteration 100: Loss = -11034.72929196928
Iteration 200: Loss = -11033.553394104189
Iteration 300: Loss = -11033.249010449805
Iteration 400: Loss = -11033.047640581817
Iteration 500: Loss = -11032.579191373039
Iteration 600: Loss = -11031.503718930642
Iteration 700: Loss = -11030.895371151377
Iteration 800: Loss = -11030.451286603138
Iteration 900: Loss = -11030.117825940742
Iteration 1000: Loss = -11029.85772564793
Iteration 1100: Loss = -11029.65471367026
Iteration 1200: Loss = -11029.465855414028
Iteration 1300: Loss = -11029.263649984328
Iteration 1400: Loss = -11029.063585215703
Iteration 1500: Loss = -11028.876931307565
Iteration 1600: Loss = -11028.70083976395
Iteration 1700: Loss = -11028.540115194253
Iteration 1800: Loss = -11028.40967778698
Iteration 1900: Loss = -11028.308436717707
Iteration 2000: Loss = -11028.234615886495
Iteration 2100: Loss = -11028.185702247907
Iteration 2200: Loss = -11028.154610719374
Iteration 2300: Loss = -11028.133959978068
Iteration 2400: Loss = -11028.119149382876
Iteration 2500: Loss = -11028.108028346347
Iteration 2600: Loss = -11028.099372816954
Iteration 2700: Loss = -11028.092423954493
Iteration 2800: Loss = -11028.086692101097
Iteration 2900: Loss = -11028.081958790382
Iteration 3000: Loss = -11028.077963575062
Iteration 3100: Loss = -11028.074597370582
Iteration 3200: Loss = -11028.071772754816
Iteration 3300: Loss = -11028.069304653733
Iteration 3400: Loss = -11028.067113880426
Iteration 3500: Loss = -11028.065912786655
Iteration 3600: Loss = -11028.063571704271
Iteration 3700: Loss = -11028.062066627599
Iteration 3800: Loss = -11028.061171919344
Iteration 3900: Loss = -11028.059547191528
Iteration 4000: Loss = -11028.058416771033
Iteration 4100: Loss = -11028.057449565758
Iteration 4200: Loss = -11028.05650061757
Iteration 4300: Loss = -11028.058221054838
1
Iteration 4400: Loss = -11028.054854239172
Iteration 4500: Loss = -11028.054119322605
Iteration 4600: Loss = -11028.05403923491
Iteration 4700: Loss = -11028.05276212511
Iteration 4800: Loss = -11028.05202605741
Iteration 4900: Loss = -11028.05130521206
Iteration 5000: Loss = -11028.050293510325
Iteration 5100: Loss = -11028.048944445709
Iteration 5200: Loss = -11028.041449252394
Iteration 5300: Loss = -11020.863798979275
Iteration 5400: Loss = -11019.949679808728
Iteration 5500: Loss = -11019.423912977152
Iteration 5600: Loss = -11019.291959032766
Iteration 5700: Loss = -11019.271788906286
Iteration 5800: Loss = -11019.261019637499
Iteration 5900: Loss = -11019.253991251855
Iteration 6000: Loss = -11019.248984839549
Iteration 6100: Loss = -11019.245226306735
Iteration 6200: Loss = -11019.242298721525
Iteration 6300: Loss = -11019.239981342287
Iteration 6400: Loss = -11019.238007753793
Iteration 6500: Loss = -11019.236428916585
Iteration 6600: Loss = -11019.235031147486
Iteration 6700: Loss = -11019.233881961
Iteration 6800: Loss = -11019.232874481451
Iteration 6900: Loss = -11019.232013662267
Iteration 7000: Loss = -11019.231229116358
Iteration 7100: Loss = -11019.230546206587
Iteration 7200: Loss = -11019.22994039234
Iteration 7300: Loss = -11019.229399904994
Iteration 7400: Loss = -11019.228917036096
Iteration 7500: Loss = -11019.22846337661
Iteration 7600: Loss = -11019.228046774018
Iteration 7700: Loss = -11019.227695756228
Iteration 7800: Loss = -11019.232801286918
1
Iteration 7900: Loss = -11019.284990703587
2
Iteration 8000: Loss = -11019.231313543996
3
Iteration 8100: Loss = -11019.260921291228
4
Iteration 8200: Loss = -11019.22631254613
Iteration 8300: Loss = -11019.226304595812
Iteration 8400: Loss = -11019.226161870378
Iteration 8500: Loss = -11019.225653622829
Iteration 8600: Loss = -11019.22673147946
1
Iteration 8700: Loss = -11019.225332624472
Iteration 8800: Loss = -11019.386684407602
1
Iteration 8900: Loss = -11019.22501705066
Iteration 9000: Loss = -11019.224882731909
Iteration 9100: Loss = -11019.22527336074
1
Iteration 9200: Loss = -11019.224659638121
Iteration 9300: Loss = -11019.226470732432
1
Iteration 9400: Loss = -11019.224483799522
Iteration 9500: Loss = -11019.224364521553
Iteration 9600: Loss = -11019.234095439131
1
Iteration 9700: Loss = -11019.224199677563
Iteration 9800: Loss = -11019.224099590823
Iteration 9900: Loss = -11019.244944917935
1
Iteration 10000: Loss = -11019.223969796236
Iteration 10100: Loss = -11019.223864145439
Iteration 10200: Loss = -11019.249049250684
1
Iteration 10300: Loss = -11019.22378721436
Iteration 10400: Loss = -11019.223721886598
Iteration 10500: Loss = -11019.227765543279
1
Iteration 10600: Loss = -11019.223629874134
Iteration 10700: Loss = -11019.243356712175
1
Iteration 10800: Loss = -11019.22600389104
2
Iteration 10900: Loss = -11019.224361885543
3
Iteration 11000: Loss = -11019.224380914678
4
Iteration 11100: Loss = -11019.250919423297
5
Iteration 11200: Loss = -11019.223367339273
Iteration 11300: Loss = -11019.231546627114
1
Iteration 11400: Loss = -11019.223311356493
Iteration 11500: Loss = -11019.225197393009
1
Iteration 11600: Loss = -11019.223597740869
2
Iteration 11700: Loss = -11019.22327173146
Iteration 11800: Loss = -11019.224350930066
1
Iteration 11900: Loss = -11019.224316149139
2
Iteration 12000: Loss = -11019.223720637912
3
Iteration 12100: Loss = -11019.223135125467
Iteration 12200: Loss = -11019.223439574062
1
Iteration 12300: Loss = -11019.223109768129
Iteration 12400: Loss = -11019.223712568013
1
Iteration 12500: Loss = -11019.223057984571
Iteration 12600: Loss = -11019.23446820624
1
Iteration 12700: Loss = -11019.223032312155
Iteration 12800: Loss = -11019.223075487616
Iteration 12900: Loss = -11019.22323259652
1
Iteration 13000: Loss = -11019.224255630521
2
Iteration 13100: Loss = -11019.224219932205
3
Iteration 13200: Loss = -11019.223814313069
4
Iteration 13300: Loss = -11019.22317995251
5
Iteration 13400: Loss = -11019.252482800417
6
Iteration 13500: Loss = -11019.222954228986
Iteration 13600: Loss = -11019.224263246537
1
Iteration 13700: Loss = -11019.224035417294
2
Iteration 13800: Loss = -11019.331494170754
3
Iteration 13900: Loss = -11019.222915467262
Iteration 14000: Loss = -11019.22305521011
1
Iteration 14100: Loss = -11019.225607967632
2
Iteration 14200: Loss = -11019.223335685785
3
Iteration 14300: Loss = -11019.223079934047
4
Iteration 14400: Loss = -11019.225319294295
5
Iteration 14500: Loss = -11019.223626846213
6
Iteration 14600: Loss = -11019.236151279663
7
Iteration 14700: Loss = -11019.228817782998
8
Iteration 14800: Loss = -11019.250118832608
9
Iteration 14900: Loss = -11019.222983998401
Iteration 15000: Loss = -11019.222915081682
Iteration 15100: Loss = -11019.237135499676
1
Iteration 15200: Loss = -11019.22356013541
2
Iteration 15300: Loss = -11019.223966844682
3
Iteration 15400: Loss = -11019.22333251454
4
Iteration 15500: Loss = -11019.225486001089
5
Iteration 15600: Loss = -11019.224013927535
6
Iteration 15700: Loss = -11019.222882351625
Iteration 15800: Loss = -11019.22313359767
1
Iteration 15900: Loss = -11019.230910077817
2
Iteration 16000: Loss = -11019.222870775666
Iteration 16100: Loss = -11019.224573528554
1
Iteration 16200: Loss = -11019.223202421725
2
Iteration 16300: Loss = -11019.225200643215
3
Iteration 16400: Loss = -11019.33313933786
4
Iteration 16500: Loss = -11019.222888547423
Iteration 16600: Loss = -11019.223072652685
1
Iteration 16700: Loss = -11019.41771676645
2
Iteration 16800: Loss = -11019.227540477668
3
Iteration 16900: Loss = -11019.223616668274
4
Iteration 17000: Loss = -11019.23613527654
5
Iteration 17100: Loss = -11019.373718135728
6
Iteration 17200: Loss = -11019.222841566478
Iteration 17300: Loss = -11019.223980852377
1
Iteration 17400: Loss = -11019.267111489185
2
Iteration 17500: Loss = -11019.223334986833
3
Iteration 17600: Loss = -11019.242194362821
4
Iteration 17700: Loss = -11019.22281087565
Iteration 17800: Loss = -11019.22316249313
1
Iteration 17900: Loss = -11019.222893502096
Iteration 18000: Loss = -11019.2228632426
Iteration 18100: Loss = -11019.308192862549
1
Iteration 18200: Loss = -11019.289289281554
2
Iteration 18300: Loss = -11019.222853144023
Iteration 18400: Loss = -11019.2228984398
Iteration 18500: Loss = -11019.234003269166
1
Iteration 18600: Loss = -11019.222793672849
Iteration 18700: Loss = -11019.223396768917
1
Iteration 18800: Loss = -11019.2763738083
2
Iteration 18900: Loss = -11019.222750317773
Iteration 19000: Loss = -11019.222790293323
Iteration 19100: Loss = -11019.224861332457
1
Iteration 19200: Loss = -11019.222827335296
Iteration 19300: Loss = -11019.223904118493
1
Iteration 19400: Loss = -11019.222802136384
Iteration 19500: Loss = -11019.303859180705
1
Iteration 19600: Loss = -11019.222806547546
Iteration 19700: Loss = -11019.533394537808
1
Iteration 19800: Loss = -11019.222780212722
Iteration 19900: Loss = -11019.234206242882
1
pi: tensor([[1.0000e+00, 6.6063e-07],
        [2.6819e-02, 9.7318e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0099, 0.9901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1933, 0.0101],
         [0.5937, 0.1604]],

        [[0.7045, 0.1179],
         [0.5310, 0.5068]],

        [[0.6108, 0.2031],
         [0.6429, 0.5689]],

        [[0.5548, 0.2247],
         [0.5625, 0.5265]],

        [[0.5933, 0.1943],
         [0.5721, 0.6081]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.012864505300896736
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0006921612735767433
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.011562904917628186
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.006809752538456861
Global Adjusted Rand Index: -0.0006718285287239074
Average Adjusted Rand Index: 0.0014529069462118761
10946.041375224544
[0.0016084095299759163, -0.0006718285287239074] [0.002923235147585964, 0.0014529069462118761] [11026.303372358832, 11019.22365568833]
-------------------------------------
This iteration is 15
True Objective function: Loss = -10793.527137263682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21209.686219964595
Iteration 100: Loss = -10925.622109334203
Iteration 200: Loss = -10924.706809256848
Iteration 300: Loss = -10924.410977215033
Iteration 400: Loss = -10924.013268587063
Iteration 500: Loss = -10922.877688860302
Iteration 600: Loss = -10921.975193829912
Iteration 700: Loss = -10921.725699521352
Iteration 800: Loss = -10921.56110172369
Iteration 900: Loss = -10921.44913213917
Iteration 1000: Loss = -10921.372096062643
Iteration 1100: Loss = -10921.32099546149
Iteration 1200: Loss = -10921.287880867725
Iteration 1300: Loss = -10921.2650859336
Iteration 1400: Loss = -10921.247873087028
Iteration 1500: Loss = -10921.230348763214
Iteration 1600: Loss = -10921.211198822803
Iteration 1700: Loss = -10921.202834237889
Iteration 1800: Loss = -10921.196973431703
Iteration 1900: Loss = -10921.192469969279
Iteration 2000: Loss = -10921.188921670533
Iteration 2100: Loss = -10921.185850221424
Iteration 2200: Loss = -10921.183161933117
Iteration 2300: Loss = -10921.180810090702
Iteration 2400: Loss = -10921.178644578828
Iteration 2500: Loss = -10921.17664677274
Iteration 2600: Loss = -10921.17478699222
Iteration 2700: Loss = -10921.172905174835
Iteration 2800: Loss = -10921.171186158175
Iteration 2900: Loss = -10921.169565830012
Iteration 3000: Loss = -10921.168049870299
Iteration 3100: Loss = -10921.16675081851
Iteration 3200: Loss = -10921.165487802116
Iteration 3300: Loss = -10921.164338119725
Iteration 3400: Loss = -10921.163245145543
Iteration 3500: Loss = -10921.16221443373
Iteration 3600: Loss = -10921.16119042186
Iteration 3700: Loss = -10921.160166487392
Iteration 3800: Loss = -10921.158972383191
Iteration 3900: Loss = -10921.15761474347
Iteration 4000: Loss = -10921.155906346135
Iteration 4100: Loss = -10921.153508964617
Iteration 4200: Loss = -10921.149601616089
Iteration 4300: Loss = -10921.141904585964
Iteration 4400: Loss = -10921.12235214175
Iteration 4500: Loss = -10921.069565852351
Iteration 4600: Loss = -10920.803364545598
Iteration 4700: Loss = -10920.0408046375
Iteration 4800: Loss = -10771.87312809277
Iteration 4900: Loss = -10771.580724091667
Iteration 5000: Loss = -10771.503669672391
Iteration 5100: Loss = -10771.399526155392
Iteration 5200: Loss = -10771.355776548316
Iteration 5300: Loss = -10771.306353348253
Iteration 5400: Loss = -10771.282118401203
Iteration 5500: Loss = -10771.272852239586
Iteration 5600: Loss = -10771.237095400988
Iteration 5700: Loss = -10771.216274982378
Iteration 5800: Loss = -10771.2154095228
Iteration 5900: Loss = -10771.200019219581
Iteration 6000: Loss = -10771.20068271861
1
Iteration 6100: Loss = -10771.184463129237
Iteration 6200: Loss = -10771.188828998831
1
Iteration 6300: Loss = -10771.175390486276
Iteration 6400: Loss = -10771.174255314885
Iteration 6500: Loss = -10771.17905084268
1
Iteration 6600: Loss = -10771.155381065768
Iteration 6700: Loss = -10771.154325722071
Iteration 6800: Loss = -10771.152453784933
Iteration 6900: Loss = -10771.147016515886
Iteration 7000: Loss = -10771.144953984769
Iteration 7100: Loss = -10771.147562038348
1
Iteration 7200: Loss = -10771.154976564134
2
Iteration 7300: Loss = -10771.154506686034
3
Iteration 7400: Loss = -10771.15034234354
4
Iteration 7500: Loss = -10771.220303242748
5
Iteration 7600: Loss = -10771.14009294964
Iteration 7700: Loss = -10771.140370120549
1
Iteration 7800: Loss = -10771.138315941846
Iteration 7900: Loss = -10771.133504264262
Iteration 8000: Loss = -10771.121882892361
Iteration 8100: Loss = -10771.114417613371
Iteration 8200: Loss = -10771.114058523379
Iteration 8300: Loss = -10771.120835518643
1
Iteration 8400: Loss = -10771.10885818484
Iteration 8500: Loss = -10771.08426201801
Iteration 8600: Loss = -10771.084580528437
1
Iteration 8700: Loss = -10771.08648149195
2
Iteration 8800: Loss = -10771.083315003738
Iteration 8900: Loss = -10771.082525827882
Iteration 9000: Loss = -10771.09576645789
1
Iteration 9100: Loss = -10771.080866892666
Iteration 9200: Loss = -10771.089624839726
1
Iteration 9300: Loss = -10771.081843596665
2
Iteration 9400: Loss = -10771.080576318185
Iteration 9500: Loss = -10771.078662195796
Iteration 9600: Loss = -10771.08095617874
1
Iteration 9700: Loss = -10771.073955330368
Iteration 9800: Loss = -10771.097557425686
1
Iteration 9900: Loss = -10771.074759814877
2
Iteration 10000: Loss = -10771.073310807164
Iteration 10100: Loss = -10771.073988956703
1
Iteration 10200: Loss = -10771.074571123703
2
Iteration 10300: Loss = -10771.069292613958
Iteration 10400: Loss = -10771.069346654973
Iteration 10500: Loss = -10771.078738356866
1
Iteration 10600: Loss = -10771.18718810946
2
Iteration 10700: Loss = -10771.082976143498
3
Iteration 10800: Loss = -10771.12752822695
4
Iteration 10900: Loss = -10771.073483371636
5
Iteration 11000: Loss = -10771.071005573998
6
Iteration 11100: Loss = -10771.073870057164
7
Iteration 11200: Loss = -10771.062968366363
Iteration 11300: Loss = -10771.063925846243
1
Iteration 11400: Loss = -10771.065841355596
2
Iteration 11500: Loss = -10771.064087780507
3
Iteration 11600: Loss = -10771.062911590489
Iteration 11700: Loss = -10771.069015873492
1
Iteration 11800: Loss = -10771.075533459196
2
Iteration 11900: Loss = -10771.071784019934
3
Iteration 12000: Loss = -10771.065968840769
4
Iteration 12100: Loss = -10771.064495502576
5
Iteration 12200: Loss = -10771.06628596038
6
Iteration 12300: Loss = -10771.06493347092
7
Iteration 12400: Loss = -10771.139604943037
8
Iteration 12500: Loss = -10771.06772070687
9
Iteration 12600: Loss = -10771.11584028401
10
Iteration 12700: Loss = -10771.077461136243
11
Iteration 12800: Loss = -10771.065123436161
12
Iteration 12900: Loss = -10771.068626913588
13
Iteration 13000: Loss = -10771.063340789311
14
Iteration 13100: Loss = -10771.081583217745
15
Stopping early at iteration 13100 due to no improvement.
pi: tensor([[0.7473, 0.2527],
        [0.2438, 0.7562]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5007, 0.4993], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2517, 0.0991],
         [0.6243, 0.1999]],

        [[0.5230, 0.0917],
         [0.5181, 0.6811]],

        [[0.5261, 0.0922],
         [0.7097, 0.6495]],

        [[0.6190, 0.0981],
         [0.7077, 0.5636]],

        [[0.7014, 0.0999],
         [0.7273, 0.5947]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446731470944564
Average Adjusted Rand Index: 0.9449667485675066
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19475.49487263403
Iteration 100: Loss = -10924.890373972981
Iteration 200: Loss = -10924.786277676078
Iteration 300: Loss = -10924.742580552764
Iteration 400: Loss = -10924.63658139999
Iteration 500: Loss = -10923.931508745598
Iteration 600: Loss = -10921.879819752072
Iteration 700: Loss = -10921.454939576872
Iteration 800: Loss = -10921.297571603864
Iteration 900: Loss = -10921.247141946445
Iteration 1000: Loss = -10921.224221780594
Iteration 1100: Loss = -10921.211633023293
Iteration 1200: Loss = -10921.203482159013
Iteration 1300: Loss = -10921.197320701458
Iteration 1400: Loss = -10921.19223488572
Iteration 1500: Loss = -10921.187842161462
Iteration 1600: Loss = -10921.18399159548
Iteration 1700: Loss = -10921.180550088007
Iteration 1800: Loss = -10921.177372194847
Iteration 1900: Loss = -10921.174435652525
Iteration 2000: Loss = -10921.17167548392
Iteration 2100: Loss = -10921.169109420298
Iteration 2200: Loss = -10921.166709405474
Iteration 2300: Loss = -10921.164503444284
Iteration 2400: Loss = -10921.162467236472
Iteration 2500: Loss = -10921.160357541652
Iteration 2600: Loss = -10921.157981004082
Iteration 2700: Loss = -10921.154497116415
Iteration 2800: Loss = -10921.146986285143
Iteration 2900: Loss = -10921.117882376402
Iteration 3000: Loss = -10920.930245331065
Iteration 3100: Loss = -10920.240892596197
Iteration 3200: Loss = -10771.942302824104
Iteration 3300: Loss = -10771.538976113225
Iteration 3400: Loss = -10771.449693743365
Iteration 3500: Loss = -10771.415657893209
Iteration 3600: Loss = -10771.37408063242
Iteration 3700: Loss = -10771.314111974456
Iteration 3800: Loss = -10771.30232959686
Iteration 3900: Loss = -10771.297726821855
Iteration 4000: Loss = -10771.286786397239
Iteration 4100: Loss = -10771.256026027597
Iteration 4200: Loss = -10771.232254822145
Iteration 4300: Loss = -10771.229265093
Iteration 4400: Loss = -10771.224313315317
Iteration 4500: Loss = -10771.218935606059
Iteration 4600: Loss = -10771.21422103604
Iteration 4700: Loss = -10771.195053325937
Iteration 4800: Loss = -10771.19122923153
Iteration 4900: Loss = -10771.185169773602
Iteration 5000: Loss = -10771.156622464145
Iteration 5100: Loss = -10771.155254909532
Iteration 5200: Loss = -10771.154069516098
Iteration 5300: Loss = -10771.14341332792
Iteration 5400: Loss = -10771.140865253976
Iteration 5500: Loss = -10771.129358341372
Iteration 5600: Loss = -10771.12933852278
Iteration 5700: Loss = -10771.128442830313
Iteration 5800: Loss = -10771.127944386406
Iteration 5900: Loss = -10771.126728552732
Iteration 6000: Loss = -10771.126102458225
Iteration 6100: Loss = -10771.125024297024
Iteration 6200: Loss = -10771.124493278161
Iteration 6300: Loss = -10771.124397407446
Iteration 6400: Loss = -10771.123970384084
Iteration 6500: Loss = -10771.122972302273
Iteration 6600: Loss = -10771.11656890333
Iteration 6700: Loss = -10771.115954185636
Iteration 6800: Loss = -10771.116062681638
1
Iteration 6900: Loss = -10771.113835838489
Iteration 7000: Loss = -10771.100760311334
Iteration 7100: Loss = -10771.080095279909
Iteration 7200: Loss = -10771.08162514172
1
Iteration 7300: Loss = -10771.095062273691
2
Iteration 7400: Loss = -10771.076946852509
Iteration 7500: Loss = -10771.077115703034
1
Iteration 7600: Loss = -10771.157487608754
2
Iteration 7700: Loss = -10771.07760084108
3
Iteration 7800: Loss = -10771.082093167937
4
Iteration 7900: Loss = -10771.077846531196
5
Iteration 8000: Loss = -10771.07637173155
Iteration 8100: Loss = -10771.075419596516
Iteration 8200: Loss = -10771.074757820632
Iteration 8300: Loss = -10771.072453559318
Iteration 8400: Loss = -10771.073465125959
1
Iteration 8500: Loss = -10771.080193127515
2
Iteration 8600: Loss = -10771.069085514808
Iteration 8700: Loss = -10771.06940229323
1
Iteration 8800: Loss = -10771.06922443397
2
Iteration 8900: Loss = -10771.068539514094
Iteration 9000: Loss = -10771.071781464625
1
Iteration 9100: Loss = -10771.068127306467
Iteration 9200: Loss = -10771.068281147278
1
Iteration 9300: Loss = -10771.068675717079
2
Iteration 9400: Loss = -10771.070706226397
3
Iteration 9500: Loss = -10771.068246680317
4
Iteration 9600: Loss = -10771.063464614528
Iteration 9700: Loss = -10771.062806930222
Iteration 9800: Loss = -10771.064466598855
1
Iteration 9900: Loss = -10771.068435967372
2
Iteration 10000: Loss = -10771.064441578497
3
Iteration 10100: Loss = -10771.062803499182
Iteration 10200: Loss = -10771.096523903907
1
Iteration 10300: Loss = -10771.063740140578
2
Iteration 10400: Loss = -10771.062970708912
3
Iteration 10500: Loss = -10771.06362309714
4
Iteration 10600: Loss = -10771.06759625209
5
Iteration 10700: Loss = -10771.083708540411
6
Iteration 10800: Loss = -10771.06375538134
7
Iteration 10900: Loss = -10771.064765684472
8
Iteration 11000: Loss = -10771.067290742829
9
Iteration 11100: Loss = -10771.06378468637
10
Iteration 11200: Loss = -10771.063476945978
11
Iteration 11300: Loss = -10771.062614289925
Iteration 11400: Loss = -10771.063210666054
1
Iteration 11500: Loss = -10771.063601610307
2
Iteration 11600: Loss = -10771.064857218147
3
Iteration 11700: Loss = -10771.067573473669
4
Iteration 11800: Loss = -10771.063556346084
5
Iteration 11900: Loss = -10771.117222482484
6
Iteration 12000: Loss = -10771.062683330643
Iteration 12100: Loss = -10771.06272710611
Iteration 12200: Loss = -10771.064450503569
1
Iteration 12300: Loss = -10771.067474608437
2
Iteration 12400: Loss = -10771.09141343559
3
Iteration 12500: Loss = -10771.067295478853
4
Iteration 12600: Loss = -10771.063404859535
5
Iteration 12700: Loss = -10771.064966825981
6
Iteration 12800: Loss = -10771.103110807304
7
Iteration 12900: Loss = -10771.062649155814
Iteration 13000: Loss = -10771.062827558628
1
Iteration 13100: Loss = -10771.097253665304
2
Iteration 13200: Loss = -10771.06555711827
3
Iteration 13300: Loss = -10771.076647738437
4
Iteration 13400: Loss = -10771.062857732682
5
Iteration 13500: Loss = -10771.06650887434
6
Iteration 13600: Loss = -10771.064159782647
7
Iteration 13700: Loss = -10771.06310150434
8
Iteration 13800: Loss = -10771.0636872316
9
Iteration 13900: Loss = -10771.063577794268
10
Iteration 14000: Loss = -10771.063468293007
11
Iteration 14100: Loss = -10771.066557363627
12
Iteration 14200: Loss = -10771.063124185814
13
Iteration 14300: Loss = -10771.065545499028
14
Iteration 14400: Loss = -10771.068812857484
15
Stopping early at iteration 14400 due to no improvement.
pi: tensor([[0.7591, 0.2409],
        [0.2531, 0.7469]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4991, 0.5009], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.0994],
         [0.5526, 0.2519]],

        [[0.6308, 0.0919],
         [0.7265, 0.6011]],

        [[0.6326, 0.0925],
         [0.7097, 0.5625]],

        [[0.6182, 0.0984],
         [0.5918, 0.6832]],

        [[0.6967, 0.1002],
         [0.6068, 0.7123]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446731470944564
Average Adjusted Rand Index: 0.9449667485675066
10793.527137263682
[0.9446731470944564, 0.9446731470944564] [0.9449667485675066, 0.9449667485675066] [10771.081583217745, 10771.068812857484]
-------------------------------------
This iteration is 16
True Objective function: Loss = -10836.40406986485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23649.94499278615
Iteration 100: Loss = -10939.558812115647
Iteration 200: Loss = -10938.55420649167
Iteration 300: Loss = -10938.248682010983
Iteration 400: Loss = -10938.088170769857
Iteration 500: Loss = -10937.963059275271
Iteration 600: Loss = -10937.811228534563
Iteration 700: Loss = -10937.558413123163
Iteration 800: Loss = -10937.218536128683
Iteration 900: Loss = -10936.767061059618
Iteration 1000: Loss = -10936.404045181269
Iteration 1100: Loss = -10936.196388607146
Iteration 1200: Loss = -10936.076005286472
Iteration 1300: Loss = -10936.003357079395
Iteration 1400: Loss = -10935.95566109414
Iteration 1500: Loss = -10935.908967211806
Iteration 1600: Loss = -10935.858908425047
Iteration 1700: Loss = -10935.827855911848
Iteration 1800: Loss = -10935.805251736801
Iteration 1900: Loss = -10935.784385451663
Iteration 2000: Loss = -10935.76325729327
Iteration 2100: Loss = -10935.738697159171
Iteration 2200: Loss = -10935.70543874208
Iteration 2300: Loss = -10935.657539921734
Iteration 2400: Loss = -10935.57191472347
Iteration 2500: Loss = -10935.341436011811
Iteration 2600: Loss = -10806.573103550205
Iteration 2700: Loss = -10795.56251778156
Iteration 2800: Loss = -10795.133273520543
Iteration 2900: Loss = -10794.916888993179
Iteration 3000: Loss = -10794.897316807634
Iteration 3100: Loss = -10794.884268464484
Iteration 3200: Loss = -10794.867903329992
Iteration 3300: Loss = -10794.860628943828
Iteration 3400: Loss = -10794.85686419913
Iteration 3500: Loss = -10794.854050755303
Iteration 3600: Loss = -10794.851879099966
Iteration 3700: Loss = -10794.848055036568
Iteration 3800: Loss = -10794.835172222185
Iteration 3900: Loss = -10794.826396261233
Iteration 4000: Loss = -10794.827214402536
1
Iteration 4100: Loss = -10794.823287704783
Iteration 4200: Loss = -10794.823921739322
1
Iteration 4300: Loss = -10794.82161323869
Iteration 4400: Loss = -10794.828161611065
1
Iteration 4500: Loss = -10794.820227887149
Iteration 4600: Loss = -10794.84235061489
1
Iteration 4700: Loss = -10794.818502701295
Iteration 4800: Loss = -10794.817341462853
Iteration 4900: Loss = -10794.798595781464
Iteration 5000: Loss = -10794.794849735395
Iteration 5100: Loss = -10794.782144333805
Iteration 5200: Loss = -10794.779457318702
Iteration 5300: Loss = -10794.755306330306
Iteration 5400: Loss = -10794.750315478317
Iteration 5500: Loss = -10791.625361688417
Iteration 5600: Loss = -10791.61858722348
Iteration 5700: Loss = -10791.627268907712
1
Iteration 5800: Loss = -10791.617404074532
Iteration 5900: Loss = -10791.61463844369
Iteration 6000: Loss = -10791.609558382295
Iteration 6100: Loss = -10791.608915373668
Iteration 6200: Loss = -10791.61044945697
1
Iteration 6300: Loss = -10791.609523198247
2
Iteration 6400: Loss = -10791.608446819746
Iteration 6500: Loss = -10791.609132160187
1
Iteration 6600: Loss = -10791.607856632982
Iteration 6700: Loss = -10791.607551856483
Iteration 6800: Loss = -10791.607420688773
Iteration 6900: Loss = -10791.607744118815
1
Iteration 7000: Loss = -10791.607363076842
Iteration 7100: Loss = -10791.60737570343
Iteration 7200: Loss = -10791.608758534048
1
Iteration 7300: Loss = -10791.60720021933
Iteration 7400: Loss = -10791.607133225547
Iteration 7500: Loss = -10791.607391222604
1
Iteration 7600: Loss = -10791.61155070021
2
Iteration 7700: Loss = -10791.608614934268
3
Iteration 7800: Loss = -10791.659663686518
4
Iteration 7900: Loss = -10791.60335091659
Iteration 8000: Loss = -10791.602114313708
Iteration 8100: Loss = -10791.601215041845
Iteration 8200: Loss = -10791.602211795758
1
Iteration 8300: Loss = -10791.600600213538
Iteration 8400: Loss = -10791.620341878513
1
Iteration 8500: Loss = -10791.588399686816
Iteration 8600: Loss = -10791.588305381658
Iteration 8700: Loss = -10791.588333571955
Iteration 8800: Loss = -10791.588271171571
Iteration 8900: Loss = -10791.591278036769
1
Iteration 9000: Loss = -10791.588162705815
Iteration 9100: Loss = -10791.588217412456
Iteration 9200: Loss = -10791.588071762002
Iteration 9300: Loss = -10791.587606082494
Iteration 9400: Loss = -10791.586125835554
Iteration 9500: Loss = -10791.586621027054
1
Iteration 9600: Loss = -10791.642717397219
2
Iteration 9700: Loss = -10791.58594976637
Iteration 9800: Loss = -10791.588359657368
1
Iteration 9900: Loss = -10791.585898600919
Iteration 10000: Loss = -10791.601445657714
1
Iteration 10100: Loss = -10791.586809696104
2
Iteration 10200: Loss = -10791.59015739074
3
Iteration 10300: Loss = -10791.581675090434
Iteration 10400: Loss = -10791.581704337677
Iteration 10500: Loss = -10791.858842864285
1
Iteration 10600: Loss = -10791.581524809562
Iteration 10700: Loss = -10791.744376202649
1
Iteration 10800: Loss = -10791.581454688247
Iteration 10900: Loss = -10791.583254730635
1
Iteration 11000: Loss = -10791.581424000666
Iteration 11100: Loss = -10791.581401454583
Iteration 11200: Loss = -10791.581511306114
1
Iteration 11300: Loss = -10791.580998171421
Iteration 11400: Loss = -10791.697637347288
1
Iteration 11500: Loss = -10791.58095578752
Iteration 11600: Loss = -10791.580916947509
Iteration 11700: Loss = -10791.606123096115
1
Iteration 11800: Loss = -10791.580787031873
Iteration 11900: Loss = -10791.595304137432
1
Iteration 12000: Loss = -10791.581541111911
2
Iteration 12100: Loss = -10791.741167625492
3
Iteration 12200: Loss = -10791.580727920718
Iteration 12300: Loss = -10791.58086710315
1
Iteration 12400: Loss = -10791.857579173755
2
Iteration 12500: Loss = -10791.580618202894
Iteration 12600: Loss = -10791.593037784967
1
Iteration 12700: Loss = -10791.580557430894
Iteration 12800: Loss = -10791.607044197712
1
Iteration 12900: Loss = -10791.580495626491
Iteration 13000: Loss = -10791.594745861696
1
Iteration 13100: Loss = -10791.580376589594
Iteration 13200: Loss = -10791.59500819885
1
Iteration 13300: Loss = -10791.580262640711
Iteration 13400: Loss = -10791.58025114288
Iteration 13500: Loss = -10791.58115853436
1
Iteration 13600: Loss = -10791.580043610264
Iteration 13700: Loss = -10791.57992625608
Iteration 13800: Loss = -10791.579341046669
Iteration 13900: Loss = -10791.575399673879
Iteration 14000: Loss = -10791.588924486883
1
Iteration 14100: Loss = -10791.57542784399
Iteration 14200: Loss = -10791.5753843415
Iteration 14300: Loss = -10791.578531453251
1
Iteration 14400: Loss = -10791.575406084066
Iteration 14500: Loss = -10791.575341865217
Iteration 14600: Loss = -10791.575382860907
Iteration 14700: Loss = -10791.575314979864
Iteration 14800: Loss = -10791.576432625658
1
Iteration 14900: Loss = -10791.57529105157
Iteration 15000: Loss = -10792.085252386345
1
Iteration 15100: Loss = -10791.57529798408
Iteration 15200: Loss = -10791.575160426833
Iteration 15300: Loss = -10791.595186420915
1
Iteration 15400: Loss = -10791.575065080455
Iteration 15500: Loss = -10791.616974979475
1
Iteration 15600: Loss = -10791.575120610087
Iteration 15700: Loss = -10791.575045162157
Iteration 15800: Loss = -10791.576865563744
1
Iteration 15900: Loss = -10791.57500867526
Iteration 16000: Loss = -10791.575665472255
1
Iteration 16100: Loss = -10791.574976899123
Iteration 16200: Loss = -10791.591762930448
1
Iteration 16300: Loss = -10791.574988504613
Iteration 16400: Loss = -10791.575073316562
Iteration 16500: Loss = -10791.575074135986
Iteration 16600: Loss = -10791.609234359015
1
Iteration 16700: Loss = -10791.575138410108
Iteration 16800: Loss = -10791.575105830341
Iteration 16900: Loss = -10791.579700436601
1
Iteration 17000: Loss = -10791.601602113029
2
Iteration 17100: Loss = -10791.57500542926
Iteration 17200: Loss = -10791.575078734673
Iteration 17300: Loss = -10791.574982111952
Iteration 17400: Loss = -10791.575079319808
Iteration 17500: Loss = -10791.57718875488
1
Iteration 17600: Loss = -10791.575034252308
Iteration 17700: Loss = -10791.574973058892
Iteration 17800: Loss = -10791.575160363196
1
Iteration 17900: Loss = -10791.574921961837
Iteration 18000: Loss = -10791.575124628902
1
Iteration 18100: Loss = -10791.633646045619
2
Iteration 18200: Loss = -10791.574918251074
Iteration 18300: Loss = -10791.742586890598
1
Iteration 18400: Loss = -10791.574934997403
Iteration 18500: Loss = -10791.579300078509
1
Iteration 18600: Loss = -10791.57489357986
Iteration 18700: Loss = -10791.575180715996
1
Iteration 18800: Loss = -10791.574935825145
Iteration 18900: Loss = -10791.587094758701
1
Iteration 19000: Loss = -10791.57492901599
Iteration 19100: Loss = -10791.574938941698
Iteration 19200: Loss = -10791.574989136196
Iteration 19300: Loss = -10791.578401731062
1
Iteration 19400: Loss = -10791.574957668956
Iteration 19500: Loss = -10791.578179414548
1
Iteration 19600: Loss = -10791.574891334174
Iteration 19700: Loss = -10791.57686725563
1
Iteration 19800: Loss = -10791.76473427723
2
Iteration 19900: Loss = -10791.574897780805
pi: tensor([[0.7612, 0.2388],
        [0.1914, 0.8086]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4316, 0.5684], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2030, 0.1023],
         [0.7217, 0.2363]],

        [[0.6986, 0.0986],
         [0.5823, 0.5004]],

        [[0.5070, 0.0983],
         [0.5183, 0.7158]],

        [[0.7216, 0.0881],
         [0.6862, 0.6913]],

        [[0.5965, 0.1042],
         [0.6116, 0.6951]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369913366172994
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 89
Adjusted Rand Index: 0.6045048763898703
Global Adjusted Rand Index: 0.7881127736935931
Average Adjusted Rand Index: 0.7916888960482138
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22043.185984891377
Iteration 100: Loss = -10939.8027144724
Iteration 200: Loss = -10938.543347640485
Iteration 300: Loss = -10938.123829363589
Iteration 400: Loss = -10937.965587526733
Iteration 500: Loss = -10937.872495060094
Iteration 600: Loss = -10937.793764217564
Iteration 700: Loss = -10937.720131080383
Iteration 800: Loss = -10937.654967120372
Iteration 900: Loss = -10937.584932347007
Iteration 1000: Loss = -10937.47526182982
Iteration 1100: Loss = -10937.221167946982
Iteration 1200: Loss = -10936.719267791288
Iteration 1300: Loss = -10936.387414073852
Iteration 1400: Loss = -10936.177416495126
Iteration 1500: Loss = -10936.034502513867
Iteration 1600: Loss = -10935.949413430892
Iteration 1700: Loss = -10935.889865432526
Iteration 1800: Loss = -10935.848976952173
Iteration 1900: Loss = -10935.816777843631
Iteration 2000: Loss = -10935.790955175426
Iteration 2100: Loss = -10935.767519659508
Iteration 2200: Loss = -10935.74414961591
Iteration 2300: Loss = -10935.719298844308
Iteration 2400: Loss = -10935.688966191517
Iteration 2500: Loss = -10935.646364117969
Iteration 2600: Loss = -10935.57242125974
Iteration 2700: Loss = -10935.344049617894
Iteration 2800: Loss = -10849.249529379576
Iteration 2900: Loss = -10806.483448505305
Iteration 3000: Loss = -10801.58553497826
Iteration 3100: Loss = -10801.3318511228
Iteration 3200: Loss = -10799.950673404654
Iteration 3300: Loss = -10799.761880779472
Iteration 3400: Loss = -10799.720502703593
Iteration 3500: Loss = -10799.711318722068
Iteration 3600: Loss = -10799.70355888722
Iteration 3700: Loss = -10799.694888101649
Iteration 3800: Loss = -10799.626160435915
Iteration 3900: Loss = -10799.591052465263
Iteration 4000: Loss = -10799.56893455584
Iteration 4100: Loss = -10799.556615687246
Iteration 4200: Loss = -10799.55389276331
Iteration 4300: Loss = -10799.551446478003
Iteration 4400: Loss = -10799.551306740279
Iteration 4500: Loss = -10799.546836393698
Iteration 4600: Loss = -10799.540170843584
Iteration 4700: Loss = -10799.291473562254
Iteration 4800: Loss = -10799.279200447405
Iteration 4900: Loss = -10797.530811094124
Iteration 5000: Loss = -10797.436115609855
Iteration 5100: Loss = -10797.43216529599
Iteration 5200: Loss = -10797.015348636742
Iteration 5300: Loss = -10797.00270984004
Iteration 5400: Loss = -10797.005776707603
1
Iteration 5500: Loss = -10796.992338602791
Iteration 5600: Loss = -10796.951254844758
Iteration 5700: Loss = -10796.944334529617
Iteration 5800: Loss = -10796.912114807481
Iteration 5900: Loss = -10796.888849250206
Iteration 6000: Loss = -10796.886465461255
Iteration 6100: Loss = -10796.883669565439
Iteration 6200: Loss = -10796.88123977208
Iteration 6300: Loss = -10796.874498168732
Iteration 6400: Loss = -10796.875971987762
1
Iteration 6500: Loss = -10796.873806766102
Iteration 6600: Loss = -10796.884393613745
1
Iteration 6700: Loss = -10796.859126611182
Iteration 6800: Loss = -10796.849837870055
Iteration 6900: Loss = -10796.498883138693
Iteration 7000: Loss = -10796.500829134084
1
Iteration 7100: Loss = -10796.464371162441
Iteration 7200: Loss = -10796.512210478888
1
Iteration 7300: Loss = -10796.462043522688
Iteration 7400: Loss = -10793.533824908534
Iteration 7500: Loss = -10793.528808734176
Iteration 7600: Loss = -10793.52224865918
Iteration 7700: Loss = -10793.522613820187
1
Iteration 7800: Loss = -10793.52162060965
Iteration 7900: Loss = -10793.37695422065
Iteration 8000: Loss = -10793.368291228011
Iteration 8100: Loss = -10793.368565942521
1
Iteration 8200: Loss = -10793.367798723188
Iteration 8300: Loss = -10793.367771597474
Iteration 8400: Loss = -10793.376290404274
1
Iteration 8500: Loss = -10793.367566068966
Iteration 8600: Loss = -10793.367526567534
Iteration 8700: Loss = -10793.368176087512
1
Iteration 8800: Loss = -10793.367368050907
Iteration 8900: Loss = -10793.367248513361
Iteration 9000: Loss = -10793.367044337087
Iteration 9100: Loss = -10793.366664721392
Iteration 9200: Loss = -10793.372738589502
1
Iteration 9300: Loss = -10793.366395406001
Iteration 9400: Loss = -10793.366207344854
Iteration 9500: Loss = -10793.37107105703
1
Iteration 9600: Loss = -10793.36539829852
Iteration 9700: Loss = -10793.37725931817
1
Iteration 9800: Loss = -10793.36347300214
Iteration 9900: Loss = -10793.363417378501
Iteration 10000: Loss = -10793.363945012876
1
Iteration 10100: Loss = -10793.179496177445
Iteration 10200: Loss = -10793.175017761574
Iteration 10300: Loss = -10793.175432513432
1
Iteration 10400: Loss = -10793.179286919287
2
Iteration 10500: Loss = -10793.174787793352
Iteration 10600: Loss = -10793.181727654111
1
Iteration 10700: Loss = -10793.017485049213
Iteration 10800: Loss = -10792.974090145106
Iteration 10900: Loss = -10792.96889358719
Iteration 11000: Loss = -10792.963838096653
Iteration 11100: Loss = -10792.9582932413
Iteration 11200: Loss = -10792.064440916954
Iteration 11300: Loss = -10792.057862655554
Iteration 11400: Loss = -10792.389789837045
1
Iteration 11500: Loss = -10792.053464603752
Iteration 11600: Loss = -10792.053177906631
Iteration 11700: Loss = -10792.049138724446
Iteration 11800: Loss = -10791.79073473313
Iteration 11900: Loss = -10791.791894582917
1
Iteration 12000: Loss = -10791.736607944658
Iteration 12100: Loss = -10791.73261400603
Iteration 12200: Loss = -10791.741565388464
1
Iteration 12300: Loss = -10791.73099691237
Iteration 12400: Loss = -10791.731062852274
Iteration 12500: Loss = -10791.730936135622
Iteration 12600: Loss = -10791.731478138052
1
Iteration 12700: Loss = -10791.730756178911
Iteration 12800: Loss = -10791.811626075176
1
Iteration 12900: Loss = -10791.729431477916
Iteration 13000: Loss = -10791.729008612327
Iteration 13100: Loss = -10791.729247463027
1
Iteration 13200: Loss = -10791.728930062018
Iteration 13300: Loss = -10791.729429337945
1
Iteration 13400: Loss = -10791.72640877195
Iteration 13500: Loss = -10791.72318233372
Iteration 13600: Loss = -10791.722572087983
Iteration 13700: Loss = -10791.722266811106
Iteration 13800: Loss = -10791.720475253907
Iteration 13900: Loss = -10791.720438689152
Iteration 14000: Loss = -10791.834979590012
1
Iteration 14100: Loss = -10791.720389460845
Iteration 14200: Loss = -10791.72035409404
Iteration 14300: Loss = -10791.743321126405
1
Iteration 14400: Loss = -10791.719192090955
Iteration 14500: Loss = -10791.71917362349
Iteration 14600: Loss = -10791.725864418224
1
Iteration 14700: Loss = -10791.71871848834
Iteration 14800: Loss = -10791.71872159497
Iteration 14900: Loss = -10791.71986712879
1
Iteration 15000: Loss = -10791.718572767362
Iteration 15100: Loss = -10791.717979651514
Iteration 15200: Loss = -10791.706063496347
Iteration 15300: Loss = -10791.705563160296
Iteration 15400: Loss = -10791.70652670481
1
Iteration 15500: Loss = -10791.70543053327
Iteration 15600: Loss = -10791.707737293638
1
Iteration 15700: Loss = -10791.705363067536
Iteration 15800: Loss = -10791.710848983548
1
Iteration 15900: Loss = -10791.705130285742
Iteration 16000: Loss = -10791.770511883587
1
Iteration 16100: Loss = -10791.704910491631
Iteration 16200: Loss = -10791.700321350992
Iteration 16300: Loss = -10791.70028920476
Iteration 16400: Loss = -10791.715877083992
1
Iteration 16500: Loss = -10791.699739804104
Iteration 16600: Loss = -10791.699659392347
Iteration 16700: Loss = -10791.791599908556
1
Iteration 16800: Loss = -10791.69909914443
Iteration 16900: Loss = -10791.715704272392
1
Iteration 17000: Loss = -10791.69697656485
Iteration 17100: Loss = -10792.0494158755
1
Iteration 17200: Loss = -10791.696943936866
Iteration 17300: Loss = -10791.69696077556
Iteration 17400: Loss = -10791.69719948598
1
Iteration 17500: Loss = -10791.696979943521
Iteration 17600: Loss = -10791.697674062256
1
Iteration 17700: Loss = -10791.696042879112
Iteration 17800: Loss = -10791.703329874319
1
Iteration 17900: Loss = -10791.69604062062
Iteration 18000: Loss = -10791.758659007744
1
Iteration 18100: Loss = -10791.696012507515
Iteration 18200: Loss = -10791.774630280393
1
Iteration 18300: Loss = -10791.695236163909
Iteration 18400: Loss = -10791.697814226853
1
Iteration 18500: Loss = -10791.695020150906
Iteration 18600: Loss = -10791.701713567727
1
Iteration 18700: Loss = -10791.692826075188
Iteration 18800: Loss = -10791.69410430153
1
Iteration 18900: Loss = -10791.692850168725
Iteration 19000: Loss = -10791.730956150908
1
Iteration 19100: Loss = -10791.692828264791
Iteration 19200: Loss = -10791.746941841016
1
Iteration 19300: Loss = -10791.692705835887
Iteration 19400: Loss = -10791.692727039808
Iteration 19500: Loss = -10791.692699823683
Iteration 19600: Loss = -10791.693185483367
1
Iteration 19700: Loss = -10791.692557213166
Iteration 19800: Loss = -10792.091106481172
1
Iteration 19900: Loss = -10791.692554379339
pi: tensor([[0.8090, 0.1910],
        [0.2382, 0.7618]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5684, 0.4316], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2363, 0.1023],
         [0.6607, 0.2030]],

        [[0.5409, 0.0986],
         [0.6402, 0.6322]],

        [[0.6943, 0.0982],
         [0.5819, 0.5158]],

        [[0.5970, 0.0879],
         [0.6474, 0.6642]],

        [[0.5400, 0.1042],
         [0.6335, 0.5180]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369913366172994
time is 1
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 11
Adjusted Rand Index: 0.6045048763898703
Global Adjusted Rand Index: 0.7952478201447712
Average Adjusted Rand Index: 0.7992034947607661
10836.40406986485
[0.7881127736935931, 0.7952478201447712] [0.7916888960482138, 0.7992034947607661] [10791.575838467757, 10791.692560472453]
-------------------------------------
This iteration is 17
True Objective function: Loss = -10866.070208632842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19920.94218646221
Iteration 100: Loss = -11011.02446550666
Iteration 200: Loss = -10969.229652380636
Iteration 300: Loss = -10968.047197037744
Iteration 400: Loss = -10967.618081234215
Iteration 500: Loss = -10967.119468822191
Iteration 600: Loss = -10966.143473211085
Iteration 700: Loss = -10965.672452791825
Iteration 800: Loss = -10965.329670438696
Iteration 900: Loss = -10964.975993409755
Iteration 1000: Loss = -10964.6857134909
Iteration 1100: Loss = -10964.331537057258
Iteration 1200: Loss = -10963.55667610976
Iteration 1300: Loss = -10963.309897068362
Iteration 1400: Loss = -10963.176311922614
Iteration 1500: Loss = -10963.10075338601
Iteration 1600: Loss = -10963.05536642052
Iteration 1700: Loss = -10963.022958775271
Iteration 1800: Loss = -10962.994555831172
Iteration 1900: Loss = -10962.961344204436
Iteration 2000: Loss = -10962.938089681793
Iteration 2100: Loss = -10962.925161794172
Iteration 2200: Loss = -10962.913565721145
Iteration 2300: Loss = -10962.903289514818
Iteration 2400: Loss = -10962.89512659554
Iteration 2500: Loss = -10962.889094304004
Iteration 2600: Loss = -10962.884475906416
Iteration 2700: Loss = -10962.880572090387
Iteration 2800: Loss = -10962.87719807082
Iteration 2900: Loss = -10962.87421339834
Iteration 3000: Loss = -10962.871524675584
Iteration 3100: Loss = -10962.86910498309
Iteration 3200: Loss = -10962.866920150334
Iteration 3300: Loss = -10962.86495847039
Iteration 3400: Loss = -10962.863085766454
Iteration 3500: Loss = -10962.861262597675
Iteration 3600: Loss = -10962.859487746364
Iteration 3700: Loss = -10962.85760828583
Iteration 3800: Loss = -10962.855474961663
Iteration 3900: Loss = -10962.852627748913
Iteration 4000: Loss = -10962.850739864962
Iteration 4100: Loss = -10962.849878323477
Iteration 4200: Loss = -10962.849060620096
Iteration 4300: Loss = -10962.84832405352
Iteration 4400: Loss = -10962.847640314036
Iteration 4500: Loss = -10962.84702655875
Iteration 4600: Loss = -10962.846485732476
Iteration 4700: Loss = -10962.845955850182
Iteration 4800: Loss = -10962.845467762483
Iteration 4900: Loss = -10962.844992712655
Iteration 5000: Loss = -10962.844622027016
Iteration 5100: Loss = -10962.844186193604
Iteration 5200: Loss = -10962.84383179178
Iteration 5300: Loss = -10962.843467922034
Iteration 5400: Loss = -10962.843131415946
Iteration 5500: Loss = -10962.842825063464
Iteration 5600: Loss = -10962.842548809373
Iteration 5700: Loss = -10962.842265343665
Iteration 5800: Loss = -10962.842036696706
Iteration 5900: Loss = -10962.841798653431
Iteration 6000: Loss = -10962.841575454502
Iteration 6100: Loss = -10962.841359560289
Iteration 6200: Loss = -10962.841161073844
Iteration 6300: Loss = -10962.840979720433
Iteration 6400: Loss = -10962.840802586215
Iteration 6500: Loss = -10962.840605009907
Iteration 6600: Loss = -10962.840457634657
Iteration 6700: Loss = -10962.840312161767
Iteration 6800: Loss = -10962.840124246395
Iteration 6900: Loss = -10962.840048875654
Iteration 7000: Loss = -10962.839918397653
Iteration 7100: Loss = -10962.839798189993
Iteration 7200: Loss = -10962.87164802749
1
Iteration 7300: Loss = -10962.83961867337
Iteration 7400: Loss = -10962.839494000924
Iteration 7500: Loss = -10962.843628116989
1
Iteration 7600: Loss = -10962.839303785617
Iteration 7700: Loss = -10962.839225298725
Iteration 7800: Loss = -10962.841374070478
1
Iteration 7900: Loss = -10962.839040751725
Iteration 8000: Loss = -10962.839023148503
Iteration 8100: Loss = -10962.838919199421
Iteration 8200: Loss = -10962.838867845541
Iteration 8300: Loss = -10963.033848445746
1
Iteration 8400: Loss = -10962.838734261519
Iteration 8500: Loss = -10962.83869704263
Iteration 8600: Loss = -10963.001248747167
1
Iteration 8700: Loss = -10962.838618812299
Iteration 8800: Loss = -10962.838560117463
Iteration 8900: Loss = -10963.039063912132
1
Iteration 9000: Loss = -10962.838425869282
Iteration 9100: Loss = -10962.838409349028
Iteration 9200: Loss = -10962.839343722468
1
Iteration 9300: Loss = -10962.83836380303
Iteration 9400: Loss = -10962.838303021827
Iteration 9500: Loss = -10962.838274734644
Iteration 9600: Loss = -10962.838241928072
Iteration 9700: Loss = -10962.838188024738
Iteration 9800: Loss = -10962.839175057934
1
Iteration 9900: Loss = -10962.838179315177
Iteration 10000: Loss = -10962.838098234484
Iteration 10100: Loss = -10962.83813737631
Iteration 10200: Loss = -10962.83812107338
Iteration 10300: Loss = -10962.83810129966
Iteration 10400: Loss = -10962.83804699162
Iteration 10500: Loss = -10962.860752697481
1
Iteration 10600: Loss = -10962.838019515622
Iteration 10700: Loss = -10962.837997885246
Iteration 10800: Loss = -10962.859428151503
1
Iteration 10900: Loss = -10962.837959008952
Iteration 11000: Loss = -10962.837918335705
Iteration 11100: Loss = -10962.876033732848
1
Iteration 11200: Loss = -10962.837939696181
Iteration 11300: Loss = -10962.837925173471
Iteration 11400: Loss = -10962.838365160847
1
Iteration 11500: Loss = -10962.838167647591
2
Iteration 11600: Loss = -10962.840091669987
3
Iteration 11700: Loss = -10962.838417401754
4
Iteration 11800: Loss = -10962.838432277867
5
Iteration 11900: Loss = -10962.862568910126
6
Iteration 12000: Loss = -10962.837845314587
Iteration 12100: Loss = -10963.082089305955
1
Iteration 12200: Loss = -10962.837856476002
Iteration 12300: Loss = -10962.923563425138
1
Iteration 12400: Loss = -10962.83780936783
Iteration 12500: Loss = -10962.837817338359
Iteration 12600: Loss = -10962.841679842039
1
Iteration 12700: Loss = -10962.837804125556
Iteration 12800: Loss = -10962.863421681359
1
Iteration 12900: Loss = -10962.837784445126
Iteration 13000: Loss = -10962.837781628354
Iteration 13100: Loss = -10962.842228195084
1
Iteration 13200: Loss = -10962.837769719852
Iteration 13300: Loss = -10962.840932128769
1
Iteration 13400: Loss = -10962.840233902145
2
Iteration 13500: Loss = -10962.837781007865
Iteration 13600: Loss = -10962.844814632526
1
Iteration 13700: Loss = -10962.837743580192
Iteration 13800: Loss = -10962.838901410958
1
Iteration 13900: Loss = -10962.837709094805
Iteration 14000: Loss = -10962.843518294234
1
Iteration 14100: Loss = -10962.837731664169
Iteration 14200: Loss = -10962.838076024766
1
Iteration 14300: Loss = -10962.837790267213
Iteration 14400: Loss = -10962.837791630654
Iteration 14500: Loss = -10962.96285157058
1
Iteration 14600: Loss = -10962.837759989417
Iteration 14700: Loss = -10962.837697799112
Iteration 14800: Loss = -10962.837860872296
1
Iteration 14900: Loss = -10962.840986659376
2
Iteration 15000: Loss = -10962.837712498243
Iteration 15100: Loss = -10962.887005161607
1
Iteration 15200: Loss = -10962.837703607278
Iteration 15300: Loss = -10962.849488021804
1
Iteration 15400: Loss = -10962.894227074146
2
Iteration 15500: Loss = -10962.837696700593
Iteration 15600: Loss = -10962.83906306645
1
Iteration 15700: Loss = -10962.837868259649
2
Iteration 15800: Loss = -10962.8378462718
3
Iteration 15900: Loss = -10962.85388156216
4
Iteration 16000: Loss = -10962.857410616136
5
Iteration 16100: Loss = -10962.83770475519
Iteration 16200: Loss = -10962.839212171059
1
Iteration 16300: Loss = -10962.844176336366
2
Iteration 16400: Loss = -10962.853670000457
3
Iteration 16500: Loss = -10962.83772991606
Iteration 16600: Loss = -10962.900660819096
1
Iteration 16700: Loss = -10962.842901140382
2
Iteration 16800: Loss = -10962.83769188767
Iteration 16900: Loss = -10962.837928498277
1
Iteration 17000: Loss = -10962.83910680657
2
Iteration 17100: Loss = -10962.838750607425
3
Iteration 17200: Loss = -10962.840185748204
4
Iteration 17300: Loss = -10962.837686330751
Iteration 17400: Loss = -10963.130673544909
1
Iteration 17500: Loss = -10962.837683086536
Iteration 17600: Loss = -10962.885089170628
1
Iteration 17700: Loss = -10962.942207346128
2
Iteration 17800: Loss = -10962.838121408442
3
Iteration 17900: Loss = -10962.837696333085
Iteration 18000: Loss = -10962.853177329034
1
Iteration 18100: Loss = -10962.837715882268
Iteration 18200: Loss = -10962.838013163684
1
Iteration 18300: Loss = -10962.888615096575
2
Iteration 18400: Loss = -10962.838271573268
3
Iteration 18500: Loss = -10962.842985279944
4
Iteration 18600: Loss = -10962.83781709641
5
Iteration 18700: Loss = -10962.84056176455
6
Iteration 18800: Loss = -10962.940262728967
7
Iteration 18900: Loss = -10962.837665918156
Iteration 19000: Loss = -10962.841254786197
1
Iteration 19100: Loss = -10962.837736222526
Iteration 19200: Loss = -10962.838071777025
1
Iteration 19300: Loss = -10962.837689820304
Iteration 19400: Loss = -10962.837669922174
Iteration 19500: Loss = -10962.838063287856
1
Iteration 19600: Loss = -10962.83785954249
2
Iteration 19700: Loss = -10963.053028896637
3
Iteration 19800: Loss = -10962.837638934061
Iteration 19900: Loss = -10962.8391518261
1
pi: tensor([[1.0000e+00, 9.8427e-08],
        [1.2332e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0718, 0.9282], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1490, 0.2048],
         [0.5129, 0.1569]],

        [[0.5962, 0.1615],
         [0.5112, 0.5446]],

        [[0.5715, 0.1931],
         [0.5951, 0.5197]],

        [[0.6255, 0.1830],
         [0.5354, 0.6115]],

        [[0.7030, 0.2384],
         [0.6009, 0.6190]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.01941443218731252
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.006998325149571616
Global Adjusted Rand Index: 0.0023204219369068195
Average Adjusted Rand Index: 0.00218029304457509
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22636.970785829395
Iteration 100: Loss = -10968.28596194522
Iteration 200: Loss = -10967.55774656265
Iteration 300: Loss = -10967.217218978043
Iteration 400: Loss = -10966.71977935221
Iteration 500: Loss = -10966.020381721526
Iteration 600: Loss = -10965.74332689747
Iteration 700: Loss = -10965.584993858909
Iteration 800: Loss = -10965.480172517242
Iteration 900: Loss = -10965.393388838209
Iteration 1000: Loss = -10965.315504551078
Iteration 1100: Loss = -10965.24408713577
Iteration 1200: Loss = -10965.178604299239
Iteration 1300: Loss = -10965.120384178139
Iteration 1400: Loss = -10965.059768292602
Iteration 1500: Loss = -10964.972613248061
Iteration 1600: Loss = -10964.814720328575
Iteration 1700: Loss = -10964.515607201542
Iteration 1800: Loss = -10964.150235086845
Iteration 1900: Loss = -10963.906624665706
Iteration 2000: Loss = -10962.285567854584
Iteration 2100: Loss = -10879.593231959809
Iteration 2200: Loss = -10875.947052477417
Iteration 2300: Loss = -10875.432322566583
Iteration 2400: Loss = -10875.249318464304
Iteration 2500: Loss = -10875.16761528168
Iteration 2600: Loss = -10875.13268636865
Iteration 2700: Loss = -10875.107834992716
Iteration 2800: Loss = -10875.094102109284
Iteration 2900: Loss = -10875.081052195721
Iteration 3000: Loss = -10875.074236855084
Iteration 3100: Loss = -10875.069858723995
Iteration 3200: Loss = -10875.10114948208
1
Iteration 3300: Loss = -10875.063120022944
Iteration 3400: Loss = -10875.060439577954
Iteration 3500: Loss = -10875.061099184377
1
Iteration 3600: Loss = -10875.057049580913
Iteration 3700: Loss = -10875.055955512551
Iteration 3800: Loss = -10875.055404846333
Iteration 3900: Loss = -10875.062846748617
1
Iteration 4000: Loss = -10875.053431115542
Iteration 4100: Loss = -10875.052722285143
Iteration 4200: Loss = -10875.053578052923
1
Iteration 4300: Loss = -10875.051455291261
Iteration 4400: Loss = -10875.052102139125
1
Iteration 4500: Loss = -10875.050278232593
Iteration 4600: Loss = -10875.049672816216
Iteration 4700: Loss = -10875.04924452708
Iteration 4800: Loss = -10875.048847399226
Iteration 4900: Loss = -10875.049233901163
1
Iteration 5000: Loss = -10875.04830294074
Iteration 5100: Loss = -10875.048080573773
Iteration 5200: Loss = -10875.048416657615
1
Iteration 5300: Loss = -10875.047696243191
Iteration 5400: Loss = -10875.047552949554
Iteration 5500: Loss = -10875.047749917108
1
Iteration 5600: Loss = -10875.04722159172
Iteration 5700: Loss = -10875.04709430257
Iteration 5800: Loss = -10875.047355554612
1
Iteration 5900: Loss = -10875.04689244841
Iteration 6000: Loss = -10875.04679425028
Iteration 6100: Loss = -10875.046782275513
Iteration 6200: Loss = -10875.046640290713
Iteration 6300: Loss = -10875.050864128678
1
Iteration 6400: Loss = -10875.046458610412
Iteration 6500: Loss = -10875.04644939578
Iteration 6600: Loss = -10875.046388922356
Iteration 6700: Loss = -10875.04631539809
Iteration 6800: Loss = -10875.04629339941
Iteration 6900: Loss = -10875.046176692933
Iteration 7000: Loss = -10875.046159053147
Iteration 7100: Loss = -10875.046804060456
1
Iteration 7200: Loss = -10875.046019924013
Iteration 7300: Loss = -10875.045833259455
Iteration 7400: Loss = -10875.045047375243
Iteration 7500: Loss = -10875.045016741104
Iteration 7600: Loss = -10875.04509385605
Iteration 7700: Loss = -10875.049682102604
1
Iteration 7800: Loss = -10875.047213360149
2
Iteration 7900: Loss = -10875.046973118866
3
Iteration 8000: Loss = -10875.044846986964
Iteration 8100: Loss = -10875.045030601093
1
Iteration 8200: Loss = -10875.045432426896
2
Iteration 8300: Loss = -10875.056021112458
3
Iteration 8400: Loss = -10875.051287449463
4
Iteration 8500: Loss = -10875.044714768183
Iteration 8600: Loss = -10875.044699613532
Iteration 8700: Loss = -10875.044703081972
Iteration 8800: Loss = -10875.044705273933
Iteration 8900: Loss = -10875.04467415808
Iteration 9000: Loss = -10875.044671245812
Iteration 9100: Loss = -10875.044698997764
Iteration 9200: Loss = -10875.044636722052
Iteration 9300: Loss = -10875.044640254811
Iteration 9400: Loss = -10875.044675474692
Iteration 9500: Loss = -10875.044576095868
Iteration 9600: Loss = -10875.145448927595
1
Iteration 9700: Loss = -10875.044562146104
Iteration 9800: Loss = -10875.04454227184
Iteration 9900: Loss = -10875.19859011191
1
Iteration 10000: Loss = -10875.04458274812
Iteration 10100: Loss = -10875.044565868297
Iteration 10200: Loss = -10875.044636194281
Iteration 10300: Loss = -10875.044526925996
Iteration 10400: Loss = -10875.044532534452
Iteration 10500: Loss = -10875.044525236231
Iteration 10600: Loss = -10875.05011145156
1
Iteration 10700: Loss = -10875.044521848982
Iteration 10800: Loss = -10875.04452524411
Iteration 10900: Loss = -10875.06439136723
1
Iteration 11000: Loss = -10875.044503779056
Iteration 11100: Loss = -10875.044488892034
Iteration 11200: Loss = -10875.080108224993
1
Iteration 11300: Loss = -10875.044490943652
Iteration 11400: Loss = -10875.044485524248
Iteration 11500: Loss = -10875.24874941032
1
Iteration 11600: Loss = -10875.044471888204
Iteration 11700: Loss = -10875.04451787669
Iteration 11800: Loss = -10875.045904785024
1
Iteration 11900: Loss = -10875.044493757545
Iteration 12000: Loss = -10875.04446753007
Iteration 12100: Loss = -10875.044750392333
1
Iteration 12200: Loss = -10875.044451105754
Iteration 12300: Loss = -10875.044513959814
Iteration 12400: Loss = -10875.044576488654
Iteration 12500: Loss = -10875.04449013438
Iteration 12600: Loss = -10875.044570384021
Iteration 12700: Loss = -10875.044497820472
Iteration 12800: Loss = -10875.046859734968
1
Iteration 12900: Loss = -10875.044479900114
Iteration 13000: Loss = -10875.369168056432
1
Iteration 13100: Loss = -10875.04446832519
Iteration 13200: Loss = -10875.04448684534
Iteration 13300: Loss = -10875.257576129323
1
Iteration 13400: Loss = -10875.044465925244
Iteration 13500: Loss = -10875.044431578244
Iteration 13600: Loss = -10875.086891072362
1
Iteration 13700: Loss = -10875.044457205315
Iteration 13800: Loss = -10875.044474425633
Iteration 13900: Loss = -10875.046513766043
1
Iteration 14000: Loss = -10875.04446091416
Iteration 14100: Loss = -10875.044440246229
Iteration 14200: Loss = -10875.046068673331
1
Iteration 14300: Loss = -10875.044416697774
Iteration 14400: Loss = -10875.044423101559
Iteration 14500: Loss = -10875.085997803564
1
Iteration 14600: Loss = -10875.044433582103
Iteration 14700: Loss = -10875.044455904548
Iteration 14800: Loss = -10875.044953832274
1
Iteration 14900: Loss = -10875.044486914236
Iteration 15000: Loss = -10875.044466332192
Iteration 15100: Loss = -10875.044608892766
1
Iteration 15200: Loss = -10875.044485814726
Iteration 15300: Loss = -10875.044462392552
Iteration 15400: Loss = -10875.045156241484
1
Iteration 15500: Loss = -10875.044464627748
Iteration 15600: Loss = -10875.04447185057
Iteration 15700: Loss = -10875.044508086254
Iteration 15800: Loss = -10875.04447209175
Iteration 15900: Loss = -10875.047021054635
1
Iteration 16000: Loss = -10875.044473357782
Iteration 16100: Loss = -10875.044453211389
Iteration 16200: Loss = -10875.355779105901
1
Iteration 16300: Loss = -10875.044455311901
Iteration 16400: Loss = -10875.044454254823
Iteration 16500: Loss = -10875.53589868876
1
Iteration 16600: Loss = -10875.04449014428
Iteration 16700: Loss = -10875.04446292449
Iteration 16800: Loss = -10875.044505455915
Iteration 16900: Loss = -10875.044730483554
1
Iteration 17000: Loss = -10875.044478971002
Iteration 17100: Loss = -10875.044475673396
Iteration 17200: Loss = -10875.044595159849
1
Iteration 17300: Loss = -10875.04448276402
Iteration 17400: Loss = -10875.044462787866
Iteration 17500: Loss = -10875.044597983218
1
Iteration 17600: Loss = -10875.044478872374
Iteration 17700: Loss = -10875.044496385573
Iteration 17800: Loss = -10875.045823129278
1
Iteration 17900: Loss = -10875.044464793538
Iteration 18000: Loss = -10875.044453956982
Iteration 18100: Loss = -10875.044737790808
1
Iteration 18200: Loss = -10875.044474493252
Iteration 18300: Loss = -10875.04480006665
1
Iteration 18400: Loss = -10875.044515213605
Iteration 18500: Loss = -10875.04447267077
Iteration 18600: Loss = -10875.057395469647
1
Iteration 18700: Loss = -10875.044497198134
Iteration 18800: Loss = -10875.044466730118
Iteration 18900: Loss = -10875.04446840682
Iteration 19000: Loss = -10875.045299589074
1
Iteration 19100: Loss = -10875.044484354437
Iteration 19200: Loss = -10875.044480073471
Iteration 19300: Loss = -10875.04453031981
Iteration 19400: Loss = -10875.044445026393
Iteration 19500: Loss = -10875.044460152134
Iteration 19600: Loss = -10875.044559368125
Iteration 19700: Loss = -10875.0444763561
Iteration 19800: Loss = -10875.044458042496
Iteration 19900: Loss = -10875.044709190728
1
pi: tensor([[0.6702, 0.3298],
        [0.2301, 0.7699]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1797, 0.0417],
         [0.5102, 0.2537]],

        [[0.6271, 0.0939],
         [0.6888, 0.6457]],

        [[0.6814, 0.0996],
         [0.6799, 0.6683]],

        [[0.6405, 0.1058],
         [0.6501, 0.6900]],

        [[0.5640, 0.1052],
         [0.5552, 0.5304]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369432436752338
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.52325404708415
Average Adjusted Rand Index: 0.6698118108387335
10866.070208632842
[0.0023204219369068195, 0.52325404708415] [0.00218029304457509, 0.6698118108387335] [10962.837793642713, 10875.044467079557]
-------------------------------------
This iteration is 18
True Objective function: Loss = -10895.275102043353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20765.019612179894
Iteration 100: Loss = -10934.080267579853
Iteration 200: Loss = -10933.390300977024
Iteration 300: Loss = -10933.225213737698
Iteration 400: Loss = -10933.146041458855
Iteration 500: Loss = -10933.101900327796
Iteration 600: Loss = -10933.07329537165
Iteration 700: Loss = -10933.051635462216
Iteration 800: Loss = -10933.032947678064
Iteration 900: Loss = -10933.015347265773
Iteration 1000: Loss = -10932.997754540693
Iteration 1100: Loss = -10932.979013251175
Iteration 1200: Loss = -10932.9581887032
Iteration 1300: Loss = -10932.934779049572
Iteration 1400: Loss = -10932.90885417917
Iteration 1500: Loss = -10932.880957406855
Iteration 1600: Loss = -10932.851776876078
Iteration 1700: Loss = -10932.822277859157
Iteration 1800: Loss = -10932.794229825691
Iteration 1900: Loss = -10932.769762171207
Iteration 2000: Loss = -10932.749973075921
Iteration 2100: Loss = -10932.735001443894
Iteration 2200: Loss = -10932.723401385718
Iteration 2300: Loss = -10932.714615669274
Iteration 2400: Loss = -10932.707761603246
Iteration 2500: Loss = -10932.702317494199
Iteration 2600: Loss = -10932.69779748874
Iteration 2700: Loss = -10932.693447527377
Iteration 2800: Loss = -10932.68859758355
Iteration 2900: Loss = -10932.682209937633
Iteration 3000: Loss = -10932.672344975614
Iteration 3100: Loss = -10932.654479483132
Iteration 3200: Loss = -10932.610743309036
Iteration 3300: Loss = -10931.353926658847
Iteration 3400: Loss = -10929.511150535222
Iteration 3500: Loss = -10929.184155692154
Iteration 3600: Loss = -10928.993797942427
Iteration 3700: Loss = -10928.908731074067
Iteration 3800: Loss = -10928.870691373726
Iteration 3900: Loss = -10928.851220871464
Iteration 4000: Loss = -10928.839571950564
Iteration 4100: Loss = -10928.831604978293
Iteration 4200: Loss = -10928.825544280306
Iteration 4300: Loss = -10928.820910443757
Iteration 4400: Loss = -10928.817281809697
Iteration 4500: Loss = -10928.814399508514
Iteration 4600: Loss = -10928.812007814231
Iteration 4700: Loss = -10928.809970588976
Iteration 4800: Loss = -10928.808174597014
Iteration 4900: Loss = -10928.806585632941
Iteration 5000: Loss = -10928.805135197914
Iteration 5100: Loss = -10928.803873369565
Iteration 5200: Loss = -10928.802813129772
Iteration 5300: Loss = -10928.801851917076
Iteration 5400: Loss = -10928.80102291365
Iteration 5500: Loss = -10928.800309181786
Iteration 5600: Loss = -10928.799597305917
Iteration 5700: Loss = -10928.79897417898
Iteration 5800: Loss = -10928.798440197203
Iteration 5900: Loss = -10928.797929124245
Iteration 6000: Loss = -10928.797454757067
Iteration 6100: Loss = -10928.797043909526
Iteration 6200: Loss = -10928.796647383404
Iteration 6300: Loss = -10928.796287776113
Iteration 6400: Loss = -10928.795909808605
Iteration 6500: Loss = -10928.79555193136
Iteration 6600: Loss = -10928.795158491466
Iteration 6700: Loss = -10928.794757040358
Iteration 6800: Loss = -10928.794256450916
Iteration 6900: Loss = -10928.793931908276
Iteration 7000: Loss = -10928.793686364936
Iteration 7100: Loss = -10928.793427728655
Iteration 7200: Loss = -10928.79326596659
Iteration 7300: Loss = -10928.793082566568
Iteration 7400: Loss = -10928.792924046855
Iteration 7500: Loss = -10928.79275349966
Iteration 7600: Loss = -10928.792648832768
Iteration 7700: Loss = -10928.857991360977
1
Iteration 7800: Loss = -10928.840103489714
2
Iteration 7900: Loss = -10928.792284257306
Iteration 8000: Loss = -10928.792369128965
Iteration 8100: Loss = -10928.792163014874
Iteration 8200: Loss = -10928.792073293684
Iteration 8300: Loss = -10928.791897090961
Iteration 8400: Loss = -10928.791805346158
Iteration 8500: Loss = -10928.791653836057
Iteration 8600: Loss = -10928.791592247726
Iteration 8700: Loss = -10928.791560004307
Iteration 8800: Loss = -10928.795777388525
1
Iteration 8900: Loss = -10928.791372549245
Iteration 9000: Loss = -10928.791312903933
Iteration 9100: Loss = -10928.7913674489
Iteration 9200: Loss = -10928.791260772363
Iteration 9300: Loss = -10928.791169528913
Iteration 9400: Loss = -10928.82114256056
1
Iteration 9500: Loss = -10928.791025514776
Iteration 9600: Loss = -10928.790974101552
Iteration 9700: Loss = -10928.791149811885
1
Iteration 9800: Loss = -10928.790997494947
Iteration 9900: Loss = -10928.793316887233
1
Iteration 10000: Loss = -10928.790559069801
Iteration 10100: Loss = -10928.802380649244
1
Iteration 10200: Loss = -10928.791247331339
2
Iteration 10300: Loss = -10928.790135163794
Iteration 10400: Loss = -10928.79729395221
1
Iteration 10500: Loss = -10928.790098048328
Iteration 10600: Loss = -10928.789995341076
Iteration 10700: Loss = -10928.789968826546
Iteration 10800: Loss = -10928.789879513692
Iteration 10900: Loss = -10928.801182251247
1
Iteration 11000: Loss = -10928.789827655779
Iteration 11100: Loss = -10928.829329362861
1
Iteration 11200: Loss = -10928.789754049028
Iteration 11300: Loss = -10928.794084042795
1
Iteration 11400: Loss = -10928.789705667188
Iteration 11500: Loss = -10928.78976291415
Iteration 11600: Loss = -10928.789809815376
Iteration 11700: Loss = -10928.790327504343
1
Iteration 11800: Loss = -10928.789563087308
Iteration 11900: Loss = -10928.789499541168
Iteration 12000: Loss = -10928.789997843289
1
Iteration 12100: Loss = -10928.939087223733
2
Iteration 12200: Loss = -10928.78936998521
Iteration 12300: Loss = -10928.789699923844
1
Iteration 12400: Loss = -10928.789315101054
Iteration 12500: Loss = -10928.897573927063
1
Iteration 12600: Loss = -10928.789138502649
Iteration 12700: Loss = -10928.789109890244
Iteration 12800: Loss = -10928.789768605477
1
Iteration 12900: Loss = -10928.789059571947
Iteration 13000: Loss = -10928.856264006057
1
Iteration 13100: Loss = -10928.789085440514
Iteration 13200: Loss = -10928.80072414369
1
Iteration 13300: Loss = -10928.789027494202
Iteration 13400: Loss = -10928.789736221906
1
Iteration 13500: Loss = -10928.78906778639
Iteration 13600: Loss = -10928.797439363596
1
Iteration 13700: Loss = -10928.789030080476
Iteration 13800: Loss = -10928.789815606833
1
Iteration 13900: Loss = -10928.935051745571
2
Iteration 14000: Loss = -10928.788990403224
Iteration 14100: Loss = -10928.792623166108
1
Iteration 14200: Loss = -10928.788995239309
Iteration 14300: Loss = -10928.795926422194
1
Iteration 14400: Loss = -10928.78898136494
Iteration 14500: Loss = -10928.789027599862
Iteration 14600: Loss = -10928.81711950834
1
Iteration 14700: Loss = -10928.788938253085
Iteration 14800: Loss = -10928.793699656384
1
Iteration 14900: Loss = -10928.788975027208
Iteration 15000: Loss = -10928.82838497345
1
Iteration 15100: Loss = -10928.78919417636
2
Iteration 15200: Loss = -10928.789126429143
3
Iteration 15300: Loss = -10928.805501845804
4
Iteration 15400: Loss = -10928.790072628228
5
Iteration 15500: Loss = -10928.7889355606
Iteration 15600: Loss = -10928.796187622967
1
Iteration 15700: Loss = -10928.789207157175
2
Iteration 15800: Loss = -10928.788970896652
Iteration 15900: Loss = -10928.790447311158
1
Iteration 16000: Loss = -10928.794627106077
2
Iteration 16100: Loss = -10928.788873935342
Iteration 16200: Loss = -10928.802511809516
1
Iteration 16300: Loss = -10928.788861962863
Iteration 16400: Loss = -10928.7889507453
Iteration 16500: Loss = -10928.882849527703
1
Iteration 16600: Loss = -10928.788860114206
Iteration 16700: Loss = -10928.788899695124
Iteration 16800: Loss = -10928.801024939805
1
Iteration 16900: Loss = -10928.788967654144
Iteration 17000: Loss = -10928.841392781735
1
Iteration 17100: Loss = -10928.788883055642
Iteration 17200: Loss = -10928.788995901385
1
Iteration 17300: Loss = -10928.867360993581
2
Iteration 17400: Loss = -10928.78886729732
Iteration 17500: Loss = -10928.792516287021
1
Iteration 17600: Loss = -10928.803152305083
2
Iteration 17700: Loss = -10928.788888790126
Iteration 17800: Loss = -10928.789052586297
1
Iteration 17900: Loss = -10928.790181690865
2
Iteration 18000: Loss = -10928.789109022182
3
Iteration 18100: Loss = -10928.789595904076
4
Iteration 18200: Loss = -10928.790268004517
5
Iteration 18300: Loss = -10928.789474283767
6
Iteration 18400: Loss = -10928.788856690788
Iteration 18500: Loss = -10928.788939180857
Iteration 18600: Loss = -10928.788770134326
Iteration 18700: Loss = -10928.800327343706
1
Iteration 18800: Loss = -10928.788715966572
Iteration 18900: Loss = -10928.829787477582
1
Iteration 19000: Loss = -10928.793012402271
2
Iteration 19100: Loss = -10928.788721923558
Iteration 19200: Loss = -10928.807134310418
1
Iteration 19300: Loss = -10928.79569828496
2
Iteration 19400: Loss = -10928.791616369355
3
Iteration 19500: Loss = -10928.788720265124
Iteration 19600: Loss = -10928.825961307575
1
Iteration 19700: Loss = -10928.788850545438
2
Iteration 19800: Loss = -10928.789315634665
3
Iteration 19900: Loss = -10928.840316767308
4
pi: tensor([[1.0000e+00, 1.9645e-07],
        [1.3092e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0367, 0.9633], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0895, 0.0935],
         [0.6039, 0.1608]],

        [[0.7004, 0.2230],
         [0.6991, 0.6184]],

        [[0.6675, 0.1910],
         [0.6830, 0.5622]],

        [[0.6826, 0.1266],
         [0.5138, 0.6126]],

        [[0.6526, 0.2047],
         [0.5029, 0.5688]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.008125898971292667
Global Adjusted Rand Index: 0.006974530971122017
Average Adjusted Rand Index: 0.0051994655659821955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21514.56574102261
Iteration 100: Loss = -10934.95724604699
Iteration 200: Loss = -10933.814783784028
Iteration 300: Loss = -10933.458827984443
Iteration 400: Loss = -10933.315326298392
Iteration 500: Loss = -10933.239384456112
Iteration 600: Loss = -10933.189452790284
Iteration 700: Loss = -10933.152365622534
Iteration 800: Loss = -10933.122918026696
Iteration 900: Loss = -10933.098534914157
Iteration 1000: Loss = -10933.077889897391
Iteration 1100: Loss = -10933.060026847066
Iteration 1200: Loss = -10933.044335365452
Iteration 1300: Loss = -10933.030313441626
Iteration 1400: Loss = -10933.017635005534
Iteration 1500: Loss = -10933.005952545054
Iteration 1600: Loss = -10932.994838373377
Iteration 1700: Loss = -10932.984176799413
Iteration 1800: Loss = -10932.973654663272
Iteration 1900: Loss = -10932.963098193235
Iteration 2000: Loss = -10932.952238860855
Iteration 2100: Loss = -10932.940934373602
Iteration 2200: Loss = -10932.928910911432
Iteration 2300: Loss = -10932.91594150208
Iteration 2400: Loss = -10932.901722427647
Iteration 2500: Loss = -10932.886122421025
Iteration 2600: Loss = -10932.868775221505
Iteration 2700: Loss = -10932.849599755502
Iteration 2800: Loss = -10932.828641482734
Iteration 2900: Loss = -10932.805994346762
Iteration 3000: Loss = -10932.78261409362
Iteration 3100: Loss = -10932.759767158155
Iteration 3200: Loss = -10932.739344989677
Iteration 3300: Loss = -10932.723091854617
Iteration 3400: Loss = -10932.711597870728
Iteration 3500: Loss = -10932.704041979274
Iteration 3600: Loss = -10932.699017280987
Iteration 3700: Loss = -10932.695345963057
Iteration 3800: Loss = -10932.692198211684
Iteration 3900: Loss = -10932.689155026166
Iteration 4000: Loss = -10932.685854151277
Iteration 4100: Loss = -10932.682156848981
Iteration 4200: Loss = -10932.677745208472
Iteration 4300: Loss = -10932.672174030726
Iteration 4400: Loss = -10932.664873055857
Iteration 4500: Loss = -10932.65459614167
Iteration 4600: Loss = -10932.638377862848
Iteration 4700: Loss = -10932.605021094776
Iteration 4800: Loss = -10932.448035725954
Iteration 4900: Loss = -10930.12719012042
Iteration 5000: Loss = -10929.452956140214
Iteration 5100: Loss = -10929.113178642452
Iteration 5200: Loss = -10928.984705553537
Iteration 5300: Loss = -10928.928428363448
Iteration 5400: Loss = -10928.89672483307
Iteration 5500: Loss = -10928.875594904233
Iteration 5600: Loss = -10928.86022837016
Iteration 5700: Loss = -10928.849203152822
Iteration 5800: Loss = -10928.841042131937
Iteration 5900: Loss = -10928.83462429537
Iteration 6000: Loss = -10928.829474386277
Iteration 6100: Loss = -10928.825202506063
Iteration 6200: Loss = -10928.821676578975
Iteration 6300: Loss = -10928.818599569157
Iteration 6400: Loss = -10928.815986629968
Iteration 6500: Loss = -10928.81365864922
Iteration 6600: Loss = -10928.81165426925
Iteration 6700: Loss = -10928.809942091211
Iteration 6800: Loss = -10928.8083289096
Iteration 6900: Loss = -10928.806935258894
Iteration 7000: Loss = -10928.805705981224
Iteration 7100: Loss = -10928.804513199926
Iteration 7200: Loss = -10928.803455711984
Iteration 7300: Loss = -10928.802533003996
Iteration 7400: Loss = -10928.80165545569
Iteration 7500: Loss = -10928.801524778446
Iteration 7600: Loss = -10928.801126861961
Iteration 7700: Loss = -10928.818958485006
1
Iteration 7800: Loss = -10928.79879054541
Iteration 7900: Loss = -10928.955806500728
1
Iteration 8000: Loss = -10928.797629564146
Iteration 8100: Loss = -10928.79702106781
Iteration 8200: Loss = -10928.815609638395
1
Iteration 8300: Loss = -10928.795781673252
Iteration 8400: Loss = -10928.79533484067
Iteration 8500: Loss = -10928.797575212486
1
Iteration 8600: Loss = -10928.794561877974
Iteration 8700: Loss = -10928.794249840998
Iteration 8800: Loss = -10928.793964120312
Iteration 8900: Loss = -10928.793756283876
Iteration 9000: Loss = -10928.79337897423
Iteration 9100: Loss = -10928.793146201855
Iteration 9200: Loss = -10928.795288022284
1
Iteration 9300: Loss = -10928.792700202099
Iteration 9400: Loss = -10928.792497997394
Iteration 9500: Loss = -10928.792293203192
Iteration 9600: Loss = -10928.792756733681
1
Iteration 9700: Loss = -10928.792012832559
Iteration 9800: Loss = -10928.791789085511
Iteration 9900: Loss = -10929.222742611262
1
Iteration 10000: Loss = -10928.791493859764
Iteration 10100: Loss = -10928.791378497814
Iteration 10200: Loss = -10928.791222847012
Iteration 10300: Loss = -10928.79136106145
1
Iteration 10400: Loss = -10928.790992745286
Iteration 10500: Loss = -10928.790869847666
Iteration 10600: Loss = -10928.848221067245
1
Iteration 10700: Loss = -10928.790627755421
Iteration 10800: Loss = -10928.790516304189
Iteration 10900: Loss = -10928.790479398338
Iteration 11000: Loss = -10928.791233699829
1
Iteration 11100: Loss = -10928.790270185516
Iteration 11200: Loss = -10928.790222551186
Iteration 11300: Loss = -10928.941113451407
1
Iteration 11400: Loss = -10928.79009786439
Iteration 11500: Loss = -10928.790020229702
Iteration 11600: Loss = -10928.790135317471
1
Iteration 11700: Loss = -10928.790030743772
Iteration 11800: Loss = -10929.041843391098
1
Iteration 11900: Loss = -10928.78979558977
Iteration 12000: Loss = -10928.942249329017
1
Iteration 12100: Loss = -10928.789705064302
Iteration 12200: Loss = -10928.789671818544
Iteration 12300: Loss = -10928.789665516058
Iteration 12400: Loss = -10928.79177583929
1
Iteration 12500: Loss = -10928.800416354241
2
Iteration 12600: Loss = -10928.78950349936
Iteration 12700: Loss = -10928.79643981611
1
Iteration 12800: Loss = -10928.789471981707
Iteration 12900: Loss = -10928.789738613506
1
Iteration 13000: Loss = -10928.790843388419
2
Iteration 13100: Loss = -10928.789448010486
Iteration 13200: Loss = -10928.791557856743
1
Iteration 13300: Loss = -10928.790316694789
2
Iteration 13400: Loss = -10928.789961446644
3
Iteration 13500: Loss = -10928.789615713136
4
Iteration 13600: Loss = -10928.78941930057
Iteration 13700: Loss = -10928.790166059196
1
Iteration 13800: Loss = -10928.789413161976
Iteration 13900: Loss = -10928.831942565475
1
Iteration 14000: Loss = -10928.795590711494
2
Iteration 14100: Loss = -10928.789274747513
Iteration 14200: Loss = -10928.794176668898
1
Iteration 14300: Loss = -10928.78937820859
2
Iteration 14400: Loss = -10928.793645636371
3
Iteration 14500: Loss = -10928.789944082331
4
Iteration 14600: Loss = -10928.789104461759
Iteration 14700: Loss = -10928.791656077772
1
Iteration 14800: Loss = -10928.789107492132
Iteration 14900: Loss = -10928.789212827998
1
Iteration 15000: Loss = -10928.789053574312
Iteration 15100: Loss = -10928.789057944005
Iteration 15200: Loss = -10928.789029923484
Iteration 15300: Loss = -10928.789103181882
Iteration 15400: Loss = -10929.028839000148
1
Iteration 15500: Loss = -10928.788945749131
Iteration 15600: Loss = -10928.872644149164
1
Iteration 15700: Loss = -10928.78917225177
2
Iteration 15800: Loss = -10928.795733161165
3
Iteration 15900: Loss = -10928.79257984647
4
Iteration 16000: Loss = -10928.789243677811
5
Iteration 16100: Loss = -10928.803197795583
6
Iteration 16200: Loss = -10928.788906402871
Iteration 16300: Loss = -10928.791143416496
1
Iteration 16400: Loss = -10928.788852689904
Iteration 16500: Loss = -10928.789364715385
1
Iteration 16600: Loss = -10928.796099562063
2
Iteration 16700: Loss = -10928.789327876837
3
Iteration 16800: Loss = -10928.789240241462
4
Iteration 16900: Loss = -10928.801987669856
5
Iteration 17000: Loss = -10928.78888898825
Iteration 17100: Loss = -10928.789953384165
1
Iteration 17200: Loss = -10928.790085696644
2
Iteration 17300: Loss = -10928.855479638574
3
Iteration 17400: Loss = -10928.789009544713
4
Iteration 17500: Loss = -10928.790544770314
5
Iteration 17600: Loss = -10928.788853571055
Iteration 17700: Loss = -10928.809535924016
1
Iteration 17800: Loss = -10928.919285057915
2
Iteration 17900: Loss = -10928.788866525698
Iteration 18000: Loss = -10928.789400183674
1
Iteration 18100: Loss = -10928.86346086438
2
Iteration 18200: Loss = -10928.788842227197
Iteration 18300: Loss = -10928.789281874278
1
Iteration 18400: Loss = -10928.78897660443
2
Iteration 18500: Loss = -10928.788827526336
Iteration 18600: Loss = -10929.036417090698
1
Iteration 18700: Loss = -10928.788840579542
Iteration 18800: Loss = -10928.813852993362
1
Iteration 18900: Loss = -10928.788851698382
Iteration 19000: Loss = -10928.788895347774
Iteration 19100: Loss = -10928.81769455245
1
Iteration 19200: Loss = -10928.788814698079
Iteration 19300: Loss = -10928.789866593668
1
Iteration 19400: Loss = -10928.789123594659
2
Iteration 19500: Loss = -10928.790001478352
3
Iteration 19600: Loss = -10928.80475501336
4
Iteration 19700: Loss = -10928.788796005649
Iteration 19800: Loss = -10928.797248381352
1
Iteration 19900: Loss = -10928.788803656496
pi: tensor([[1.0000e+00, 5.8757e-08],
        [6.3276e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9634, 0.0366], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1606, 0.0936],
         [0.5553, 0.0896]],

        [[0.6181, 0.2231],
         [0.5706, 0.5856]],

        [[0.5708, 0.1914],
         [0.6610, 0.6493]],

        [[0.6214, 0.1269],
         [0.7275, 0.5273]],

        [[0.6730, 0.2050],
         [0.5255, 0.6128]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
Global Adjusted Rand Index: 0.006974530971122017
Average Adjusted Rand Index: 0.0051994655659821955
10895.275102043353
[0.006974530971122017, 0.006974530971122017] [0.0051994655659821955, 0.0051994655659821955] [10928.788713308862, 10928.796272008765]
-------------------------------------
This iteration is 19
True Objective function: Loss = -10870.479244653518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21679.49045236079
Iteration 100: Loss = -10943.211746453799
Iteration 200: Loss = -10941.9038285478
Iteration 300: Loss = -10941.240389906601
Iteration 400: Loss = -10939.680822614175
Iteration 500: Loss = -10939.362360768326
Iteration 600: Loss = -10939.23640833003
Iteration 700: Loss = -10939.141865203059
Iteration 800: Loss = -10939.05290852692
Iteration 900: Loss = -10938.957257940463
Iteration 1000: Loss = -10938.84622176394
Iteration 1100: Loss = -10938.704123881957
Iteration 1200: Loss = -10938.385441439868
Iteration 1300: Loss = -10937.887312178045
Iteration 1400: Loss = -10937.659092863403
Iteration 1500: Loss = -10937.515253710033
Iteration 1600: Loss = -10937.236063522327
Iteration 1700: Loss = -10918.421004593998
Iteration 1800: Loss = -10918.326074302719
Iteration 1900: Loss = -10918.303554320459
Iteration 2000: Loss = -10918.291731668087
Iteration 2100: Loss = -10918.282844401288
Iteration 2200: Loss = -10918.273306182262
Iteration 2300: Loss = -10918.254313553976
Iteration 2400: Loss = -10917.991612152788
Iteration 2500: Loss = -10914.123235548548
Iteration 2600: Loss = -10913.745069881015
Iteration 2700: Loss = -10913.643881427193
Iteration 2800: Loss = -10913.591371550901
Iteration 2900: Loss = -10913.56396657977
Iteration 3000: Loss = -10913.534297325003
Iteration 3100: Loss = -10913.535194722326
1
Iteration 3200: Loss = -10913.511459002662
Iteration 3300: Loss = -10913.50285798751
Iteration 3400: Loss = -10913.497298668455
Iteration 3500: Loss = -10913.491013121362
Iteration 3600: Loss = -10913.487336040158
Iteration 3700: Loss = -10913.482785368113
Iteration 3800: Loss = -10913.479598057906
Iteration 3900: Loss = -10913.48118068872
1
Iteration 4000: Loss = -10913.474313103945
Iteration 4100: Loss = -10913.47223267905
Iteration 4200: Loss = -10913.47030595745
Iteration 4300: Loss = -10913.468608603554
Iteration 4400: Loss = -10913.467577881358
Iteration 4500: Loss = -10913.466187383536
Iteration 4600: Loss = -10913.468426351546
1
Iteration 4700: Loss = -10913.464365611811
Iteration 4800: Loss = -10913.462587723967
Iteration 4900: Loss = -10913.46181121913
Iteration 5000: Loss = -10913.460857354965
Iteration 5100: Loss = -10913.461442441981
1
Iteration 5200: Loss = -10913.459437959527
Iteration 5300: Loss = -10913.458755227497
Iteration 5400: Loss = -10913.459121896587
1
Iteration 5500: Loss = -10913.457594199026
Iteration 5600: Loss = -10913.45709762992
Iteration 5700: Loss = -10913.456770725385
Iteration 5800: Loss = -10913.456168786854
Iteration 5900: Loss = -10913.455718551175
Iteration 6000: Loss = -10913.4553453831
Iteration 6100: Loss = -10913.455455935173
1
Iteration 6200: Loss = -10913.454742423626
Iteration 6300: Loss = -10913.454364716927
Iteration 6400: Loss = -10913.456472452443
1
Iteration 6500: Loss = -10913.453765475606
Iteration 6600: Loss = -10913.453521952744
Iteration 6700: Loss = -10913.453259174355
Iteration 6800: Loss = -10913.453059191132
Iteration 6900: Loss = -10913.452810174937
Iteration 7000: Loss = -10913.452604325768
Iteration 7100: Loss = -10913.453101820858
1
Iteration 7200: Loss = -10913.452968139687
2
Iteration 7300: Loss = -10913.453111370973
3
Iteration 7400: Loss = -10913.451855529302
Iteration 7500: Loss = -10913.452328952057
1
Iteration 7600: Loss = -10913.45157637305
Iteration 7700: Loss = -10913.45528868947
1
Iteration 7800: Loss = -10913.451313447364
Iteration 7900: Loss = -10913.451206342079
Iteration 8000: Loss = -10913.451134263052
Iteration 8100: Loss = -10913.450943963333
Iteration 8200: Loss = -10913.470772845065
1
Iteration 8300: Loss = -10913.450715744157
Iteration 8400: Loss = -10913.450667333233
Iteration 8500: Loss = -10913.450744668165
Iteration 8600: Loss = -10913.456672253664
1
Iteration 8700: Loss = -10913.461576003323
2
Iteration 8800: Loss = -10913.458580078372
3
Iteration 8900: Loss = -10913.450288457248
Iteration 9000: Loss = -10913.450509051467
1
Iteration 9100: Loss = -10913.450165194876
Iteration 9200: Loss = -10913.450591805375
1
Iteration 9300: Loss = -10913.450024582777
Iteration 9400: Loss = -10913.470379184493
1
Iteration 9500: Loss = -10913.449931805611
Iteration 9600: Loss = -10913.449850914294
Iteration 9700: Loss = -10913.44985497267
Iteration 9800: Loss = -10913.449748957175
Iteration 9900: Loss = -10913.497201795482
1
Iteration 10000: Loss = -10913.449700463534
Iteration 10100: Loss = -10913.449652478439
Iteration 10200: Loss = -10913.449861100069
1
Iteration 10300: Loss = -10913.449585321981
Iteration 10400: Loss = -10913.648670224973
1
Iteration 10500: Loss = -10913.449519405185
Iteration 10600: Loss = -10913.449462149065
Iteration 10700: Loss = -10913.468796880077
1
Iteration 10800: Loss = -10913.449448641812
Iteration 10900: Loss = -10913.449417456122
Iteration 11000: Loss = -10913.451571067246
1
Iteration 11100: Loss = -10913.449348649905
Iteration 11200: Loss = -10913.449380329137
Iteration 11300: Loss = -10913.449737404755
1
Iteration 11400: Loss = -10913.729405121825
2
Iteration 11500: Loss = -10913.449304210892
Iteration 11600: Loss = -10913.477058239512
1
Iteration 11700: Loss = -10913.449274189405
Iteration 11800: Loss = -10913.449305446244
Iteration 11900: Loss = -10913.490605154431
1
Iteration 12000: Loss = -10913.44920035565
Iteration 12100: Loss = -10913.454407371646
1
Iteration 12200: Loss = -10913.449165676318
Iteration 12300: Loss = -10913.453573680674
1
Iteration 12400: Loss = -10913.449155920442
Iteration 12500: Loss = -10913.468103102981
1
Iteration 12600: Loss = -10913.449099064428
Iteration 12700: Loss = -10913.45563304593
1
Iteration 12800: Loss = -10913.449335254178
2
Iteration 12900: Loss = -10913.449144147538
Iteration 13000: Loss = -10913.458446260467
1
Iteration 13100: Loss = -10913.449078144024
Iteration 13200: Loss = -10913.45042825953
1
Iteration 13300: Loss = -10913.449072620379
Iteration 13400: Loss = -10913.449587780644
1
Iteration 13500: Loss = -10913.449062387155
Iteration 13600: Loss = -10913.449440375138
1
Iteration 13700: Loss = -10913.449002410789
Iteration 13800: Loss = -10913.510443806721
1
Iteration 13900: Loss = -10913.449016930414
Iteration 14000: Loss = -10913.44903061605
Iteration 14100: Loss = -10913.4561187013
1
Iteration 14200: Loss = -10913.449006986972
Iteration 14300: Loss = -10913.44898586627
Iteration 14400: Loss = -10913.451813614376
1
Iteration 14500: Loss = -10913.449005548218
Iteration 14600: Loss = -10913.448999466995
Iteration 14700: Loss = -10913.4500228321
1
Iteration 14800: Loss = -10913.461765855716
2
Iteration 14900: Loss = -10913.448980739393
Iteration 15000: Loss = -10913.477610187692
1
Iteration 15100: Loss = -10913.448972882328
Iteration 15200: Loss = -10913.474615439736
1
Iteration 15300: Loss = -10913.448930739785
Iteration 15400: Loss = -10913.449881061977
1
Iteration 15500: Loss = -10913.448913806238
Iteration 15600: Loss = -10913.448963983376
Iteration 15700: Loss = -10913.453155587933
1
Iteration 15800: Loss = -10913.46603976862
2
Iteration 15900: Loss = -10913.44894045862
Iteration 16000: Loss = -10913.44995710482
1
Iteration 16100: Loss = -10913.44892880821
Iteration 16200: Loss = -10913.449422859427
1
Iteration 16300: Loss = -10913.448953303101
Iteration 16400: Loss = -10913.448981705906
Iteration 16500: Loss = -10913.448956564973
Iteration 16600: Loss = -10913.448985757439
Iteration 16700: Loss = -10913.448942761901
Iteration 16800: Loss = -10913.44930460365
1
Iteration 16900: Loss = -10913.448940666873
Iteration 17000: Loss = -10913.470975910499
1
Iteration 17100: Loss = -10913.448898534143
Iteration 17200: Loss = -10913.44892081258
Iteration 17300: Loss = -10913.449055674106
1
Iteration 17400: Loss = -10913.448910298048
Iteration 17500: Loss = -10913.450934941158
1
Iteration 17600: Loss = -10913.44893135104
Iteration 17700: Loss = -10913.45008266152
1
Iteration 17800: Loss = -10913.448913844604
Iteration 17900: Loss = -10913.450770512884
1
Iteration 18000: Loss = -10913.44891963376
Iteration 18100: Loss = -10913.4489159161
Iteration 18200: Loss = -10913.449146010718
1
Iteration 18300: Loss = -10913.448933226948
Iteration 18400: Loss = -10913.448924647
Iteration 18500: Loss = -10913.448999493166
Iteration 18600: Loss = -10913.448922615336
Iteration 18700: Loss = -10913.459265494968
1
Iteration 18800: Loss = -10913.448906015765
Iteration 18900: Loss = -10913.448905147421
Iteration 19000: Loss = -10913.460950636281
1
Iteration 19100: Loss = -10913.448916061263
Iteration 19200: Loss = -10913.4489267304
Iteration 19300: Loss = -10913.464529381723
1
Iteration 19400: Loss = -10913.44893652057
Iteration 19500: Loss = -10913.448901895665
Iteration 19600: Loss = -10913.44891503015
Iteration 19700: Loss = -10913.44896672455
Iteration 19800: Loss = -10913.449673534964
1
Iteration 19900: Loss = -10913.448923639511
pi: tensor([[1.0000e+00, 6.4633e-08],
        [4.5659e-01, 5.4341e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5087, 0.4913], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1639, 0.1119],
         [0.5860, 0.2842]],

        [[0.5765, 0.1319],
         [0.6531, 0.6848]],

        [[0.6602, 0.1326],
         [0.6855, 0.5069]],

        [[0.7267, 0.1018],
         [0.5069, 0.5698]],

        [[0.6336, 0.1653],
         [0.5727, 0.7076]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369480537608971
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 78
Adjusted Rand Index: 0.3077261613691932
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 68
Adjusted Rand Index: 0.12080808080808081
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.001684040076915292
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.1342766880329306
Average Adjusted Rand Index: 0.23327634417536364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21132.615664075212
Iteration 100: Loss = -10942.237633022798
Iteration 200: Loss = -10941.310760452208
Iteration 300: Loss = -10940.236730560206
Iteration 400: Loss = -10939.312643813506
Iteration 500: Loss = -10939.162459236797
Iteration 600: Loss = -10939.084246936178
Iteration 700: Loss = -10939.0169834046
Iteration 800: Loss = -10938.930530532583
Iteration 900: Loss = -10938.746382367228
Iteration 1000: Loss = -10938.267044748021
Iteration 1100: Loss = -10937.860566192829
Iteration 1200: Loss = -10937.549819659353
Iteration 1300: Loss = -10928.019973854436
Iteration 1400: Loss = -10918.730922512725
Iteration 1500: Loss = -10918.618792249043
Iteration 1600: Loss = -10918.563341715799
Iteration 1700: Loss = -10918.27669550907
Iteration 1800: Loss = -10913.005384019763
Iteration 1900: Loss = -10903.102482129065
Iteration 2000: Loss = -10848.737946398416
Iteration 2100: Loss = -10828.692020857308
Iteration 2200: Loss = -10827.913105620388
Iteration 2300: Loss = -10827.767702771047
Iteration 2400: Loss = -10827.671776302615
Iteration 2500: Loss = -10827.63398689313
Iteration 2600: Loss = -10827.612257977855
Iteration 2700: Loss = -10827.59401390595
Iteration 2800: Loss = -10827.577573626997
Iteration 2900: Loss = -10827.556620107407
Iteration 3000: Loss = -10827.408419829932
Iteration 3100: Loss = -10827.395372839539
Iteration 3200: Loss = -10827.42554389805
1
Iteration 3300: Loss = -10827.371205981388
Iteration 3400: Loss = -10827.359062127714
Iteration 3500: Loss = -10827.356767669138
Iteration 3600: Loss = -10827.3540160311
Iteration 3700: Loss = -10827.352030115657
Iteration 3800: Loss = -10827.350983309874
Iteration 3900: Loss = -10827.348769445442
Iteration 4000: Loss = -10827.34601733434
Iteration 4100: Loss = -10827.34266682984
Iteration 4200: Loss = -10827.333201638092
Iteration 4300: Loss = -10827.330713514475
Iteration 4400: Loss = -10827.329370837453
Iteration 4500: Loss = -10827.32840806747
Iteration 4600: Loss = -10827.327217883758
Iteration 4700: Loss = -10827.326270840127
Iteration 4800: Loss = -10827.325501279935
Iteration 4900: Loss = -10827.32532225668
Iteration 5000: Loss = -10827.323935713428
Iteration 5100: Loss = -10827.327198388586
1
Iteration 5200: Loss = -10827.309860134601
Iteration 5300: Loss = -10827.312877419472
1
Iteration 5400: Loss = -10827.306230330325
Iteration 5500: Loss = -10827.305807424122
Iteration 5600: Loss = -10827.304686899928
Iteration 5700: Loss = -10827.303020450478
Iteration 5800: Loss = -10827.3006290607
Iteration 5900: Loss = -10827.29899447087
Iteration 6000: Loss = -10827.285336746341
Iteration 6100: Loss = -10827.287534471028
1
Iteration 6200: Loss = -10827.28342545064
Iteration 6300: Loss = -10827.28371641428
1
Iteration 6400: Loss = -10827.283623408284
2
Iteration 6500: Loss = -10827.28255646483
Iteration 6600: Loss = -10827.283251854555
1
Iteration 6700: Loss = -10827.282643591974
Iteration 6800: Loss = -10827.281510908233
Iteration 6900: Loss = -10827.283640226531
1
Iteration 7000: Loss = -10827.278617190683
Iteration 7100: Loss = -10827.268601867872
Iteration 7200: Loss = -10827.26327004097
Iteration 7300: Loss = -10827.262968522278
Iteration 7400: Loss = -10827.262680102394
Iteration 7500: Loss = -10827.26241591445
Iteration 7600: Loss = -10827.262129112245
Iteration 7700: Loss = -10827.262692037364
1
Iteration 7800: Loss = -10827.261474147523
Iteration 7900: Loss = -10827.23007056395
Iteration 8000: Loss = -10827.333028325564
1
Iteration 8100: Loss = -10827.229423880513
Iteration 8200: Loss = -10827.274831144712
1
Iteration 8300: Loss = -10827.229279845207
Iteration 8400: Loss = -10827.22921573716
Iteration 8500: Loss = -10827.229236885722
Iteration 8600: Loss = -10827.352936919893
1
Iteration 8700: Loss = -10827.228947384207
Iteration 8800: Loss = -10827.23677061836
1
Iteration 8900: Loss = -10827.228584237013
Iteration 9000: Loss = -10827.227295674904
Iteration 9100: Loss = -10827.231737447195
1
Iteration 9200: Loss = -10827.226478022269
Iteration 9300: Loss = -10827.225151165387
Iteration 9400: Loss = -10827.223395665033
Iteration 9500: Loss = -10827.223669329516
1
Iteration 9600: Loss = -10827.224876448947
2
Iteration 9700: Loss = -10826.963845433658
Iteration 9800: Loss = -10827.077901891425
1
Iteration 9900: Loss = -10826.898480581798
Iteration 10000: Loss = -10826.895717600893
Iteration 10100: Loss = -10826.89406356491
Iteration 10200: Loss = -10826.844751276214
Iteration 10300: Loss = -10826.840472551654
Iteration 10400: Loss = -10826.86146921215
1
Iteration 10500: Loss = -10826.844699662786
2
Iteration 10600: Loss = -10826.839256143527
Iteration 10700: Loss = -10826.84273925398
1
Iteration 10800: Loss = -10826.837203232792
Iteration 10900: Loss = -10826.836918386975
Iteration 11000: Loss = -10826.833984236759
Iteration 11100: Loss = -10827.164666813496
1
Iteration 11200: Loss = -10826.831557616497
Iteration 11300: Loss = -10826.810873332424
Iteration 11400: Loss = -10826.815076487148
1
Iteration 11500: Loss = -10826.812201472943
2
Iteration 11600: Loss = -10826.81783393246
3
Iteration 11700: Loss = -10826.807782578351
Iteration 11800: Loss = -10826.816085258877
1
Iteration 11900: Loss = -10826.807660258688
Iteration 12000: Loss = -10826.804182991014
Iteration 12100: Loss = -10826.825037027213
1
Iteration 12200: Loss = -10826.81583819315
2
Iteration 12300: Loss = -10826.804331750856
3
Iteration 12400: Loss = -10823.742194501749
Iteration 12500: Loss = -10823.695557282124
Iteration 12600: Loss = -10823.71752671255
1
Iteration 12700: Loss = -10823.672289211985
Iteration 12800: Loss = -10823.672185753121
Iteration 12900: Loss = -10824.066480791875
1
Iteration 13000: Loss = -10823.671269273305
Iteration 13100: Loss = -10823.63562941939
Iteration 13200: Loss = -10823.636515341877
1
Iteration 13300: Loss = -10823.628005150631
Iteration 13400: Loss = -10823.903895925294
1
Iteration 13500: Loss = -10823.627986870084
Iteration 13600: Loss = -10823.627966075745
Iteration 13700: Loss = -10823.628064586976
Iteration 13800: Loss = -10823.627884687308
Iteration 13900: Loss = -10823.627778687587
Iteration 14000: Loss = -10823.626741088947
Iteration 14100: Loss = -10823.625844338072
Iteration 14200: Loss = -10823.624761272033
Iteration 14300: Loss = -10823.626704981893
1
Iteration 14400: Loss = -10823.61728183166
Iteration 14500: Loss = -10823.619347894102
1
Iteration 14600: Loss = -10823.617172596976
Iteration 14700: Loss = -10823.619350189212
1
Iteration 14800: Loss = -10823.617143516622
Iteration 14900: Loss = -10823.68876341806
1
Iteration 15000: Loss = -10823.617124590139
Iteration 15100: Loss = -10823.617140651697
Iteration 15200: Loss = -10823.617218889249
Iteration 15300: Loss = -10823.61687766961
Iteration 15400: Loss = -10823.665222305135
1
Iteration 15500: Loss = -10823.616857559022
Iteration 15600: Loss = -10823.616956119167
Iteration 15700: Loss = -10823.61678466137
Iteration 15800: Loss = -10823.616712740508
Iteration 15900: Loss = -10823.617084414787
1
Iteration 16000: Loss = -10823.616681070052
Iteration 16100: Loss = -10823.620200574414
1
Iteration 16200: Loss = -10823.616678217819
Iteration 16300: Loss = -10823.64399374187
1
Iteration 16400: Loss = -10823.616635469452
Iteration 16500: Loss = -10823.616603683287
Iteration 16600: Loss = -10823.616579332174
Iteration 16700: Loss = -10823.616013697443
Iteration 16800: Loss = -10823.605657740241
Iteration 16900: Loss = -10823.603471760714
Iteration 17000: Loss = -10823.921963091496
1
Iteration 17100: Loss = -10823.603479469157
Iteration 17200: Loss = -10823.603499108482
Iteration 17300: Loss = -10823.60372890176
1
Iteration 17400: Loss = -10823.603480146365
Iteration 17500: Loss = -10823.61559143976
1
Iteration 17600: Loss = -10823.603497105994
Iteration 17700: Loss = -10823.603310705892
Iteration 17800: Loss = -10823.60101428501
Iteration 17900: Loss = -10823.600899477115
Iteration 18000: Loss = -10823.661418088097
1
Iteration 18100: Loss = -10823.600904332992
Iteration 18200: Loss = -10823.60090984657
Iteration 18300: Loss = -10823.601529044478
1
Iteration 18400: Loss = -10823.60090271423
Iteration 18500: Loss = -10823.604917525558
1
Iteration 18600: Loss = -10823.600899824803
Iteration 18700: Loss = -10823.598686868527
Iteration 18800: Loss = -10823.598700486727
Iteration 18900: Loss = -10823.638195196001
1
Iteration 19000: Loss = -10823.598156483667
Iteration 19100: Loss = -10823.617908717226
1
Iteration 19200: Loss = -10823.598122996156
Iteration 19300: Loss = -10823.60054809854
1
Iteration 19400: Loss = -10823.598098795837
Iteration 19500: Loss = -10823.60336327971
1
Iteration 19600: Loss = -10823.598083235993
Iteration 19700: Loss = -10823.598904471135
1
Iteration 19800: Loss = -10823.590407918631
Iteration 19900: Loss = -10823.585678170051
pi: tensor([[0.7294, 0.2706],
        [0.2513, 0.7487]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5409, 0.4591], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2479, 0.1062],
         [0.5054, 0.1986]],

        [[0.5343, 0.1056],
         [0.6258, 0.5962]],

        [[0.6958, 0.0964],
         [0.7012, 0.7197]],

        [[0.5318, 0.0925],
         [0.6574, 0.7280]],

        [[0.6367, 0.1014],
         [0.6638, 0.6658]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026262626262626
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.8534824373504019
Average Adjusted Rand Index: 0.8546193945068566
10870.479244653518
[0.1342766880329306, 0.8534824373504019] [0.23327634417536364, 0.8546193945068566] [10913.448952655177, 10823.586015591729]
-------------------------------------
This iteration is 20
True Objective function: Loss = -10885.618121766836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23383.58499078557
Iteration 100: Loss = -11009.415119701243
Iteration 200: Loss = -11008.035577107921
Iteration 300: Loss = -11007.489480417746
Iteration 400: Loss = -11006.285689083688
Iteration 500: Loss = -11001.402996687031
Iteration 600: Loss = -11000.488112459874
Iteration 700: Loss = -11000.088036233981
Iteration 800: Loss = -10999.861740810044
Iteration 900: Loss = -10999.722104667857
Iteration 1000: Loss = -10999.63188702506
Iteration 1100: Loss = -10999.5695822807
Iteration 1200: Loss = -10999.523919860392
Iteration 1300: Loss = -10999.490211729277
Iteration 1400: Loss = -10999.465201753428
Iteration 1500: Loss = -10999.446355776608
Iteration 1600: Loss = -10999.431721499406
Iteration 1700: Loss = -10999.420116183046
Iteration 1800: Loss = -10999.410680805147
Iteration 1900: Loss = -10999.403046557632
Iteration 2000: Loss = -10999.396756980357
Iteration 2100: Loss = -10999.391407913883
Iteration 2200: Loss = -10999.386896954114
Iteration 2300: Loss = -10999.382926386741
Iteration 2400: Loss = -10999.379420632284
Iteration 2500: Loss = -10999.376211110117
Iteration 2600: Loss = -10999.37339800721
Iteration 2700: Loss = -10999.37081158435
Iteration 2800: Loss = -10999.36841003795
Iteration 2900: Loss = -10999.366244322242
Iteration 3000: Loss = -10999.364231089128
Iteration 3100: Loss = -10999.36239759673
Iteration 3200: Loss = -10999.360648915941
Iteration 3300: Loss = -10999.359102769424
Iteration 3400: Loss = -10999.357652030869
Iteration 3500: Loss = -10999.356248043148
Iteration 3600: Loss = -10999.355000378102
Iteration 3700: Loss = -10999.35382605789
Iteration 3800: Loss = -10999.35270269411
Iteration 3900: Loss = -10999.35168926559
Iteration 4000: Loss = -10999.350731794146
Iteration 4100: Loss = -10999.34986393779
Iteration 4200: Loss = -10999.349007842373
Iteration 4300: Loss = -10999.348238073673
Iteration 4400: Loss = -10999.347621174951
Iteration 4500: Loss = -10999.346824252387
Iteration 4600: Loss = -10999.346169884218
Iteration 4700: Loss = -10999.347820942623
1
Iteration 4800: Loss = -10999.344994187819
Iteration 4900: Loss = -10999.344469885003
Iteration 5000: Loss = -10999.34916524858
1
Iteration 5100: Loss = -10999.343544680925
Iteration 5200: Loss = -10999.343098011617
Iteration 5300: Loss = -10999.342688871106
Iteration 5400: Loss = -10999.342375151107
Iteration 5500: Loss = -10999.342509391063
1
Iteration 5600: Loss = -10999.341570151268
Iteration 5700: Loss = -10999.341260836001
Iteration 5800: Loss = -10999.341127986381
Iteration 5900: Loss = -10999.340861126117
Iteration 6000: Loss = -10999.340453038989
Iteration 6100: Loss = -10999.340244679877
Iteration 6200: Loss = -10999.339863443734
Iteration 6300: Loss = -10999.339929230255
Iteration 6400: Loss = -10999.33982575506
Iteration 6500: Loss = -10999.341800080581
1
Iteration 6600: Loss = -10999.343382175606
2
Iteration 6700: Loss = -10999.342800141832
3
Iteration 6800: Loss = -10999.338898026079
Iteration 6900: Loss = -10999.338657223483
Iteration 7000: Loss = -10999.338521080468
Iteration 7100: Loss = -10999.338167320388
Iteration 7200: Loss = -10999.338011417596
Iteration 7300: Loss = -10999.337911337361
Iteration 7400: Loss = -10999.337757146533
Iteration 7500: Loss = -10999.337631429911
Iteration 7600: Loss = -10999.337557350567
Iteration 7700: Loss = -10999.337389633974
Iteration 7800: Loss = -10999.337493995708
1
Iteration 7900: Loss = -10999.33721126606
Iteration 8000: Loss = -10999.337278232319
Iteration 8100: Loss = -10999.339352965044
1
Iteration 8200: Loss = -10999.336927009295
Iteration 8300: Loss = -10999.544024093662
1
Iteration 8400: Loss = -10999.336810458954
Iteration 8500: Loss = -10999.343064860059
1
Iteration 8600: Loss = -10999.336652090555
Iteration 8700: Loss = -10999.336590122764
Iteration 8800: Loss = -10999.343941378118
1
Iteration 8900: Loss = -10999.336453919233
Iteration 9000: Loss = -10999.336421764056
Iteration 9100: Loss = -10999.353208382587
1
Iteration 9200: Loss = -10999.336234703427
Iteration 9300: Loss = -10999.336241603505
Iteration 9400: Loss = -10999.336210820558
Iteration 9500: Loss = -10999.336307025673
Iteration 9600: Loss = -10999.336143931083
Iteration 9700: Loss = -10999.344040453752
1
Iteration 9800: Loss = -10999.336051305587
Iteration 9900: Loss = -10999.335986867938
Iteration 10000: Loss = -10999.336381407436
1
Iteration 10100: Loss = -10999.335958039284
Iteration 10200: Loss = -10999.33589859744
Iteration 10300: Loss = -10999.430312110084
1
Iteration 10400: Loss = -10999.335824373793
Iteration 10500: Loss = -10999.335803904267
Iteration 10600: Loss = -10999.340369439127
1
Iteration 10700: Loss = -10999.335747988787
Iteration 10800: Loss = -10999.335763353847
Iteration 10900: Loss = -10999.336283158238
1
Iteration 11000: Loss = -10999.335729646993
Iteration 11100: Loss = -10999.335687750236
Iteration 11200: Loss = -10999.33594520569
1
Iteration 11300: Loss = -10999.335684121248
Iteration 11400: Loss = -10999.335612321383
Iteration 11500: Loss = -10999.335860422181
1
Iteration 11600: Loss = -10999.335568840634
Iteration 11700: Loss = -10999.343405883968
1
Iteration 11800: Loss = -10999.33688898279
2
Iteration 11900: Loss = -10999.335696771594
3
Iteration 12000: Loss = -10999.396088942538
4
Iteration 12100: Loss = -10999.335554720177
Iteration 12200: Loss = -10999.337146883689
1
Iteration 12300: Loss = -10999.340343467444
2
Iteration 12400: Loss = -10999.33556540792
Iteration 12500: Loss = -10999.33579181195
1
Iteration 12600: Loss = -10999.39581160237
2
Iteration 12700: Loss = -10999.335524014079
Iteration 12800: Loss = -10999.381539035634
1
Iteration 12900: Loss = -10999.335465988905
Iteration 13000: Loss = -10999.354639584353
1
Iteration 13100: Loss = -10999.33547905103
Iteration 13200: Loss = -10999.335577256245
Iteration 13300: Loss = -10999.335669558975
Iteration 13400: Loss = -10999.335495154575
Iteration 13500: Loss = -10999.335433999438
Iteration 13600: Loss = -10999.335695096097
1
Iteration 13700: Loss = -10999.335413716028
Iteration 13800: Loss = -10999.338401601473
1
Iteration 13900: Loss = -10999.335463138947
Iteration 14000: Loss = -10999.33557114415
1
Iteration 14100: Loss = -10999.336039685011
2
Iteration 14200: Loss = -10999.335406385682
Iteration 14300: Loss = -10999.3353588302
Iteration 14400: Loss = -10999.335669804725
1
Iteration 14500: Loss = -10999.335370901925
Iteration 14600: Loss = -10999.352501994841
1
Iteration 14700: Loss = -10999.335378474976
Iteration 14800: Loss = -10999.335387337427
Iteration 14900: Loss = -10999.40983715099
1
Iteration 15000: Loss = -10999.342208337212
2
Iteration 15100: Loss = -10999.366355938606
3
Iteration 15200: Loss = -10999.335381762776
Iteration 15300: Loss = -10999.33556352914
1
Iteration 15400: Loss = -10999.430174955354
2
Iteration 15500: Loss = -10999.335349355244
Iteration 15600: Loss = -10999.36055083704
1
Iteration 15700: Loss = -10999.34242456504
2
Iteration 15800: Loss = -10999.379143099075
3
Iteration 15900: Loss = -10999.335441258278
Iteration 16000: Loss = -10999.335693685023
1
Iteration 16100: Loss = -10999.398119585776
2
Iteration 16200: Loss = -10999.335356664375
Iteration 16300: Loss = -10999.44012043425
1
Iteration 16400: Loss = -10999.335344337791
Iteration 16500: Loss = -10999.348851310899
1
Iteration 16600: Loss = -10999.338036901157
2
Iteration 16700: Loss = -10999.338020092162
3
Iteration 16800: Loss = -10999.336119791129
4
Iteration 16900: Loss = -10999.335369909075
Iteration 17000: Loss = -10999.356228550452
1
Iteration 17100: Loss = -10999.33537515393
Iteration 17200: Loss = -10999.335337461838
Iteration 17300: Loss = -10999.374560689666
1
Iteration 17400: Loss = -10999.455758504417
2
Iteration 17500: Loss = -10999.335347813958
Iteration 17600: Loss = -10999.336401815905
1
Iteration 17700: Loss = -10999.33531980637
Iteration 17800: Loss = -10999.335410071784
Iteration 17900: Loss = -10999.335476971964
Iteration 18000: Loss = -10999.335466227507
Iteration 18100: Loss = -10999.335353060387
Iteration 18200: Loss = -10999.335340162816
Iteration 18300: Loss = -10999.335865052542
1
Iteration 18400: Loss = -10999.33553061148
2
Iteration 18500: Loss = -10999.335324154054
Iteration 18600: Loss = -10999.388141250776
1
Iteration 18700: Loss = -10999.335325230615
Iteration 18800: Loss = -10999.338388994258
1
Iteration 18900: Loss = -10999.346900668957
2
Iteration 19000: Loss = -10999.336725549196
3
Iteration 19100: Loss = -10999.335473172396
4
Iteration 19200: Loss = -10999.335904259342
5
Iteration 19300: Loss = -10999.335989612793
6
Iteration 19400: Loss = -10999.345752965504
7
Iteration 19500: Loss = -10999.335282655504
Iteration 19600: Loss = -10999.335483001818
1
Iteration 19700: Loss = -10999.335682446566
2
Iteration 19800: Loss = -10999.406767925628
3
Iteration 19900: Loss = -10999.355528186585
4
pi: tensor([[1.0893e-06, 1.0000e+00],
        [6.6028e-02, 9.3397e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0960, 0.9040], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2850, 0.2079],
         [0.7113, 0.1543]],

        [[0.6587, 0.2710],
         [0.5728, 0.6858]],

        [[0.5575, 0.2061],
         [0.6025, 0.7074]],

        [[0.6724, 0.1894],
         [0.5930, 0.5328]],

        [[0.6449, 0.2261],
         [0.7176, 0.5958]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.001451180515704904
Average Adjusted Rand Index: -0.0016118884167510607
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22362.90756399564
Iteration 100: Loss = -11009.087346966693
Iteration 200: Loss = -11007.8629031371
Iteration 300: Loss = -11007.407305747416
Iteration 400: Loss = -11007.007912614035
Iteration 500: Loss = -11005.99844652442
Iteration 600: Loss = -11002.44419405598
Iteration 700: Loss = -11000.879753957224
Iteration 800: Loss = -11000.061425071986
Iteration 900: Loss = -10999.829295691019
Iteration 1000: Loss = -10999.710418222761
Iteration 1100: Loss = -10999.635306278384
Iteration 1200: Loss = -10999.582474931365
Iteration 1300: Loss = -10999.543552150115
Iteration 1400: Loss = -10999.513764569687
Iteration 1500: Loss = -10999.490312564985
Iteration 1600: Loss = -10999.471329300488
Iteration 1700: Loss = -10999.455624455468
Iteration 1800: Loss = -10999.442415554287
Iteration 1900: Loss = -10999.431204615317
Iteration 2000: Loss = -10999.421585522145
Iteration 2100: Loss = -10999.413332631882
Iteration 2200: Loss = -10999.406099608765
Iteration 2300: Loss = -10999.399843703919
Iteration 2400: Loss = -10999.394280201017
Iteration 2500: Loss = -10999.389422047974
Iteration 2600: Loss = -10999.384993434138
Iteration 2700: Loss = -10999.381141112335
Iteration 2800: Loss = -10999.377603510591
Iteration 2900: Loss = -10999.374456044425
Iteration 3000: Loss = -10999.371583203574
Iteration 3100: Loss = -10999.369019751754
Iteration 3200: Loss = -10999.366661728327
Iteration 3300: Loss = -10999.364491209373
Iteration 3400: Loss = -10999.362535438497
Iteration 3500: Loss = -10999.360700662286
Iteration 3600: Loss = -10999.35905653271
Iteration 3700: Loss = -10999.357505864473
Iteration 3800: Loss = -10999.356099539378
Iteration 3900: Loss = -10999.354790306448
Iteration 4000: Loss = -10999.354291024312
Iteration 4100: Loss = -10999.352482481341
Iteration 4200: Loss = -10999.351617501321
Iteration 4300: Loss = -10999.350483641887
Iteration 4400: Loss = -10999.349589737503
Iteration 4500: Loss = -10999.348722090006
Iteration 4600: Loss = -10999.347929071533
Iteration 4700: Loss = -10999.35026291223
1
Iteration 4800: Loss = -10999.347377483013
Iteration 4900: Loss = -10999.345881575555
Iteration 5000: Loss = -10999.345394452086
Iteration 5100: Loss = -10999.344910644453
Iteration 5200: Loss = -10999.34526276115
1
Iteration 5300: Loss = -10999.344624883779
Iteration 5400: Loss = -10999.344248020214
Iteration 5500: Loss = -10999.343879092085
Iteration 5600: Loss = -10999.344793197924
1
Iteration 5700: Loss = -10999.344093244996
2
Iteration 5800: Loss = -10999.343300396029
Iteration 5900: Loss = -10999.341417458925
Iteration 6000: Loss = -10999.344084009514
1
Iteration 6100: Loss = -10999.341354344373
Iteration 6200: Loss = -10999.340510621534
Iteration 6300: Loss = -10999.340210653112
Iteration 6400: Loss = -10999.339986567515
Iteration 6500: Loss = -10999.342707117641
1
Iteration 6600: Loss = -10999.33961665901
Iteration 6700: Loss = -10999.339262892803
Iteration 6800: Loss = -10999.339047561403
Iteration 6900: Loss = -10999.33886223713
Iteration 7000: Loss = -10999.338681587047
Iteration 7100: Loss = -10999.338480951546
Iteration 7200: Loss = -10999.342051360532
1
Iteration 7300: Loss = -10999.338189649472
Iteration 7400: Loss = -10999.338509015994
1
Iteration 7500: Loss = -10999.3379230321
Iteration 7600: Loss = -10999.3756227774
1
Iteration 7700: Loss = -10999.337651829825
Iteration 7800: Loss = -10999.350235433802
1
Iteration 7900: Loss = -10999.33740465421
Iteration 8000: Loss = -10999.337682249838
1
Iteration 8100: Loss = -10999.337316999447
Iteration 8200: Loss = -10999.337141427312
Iteration 8300: Loss = -10999.337023044778
Iteration 8400: Loss = -10999.336941772117
Iteration 8500: Loss = -10999.336874809724
Iteration 8600: Loss = -10999.368574081283
1
Iteration 8700: Loss = -10999.336684981992
Iteration 8800: Loss = -10999.336651269312
Iteration 8900: Loss = -10999.345542884706
1
Iteration 9000: Loss = -10999.33648313079
Iteration 9100: Loss = -10999.336457980782
Iteration 9200: Loss = -10999.336379035085
Iteration 9300: Loss = -10999.336325164686
Iteration 9400: Loss = -10999.467203283517
1
Iteration 9500: Loss = -10999.33629576153
Iteration 9600: Loss = -10999.336208008943
Iteration 9700: Loss = -10999.409503274986
1
Iteration 9800: Loss = -10999.336135444515
Iteration 9900: Loss = -10999.336055648382
Iteration 10000: Loss = -10999.338497573524
1
Iteration 10100: Loss = -10999.336019347984
Iteration 10200: Loss = -10999.335966795583
Iteration 10300: Loss = -10999.34486922718
1
Iteration 10400: Loss = -10999.335894520435
Iteration 10500: Loss = -10999.335880131704
Iteration 10600: Loss = -10999.338290239239
1
Iteration 10700: Loss = -10999.335813973243
Iteration 10800: Loss = -10999.33582869946
Iteration 10900: Loss = -10999.337135852324
1
Iteration 11000: Loss = -10999.335722593694
Iteration 11100: Loss = -10999.335713105165
Iteration 11200: Loss = -10999.336974222482
1
Iteration 11300: Loss = -10999.335692504268
Iteration 11400: Loss = -10999.33566201365
Iteration 11500: Loss = -10999.336117468436
1
Iteration 11600: Loss = -10999.337567621085
2
Iteration 11700: Loss = -10999.33863458251
3
Iteration 11800: Loss = -10999.33561567614
Iteration 11900: Loss = -10999.33743546998
1
Iteration 12000: Loss = -10999.335579364495
Iteration 12100: Loss = -10999.350215151373
1
Iteration 12200: Loss = -10999.33554532289
Iteration 12300: Loss = -10999.336697928984
1
Iteration 12400: Loss = -10999.335503505592
Iteration 12500: Loss = -10999.343882934969
1
Iteration 12600: Loss = -10999.33549581383
Iteration 12700: Loss = -10999.398954748955
1
Iteration 12800: Loss = -10999.335479004973
Iteration 12900: Loss = -10999.33546639989
Iteration 13000: Loss = -10999.405422531732
1
Iteration 13100: Loss = -10999.335432513644
Iteration 13200: Loss = -10999.335448023705
Iteration 13300: Loss = -10999.335583710636
1
Iteration 13400: Loss = -10999.36438832643
2
Iteration 13500: Loss = -10999.335432012584
Iteration 13600: Loss = -10999.3354129717
Iteration 13700: Loss = -10999.335581418836
1
Iteration 13800: Loss = -10999.335437395268
Iteration 13900: Loss = -10999.338710285498
1
Iteration 14000: Loss = -10999.335399540352
Iteration 14100: Loss = -10999.335431897889
Iteration 14200: Loss = -10999.335646163685
1
Iteration 14300: Loss = -10999.765119896223
2
Iteration 14400: Loss = -10999.335406227277
Iteration 14500: Loss = -10999.335372469055
Iteration 14600: Loss = -10999.335685869513
1
Iteration 14700: Loss = -10999.335380281242
Iteration 14800: Loss = -10999.336074176033
1
Iteration 14900: Loss = -10999.335959812597
2
Iteration 15000: Loss = -10999.33536672096
Iteration 15100: Loss = -10999.527394051505
1
Iteration 15200: Loss = -10999.335354123934
Iteration 15300: Loss = -10999.335414874295
Iteration 15400: Loss = -10999.355227025064
1
Iteration 15500: Loss = -10999.335378341015
Iteration 15600: Loss = -10999.337455465547
1
Iteration 15700: Loss = -10999.335427332784
Iteration 15800: Loss = -10999.36369018304
1
Iteration 15900: Loss = -10999.338883531056
2
Iteration 16000: Loss = -10999.338678761686
3
Iteration 16100: Loss = -10999.336064852687
4
Iteration 16200: Loss = -10999.335596330648
5
Iteration 16300: Loss = -10999.335531699298
6
Iteration 16400: Loss = -10999.337825374556
7
Iteration 16500: Loss = -10999.337788559418
8
Iteration 16600: Loss = -10999.335356547279
Iteration 16700: Loss = -10999.338490310258
1
Iteration 16800: Loss = -10999.33658804294
2
Iteration 16900: Loss = -10999.335371963252
Iteration 17000: Loss = -10999.337583662627
1
Iteration 17100: Loss = -10999.33825192776
2
Iteration 17200: Loss = -10999.508570669173
3
Iteration 17300: Loss = -10999.335370879864
Iteration 17400: Loss = -10999.335543762865
1
Iteration 17500: Loss = -10999.35060343461
2
Iteration 17600: Loss = -10999.33557820807
3
Iteration 17700: Loss = -10999.336522380285
4
Iteration 17800: Loss = -10999.335362977403
Iteration 17900: Loss = -10999.335570969606
1
Iteration 18000: Loss = -10999.335674861586
2
Iteration 18100: Loss = -10999.338026257863
3
Iteration 18200: Loss = -10999.336387232783
4
Iteration 18300: Loss = -10999.335638177357
5
Iteration 18400: Loss = -10999.335676525649
6
Iteration 18500: Loss = -10999.335705218115
7
Iteration 18600: Loss = -10999.3461599379
8
Iteration 18700: Loss = -10999.398098190075
9
Iteration 18800: Loss = -10999.34577474021
10
Iteration 18900: Loss = -10999.749578532357
11
Iteration 19000: Loss = -10999.335333239174
Iteration 19100: Loss = -10999.357487302426
1
Iteration 19200: Loss = -10999.360072113835
2
Iteration 19300: Loss = -10999.335625851412
3
Iteration 19400: Loss = -10999.33533360937
Iteration 19500: Loss = -10999.336105236704
1
Iteration 19600: Loss = -10999.364526138974
2
Iteration 19700: Loss = -10999.33893637121
3
Iteration 19800: Loss = -10999.33533523225
Iteration 19900: Loss = -10999.337145893995
1
pi: tensor([[9.3438e-01, 6.5617e-02],
        [1.0000e+00, 1.1564e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9048, 0.0952], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1544, 0.2090],
         [0.7214, 0.2865]],

        [[0.6058, 0.2711],
         [0.5090, 0.5071]],

        [[0.5438, 0.2073],
         [0.5078, 0.7303]],

        [[0.6846, 0.1900],
         [0.5828, 0.7292]],

        [[0.6077, 0.2265],
         [0.5438, 0.5251]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.001451180515704904
Average Adjusted Rand Index: -0.0016118884167510607
10885.618121766836
[-0.001451180515704904, -0.001451180515704904] [-0.0016118884167510607, -0.0016118884167510607] [10999.339153978815, 10999.343771235077]
-------------------------------------
This iteration is 21
True Objective function: Loss = -10798.484455847329
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22088.805565668088
Iteration 100: Loss = -10878.09083676804
Iteration 200: Loss = -10877.022800970879
Iteration 300: Loss = -10876.775606851124
Iteration 400: Loss = -10876.573016191986
Iteration 500: Loss = -10876.30138883832
Iteration 600: Loss = -10876.131445808003
Iteration 700: Loss = -10876.043603096117
Iteration 800: Loss = -10875.959128507775
Iteration 900: Loss = -10875.86194868673
Iteration 1000: Loss = -10875.758931341814
Iteration 1100: Loss = -10875.673210114677
Iteration 1200: Loss = -10875.598026891905
Iteration 1300: Loss = -10875.519279690185
Iteration 1400: Loss = -10875.442102944497
Iteration 1500: Loss = -10875.366327342817
Iteration 1600: Loss = -10875.281038016994
Iteration 1700: Loss = -10875.181449111244
Iteration 1800: Loss = -10875.019330830757
Iteration 1900: Loss = -10874.713017429263
Iteration 2000: Loss = -10874.483928018182
Iteration 2100: Loss = -10874.401331509891
Iteration 2200: Loss = -10874.380360613644
Iteration 2300: Loss = -10874.371146179463
Iteration 2400: Loss = -10874.35816177302
Iteration 2500: Loss = -10874.320203282841
Iteration 2600: Loss = -10874.056307403578
Iteration 2700: Loss = -10867.953491355156
Iteration 2800: Loss = -10763.719766511518
Iteration 2900: Loss = -10763.554178544873
Iteration 3000: Loss = -10763.482711129329
Iteration 3100: Loss = -10763.451627844597
Iteration 3200: Loss = -10763.434939152843
Iteration 3300: Loss = -10763.427458877943
Iteration 3400: Loss = -10763.421308468116
Iteration 3500: Loss = -10763.414316843899
Iteration 3600: Loss = -10763.397199784515
Iteration 3700: Loss = -10763.370258594665
Iteration 3800: Loss = -10763.361873913937
Iteration 3900: Loss = -10763.360652085654
Iteration 4000: Loss = -10763.359515323844
Iteration 4100: Loss = -10763.358411197189
Iteration 4200: Loss = -10763.356967606958
Iteration 4300: Loss = -10763.357060407081
Iteration 4400: Loss = -10763.305585759763
Iteration 4500: Loss = -10763.304962048018
Iteration 4600: Loss = -10763.3061808332
1
Iteration 4700: Loss = -10763.30415555184
Iteration 4800: Loss = -10763.30391980088
Iteration 4900: Loss = -10763.315642367064
1
Iteration 5000: Loss = -10763.302208412259
Iteration 5100: Loss = -10763.30180932486
Iteration 5200: Loss = -10763.30806862066
1
Iteration 5300: Loss = -10763.299289269415
Iteration 5400: Loss = -10763.296815904268
Iteration 5500: Loss = -10763.292545209619
Iteration 5600: Loss = -10763.2924230847
Iteration 5700: Loss = -10763.299114199226
1
Iteration 5800: Loss = -10763.292134209441
Iteration 5900: Loss = -10763.292154533972
Iteration 6000: Loss = -10763.292726882415
1
Iteration 6100: Loss = -10763.291562449305
Iteration 6200: Loss = -10763.29035747869
Iteration 6300: Loss = -10763.28918521405
Iteration 6400: Loss = -10763.289057785236
Iteration 6500: Loss = -10763.28884830329
Iteration 6600: Loss = -10763.297200301207
1
Iteration 6700: Loss = -10763.289011036943
2
Iteration 6800: Loss = -10763.288706140911
Iteration 6900: Loss = -10763.28872776781
Iteration 7000: Loss = -10763.289605393655
1
Iteration 7100: Loss = -10763.288656995637
Iteration 7200: Loss = -10763.288718825983
Iteration 7300: Loss = -10763.28862397226
Iteration 7400: Loss = -10763.288629835897
Iteration 7500: Loss = -10763.288704689607
Iteration 7600: Loss = -10763.288607131062
Iteration 7700: Loss = -10763.290787694594
1
Iteration 7800: Loss = -10763.288634174685
Iteration 7900: Loss = -10763.28856086004
Iteration 8000: Loss = -10763.290120104173
1
Iteration 8100: Loss = -10763.288551927793
Iteration 8200: Loss = -10763.289452408446
1
Iteration 8300: Loss = -10763.289237228588
2
Iteration 8400: Loss = -10763.288422223653
Iteration 8500: Loss = -10763.288441054829
Iteration 8600: Loss = -10763.29348815052
1
Iteration 8700: Loss = -10763.302034104274
2
Iteration 8800: Loss = -10763.323325455243
3
Iteration 8900: Loss = -10763.288532950959
Iteration 9000: Loss = -10763.288297699255
Iteration 9100: Loss = -10763.307627666925
1
Iteration 9200: Loss = -10763.29009223477
2
Iteration 9300: Loss = -10763.288009722646
Iteration 9400: Loss = -10763.290285680618
1
Iteration 9500: Loss = -10763.287917985403
Iteration 9600: Loss = -10763.319800449524
1
Iteration 9700: Loss = -10763.287865132648
Iteration 9800: Loss = -10763.30638173804
1
Iteration 9900: Loss = -10763.287849117036
Iteration 10000: Loss = -10763.28832744608
1
Iteration 10100: Loss = -10763.294482262128
2
Iteration 10200: Loss = -10763.288093510668
3
Iteration 10300: Loss = -10763.288444348322
4
Iteration 10400: Loss = -10763.34780220292
5
Iteration 10500: Loss = -10763.320176617151
6
Iteration 10600: Loss = -10763.423038381983
7
Iteration 10700: Loss = -10763.301398565794
8
Iteration 10800: Loss = -10763.296905382704
9
Iteration 10900: Loss = -10763.291145811696
10
Iteration 11000: Loss = -10763.5786137732
11
Iteration 11100: Loss = -10763.287826843061
Iteration 11200: Loss = -10763.291395326605
1
Iteration 11300: Loss = -10763.287863326894
Iteration 11400: Loss = -10763.289538432615
1
Iteration 11500: Loss = -10763.287887420895
Iteration 11600: Loss = -10763.288383410036
1
Iteration 11700: Loss = -10763.338992110685
2
Iteration 11800: Loss = -10763.287843315038
Iteration 11900: Loss = -10763.287959963665
1
Iteration 12000: Loss = -10763.294037709496
2
Iteration 12100: Loss = -10763.290112978184
3
Iteration 12200: Loss = -10763.288912277987
4
Iteration 12300: Loss = -10763.293737274764
5
Iteration 12400: Loss = -10763.294147294593
6
Iteration 12500: Loss = -10763.303446924692
7
Iteration 12600: Loss = -10763.287844420169
Iteration 12700: Loss = -10763.28788387857
Iteration 12800: Loss = -10763.304098507442
1
Iteration 12900: Loss = -10763.287811332797
Iteration 13000: Loss = -10763.321249945795
1
Iteration 13100: Loss = -10763.2877910115
Iteration 13200: Loss = -10763.45264713208
1
Iteration 13300: Loss = -10763.287791446022
Iteration 13400: Loss = -10763.287809133219
Iteration 13500: Loss = -10763.287963380866
1
Iteration 13600: Loss = -10763.288490305617
2
Iteration 13700: Loss = -10763.287820472367
Iteration 13800: Loss = -10763.287826987098
Iteration 13900: Loss = -10763.288484729332
1
Iteration 14000: Loss = -10763.28817125645
2
Iteration 14100: Loss = -10763.294708570895
3
Iteration 14200: Loss = -10763.288136577787
4
Iteration 14300: Loss = -10763.29035924797
5
Iteration 14400: Loss = -10763.291364669334
6
Iteration 14500: Loss = -10763.287966736718
7
Iteration 14600: Loss = -10763.287859674623
Iteration 14700: Loss = -10763.351454759375
1
Iteration 14800: Loss = -10763.304192856425
2
Iteration 14900: Loss = -10763.30406019417
3
Iteration 15000: Loss = -10763.304970258703
4
Iteration 15100: Loss = -10763.28809019071
5
Iteration 15200: Loss = -10763.287880310349
Iteration 15300: Loss = -10763.28882249347
1
Iteration 15400: Loss = -10763.288867991678
2
Iteration 15500: Loss = -10763.291326467948
3
Iteration 15600: Loss = -10763.28786596185
Iteration 15700: Loss = -10763.288536462767
1
Iteration 15800: Loss = -10763.311016491849
2
Iteration 15900: Loss = -10763.287813395616
Iteration 16000: Loss = -10763.288062622156
1
Iteration 16100: Loss = -10763.29695200965
2
Iteration 16200: Loss = -10763.28780298158
Iteration 16300: Loss = -10763.291067540607
1
Iteration 16400: Loss = -10763.287850194498
Iteration 16500: Loss = -10763.289271065718
1
Iteration 16600: Loss = -10763.287794772654
Iteration 16700: Loss = -10763.321436893093
1
Iteration 16800: Loss = -10763.288721631741
2
Iteration 16900: Loss = -10763.378398178203
3
Iteration 17000: Loss = -10763.28782443898
Iteration 17100: Loss = -10763.291883409933
1
Iteration 17200: Loss = -10763.287821917289
Iteration 17300: Loss = -10763.28858111684
1
Iteration 17400: Loss = -10763.287796362962
Iteration 17500: Loss = -10763.287914459126
1
Iteration 17600: Loss = -10763.291809244438
2
Iteration 17700: Loss = -10763.287808460485
Iteration 17800: Loss = -10763.402947796005
1
Iteration 17900: Loss = -10763.28894995186
2
Iteration 18000: Loss = -10763.290763302999
3
Iteration 18100: Loss = -10763.393866850647
4
Iteration 18200: Loss = -10763.28783468409
Iteration 18300: Loss = -10763.287984212624
1
Iteration 18400: Loss = -10763.358765574898
2
Iteration 18500: Loss = -10763.287808364143
Iteration 18600: Loss = -10763.28860336327
1
Iteration 18700: Loss = -10763.28784697869
Iteration 18800: Loss = -10763.288021346572
1
Iteration 18900: Loss = -10763.296600268379
2
Iteration 19000: Loss = -10763.288255416343
3
Iteration 19100: Loss = -10763.287851499701
Iteration 19200: Loss = -10763.401326931616
1
Iteration 19300: Loss = -10763.287937054734
Iteration 19400: Loss = -10763.298252716235
1
Iteration 19500: Loss = -10763.28783783205
Iteration 19600: Loss = -10763.288520487953
1
Iteration 19700: Loss = -10763.357724353114
2
Iteration 19800: Loss = -10763.287820585763
Iteration 19900: Loss = -10763.344801833642
1
pi: tensor([[0.7891, 0.2109],
        [0.2145, 0.7855]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5079, 0.4921], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2428, 0.0951],
         [0.5945, 0.1944]],

        [[0.6991, 0.1032],
         [0.7012, 0.6052]],

        [[0.5364, 0.1032],
         [0.6050, 0.5345]],

        [[0.5211, 0.1086],
         [0.5586, 0.6246]],

        [[0.6396, 0.0967],
         [0.5372, 0.6245]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9137632467975612
Average Adjusted Rand Index: 0.9136096949077992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21045.614791436554
Iteration 100: Loss = -10877.518045632825
Iteration 200: Loss = -10877.050686013625
Iteration 300: Loss = -10876.934859321897
Iteration 400: Loss = -10876.851179231837
Iteration 500: Loss = -10876.756886004252
Iteration 600: Loss = -10876.594453285683
Iteration 700: Loss = -10876.32085288655
Iteration 800: Loss = -10876.142854727044
Iteration 900: Loss = -10876.031737231835
Iteration 1000: Loss = -10875.927807794158
Iteration 1100: Loss = -10875.82279039206
Iteration 1200: Loss = -10875.715016347267
Iteration 1300: Loss = -10875.60382665615
Iteration 1400: Loss = -10875.487747373745
Iteration 1500: Loss = -10875.36838341912
Iteration 1600: Loss = -10875.247071987178
Iteration 1700: Loss = -10875.12528559425
Iteration 1800: Loss = -10875.012288030302
Iteration 1900: Loss = -10874.924197861134
Iteration 2000: Loss = -10874.86798720015
Iteration 2100: Loss = -10874.8335861579
Iteration 2200: Loss = -10874.810101785177
Iteration 2300: Loss = -10874.792075600777
Iteration 2400: Loss = -10874.77609368218
Iteration 2500: Loss = -10874.75746385628
Iteration 2600: Loss = -10874.718837861823
Iteration 2700: Loss = -10874.48648066052
Iteration 2800: Loss = -10874.400387852153
Iteration 2900: Loss = -10874.394551379204
Iteration 3000: Loss = -10874.392399297065
Iteration 3100: Loss = -10874.391572265058
Iteration 3200: Loss = -10874.391235195742
Iteration 3300: Loss = -10874.391046021268
Iteration 3400: Loss = -10874.390982402954
Iteration 3500: Loss = -10874.390934999781
Iteration 3600: Loss = -10874.390889864486
Iteration 3700: Loss = -10874.390888964412
Iteration 3800: Loss = -10874.390878820226
Iteration 3900: Loss = -10874.390823934425
Iteration 4000: Loss = -10874.390822325653
Iteration 4100: Loss = -10874.391320942814
1
Iteration 4200: Loss = -10874.39082467282
Iteration 4300: Loss = -10874.390819054875
Iteration 4400: Loss = -10874.390818687834
Iteration 4500: Loss = -10874.390777017226
Iteration 4600: Loss = -10874.390929877716
1
Iteration 4700: Loss = -10874.390819713504
Iteration 4800: Loss = -10874.395018477824
1
Iteration 4900: Loss = -10874.390782620387
Iteration 5000: Loss = -10874.390791291235
Iteration 5100: Loss = -10874.390783094843
Iteration 5200: Loss = -10874.390780905238
Iteration 5300: Loss = -10874.390883994247
1
Iteration 5400: Loss = -10874.390803840099
Iteration 5500: Loss = -10874.390787338341
Iteration 5600: Loss = -10874.390782986757
Iteration 5700: Loss = -10874.390823666625
Iteration 5800: Loss = -10874.390807478463
Iteration 5900: Loss = -10874.390761513156
Iteration 6000: Loss = -10874.39083420935
Iteration 6100: Loss = -10874.390789999008
Iteration 6200: Loss = -10874.390955163031
1
Iteration 6300: Loss = -10874.390790206797
Iteration 6400: Loss = -10874.393621055866
1
Iteration 6500: Loss = -10874.390789780065
Iteration 6600: Loss = -10874.39078541838
Iteration 6700: Loss = -10874.39084077591
Iteration 6800: Loss = -10874.390837128441
Iteration 6900: Loss = -10874.391071324832
1
Iteration 7000: Loss = -10874.39079491886
Iteration 7100: Loss = -10874.390799461904
Iteration 7200: Loss = -10874.391377550219
1
Iteration 7300: Loss = -10874.390782123679
Iteration 7400: Loss = -10874.394416511004
1
Iteration 7500: Loss = -10874.390822705856
Iteration 7600: Loss = -10874.390918077235
Iteration 7700: Loss = -10874.390823136007
Iteration 7800: Loss = -10874.397449416365
1
Iteration 7900: Loss = -10874.390766724964
Iteration 8000: Loss = -10874.391913912385
1
Iteration 8100: Loss = -10874.390813982434
Iteration 8200: Loss = -10874.39077900096
Iteration 8300: Loss = -10874.390892343854
1
Iteration 8400: Loss = -10874.390797917627
Iteration 8500: Loss = -10874.390802991215
Iteration 8600: Loss = -10874.390798301429
Iteration 8700: Loss = -10874.390782674102
Iteration 8800: Loss = -10874.390780159065
Iteration 8900: Loss = -10874.390914976022
1
Iteration 9000: Loss = -10874.390773958568
Iteration 9100: Loss = -10874.390743040558
Iteration 9200: Loss = -10874.391397406669
1
Iteration 9300: Loss = -10874.390759983366
Iteration 9400: Loss = -10874.390803339738
Iteration 9500: Loss = -10874.394356546178
1
Iteration 9600: Loss = -10874.390740905801
Iteration 9700: Loss = -10874.390783936811
Iteration 9800: Loss = -10874.404459073716
1
Iteration 9900: Loss = -10874.390781349392
Iteration 10000: Loss = -10874.390778729801
Iteration 10100: Loss = -10874.392416093988
1
Iteration 10200: Loss = -10874.390815774805
Iteration 10300: Loss = -10874.390781339962
Iteration 10400: Loss = -10874.41591537436
1
Iteration 10500: Loss = -10874.390777234428
Iteration 10600: Loss = -10874.39079122249
Iteration 10700: Loss = -10874.41962434729
1
Iteration 10800: Loss = -10874.390800635618
Iteration 10900: Loss = -10874.390798664504
Iteration 11000: Loss = -10874.409069631083
1
Iteration 11100: Loss = -10874.390782995546
Iteration 11200: Loss = -10874.390798824183
Iteration 11300: Loss = -10874.393961338897
1
Iteration 11400: Loss = -10874.390800105912
Iteration 11500: Loss = -10874.390761562145
Iteration 11600: Loss = -10874.418763449969
1
Iteration 11700: Loss = -10874.390786265814
Iteration 11800: Loss = -10874.390773796493
Iteration 11900: Loss = -10874.390871242713
Iteration 12000: Loss = -10874.390803298622
Iteration 12100: Loss = -10874.391005210551
1
Iteration 12200: Loss = -10874.391742754651
2
Iteration 12300: Loss = -10874.390837210934
Iteration 12400: Loss = -10874.410660318807
1
Iteration 12500: Loss = -10874.390945186125
2
Iteration 12600: Loss = -10874.391459268847
3
Iteration 12700: Loss = -10874.3912757317
4
Iteration 12800: Loss = -10874.390803062937
Iteration 12900: Loss = -10874.454503541334
1
Iteration 13000: Loss = -10874.401948380564
2
Iteration 13100: Loss = -10874.391277049994
3
Iteration 13200: Loss = -10874.397887025609
4
Iteration 13300: Loss = -10874.39100328389
5
Iteration 13400: Loss = -10874.394377253071
6
Iteration 13500: Loss = -10874.399310974904
7
Iteration 13600: Loss = -10874.390835171316
Iteration 13700: Loss = -10874.391882591213
1
Iteration 13800: Loss = -10874.39087695187
Iteration 13900: Loss = -10874.39143692005
1
Iteration 14000: Loss = -10874.392393486163
2
Iteration 14100: Loss = -10874.41148863212
3
Iteration 14200: Loss = -10874.391332840309
4
Iteration 14300: Loss = -10874.391566880046
5
Iteration 14400: Loss = -10874.390895118613
Iteration 14500: Loss = -10874.396548417513
1
Iteration 14600: Loss = -10874.453136034826
2
Iteration 14700: Loss = -10874.393617756159
3
Iteration 14800: Loss = -10874.39095777258
Iteration 14900: Loss = -10874.442333017067
1
Iteration 15000: Loss = -10874.390811639538
Iteration 15100: Loss = -10874.390899897366
Iteration 15200: Loss = -10874.392669974883
1
Iteration 15300: Loss = -10874.390968537376
Iteration 15400: Loss = -10874.391023948994
Iteration 15500: Loss = -10874.398569872421
1
Iteration 15600: Loss = -10874.390950413921
Iteration 15700: Loss = -10874.392170191688
1
Iteration 15800: Loss = -10874.444770594722
2
Iteration 15900: Loss = -10874.401056421188
3
Iteration 16000: Loss = -10874.417154933031
4
Iteration 16100: Loss = -10874.390883416783
Iteration 16200: Loss = -10874.391975768965
1
Iteration 16300: Loss = -10874.391222124394
2
Iteration 16400: Loss = -10874.3920142678
3
Iteration 16500: Loss = -10874.409173788603
4
Iteration 16600: Loss = -10874.390884352293
Iteration 16700: Loss = -10874.390820367988
Iteration 16800: Loss = -10874.391312629696
1
Iteration 16900: Loss = -10874.392690721637
2
Iteration 17000: Loss = -10874.401666945228
3
Iteration 17100: Loss = -10874.39364884299
4
Iteration 17200: Loss = -10874.391268271283
5
Iteration 17300: Loss = -10874.392502167791
6
Iteration 17400: Loss = -10874.390822779758
Iteration 17500: Loss = -10874.391387236166
1
Iteration 17600: Loss = -10874.395226746508
2
Iteration 17700: Loss = -10874.3909147642
Iteration 17800: Loss = -10874.390923317138
Iteration 17900: Loss = -10874.496853718954
1
Iteration 18000: Loss = -10874.390871794772
Iteration 18100: Loss = -10874.396638261851
1
Iteration 18200: Loss = -10874.390761265538
Iteration 18300: Loss = -10874.397012634498
1
Iteration 18400: Loss = -10874.568632791
2
Iteration 18500: Loss = -10874.390896486138
3
Iteration 18600: Loss = -10874.42835988445
4
Iteration 18700: Loss = -10874.391034426557
5
Iteration 18800: Loss = -10874.399739749282
6
Iteration 18900: Loss = -10874.390802293969
Iteration 19000: Loss = -10874.396187981009
1
Iteration 19100: Loss = -10874.39197057233
2
Iteration 19200: Loss = -10874.396148111202
3
Iteration 19300: Loss = -10874.393567942365
4
Iteration 19400: Loss = -10874.39125939924
5
Iteration 19500: Loss = -10874.394945484617
6
Iteration 19600: Loss = -10874.390853088851
Iteration 19700: Loss = -10874.440064797946
1
Iteration 19800: Loss = -10874.392107732847
2
Iteration 19900: Loss = -10874.392149339876
3
pi: tensor([[0.9556, 0.0444],
        [0.8863, 0.1137]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9573, 0.0427], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1620, 0.1140],
         [0.6971, 0.0897]],

        [[0.5532, 0.1071],
         [0.6924, 0.6518]],

        [[0.5628, 0.1122],
         [0.7269, 0.5857]],

        [[0.5594, 0.2302],
         [0.5554, 0.5077]],

        [[0.6999, 0.1224],
         [0.6508, 0.6709]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -2.964163584753204e-05
Average Adjusted Rand Index: -0.0001522251028708703
10798.484455847329
[0.9137632467975612, -2.964163584753204e-05] [0.9136096949077992, -0.0001522251028708703] [10763.288011583532, 10874.427868323044]
-------------------------------------
This iteration is 22
True Objective function: Loss = -10761.487931096859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21393.623768113514
Iteration 100: Loss = -10852.20762477058
Iteration 200: Loss = -10851.880114077258
Iteration 300: Loss = -10851.648734149478
Iteration 400: Loss = -10850.265588734877
Iteration 500: Loss = -10842.452153229344
Iteration 600: Loss = -10840.337526138253
Iteration 700: Loss = -10839.461989945454
Iteration 800: Loss = -10839.110463840909
Iteration 900: Loss = -10837.48341101323
Iteration 1000: Loss = -10836.374222412807
Iteration 1100: Loss = -10836.068810458975
Iteration 1200: Loss = -10835.991781089877
Iteration 1300: Loss = -10835.97305951956
Iteration 1400: Loss = -10835.968150068999
Iteration 1500: Loss = -10835.96644037788
Iteration 1600: Loss = -10835.96560372224
Iteration 1700: Loss = -10835.965066130293
Iteration 1800: Loss = -10835.964716672845
Iteration 1900: Loss = -10835.964420938642
Iteration 2000: Loss = -10835.964134647409
Iteration 2100: Loss = -10835.964000091539
Iteration 2200: Loss = -10835.963815619623
Iteration 2300: Loss = -10835.963698124873
Iteration 2400: Loss = -10835.96355671114
Iteration 2500: Loss = -10835.96348505702
Iteration 2600: Loss = -10835.96406073763
1
Iteration 2700: Loss = -10835.963351786053
Iteration 2800: Loss = -10835.96324576158
Iteration 2900: Loss = -10835.96537448242
1
Iteration 3000: Loss = -10835.96316108448
Iteration 3100: Loss = -10835.963136639833
Iteration 3200: Loss = -10835.963081307702
Iteration 3300: Loss = -10835.963055932865
Iteration 3400: Loss = -10835.963227175102
1
Iteration 3500: Loss = -10835.962966097131
Iteration 3600: Loss = -10835.96294676883
Iteration 3700: Loss = -10835.962925229389
Iteration 3800: Loss = -10835.962882693895
Iteration 3900: Loss = -10835.966586881786
1
Iteration 4000: Loss = -10835.962887138994
Iteration 4100: Loss = -10835.962847746745
Iteration 4200: Loss = -10835.962862358925
Iteration 4300: Loss = -10835.96278609948
Iteration 4400: Loss = -10835.962741086305
Iteration 4500: Loss = -10835.962717024358
Iteration 4600: Loss = -10835.962700784065
Iteration 4700: Loss = -10835.962701933815
Iteration 4800: Loss = -10835.962753839609
Iteration 4900: Loss = -10835.962719394027
Iteration 5000: Loss = -10835.962910691935
1
Iteration 5100: Loss = -10835.963306935348
2
Iteration 5200: Loss = -10835.962670258708
Iteration 5300: Loss = -10835.962640433518
Iteration 5400: Loss = -10835.962654045843
Iteration 5500: Loss = -10835.962645643634
Iteration 5600: Loss = -10835.962691954792
Iteration 5700: Loss = -10835.962643814026
Iteration 5800: Loss = -10835.965449610352
1
Iteration 5900: Loss = -10835.962614326758
Iteration 6000: Loss = -10835.966774579523
1
Iteration 6100: Loss = -10835.962616867768
Iteration 6200: Loss = -10835.96277684321
1
Iteration 6300: Loss = -10835.962791189359
2
Iteration 6400: Loss = -10835.962625019656
Iteration 6500: Loss = -10835.962608208809
Iteration 6600: Loss = -10835.962746462095
1
Iteration 6700: Loss = -10835.963022587111
2
Iteration 6800: Loss = -10835.969112833882
3
Iteration 6900: Loss = -10835.963047007524
4
Iteration 7000: Loss = -10835.962575910607
Iteration 7100: Loss = -10835.962840627604
1
Iteration 7200: Loss = -10835.962611732097
Iteration 7300: Loss = -10835.964984210881
1
Iteration 7400: Loss = -10835.962579894554
Iteration 7500: Loss = -10835.963335721692
1
Iteration 7600: Loss = -10835.963205855687
2
Iteration 7700: Loss = -10835.96258925129
Iteration 7800: Loss = -10835.96261878
Iteration 7900: Loss = -10835.96283860051
1
Iteration 8000: Loss = -10835.963575160913
2
Iteration 8100: Loss = -10835.968374015727
3
Iteration 8200: Loss = -10835.96278607856
4
Iteration 8300: Loss = -10835.962651208805
Iteration 8400: Loss = -10835.964550251976
1
Iteration 8500: Loss = -10835.962574755262
Iteration 8600: Loss = -10835.962655633108
Iteration 8700: Loss = -10835.962579773399
Iteration 8800: Loss = -10835.963361802511
1
Iteration 8900: Loss = -10835.962598375367
Iteration 9000: Loss = -10835.966999852859
1
Iteration 9100: Loss = -10835.962572652576
Iteration 9200: Loss = -10835.962568931915
Iteration 9300: Loss = -10835.962970787717
1
Iteration 9400: Loss = -10835.962572850573
Iteration 9500: Loss = -10836.004455506754
1
Iteration 9600: Loss = -10835.96258193374
Iteration 9700: Loss = -10835.964323536455
1
Iteration 9800: Loss = -10835.96257759588
Iteration 9900: Loss = -10835.96259892044
Iteration 10000: Loss = -10835.964718406556
1
Iteration 10100: Loss = -10835.96257109811
Iteration 10200: Loss = -10835.962578204393
Iteration 10300: Loss = -10835.96311786552
1
Iteration 10400: Loss = -10835.962585930394
Iteration 10500: Loss = -10835.962591219748
Iteration 10600: Loss = -10835.962670358535
Iteration 10700: Loss = -10835.96258544697
Iteration 10800: Loss = -10835.968578012606
1
Iteration 10900: Loss = -10835.962612043684
Iteration 11000: Loss = -10835.962594742103
Iteration 11100: Loss = -10836.03693599067
1
Iteration 11200: Loss = -10835.96256969975
Iteration 11300: Loss = -10835.962584380972
Iteration 11400: Loss = -10835.962573695208
Iteration 11500: Loss = -10835.967786863728
1
Iteration 11600: Loss = -10836.089534753659
2
Iteration 11700: Loss = -10835.962589863468
Iteration 11800: Loss = -10835.995144084185
1
Iteration 11900: Loss = -10835.962584152288
Iteration 12000: Loss = -10835.96316876383
1
Iteration 12100: Loss = -10835.977689675596
2
Iteration 12200: Loss = -10835.962642960174
Iteration 12300: Loss = -10835.963349815454
1
Iteration 12400: Loss = -10835.962634327434
Iteration 12500: Loss = -10835.962652701182
Iteration 12600: Loss = -10835.962579427955
Iteration 12700: Loss = -10835.965615428267
1
Iteration 12800: Loss = -10835.96258066932
Iteration 12900: Loss = -10835.96514388053
1
Iteration 13000: Loss = -10835.962618206657
Iteration 13100: Loss = -10835.962579060555
Iteration 13200: Loss = -10835.962634824558
Iteration 13300: Loss = -10835.962589785617
Iteration 13400: Loss = -10835.98517139222
1
Iteration 13500: Loss = -10835.962612951562
Iteration 13600: Loss = -10835.962781689392
1
Iteration 13700: Loss = -10836.0340122266
2
Iteration 13800: Loss = -10835.962623826012
Iteration 13900: Loss = -10836.071690614608
1
Iteration 14000: Loss = -10835.962600246336
Iteration 14100: Loss = -10835.962942284339
1
Iteration 14200: Loss = -10835.962640996968
Iteration 14300: Loss = -10835.985581970224
1
Iteration 14400: Loss = -10835.962605211322
Iteration 14500: Loss = -10835.969012118123
1
Iteration 14600: Loss = -10835.962587448801
Iteration 14700: Loss = -10835.965951218157
1
Iteration 14800: Loss = -10835.962611488703
Iteration 14900: Loss = -10835.962641280106
Iteration 15000: Loss = -10835.966475070063
1
Iteration 15100: Loss = -10835.962606485851
Iteration 15200: Loss = -10835.96305283228
1
Iteration 15300: Loss = -10835.96297588404
2
Iteration 15400: Loss = -10835.963912551177
3
Iteration 15500: Loss = -10836.045335459296
4
Iteration 15600: Loss = -10835.965336539062
5
Iteration 15700: Loss = -10835.96262081455
Iteration 15800: Loss = -10835.962795434463
1
Iteration 15900: Loss = -10835.971101694577
2
Iteration 16000: Loss = -10835.967144887167
3
Iteration 16100: Loss = -10835.962575671127
Iteration 16200: Loss = -10836.151119924703
1
Iteration 16300: Loss = -10835.962620240743
Iteration 16400: Loss = -10835.963542728186
1
Iteration 16500: Loss = -10835.962590875937
Iteration 16600: Loss = -10835.977470513208
1
Iteration 16700: Loss = -10835.96367084534
2
Iteration 16800: Loss = -10835.962598358057
Iteration 16900: Loss = -10835.962787420633
1
Iteration 17000: Loss = -10835.977138160695
2
Iteration 17100: Loss = -10835.962583602908
Iteration 17200: Loss = -10835.962755990977
1
Iteration 17300: Loss = -10835.98871366276
2
Iteration 17400: Loss = -10835.96260147552
Iteration 17500: Loss = -10835.96435074107
1
Iteration 17600: Loss = -10835.963684677336
2
Iteration 17700: Loss = -10835.96363183833
3
Iteration 17800: Loss = -10835.97800152092
4
Iteration 17900: Loss = -10835.96257400831
Iteration 18000: Loss = -10835.983916911322
1
Iteration 18100: Loss = -10835.96260388782
Iteration 18200: Loss = -10835.970357692642
1
Iteration 18300: Loss = -10835.962614500037
Iteration 18400: Loss = -10835.963571167535
1
Iteration 18500: Loss = -10835.962589037948
Iteration 18600: Loss = -10835.963955896794
1
Iteration 18700: Loss = -10835.962600353223
Iteration 18800: Loss = -10835.98727163799
1
Iteration 18900: Loss = -10835.963252868974
2
Iteration 19000: Loss = -10835.978760481688
3
Iteration 19100: Loss = -10836.089772165156
4
Iteration 19200: Loss = -10835.96428662209
5
Iteration 19300: Loss = -10835.962686311761
Iteration 19400: Loss = -10835.963160765861
1
Iteration 19500: Loss = -10835.962958866701
2
Iteration 19600: Loss = -10836.039753269402
3
Iteration 19700: Loss = -10836.080781866029
4
Iteration 19800: Loss = -10835.964898009484
5
Iteration 19900: Loss = -10835.962661194237
pi: tensor([[0.5229, 0.4771],
        [0.0436, 0.9564]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0373, 0.9627], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4823, 0.2113],
         [0.5439, 0.1581]],

        [[0.5039, 0.1912],
         [0.6526, 0.5682]],

        [[0.5888, 0.2183],
         [0.7105, 0.5517]],

        [[0.6340, 0.0611],
         [0.5659, 0.5387]],

        [[0.6141, 0.1027],
         [0.5011, 0.6970]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0056450870649491815
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.020872495483242322
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.03604979046254031
Global Adjusted Rand Index: 0.0019213332906033497
Average Adjusted Rand Index: 0.012095138074352639
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22715.99774034865
Iteration 100: Loss = -10853.820108291822
Iteration 200: Loss = -10850.809402792
Iteration 300: Loss = -10849.46684083576
Iteration 400: Loss = -10848.342599594063
Iteration 500: Loss = -10847.017390717421
Iteration 600: Loss = -10845.353086108295
Iteration 700: Loss = -10843.286720037304
Iteration 800: Loss = -10842.106463812403
Iteration 900: Loss = -10841.116299755648
Iteration 1000: Loss = -10840.33087382755
Iteration 1100: Loss = -10840.088046542578
Iteration 1200: Loss = -10839.582724612159
Iteration 1300: Loss = -10838.632782980105
Iteration 1400: Loss = -10838.318950536355
Iteration 1500: Loss = -10838.14200723801
Iteration 1600: Loss = -10838.029466844839
Iteration 1700: Loss = -10837.95102434466
Iteration 1800: Loss = -10837.89452619443
Iteration 1900: Loss = -10837.849885211654
Iteration 2000: Loss = -10837.811169764505
Iteration 2100: Loss = -10837.779646994377
Iteration 2200: Loss = -10837.753289433449
Iteration 2300: Loss = -10837.734761550953
Iteration 2400: Loss = -10837.721327938581
Iteration 2500: Loss = -10837.710114834565
Iteration 2600: Loss = -10837.700493644983
Iteration 2700: Loss = -10837.692223691698
Iteration 2800: Loss = -10837.685061738039
Iteration 2900: Loss = -10837.678680122126
Iteration 3000: Loss = -10837.673039652098
Iteration 3100: Loss = -10837.66796476778
Iteration 3200: Loss = -10837.663424274426
Iteration 3300: Loss = -10837.659358603327
Iteration 3400: Loss = -10837.655671878245
Iteration 3500: Loss = -10837.652257688093
Iteration 3600: Loss = -10837.649120029493
Iteration 3700: Loss = -10837.646141347395
Iteration 3800: Loss = -10837.643447130546
Iteration 3900: Loss = -10837.640893921323
Iteration 4000: Loss = -10837.638504460621
Iteration 4100: Loss = -10837.636303165515
Iteration 4200: Loss = -10837.634183385439
Iteration 4300: Loss = -10837.632609026345
Iteration 4400: Loss = -10837.630371013758
Iteration 4500: Loss = -10837.628621117872
Iteration 4600: Loss = -10837.627045180454
Iteration 4700: Loss = -10837.62543910822
Iteration 4800: Loss = -10837.623964130968
Iteration 4900: Loss = -10837.622590476081
Iteration 5000: Loss = -10837.621302991953
Iteration 5100: Loss = -10837.620092973328
Iteration 5200: Loss = -10837.618929913882
Iteration 5300: Loss = -10837.617815495547
Iteration 5400: Loss = -10837.616768250782
Iteration 5500: Loss = -10837.615970025787
Iteration 5600: Loss = -10837.614868139133
Iteration 5700: Loss = -10837.614002151478
Iteration 5800: Loss = -10837.6131514266
Iteration 5900: Loss = -10837.612369082179
Iteration 6000: Loss = -10837.611592223153
Iteration 6100: Loss = -10837.610909907884
Iteration 6200: Loss = -10837.61035283507
Iteration 6300: Loss = -10837.611158361862
1
Iteration 6400: Loss = -10837.61074259842
2
Iteration 6500: Loss = -10837.6084183215
Iteration 6600: Loss = -10837.607871907247
Iteration 6700: Loss = -10837.607509697622
Iteration 6800: Loss = -10837.606856467644
Iteration 6900: Loss = -10837.606415065136
Iteration 7000: Loss = -10837.606122369085
Iteration 7100: Loss = -10837.605562530862
Iteration 7200: Loss = -10837.605785540552
1
Iteration 7300: Loss = -10837.604810705878
Iteration 7400: Loss = -10837.614264142909
1
Iteration 7500: Loss = -10837.604080678308
Iteration 7600: Loss = -10837.603751728564
Iteration 7700: Loss = -10837.603450055427
Iteration 7800: Loss = -10837.603171607607
Iteration 7900: Loss = -10837.60295732385
Iteration 8000: Loss = -10837.602649340972
Iteration 8100: Loss = -10837.60425005341
1
Iteration 8200: Loss = -10837.602190146688
Iteration 8300: Loss = -10837.606313943423
1
Iteration 8400: Loss = -10837.60685682271
2
Iteration 8500: Loss = -10837.601565726376
Iteration 8600: Loss = -10837.608788516105
1
Iteration 8700: Loss = -10837.60120591901
Iteration 8800: Loss = -10837.601012423365
Iteration 8900: Loss = -10837.603011837491
1
Iteration 9000: Loss = -10837.60066953615
Iteration 9100: Loss = -10837.602555247995
1
Iteration 9200: Loss = -10837.601023132493
2
Iteration 9300: Loss = -10837.601212383037
3
Iteration 9400: Loss = -10837.60013300363
Iteration 9500: Loss = -10837.600395812156
1
Iteration 9600: Loss = -10837.59994371404
Iteration 9700: Loss = -10837.602498852979
1
Iteration 9800: Loss = -10837.599715487888
Iteration 9900: Loss = -10838.207323116532
1
Iteration 10000: Loss = -10837.599529732752
Iteration 10100: Loss = -10837.59943288089
Iteration 10200: Loss = -10837.5993439611
Iteration 10300: Loss = -10837.599260760715
Iteration 10400: Loss = -10837.59918116696
Iteration 10500: Loss = -10837.600663234747
1
Iteration 10600: Loss = -10837.599059050415
Iteration 10700: Loss = -10837.599336630454
1
Iteration 10800: Loss = -10837.598988523821
Iteration 10900: Loss = -10837.5988773051
Iteration 11000: Loss = -10837.611032147852
1
Iteration 11100: Loss = -10837.598784195552
Iteration 11200: Loss = -10837.598728818584
Iteration 11300: Loss = -10837.600388952715
1
Iteration 11400: Loss = -10837.598663738412
Iteration 11500: Loss = -10837.598608572558
Iteration 11600: Loss = -10837.598824822086
1
Iteration 11700: Loss = -10837.598474642444
Iteration 11800: Loss = -10837.598630852577
1
Iteration 11900: Loss = -10837.598440293556
Iteration 12000: Loss = -10837.598401768857
Iteration 12100: Loss = -10837.59837185121
Iteration 12200: Loss = -10837.601965159696
1
Iteration 12300: Loss = -10837.598312000697
Iteration 12400: Loss = -10837.598253744996
Iteration 12500: Loss = -10837.598522559032
1
Iteration 12600: Loss = -10837.598220728863
Iteration 12700: Loss = -10837.632794608819
1
Iteration 12800: Loss = -10837.598196077684
Iteration 12900: Loss = -10837.59814597107
Iteration 13000: Loss = -10837.598483819811
1
Iteration 13100: Loss = -10837.598110678897
Iteration 13200: Loss = -10837.619717046258
1
Iteration 13300: Loss = -10837.598127359715
Iteration 13400: Loss = -10837.598074288962
Iteration 13500: Loss = -10837.621959535098
1
Iteration 13600: Loss = -10837.59805425247
Iteration 13700: Loss = -10837.598024235791
Iteration 13800: Loss = -10837.598147705086
1
Iteration 13900: Loss = -10837.59802011019
Iteration 14000: Loss = -10837.598000695058
Iteration 14100: Loss = -10837.598049010307
Iteration 14200: Loss = -10837.597952450775
Iteration 14300: Loss = -10838.030104227404
1
Iteration 14400: Loss = -10837.597961510712
Iteration 14500: Loss = -10837.597962708618
Iteration 14600: Loss = -10837.606444450726
1
Iteration 14700: Loss = -10837.597939741037
Iteration 14800: Loss = -10837.600213625234
1
Iteration 14900: Loss = -10837.597935900438
Iteration 15000: Loss = -10837.597914645849
Iteration 15100: Loss = -10837.602450404569
1
Iteration 15200: Loss = -10837.597863917876
Iteration 15300: Loss = -10837.597921141472
Iteration 15400: Loss = -10837.597934877993
Iteration 15500: Loss = -10837.597874283349
Iteration 15600: Loss = -10837.731458092643
1
Iteration 15700: Loss = -10837.597848901065
Iteration 15800: Loss = -10837.59786631567
Iteration 15900: Loss = -10837.59950027118
1
Iteration 16000: Loss = -10837.597831033301
Iteration 16100: Loss = -10837.67077862603
1
Iteration 16200: Loss = -10837.59782776584
Iteration 16300: Loss = -10837.597853245075
Iteration 16400: Loss = -10837.59873718652
1
Iteration 16500: Loss = -10837.597832654492
Iteration 16600: Loss = -10837.597948721577
1
Iteration 16700: Loss = -10837.597851054597
Iteration 16800: Loss = -10837.597766700288
Iteration 16900: Loss = -10837.615136401037
1
Iteration 17000: Loss = -10837.597827972304
Iteration 17100: Loss = -10837.597796702958
Iteration 17200: Loss = -10837.59795568621
1
Iteration 17300: Loss = -10837.597792706127
Iteration 17400: Loss = -10837.752975270005
1
Iteration 17500: Loss = -10837.597807301823
Iteration 17600: Loss = -10837.59776748944
Iteration 17700: Loss = -10837.598946287619
1
Iteration 17800: Loss = -10837.597807449505
Iteration 17900: Loss = -10837.987747907013
1
Iteration 18000: Loss = -10837.597790322547
Iteration 18100: Loss = -10837.597793567467
Iteration 18200: Loss = -10837.62592085759
1
Iteration 18300: Loss = -10837.597767491827
Iteration 18400: Loss = -10837.597749438337
Iteration 18500: Loss = -10837.59859108388
1
Iteration 18600: Loss = -10837.59780444313
Iteration 18700: Loss = -10837.621433625982
1
Iteration 18800: Loss = -10837.597785643253
Iteration 18900: Loss = -10837.597795220432
Iteration 19000: Loss = -10837.598709525359
1
Iteration 19100: Loss = -10837.597774945469
Iteration 19200: Loss = -10837.62666666277
1
Iteration 19300: Loss = -10837.597749980348
Iteration 19400: Loss = -10837.59779120827
Iteration 19500: Loss = -10837.67794606051
1
Iteration 19600: Loss = -10837.597754942095
Iteration 19700: Loss = -10837.59775420949
Iteration 19800: Loss = -10837.604928196437
1
Iteration 19900: Loss = -10837.597798857661
pi: tensor([[7.7208e-01, 2.2792e-01],
        [2.6412e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2076, 0.7924], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2725, 0.1802],
         [0.6268, 0.1483]],

        [[0.6979, 0.1761],
         [0.6153, 0.6852]],

        [[0.5422, 0.2143],
         [0.6892, 0.7149]],

        [[0.5957, 0.1753],
         [0.7247, 0.5551]],

        [[0.5206, 0.1841],
         [0.5746, 0.6915]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.004955061857865551
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.009264024704065878
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.027613264228709862
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.003702958125914224
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: 0.0074898932526849225
Average Adjusted Rand Index: 0.007364465032805372
10761.487931096859
[0.0019213332906033497, 0.0074898932526849225] [0.012095138074352639, 0.007364465032805372] [10835.963384074526, 10837.597771025694]
-------------------------------------
This iteration is 23
True Objective function: Loss = -10752.638485912514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25109.63525979869
Iteration 100: Loss = -10833.843741246272
Iteration 200: Loss = -10832.497362345823
Iteration 300: Loss = -10832.146638459119
Iteration 400: Loss = -10832.003483740753
Iteration 500: Loss = -10831.92170822695
Iteration 600: Loss = -10831.857282089513
Iteration 700: Loss = -10831.791187108816
Iteration 800: Loss = -10831.70909837021
Iteration 900: Loss = -10831.59186705316
Iteration 1000: Loss = -10831.417914268772
Iteration 1100: Loss = -10831.2188339645
Iteration 1200: Loss = -10831.082166412567
Iteration 1300: Loss = -10830.984708936385
Iteration 1400: Loss = -10830.903915068358
Iteration 1500: Loss = -10830.834095733217
Iteration 1600: Loss = -10830.769549943225
Iteration 1700: Loss = -10830.708517576875
Iteration 1800: Loss = -10830.651148757495
Iteration 1900: Loss = -10830.599366858196
Iteration 2000: Loss = -10830.55557802621
Iteration 2100: Loss = -10830.518496138007
Iteration 2200: Loss = -10830.486232226425
Iteration 2300: Loss = -10830.459027818386
Iteration 2400: Loss = -10830.437797555009
Iteration 2500: Loss = -10830.419023883622
Iteration 2600: Loss = -10830.404291141502
Iteration 2700: Loss = -10830.393582500456
Iteration 2800: Loss = -10830.386205730383
Iteration 2900: Loss = -10830.380879618935
Iteration 3000: Loss = -10830.37644663641
Iteration 3100: Loss = -10830.372332949712
Iteration 3200: Loss = -10830.36849908038
Iteration 3300: Loss = -10830.36514687316
Iteration 3400: Loss = -10830.362065987762
Iteration 3500: Loss = -10830.359187468532
Iteration 3600: Loss = -10830.356446645195
Iteration 3700: Loss = -10830.353984889278
Iteration 3800: Loss = -10830.351724985361
Iteration 3900: Loss = -10830.34971161437
Iteration 4000: Loss = -10830.347783800971
Iteration 4100: Loss = -10830.345975748125
Iteration 4200: Loss = -10830.344334100795
Iteration 4300: Loss = -10830.342793209382
Iteration 4400: Loss = -10830.341358950613
Iteration 4500: Loss = -10830.340113894443
Iteration 4600: Loss = -10830.33888264719
Iteration 4700: Loss = -10830.337800637057
Iteration 4800: Loss = -10830.336759097736
Iteration 4900: Loss = -10830.335823232117
Iteration 5000: Loss = -10830.334951496412
Iteration 5100: Loss = -10830.33413569792
Iteration 5200: Loss = -10830.333403584234
Iteration 5300: Loss = -10830.332708033333
Iteration 5400: Loss = -10830.332065399456
Iteration 5500: Loss = -10830.331466899099
Iteration 5600: Loss = -10830.33090700091
Iteration 5700: Loss = -10830.330407214336
Iteration 5800: Loss = -10830.329900483846
Iteration 5900: Loss = -10830.329443871226
Iteration 6000: Loss = -10830.329129825339
Iteration 6100: Loss = -10830.328660113219
Iteration 6200: Loss = -10830.331410464076
1
Iteration 6300: Loss = -10830.327952216676
Iteration 6400: Loss = -10830.327621192
Iteration 6500: Loss = -10830.327304924565
Iteration 6600: Loss = -10830.327065099656
Iteration 6700: Loss = -10830.326807629575
Iteration 6800: Loss = -10830.326554502637
Iteration 6900: Loss = -10830.326295451338
Iteration 7000: Loss = -10830.326138516144
Iteration 7100: Loss = -10830.325919700577
Iteration 7200: Loss = -10830.325745597083
Iteration 7300: Loss = -10830.325515505425
Iteration 7400: Loss = -10830.3273550911
1
Iteration 7500: Loss = -10830.325197641469
Iteration 7600: Loss = -10830.32615558477
1
Iteration 7700: Loss = -10830.324928432896
Iteration 7800: Loss = -10830.326027038602
1
Iteration 7900: Loss = -10830.324663406082
Iteration 8000: Loss = -10830.324595220061
Iteration 8100: Loss = -10830.324456479459
Iteration 8200: Loss = -10830.324358943024
Iteration 8300: Loss = -10830.324250730157
Iteration 8400: Loss = -10830.327550699412
1
Iteration 8500: Loss = -10830.35787061416
2
Iteration 8600: Loss = -10830.324001541894
Iteration 8700: Loss = -10830.347675778023
1
Iteration 8800: Loss = -10830.32386833215
Iteration 8900: Loss = -10830.323796034636
Iteration 9000: Loss = -10830.324150772352
1
Iteration 9100: Loss = -10830.323610553254
Iteration 9200: Loss = -10830.33299499021
1
Iteration 9300: Loss = -10830.323519204074
Iteration 9400: Loss = -10830.32347484696
Iteration 9500: Loss = -10830.32340703305
Iteration 9600: Loss = -10830.324691481685
1
Iteration 9700: Loss = -10830.323353068805
Iteration 9800: Loss = -10830.323266048696
Iteration 9900: Loss = -10830.32802211198
1
Iteration 10000: Loss = -10830.32320705764
Iteration 10100: Loss = -10830.323172125252
Iteration 10200: Loss = -10830.32479941343
1
Iteration 10300: Loss = -10830.323150478534
Iteration 10400: Loss = -10830.323094834946
Iteration 10500: Loss = -10830.325436839737
1
Iteration 10600: Loss = -10830.323015843935
Iteration 10700: Loss = -10830.323025195665
Iteration 10800: Loss = -10830.32316470678
1
Iteration 10900: Loss = -10830.322962232021
Iteration 11000: Loss = -10830.322920138631
Iteration 11100: Loss = -10830.322931076727
Iteration 11200: Loss = -10830.322919455088
Iteration 11300: Loss = -10830.32293396701
Iteration 11400: Loss = -10830.324523646519
1
Iteration 11500: Loss = -10830.32289208012
Iteration 11600: Loss = -10830.367805558046
1
Iteration 11700: Loss = -10830.322825345303
Iteration 11800: Loss = -10830.323314681991
1
Iteration 11900: Loss = -10830.322770197577
Iteration 12000: Loss = -10830.324727611835
1
Iteration 12100: Loss = -10830.322746576772
Iteration 12200: Loss = -10830.323039594696
1
Iteration 12300: Loss = -10830.322737658842
Iteration 12400: Loss = -10830.322846770587
1
Iteration 12500: Loss = -10830.322728687217
Iteration 12600: Loss = -10830.322893546912
1
Iteration 12700: Loss = -10830.322687088837
Iteration 12800: Loss = -10830.323134864831
1
Iteration 12900: Loss = -10830.326612646053
2
Iteration 13000: Loss = -10830.323238876204
3
Iteration 13100: Loss = -10830.322755855099
Iteration 13200: Loss = -10830.323137076928
1
Iteration 13300: Loss = -10830.324620913643
2
Iteration 13400: Loss = -10830.323047715307
3
Iteration 13500: Loss = -10830.323556039633
4
Iteration 13600: Loss = -10830.35872174222
5
Iteration 13700: Loss = -10830.327701970748
6
Iteration 13800: Loss = -10830.322634342536
Iteration 13900: Loss = -10830.324384154188
1
Iteration 14000: Loss = -10830.323715385208
2
Iteration 14100: Loss = -10830.325228309342
3
Iteration 14200: Loss = -10830.322648278905
Iteration 14300: Loss = -10830.323505540135
1
Iteration 14400: Loss = -10830.322655228467
Iteration 14500: Loss = -10830.385694795295
1
Iteration 14600: Loss = -10830.323193027754
2
Iteration 14700: Loss = -10830.324932908397
3
Iteration 14800: Loss = -10830.322813672374
4
Iteration 14900: Loss = -10830.32259923967
Iteration 15000: Loss = -10830.339097557642
1
Iteration 15100: Loss = -10830.322587133565
Iteration 15200: Loss = -10830.322581777475
Iteration 15300: Loss = -10830.351135811807
1
Iteration 15400: Loss = -10830.322626961382
Iteration 15500: Loss = -10830.328943740322
1
Iteration 15600: Loss = -10830.32274957943
2
Iteration 15700: Loss = -10830.323685554935
3
Iteration 15800: Loss = -10830.42451200034
4
Iteration 15900: Loss = -10830.377695818994
5
Iteration 16000: Loss = -10830.322563262262
Iteration 16100: Loss = -10830.322638988078
Iteration 16200: Loss = -10830.53156270956
1
Iteration 16300: Loss = -10830.322564048764
Iteration 16400: Loss = -10830.326961853767
1
Iteration 16500: Loss = -10830.329018347698
2
Iteration 16600: Loss = -10830.411427379471
3
Iteration 16700: Loss = -10830.32258464187
Iteration 16800: Loss = -10830.394121373138
1
Iteration 16900: Loss = -10830.323172192768
2
Iteration 17000: Loss = -10830.32312149057
3
Iteration 17100: Loss = -10830.324389221512
4
Iteration 17200: Loss = -10830.544773125575
5
Iteration 17300: Loss = -10830.328028351541
6
Iteration 17400: Loss = -10830.32716717424
7
Iteration 17500: Loss = -10830.325551633528
8
Iteration 17600: Loss = -10830.32258531529
Iteration 17700: Loss = -10830.324001554543
1
Iteration 17800: Loss = -10830.336033797512
2
Iteration 17900: Loss = -10830.322537037424
Iteration 18000: Loss = -10830.324550096346
1
Iteration 18100: Loss = -10830.322536356876
Iteration 18200: Loss = -10830.32322546588
1
Iteration 18300: Loss = -10830.322500665647
Iteration 18400: Loss = -10830.323868477866
1
Iteration 18500: Loss = -10830.322521885244
Iteration 18600: Loss = -10830.322968449002
1
Iteration 18700: Loss = -10830.32253676487
Iteration 18800: Loss = -10830.322658265923
1
Iteration 18900: Loss = -10830.324339835934
2
Iteration 19000: Loss = -10830.328117622108
3
Iteration 19100: Loss = -10830.341984044204
4
Iteration 19200: Loss = -10830.32339343509
5
Iteration 19300: Loss = -10830.322706183775
6
Iteration 19400: Loss = -10830.322776937011
7
Iteration 19500: Loss = -10830.324304762897
8
Iteration 19600: Loss = -10830.457717769432
9
Iteration 19700: Loss = -10830.325285372299
10
Iteration 19800: Loss = -10830.332470254983
11
Iteration 19900: Loss = -10830.323825615153
12
pi: tensor([[1.0000e+00, 5.0129e-07],
        [4.0431e-01, 5.9569e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8800, 0.1200], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1555, 0.1818],
         [0.6291, 0.2308]],

        [[0.5018, 0.1979],
         [0.6847, 0.5283]],

        [[0.5247, 0.1902],
         [0.5726, 0.6685]],

        [[0.6751, 0.1978],
         [0.6711, 0.7258]],

        [[0.5313, 0.1236],
         [0.5029, 0.7039]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.004212316740111065
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00018273134585092686
Average Adjusted Rand Index: -0.0011188873870328106
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22990.358321904223
Iteration 100: Loss = -10833.52584157397
Iteration 200: Loss = -10832.524833699
Iteration 300: Loss = -10832.228256649341
Iteration 400: Loss = -10832.085911086182
Iteration 500: Loss = -10831.995469783145
Iteration 600: Loss = -10831.927537313593
Iteration 700: Loss = -10831.863290002882
Iteration 800: Loss = -10831.7810878693
Iteration 900: Loss = -10831.652345915065
Iteration 1000: Loss = -10831.50216818111
Iteration 1100: Loss = -10831.402533501681
Iteration 1200: Loss = -10831.318101675677
Iteration 1300: Loss = -10831.233619853889
Iteration 1400: Loss = -10831.123369418405
Iteration 1500: Loss = -10830.879078225946
Iteration 1600: Loss = -10830.37102977953
Iteration 1700: Loss = -10829.961124341746
Iteration 1800: Loss = -10829.671436452087
Iteration 1900: Loss = -10829.462694796825
Iteration 2000: Loss = -10829.347114536547
Iteration 2100: Loss = -10829.295729006039
Iteration 2200: Loss = -10829.266332372701
Iteration 2300: Loss = -10829.246525597231
Iteration 2400: Loss = -10829.231940206773
Iteration 2500: Loss = -10829.22035205273
Iteration 2600: Loss = -10829.210934160734
Iteration 2700: Loss = -10829.203154907378
Iteration 2800: Loss = -10829.196299012558
Iteration 2900: Loss = -10829.190131392035
Iteration 3000: Loss = -10829.184268754334
Iteration 3100: Loss = -10829.178399929318
Iteration 3200: Loss = -10829.17241244407
Iteration 3300: Loss = -10829.165778083026
Iteration 3400: Loss = -10829.158151366106
Iteration 3500: Loss = -10829.14888183266
Iteration 3600: Loss = -10829.137525040744
Iteration 3700: Loss = -10829.124054978272
Iteration 3800: Loss = -10829.109512810497
Iteration 3900: Loss = -10829.095276365368
Iteration 4000: Loss = -10829.08250731602
Iteration 4100: Loss = -10829.071536471833
Iteration 4200: Loss = -10829.062498122628
Iteration 4300: Loss = -10829.055143590727
Iteration 4400: Loss = -10829.049186508619
Iteration 4500: Loss = -10829.044273501055
Iteration 4600: Loss = -10829.04023882592
Iteration 4700: Loss = -10829.03680119458
Iteration 4800: Loss = -10829.033892896798
Iteration 4900: Loss = -10829.031397087314
Iteration 5000: Loss = -10829.029239166248
Iteration 5100: Loss = -10829.027338506356
Iteration 5200: Loss = -10829.025634975538
Iteration 5300: Loss = -10829.02413949202
Iteration 5400: Loss = -10829.022769723264
Iteration 5500: Loss = -10829.021560087243
Iteration 5600: Loss = -10829.020487295706
Iteration 5700: Loss = -10829.019441487158
Iteration 5800: Loss = -10829.01854609095
Iteration 5900: Loss = -10829.017716375845
Iteration 6000: Loss = -10829.016982038343
Iteration 6100: Loss = -10829.016316316287
Iteration 6200: Loss = -10829.015704251005
Iteration 6300: Loss = -10829.015167077652
Iteration 6400: Loss = -10829.014637214557
Iteration 6500: Loss = -10829.014170475433
Iteration 6600: Loss = -10829.013723249
Iteration 6700: Loss = -10829.01332740881
Iteration 6800: Loss = -10829.012898299388
Iteration 6900: Loss = -10829.012466693306
Iteration 7000: Loss = -10829.012076612406
Iteration 7100: Loss = -10829.011758255743
Iteration 7200: Loss = -10829.011429542705
Iteration 7300: Loss = -10829.011553174125
1
Iteration 7400: Loss = -10829.010886308659
Iteration 7500: Loss = -10829.010695107478
Iteration 7600: Loss = -10829.010352172003
Iteration 7700: Loss = -10829.010318624487
Iteration 7800: Loss = -10829.00995334596
Iteration 7900: Loss = -10829.010161912634
1
Iteration 8000: Loss = -10829.009649422589
Iteration 8100: Loss = -10829.017962679885
1
Iteration 8200: Loss = -10829.00934541889
Iteration 8300: Loss = -10829.00966960791
1
Iteration 8400: Loss = -10829.008770535629
Iteration 8500: Loss = -10829.00887260512
1
Iteration 8600: Loss = -10829.008495721306
Iteration 8700: Loss = -10829.008328486618
Iteration 8800: Loss = -10829.008623414536
1
Iteration 8900: Loss = -10829.008129842043
Iteration 9000: Loss = -10829.008018216724
Iteration 9100: Loss = -10829.012741535425
1
Iteration 9200: Loss = -10829.007869294155
Iteration 9300: Loss = -10829.00774665318
Iteration 9400: Loss = -10829.01997563287
1
Iteration 9500: Loss = -10829.007642047674
Iteration 9600: Loss = -10829.007541101684
Iteration 9700: Loss = -10829.00771700813
1
Iteration 9800: Loss = -10829.00727111952
Iteration 9900: Loss = -10829.00712473441
Iteration 10000: Loss = -10829.007054022519
Iteration 10100: Loss = -10829.00695450692
Iteration 10200: Loss = -10829.007707229828
1
Iteration 10300: Loss = -10829.006480125627
Iteration 10400: Loss = -10829.006381682459
Iteration 10500: Loss = -10829.006344824924
Iteration 10600: Loss = -10829.006368885974
Iteration 10700: Loss = -10829.006252647252
Iteration 10800: Loss = -10829.00623232893
Iteration 10900: Loss = -10829.010323595305
1
Iteration 11000: Loss = -10829.006159951276
Iteration 11100: Loss = -10829.006107258316
Iteration 11200: Loss = -10829.025135465688
1
Iteration 11300: Loss = -10829.00608490568
Iteration 11400: Loss = -10829.01030485203
1
Iteration 11500: Loss = -10829.006024863367
Iteration 11600: Loss = -10829.006000678473
Iteration 11700: Loss = -10829.005968660436
Iteration 11800: Loss = -10829.00590975133
Iteration 11900: Loss = -10829.007192938476
1
Iteration 12000: Loss = -10829.005922230876
Iteration 12100: Loss = -10829.005829154938
Iteration 12200: Loss = -10829.008330506205
1
Iteration 12300: Loss = -10829.005781878297
Iteration 12400: Loss = -10829.005802823092
Iteration 12500: Loss = -10829.00580553336
Iteration 12600: Loss = -10829.006361822407
1
Iteration 12700: Loss = -10829.005764747722
Iteration 12800: Loss = -10829.005742872541
Iteration 12900: Loss = -10829.006140693651
1
Iteration 13000: Loss = -10829.00571927665
Iteration 13100: Loss = -10829.005718537077
Iteration 13200: Loss = -10829.005724996596
Iteration 13300: Loss = -10829.00715412962
1
Iteration 13400: Loss = -10829.005670470297
Iteration 13500: Loss = -10829.005660803481
Iteration 13600: Loss = -10829.444848754065
1
Iteration 13700: Loss = -10829.00565310955
Iteration 13800: Loss = -10829.005617765499
Iteration 13900: Loss = -10829.045388785573
1
Iteration 14000: Loss = -10829.005747560674
2
Iteration 14100: Loss = -10829.005684326588
Iteration 14200: Loss = -10829.037142323137
1
Iteration 14300: Loss = -10829.005643525114
Iteration 14400: Loss = -10829.007439481757
1
Iteration 14500: Loss = -10829.005595534698
Iteration 14600: Loss = -10829.010971668129
1
Iteration 14700: Loss = -10829.005816296532
2
Iteration 14800: Loss = -10829.005555503349
Iteration 14900: Loss = -10829.081806669728
1
Iteration 15000: Loss = -10829.005514369735
Iteration 15100: Loss = -10829.247061035094
1
Iteration 15200: Loss = -10829.005557272716
Iteration 15300: Loss = -10829.005500227644
Iteration 15400: Loss = -10829.005953210895
1
Iteration 15500: Loss = -10829.005505041358
Iteration 15600: Loss = -10829.025997669261
1
Iteration 15700: Loss = -10829.005499771802
Iteration 15800: Loss = -10829.005673005968
1
Iteration 15900: Loss = -10829.005499324681
Iteration 16000: Loss = -10829.022004995886
1
Iteration 16100: Loss = -10829.005492610813
Iteration 16200: Loss = -10829.006998141722
1
Iteration 16300: Loss = -10829.005511718726
Iteration 16400: Loss = -10829.005502536736
Iteration 16500: Loss = -10829.006631357004
1
Iteration 16600: Loss = -10829.005465364633
Iteration 16700: Loss = -10829.101191194597
1
Iteration 16800: Loss = -10829.005528059475
Iteration 16900: Loss = -10829.0055059714
Iteration 17000: Loss = -10829.006744887614
1
Iteration 17100: Loss = -10829.005504574028
Iteration 17200: Loss = -10829.006922451548
1
Iteration 17300: Loss = -10829.005541798677
Iteration 17400: Loss = -10829.005787948101
1
Iteration 17500: Loss = -10829.005577677699
Iteration 17600: Loss = -10829.005474877808
Iteration 17700: Loss = -10829.005968844085
1
Iteration 17800: Loss = -10829.00549620282
Iteration 17900: Loss = -10829.005566255431
Iteration 18000: Loss = -10829.005479809945
Iteration 18100: Loss = -10829.005883348282
1
Iteration 18200: Loss = -10829.0101063599
2
Iteration 18300: Loss = -10829.005826262499
3
Iteration 18400: Loss = -10829.006797028229
4
Iteration 18500: Loss = -10829.005552675184
Iteration 18600: Loss = -10829.010626995636
1
Iteration 18700: Loss = -10829.007059550493
2
Iteration 18800: Loss = -10829.005458687338
Iteration 18900: Loss = -10829.057239393984
1
Iteration 19000: Loss = -10829.007140527183
2
Iteration 19100: Loss = -10829.006225877849
3
Iteration 19200: Loss = -10829.0140137182
4
Iteration 19300: Loss = -10829.005503515567
Iteration 19400: Loss = -10829.01363511524
1
Iteration 19500: Loss = -10829.006259184896
2
Iteration 19600: Loss = -10829.005498615592
Iteration 19700: Loss = -10829.006209844809
1
Iteration 19800: Loss = -10829.005516201889
Iteration 19900: Loss = -10829.005461325783
pi: tensor([[9.9073e-01, 9.2718e-03],
        [2.0752e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9986, 0.0014], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1597, 0.1497],
         [0.5866, 0.1234]],

        [[0.7148, 0.2562],
         [0.5146, 0.6693]],

        [[0.6997, 0.2275],
         [0.5791, 0.7106]],

        [[0.7053, 0.0999],
         [0.6854, 0.6924]],

        [[0.5355, 0.1070],
         [0.5448, 0.5942]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
Global Adjusted Rand Index: -0.00018273134585092686
Average Adjusted Rand Index: -0.001017273417117706
10752.638485912514
[-0.00018273134585092686, -0.00018273134585092686] [-0.0011188873870328106, -0.001017273417117706] [10830.323132803594, 10829.016704247384]
-------------------------------------
This iteration is 24
True Objective function: Loss = -10749.023436252
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20609.19383770799
Iteration 100: Loss = -10821.892773219406
Iteration 200: Loss = -10820.932081244126
Iteration 300: Loss = -10820.59550389872
Iteration 400: Loss = -10820.39322830539
Iteration 500: Loss = -10820.257787452674
Iteration 600: Loss = -10820.096139620417
Iteration 700: Loss = -10819.799817280578
Iteration 800: Loss = -10819.559961843284
Iteration 900: Loss = -10819.4079219247
Iteration 1000: Loss = -10819.098430817516
Iteration 1100: Loss = -10818.487449308235
Iteration 1200: Loss = -10818.059766877039
Iteration 1300: Loss = -10817.786719184458
Iteration 1400: Loss = -10817.582649373895
Iteration 1500: Loss = -10817.362641036041
Iteration 1600: Loss = -10817.060045293763
Iteration 1700: Loss = -10816.685997226694
Iteration 1800: Loss = -10816.448267394166
Iteration 1900: Loss = -10816.328271544895
Iteration 2000: Loss = -10816.250506658309
Iteration 2100: Loss = -10816.197180108176
Iteration 2200: Loss = -10816.161105079374
Iteration 2300: Loss = -10816.132677312515
Iteration 2400: Loss = -10816.111500959463
Iteration 2500: Loss = -10816.091709287006
Iteration 2600: Loss = -10816.073140789042
Iteration 2700: Loss = -10816.058654682425
Iteration 2800: Loss = -10816.048362312655
Iteration 2900: Loss = -10816.03939148824
Iteration 3000: Loss = -10816.031173495865
Iteration 3100: Loss = -10816.024645270356
Iteration 3200: Loss = -10816.019566958197
Iteration 3300: Loss = -10816.015193516821
Iteration 3400: Loss = -10816.011403829176
Iteration 3500: Loss = -10816.008036232828
Iteration 3600: Loss = -10816.005013143646
Iteration 3700: Loss = -10816.002345816587
Iteration 3800: Loss = -10815.999912409832
Iteration 3900: Loss = -10815.997623383982
Iteration 4000: Loss = -10815.995588024585
Iteration 4100: Loss = -10815.993733011555
Iteration 4200: Loss = -10815.991980563613
Iteration 4300: Loss = -10815.990333154747
Iteration 4400: Loss = -10815.988729880706
Iteration 4500: Loss = -10815.987141656398
Iteration 4600: Loss = -10815.985649342185
Iteration 4700: Loss = -10815.984413462327
Iteration 4800: Loss = -10815.983385472458
Iteration 4900: Loss = -10815.982432683993
Iteration 5000: Loss = -10815.98155071171
Iteration 5100: Loss = -10815.980738705404
Iteration 5200: Loss = -10815.97996091828
Iteration 5300: Loss = -10815.979224440684
Iteration 5400: Loss = -10815.978518565782
Iteration 5500: Loss = -10815.977939993407
Iteration 5600: Loss = -10815.977254245383
Iteration 5700: Loss = -10815.978740186973
1
Iteration 5800: Loss = -10815.97603086292
Iteration 5900: Loss = -10815.975443346182
Iteration 6000: Loss = -10815.97479660145
Iteration 6100: Loss = -10815.974056249504
Iteration 6200: Loss = -10815.973430976597
Iteration 6300: Loss = -10815.972794542982
Iteration 6400: Loss = -10815.972179983957
Iteration 6500: Loss = -10815.971631765144
Iteration 6600: Loss = -10815.971105729459
Iteration 6700: Loss = -10815.970589572107
Iteration 6800: Loss = -10815.970121432665
Iteration 6900: Loss = -10815.969538812464
Iteration 7000: Loss = -10815.96902041419
Iteration 7100: Loss = -10815.968687303934
Iteration 7200: Loss = -10815.968326588327
Iteration 7300: Loss = -10815.968293819775
Iteration 7400: Loss = -10815.967888876105
Iteration 7500: Loss = -10815.970140651843
1
Iteration 7600: Loss = -10815.967516796352
Iteration 7700: Loss = -10815.967997864384
1
Iteration 7800: Loss = -10815.967240296926
Iteration 7900: Loss = -10815.967433374457
1
Iteration 8000: Loss = -10815.966957847082
Iteration 8100: Loss = -10815.968623937284
1
Iteration 8200: Loss = -10815.966725094033
Iteration 8300: Loss = -10816.00037167697
1
Iteration 8400: Loss = -10815.966149080881
Iteration 8500: Loss = -10816.194508242057
1
Iteration 8600: Loss = -10815.9654014322
Iteration 8700: Loss = -10815.965223941601
Iteration 8800: Loss = -10815.965355807433
1
Iteration 8900: Loss = -10815.965073013722
Iteration 9000: Loss = -10815.965049102899
Iteration 9100: Loss = -10815.965119963714
Iteration 9200: Loss = -10815.964857795529
Iteration 9300: Loss = -10815.964822289501
Iteration 9400: Loss = -10815.964847339088
Iteration 9500: Loss = -10815.964722304141
Iteration 9600: Loss = -10815.996500195883
1
Iteration 9700: Loss = -10815.964586782906
Iteration 9800: Loss = -10815.964562712883
Iteration 9900: Loss = -10816.00018984436
1
Iteration 10000: Loss = -10815.964462446052
Iteration 10100: Loss = -10815.964437466611
Iteration 10200: Loss = -10815.964911103028
1
Iteration 10300: Loss = -10815.964326226942
Iteration 10400: Loss = -10815.968333115059
1
Iteration 10500: Loss = -10815.964966995181
2
Iteration 10600: Loss = -10815.96425378244
Iteration 10700: Loss = -10815.967181315853
1
Iteration 10800: Loss = -10815.964123247903
Iteration 10900: Loss = -10815.964095012248
Iteration 11000: Loss = -10815.964957924865
1
Iteration 11100: Loss = -10815.964039227889
Iteration 11200: Loss = -10815.9640410169
Iteration 11300: Loss = -10815.964200358356
1
Iteration 11400: Loss = -10815.963980360784
Iteration 11500: Loss = -10815.96405148368
Iteration 11600: Loss = -10815.964001700842
Iteration 11700: Loss = -10815.999349401789
1
Iteration 11800: Loss = -10815.963917784857
Iteration 11900: Loss = -10815.99351807948
1
Iteration 12000: Loss = -10815.963903670108
Iteration 12100: Loss = -10816.379481383472
1
Iteration 12200: Loss = -10815.963835850149
Iteration 12300: Loss = -10815.96389826569
Iteration 12400: Loss = -10815.963930293638
Iteration 12500: Loss = -10815.964050250943
1
Iteration 12600: Loss = -10815.96713600917
2
Iteration 12700: Loss = -10815.963667407272
Iteration 12800: Loss = -10815.969996801838
1
Iteration 12900: Loss = -10815.963690073417
Iteration 13000: Loss = -10816.01129092318
1
Iteration 13100: Loss = -10815.963697816249
Iteration 13200: Loss = -10815.96366232938
Iteration 13300: Loss = -10815.963738704444
Iteration 13400: Loss = -10815.963636608738
Iteration 13500: Loss = -10815.99557341596
1
Iteration 13600: Loss = -10815.963626856992
Iteration 13700: Loss = -10815.963586588572
Iteration 13800: Loss = -10815.96383388919
1
Iteration 13900: Loss = -10815.963612078718
Iteration 14000: Loss = -10815.980234283255
1
Iteration 14100: Loss = -10815.963607292837
Iteration 14200: Loss = -10815.964955745118
1
Iteration 14300: Loss = -10815.963579204514
Iteration 14400: Loss = -10815.965350662153
1
Iteration 14500: Loss = -10815.963563901418
Iteration 14600: Loss = -10815.994274214563
1
Iteration 14700: Loss = -10815.963570556338
Iteration 14800: Loss = -10815.981678456998
1
Iteration 14900: Loss = -10815.963613975706
Iteration 15000: Loss = -10815.963761248753
1
Iteration 15100: Loss = -10815.96365320934
Iteration 15200: Loss = -10815.971137246239
1
Iteration 15300: Loss = -10815.963606830777
Iteration 15400: Loss = -10815.963527120066
Iteration 15500: Loss = -10815.963613913287
Iteration 15600: Loss = -10815.963757696705
1
Iteration 15700: Loss = -10815.963596939191
Iteration 15800: Loss = -10815.963709342639
1
Iteration 15900: Loss = -10815.963599854264
Iteration 16000: Loss = -10815.963502096713
Iteration 16100: Loss = -10815.96383891811
1
Iteration 16200: Loss = -10815.965514017616
2
Iteration 16300: Loss = -10816.000213360425
3
Iteration 16400: Loss = -10815.963578273033
Iteration 16500: Loss = -10815.965419724764
1
Iteration 16600: Loss = -10815.963551267081
Iteration 16700: Loss = -10815.963616282
Iteration 16800: Loss = -10816.172754902345
1
Iteration 16900: Loss = -10815.963532886155
Iteration 17000: Loss = -10816.007126419929
1
Iteration 17100: Loss = -10815.963546397057
Iteration 17200: Loss = -10815.965998067877
1
Iteration 17300: Loss = -10815.964044443312
2
Iteration 17400: Loss = -10815.964343381529
3
Iteration 17500: Loss = -10816.006952460235
4
Iteration 17600: Loss = -10815.96354986235
Iteration 17700: Loss = -10815.963607347521
Iteration 17800: Loss = -10815.965300309628
1
Iteration 17900: Loss = -10815.963555417435
Iteration 18000: Loss = -10815.966478087346
1
Iteration 18100: Loss = -10815.963504162262
Iteration 18200: Loss = -10815.96527991784
1
Iteration 18300: Loss = -10815.963490918752
Iteration 18400: Loss = -10815.975162676068
1
Iteration 18500: Loss = -10815.963542018057
Iteration 18600: Loss = -10815.990421220296
1
Iteration 18700: Loss = -10815.963514781757
Iteration 18800: Loss = -10815.963581368713
Iteration 18900: Loss = -10815.964436659358
1
Iteration 19000: Loss = -10815.964162083295
2
Iteration 19100: Loss = -10815.963472896727
Iteration 19200: Loss = -10815.96372445524
1
Iteration 19300: Loss = -10815.968310583092
2
Iteration 19400: Loss = -10815.963499817712
Iteration 19500: Loss = -10815.963529528733
Iteration 19600: Loss = -10815.963744432838
1
Iteration 19700: Loss = -10815.972675299392
2
Iteration 19800: Loss = -10815.972949791834
3
Iteration 19900: Loss = -10815.96347527255
pi: tensor([[1.0000e+00, 4.4219e-07],
        [4.2262e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1012, 0.8988], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2334, 0.1557],
         [0.5200, 0.1525]],

        [[0.5168, 0.1640],
         [0.6891, 0.5742]],

        [[0.5449, 0.1724],
         [0.6063, 0.5255]],

        [[0.6920, 0.2221],
         [0.7256, 0.7243]],

        [[0.5576, 0.1943],
         [0.6683, 0.6332]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.005311456456244215
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0034769840210040707
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0054356419077092166
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0012012141646084548
Average Adjusted Rand Index: 0.0006899818342193905
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19551.91200373465
Iteration 100: Loss = -10822.46429602229
Iteration 200: Loss = -10822.016478607256
Iteration 300: Loss = -10821.816911760088
Iteration 400: Loss = -10821.151588404793
Iteration 500: Loss = -10820.286160163276
Iteration 600: Loss = -10820.011952729436
Iteration 700: Loss = -10819.845873850347
Iteration 800: Loss = -10819.725204489228
Iteration 900: Loss = -10819.64201282526
Iteration 1000: Loss = -10819.578755642333
Iteration 1100: Loss = -10819.524533397109
Iteration 1200: Loss = -10819.474535672241
Iteration 1300: Loss = -10819.421795432778
Iteration 1400: Loss = -10819.338602114136
Iteration 1500: Loss = -10819.050730008938
Iteration 1600: Loss = -10818.409698117583
Iteration 1700: Loss = -10817.992116396088
Iteration 1800: Loss = -10817.727005011737
Iteration 1900: Loss = -10817.34274879715
Iteration 2000: Loss = -10816.915657231983
Iteration 2100: Loss = -10816.621021725334
Iteration 2200: Loss = -10816.456164628713
Iteration 2300: Loss = -10816.357392394626
Iteration 2400: Loss = -10816.283233770588
Iteration 2500: Loss = -10816.236326722095
Iteration 2600: Loss = -10816.184448568001
Iteration 2700: Loss = -10816.158414366753
Iteration 2800: Loss = -10816.136931599018
Iteration 2900: Loss = -10816.117195932364
Iteration 3000: Loss = -10816.10251271129
Iteration 3100: Loss = -10816.091481032765
Iteration 3200: Loss = -10816.082432542633
Iteration 3300: Loss = -10816.07514328759
Iteration 3400: Loss = -10816.06892964093
Iteration 3500: Loss = -10816.063267285768
Iteration 3600: Loss = -10816.057718208314
Iteration 3700: Loss = -10816.053574740374
Iteration 3800: Loss = -10816.050154654075
Iteration 3900: Loss = -10816.04653759994
Iteration 4000: Loss = -10816.03809567799
Iteration 4100: Loss = -10816.034651410646
Iteration 4200: Loss = -10816.032042677469
Iteration 4300: Loss = -10816.029678440527
Iteration 4400: Loss = -10816.027632248652
Iteration 4500: Loss = -10816.025752570864
Iteration 4600: Loss = -10816.023252656663
Iteration 4700: Loss = -10816.018047429976
Iteration 4800: Loss = -10816.01422590616
Iteration 4900: Loss = -10816.006501211314
Iteration 5000: Loss = -10816.002718969798
Iteration 5100: Loss = -10816.001297776207
Iteration 5200: Loss = -10816.00004857931
Iteration 5300: Loss = -10815.99584608886
Iteration 5400: Loss = -10815.993676778866
Iteration 5500: Loss = -10815.992836454541
Iteration 5600: Loss = -10815.995856097454
1
Iteration 5700: Loss = -10815.991184036955
Iteration 5800: Loss = -10815.990147893073
Iteration 5900: Loss = -10815.989436762786
Iteration 6000: Loss = -10815.989517653417
Iteration 6100: Loss = -10815.98845432104
Iteration 6200: Loss = -10815.987996103479
Iteration 6300: Loss = -10815.987532132362
Iteration 6400: Loss = -10815.986992033866
Iteration 6500: Loss = -10815.986310046052
Iteration 6600: Loss = -10815.985589950007
Iteration 6700: Loss = -10815.985154273198
Iteration 6800: Loss = -10815.984819394447
Iteration 6900: Loss = -10815.984631443402
Iteration 7000: Loss = -10815.984206928943
Iteration 7100: Loss = -10815.983796573057
Iteration 7200: Loss = -10815.982886440604
Iteration 7300: Loss = -10815.982932328083
Iteration 7400: Loss = -10815.981738438364
Iteration 7500: Loss = -10815.981424692985
Iteration 7600: Loss = -10815.981036796633
Iteration 7700: Loss = -10815.982186466761
1
Iteration 7800: Loss = -10816.034283846058
2
Iteration 7900: Loss = -10815.980294715251
Iteration 8000: Loss = -10815.980627127788
1
Iteration 8100: Loss = -10815.980013666733
Iteration 8200: Loss = -10815.98028475273
1
Iteration 8300: Loss = -10815.979600015353
Iteration 8400: Loss = -10815.982091295202
1
Iteration 8500: Loss = -10815.97826314564
Iteration 8600: Loss = -10815.97812604924
Iteration 8700: Loss = -10815.97739533952
Iteration 8800: Loss = -10815.977123907363
Iteration 8900: Loss = -10815.980400635795
1
Iteration 9000: Loss = -10815.976218451377
Iteration 9100: Loss = -10815.976147598116
Iteration 9200: Loss = -10815.984535829184
1
Iteration 9300: Loss = -10815.975818473584
Iteration 9400: Loss = -10815.97523951367
Iteration 9500: Loss = -10815.976657885541
1
Iteration 9600: Loss = -10815.974183976941
Iteration 9700: Loss = -10815.973870271233
Iteration 9800: Loss = -10815.973800640864
Iteration 9900: Loss = -10815.973777732192
Iteration 10000: Loss = -10815.973883160817
1
Iteration 10100: Loss = -10815.973647509503
Iteration 10200: Loss = -10815.973450975687
Iteration 10300: Loss = -10816.139643531096
1
Iteration 10400: Loss = -10815.972522766562
Iteration 10500: Loss = -10815.972376057854
Iteration 10600: Loss = -10815.988336342143
1
Iteration 10700: Loss = -10815.971463122805
Iteration 10800: Loss = -10815.971422069017
Iteration 10900: Loss = -10815.971944514917
1
Iteration 11000: Loss = -10815.971368909793
Iteration 11100: Loss = -10815.971518872522
1
Iteration 11200: Loss = -10815.971320231256
Iteration 11300: Loss = -10815.977664659222
1
Iteration 11400: Loss = -10815.97124633129
Iteration 11500: Loss = -10815.97125474794
Iteration 11600: Loss = -10815.97248072335
1
Iteration 11700: Loss = -10815.971195229
Iteration 11800: Loss = -10815.97154518926
1
Iteration 11900: Loss = -10815.973025100937
2
Iteration 12000: Loss = -10815.971159588189
Iteration 12100: Loss = -10815.971815420642
1
Iteration 12200: Loss = -10815.971837649771
2
Iteration 12300: Loss = -10815.971129217383
Iteration 12400: Loss = -10815.970252061243
Iteration 12500: Loss = -10815.981052296935
1
Iteration 12600: Loss = -10815.970151715996
Iteration 12700: Loss = -10815.974294119549
1
Iteration 12800: Loss = -10815.970291337855
2
Iteration 12900: Loss = -10815.970074758534
Iteration 13000: Loss = -10815.971524510212
1
Iteration 13100: Loss = -10816.041834999127
2
Iteration 13200: Loss = -10815.970055355696
Iteration 13300: Loss = -10815.973016082251
1
Iteration 13400: Loss = -10815.97060028803
2
Iteration 13500: Loss = -10815.970564188861
3
Iteration 13600: Loss = -10815.975789966491
4
Iteration 13700: Loss = -10815.9746343366
5
Iteration 13800: Loss = -10815.970122930603
Iteration 13900: Loss = -10815.970140155328
Iteration 14000: Loss = -10815.994955700537
1
Iteration 14100: Loss = -10815.970007874423
Iteration 14200: Loss = -10816.035646925448
1
Iteration 14300: Loss = -10815.968031311228
Iteration 14400: Loss = -10816.023077957976
1
Iteration 14500: Loss = -10815.968067503512
Iteration 14600: Loss = -10815.968026743798
Iteration 14700: Loss = -10815.968116824946
Iteration 14800: Loss = -10815.968030505544
Iteration 14900: Loss = -10816.220854646177
1
Iteration 15000: Loss = -10815.967969723028
Iteration 15100: Loss = -10815.967745716662
Iteration 15200: Loss = -10815.966301856515
Iteration 15300: Loss = -10815.966221656547
Iteration 15400: Loss = -10815.967297004658
1
Iteration 15500: Loss = -10815.96626192501
Iteration 15600: Loss = -10815.966261114092
Iteration 15700: Loss = -10815.966203820342
Iteration 15800: Loss = -10815.966802146475
1
Iteration 15900: Loss = -10815.96621159725
Iteration 16000: Loss = -10815.966810837239
1
Iteration 16100: Loss = -10815.96694398421
2
Iteration 16200: Loss = -10815.966812144627
3
Iteration 16300: Loss = -10815.966232871717
Iteration 16400: Loss = -10815.966217463503
Iteration 16500: Loss = -10816.018849113792
1
Iteration 16600: Loss = -10815.965548626817
Iteration 16700: Loss = -10815.965532677385
Iteration 16800: Loss = -10815.992537638675
1
Iteration 16900: Loss = -10816.086518929156
2
Iteration 17000: Loss = -10815.965559699836
Iteration 17100: Loss = -10815.965777944813
1
Iteration 17200: Loss = -10816.10964187367
2
Iteration 17300: Loss = -10815.96556374622
Iteration 17400: Loss = -10815.966306240776
1
Iteration 17500: Loss = -10815.96558439476
Iteration 17600: Loss = -10815.965545812016
Iteration 17700: Loss = -10815.965742402046
1
Iteration 17800: Loss = -10815.983433295409
2
Iteration 17900: Loss = -10815.96553149944
Iteration 18000: Loss = -10815.989115264363
1
Iteration 18100: Loss = -10815.965527450382
Iteration 18200: Loss = -10815.96553345825
Iteration 18300: Loss = -10815.973514442307
1
Iteration 18400: Loss = -10816.007216139826
2
Iteration 18500: Loss = -10815.966008259276
3
Iteration 18600: Loss = -10815.965205103872
Iteration 18700: Loss = -10815.978294148132
1
Iteration 18800: Loss = -10815.965205766057
Iteration 18900: Loss = -10815.965166588547
Iteration 19000: Loss = -10816.25601064027
1
Iteration 19100: Loss = -10815.965121886871
Iteration 19200: Loss = -10816.010051355184
1
Iteration 19300: Loss = -10815.965125015324
Iteration 19400: Loss = -10815.96515713831
Iteration 19500: Loss = -10815.965634587277
1
Iteration 19600: Loss = -10815.965090783402
Iteration 19700: Loss = -10815.982655506516
1
Iteration 19800: Loss = -10815.964637800254
Iteration 19900: Loss = -10815.964623992595
pi: tensor([[1.0000e+00, 4.8122e-07],
        [5.2513e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1012, 0.8988], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2331, 0.1558],
         [0.6906, 0.1524]],

        [[0.6964, 0.1639],
         [0.5199, 0.5063]],

        [[0.6917, 0.1723],
         [0.6291, 0.5211]],

        [[0.7052, 0.2221],
         [0.5507, 0.7174]],

        [[0.6806, 0.1940],
         [0.5949, 0.6898]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.005311456456244215
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0034769840210040707
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.0054356419077092166
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.003485103334991517
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0012012141646084548
Average Adjusted Rand Index: 0.0006899818342193905
10749.023436252
[0.0012012141646084548, 0.0012012141646084548] [0.0006899818342193905, 0.0006899818342193905] [10815.96449165798, 10815.965637494983]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11019.609312180504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22568.456280362054
Iteration 100: Loss = -11168.983351505565
Iteration 200: Loss = -11168.262532334084
Iteration 300: Loss = -11167.784621222663
Iteration 400: Loss = -11167.402586255948
Iteration 500: Loss = -11166.556111410664
Iteration 600: Loss = -11163.383305686988
Iteration 700: Loss = -11102.834635318397
Iteration 800: Loss = -11030.345645136285
Iteration 900: Loss = -11025.386077231697
Iteration 1000: Loss = -11024.693074670251
Iteration 1100: Loss = -11024.675874054075
Iteration 1200: Loss = -11021.4029123012
Iteration 1300: Loss = -11017.20440229376
Iteration 1400: Loss = -11004.355030300052
Iteration 1500: Loss = -10992.440237066969
Iteration 1600: Loss = -10990.174087748606
Iteration 1700: Loss = -10982.041997051585
Iteration 1800: Loss = -10982.007890546129
Iteration 1900: Loss = -10982.005139373014
Iteration 2000: Loss = -10982.003257763681
Iteration 2100: Loss = -10982.00169255133
Iteration 2200: Loss = -10981.997844143683
Iteration 2300: Loss = -10976.270205242934
Iteration 2400: Loss = -10976.267723371995
Iteration 2500: Loss = -10976.266323061844
Iteration 2600: Loss = -10976.263551452363
Iteration 2700: Loss = -10976.181350040184
Iteration 2800: Loss = -10976.180737645444
Iteration 2900: Loss = -10976.180438660325
Iteration 3000: Loss = -10976.180138265117
Iteration 3100: Loss = -10976.190872213574
1
Iteration 3200: Loss = -10976.17951118347
Iteration 3300: Loss = -10976.179034307008
Iteration 3400: Loss = -10976.178866229455
Iteration 3500: Loss = -10976.178035142862
Iteration 3600: Loss = -10976.178281880646
1
Iteration 3700: Loss = -10976.177377310536
Iteration 3800: Loss = -10976.176444982179
Iteration 3900: Loss = -10976.16548230894
Iteration 4000: Loss = -10976.16145352043
Iteration 4100: Loss = -10976.16128158511
Iteration 4200: Loss = -10976.161105686122
Iteration 4300: Loss = -10976.160873160119
Iteration 4400: Loss = -10976.167015934843
1
Iteration 4500: Loss = -10976.142521295624
Iteration 4600: Loss = -10976.142009007439
Iteration 4700: Loss = -10976.141219717292
Iteration 4800: Loss = -10976.141111018638
Iteration 4900: Loss = -10974.86037586342
Iteration 5000: Loss = -10974.192729744196
Iteration 5100: Loss = -10974.192014013199
Iteration 5200: Loss = -10974.19157137615
Iteration 5300: Loss = -10974.19140807747
Iteration 5400: Loss = -10974.192198186007
1
Iteration 5500: Loss = -10974.191056299536
Iteration 5600: Loss = -10974.19079830973
Iteration 5700: Loss = -10974.189277842885
Iteration 5800: Loss = -10974.075015092738
Iteration 5900: Loss = -10974.07332167828
Iteration 6000: Loss = -10974.088279145422
1
Iteration 6100: Loss = -10974.072981079662
Iteration 6200: Loss = -10974.07293675955
Iteration 6300: Loss = -10974.088769488257
1
Iteration 6400: Loss = -10974.072884219268
Iteration 6500: Loss = -10974.072859316973
Iteration 6600: Loss = -10974.076564298197
1
Iteration 6700: Loss = -10974.072739874537
Iteration 6800: Loss = -10974.072724597961
Iteration 6900: Loss = -10974.072748045475
Iteration 7000: Loss = -10974.072672014765
Iteration 7100: Loss = -10974.094142204116
1
Iteration 7200: Loss = -10974.072603270828
Iteration 7300: Loss = -10974.072566142282
Iteration 7400: Loss = -10974.072504918451
Iteration 7500: Loss = -10974.072456770054
Iteration 7600: Loss = -10974.072408000946
Iteration 7700: Loss = -10974.113533689148
1
Iteration 7800: Loss = -10974.072162302104
Iteration 7900: Loss = -10974.072071579745
Iteration 8000: Loss = -10974.072114916798
Iteration 8100: Loss = -10974.072010469612
Iteration 8200: Loss = -10974.07248771821
1
Iteration 8300: Loss = -10974.07195423656
Iteration 8400: Loss = -10974.071934669733
Iteration 8500: Loss = -10974.058693158942
Iteration 8600: Loss = -10974.056671727349
Iteration 8700: Loss = -10974.056031419592
Iteration 8800: Loss = -10974.056014897082
Iteration 8900: Loss = -10974.056578799704
1
Iteration 9000: Loss = -10974.055972677006
Iteration 9100: Loss = -10974.062207135079
1
Iteration 9200: Loss = -10974.055849689581
Iteration 9300: Loss = -10974.061335293092
1
Iteration 9400: Loss = -10974.055820294172
Iteration 9500: Loss = -10974.06056111186
1
Iteration 9600: Loss = -10974.05581215205
Iteration 9700: Loss = -10974.084524907956
1
Iteration 9800: Loss = -10974.055833154309
Iteration 9900: Loss = -10974.058142046506
1
Iteration 10000: Loss = -10974.05537201626
Iteration 10100: Loss = -10974.058269129046
1
Iteration 10200: Loss = -10974.05515883077
Iteration 10300: Loss = -10974.056174885785
1
Iteration 10400: Loss = -10974.060791491696
2
Iteration 10500: Loss = -10974.055170982112
Iteration 10600: Loss = -10974.056355727349
1
Iteration 10700: Loss = -10974.055918848508
2
Iteration 10800: Loss = -10974.056908875718
3
Iteration 10900: Loss = -10974.057224257165
4
Iteration 11000: Loss = -10974.306934614486
5
Iteration 11100: Loss = -10974.055049687795
Iteration 11200: Loss = -10974.088142395234
1
Iteration 11300: Loss = -10974.053829261842
Iteration 11400: Loss = -10974.05375343323
Iteration 11500: Loss = -10974.053701416227
Iteration 11600: Loss = -10974.05359504095
Iteration 11700: Loss = -10974.054244285733
1
Iteration 11800: Loss = -10974.069381965544
2
Iteration 11900: Loss = -10974.055379570485
3
Iteration 12000: Loss = -10974.053604164745
Iteration 12100: Loss = -10974.056103083843
1
Iteration 12200: Loss = -10974.056410262647
2
Iteration 12300: Loss = -10974.063755302395
3
Iteration 12400: Loss = -10974.005697868439
Iteration 12500: Loss = -10974.005858106515
1
Iteration 12600: Loss = -10974.04585283249
2
Iteration 12700: Loss = -10974.274321847588
3
Iteration 12800: Loss = -10974.007923890498
4
Iteration 12900: Loss = -10974.01719428847
5
Iteration 13000: Loss = -10974.006189083477
6
Iteration 13100: Loss = -10974.0056983076
Iteration 13200: Loss = -10974.012092979363
1
Iteration 13300: Loss = -10974.02728806735
2
Iteration 13400: Loss = -10974.00570436334
Iteration 13500: Loss = -10974.06937704726
1
Iteration 13600: Loss = -10974.005266742659
Iteration 13700: Loss = -10974.012536441507
1
Iteration 13800: Loss = -10973.994218498261
Iteration 13900: Loss = -10973.994792489944
1
Iteration 14000: Loss = -10974.005964858534
2
Iteration 14100: Loss = -10973.994945981965
3
Iteration 14200: Loss = -10974.019971334155
4
Iteration 14300: Loss = -10973.994143288686
Iteration 14400: Loss = -10973.994762901433
1
Iteration 14500: Loss = -10973.994131205278
Iteration 14600: Loss = -10973.995316672814
1
Iteration 14700: Loss = -10973.994121273492
Iteration 14800: Loss = -10973.994122026503
Iteration 14900: Loss = -10973.994705213396
1
Iteration 15000: Loss = -10973.994027214583
Iteration 15100: Loss = -10973.994458558098
1
Iteration 15200: Loss = -10973.997479169699
2
Iteration 15300: Loss = -10973.996527603136
3
Iteration 15400: Loss = -10973.993663329518
Iteration 15500: Loss = -10973.99369400119
Iteration 15600: Loss = -10973.994028562303
1
Iteration 15700: Loss = -10974.257934602654
2
Iteration 15800: Loss = -10973.993744274094
Iteration 15900: Loss = -10973.99891680164
1
Iteration 16000: Loss = -10974.044159907586
2
Iteration 16100: Loss = -10973.994721677705
3
Iteration 16200: Loss = -10973.99349174907
Iteration 16300: Loss = -10973.99375860525
1
Iteration 16400: Loss = -10974.453853779947
2
Iteration 16500: Loss = -10973.993451153561
Iteration 16600: Loss = -10973.99299810828
Iteration 16700: Loss = -10973.99510297675
1
Iteration 16800: Loss = -10973.99300817803
Iteration 16900: Loss = -10973.994297632993
1
Iteration 17000: Loss = -10973.993325832116
2
Iteration 17100: Loss = -10973.994285965906
3
Iteration 17200: Loss = -10974.079706883624
4
Iteration 17300: Loss = -10973.992978864502
Iteration 17400: Loss = -10973.993124897419
1
Iteration 17500: Loss = -10973.996285313058
2
Iteration 17600: Loss = -10974.0035824006
3
Iteration 17700: Loss = -10973.993074929487
Iteration 17800: Loss = -10973.990653633364
Iteration 17900: Loss = -10974.033340057458
1
Iteration 18000: Loss = -10974.000795050375
2
Iteration 18100: Loss = -10973.989136398197
Iteration 18200: Loss = -10973.997483093257
1
Iteration 18300: Loss = -10973.989072435319
Iteration 18400: Loss = -10973.989029404194
Iteration 18500: Loss = -10973.992003671043
1
Iteration 18600: Loss = -10974.101736843435
2
Iteration 18700: Loss = -10973.989011188074
Iteration 18800: Loss = -10973.990048372156
1
Iteration 18900: Loss = -10973.990438161247
2
Iteration 19000: Loss = -10973.990389971119
3
Iteration 19100: Loss = -10973.989131340533
4
Iteration 19200: Loss = -10973.988918139803
Iteration 19300: Loss = -10973.988967948508
Iteration 19400: Loss = -10973.991599278672
1
Iteration 19500: Loss = -10974.039949229415
2
Iteration 19600: Loss = -10973.988891974699
Iteration 19700: Loss = -10973.98945590349
1
Iteration 19800: Loss = -10973.9889901908
Iteration 19900: Loss = -10973.988892052535
pi: tensor([[0.8136, 0.1864],
        [0.2584, 0.7416]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4967, 0.5033], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2593, 0.1060],
         [0.5646, 0.1937]],

        [[0.7299, 0.0936],
         [0.5158, 0.5473]],

        [[0.5112, 0.0911],
         [0.5854, 0.5361]],

        [[0.7182, 0.1073],
         [0.5280, 0.6233]],

        [[0.6878, 0.1037],
         [0.5629, 0.6319]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8075424217019954
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721167107229054
Global Adjusted Rand Index: 0.8683472525728395
Average Adjusted Rand Index: 0.8685760052774019
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23038.680905355908
Iteration 100: Loss = -11170.284095725017
Iteration 200: Loss = -11169.207279244709
Iteration 300: Loss = -11168.97827932223
Iteration 400: Loss = -11168.863488691102
Iteration 500: Loss = -11168.76385229509
Iteration 600: Loss = -11168.658530456776
Iteration 700: Loss = -11168.552120368118
Iteration 800: Loss = -11168.458508665652
Iteration 900: Loss = -11168.376692659174
Iteration 1000: Loss = -11168.298184104997
Iteration 1100: Loss = -11168.21777745242
Iteration 1200: Loss = -11168.1316836614
Iteration 1300: Loss = -11168.030741697565
Iteration 1400: Loss = -11167.838716389748
Iteration 1500: Loss = -11166.979505498692
Iteration 1600: Loss = -11166.337677820291
Iteration 1700: Loss = -11119.22869679749
Iteration 1800: Loss = -11017.410237903723
Iteration 1900: Loss = -11017.055583697264
Iteration 2000: Loss = -11016.929296369923
Iteration 2100: Loss = -11016.875402100874
Iteration 2200: Loss = -11016.840061347075
Iteration 2300: Loss = -11016.799661958896
Iteration 2400: Loss = -11016.783095707768
Iteration 2500: Loss = -11016.74310990879
Iteration 2600: Loss = -11016.73517002529
Iteration 2700: Loss = -11016.728496236752
Iteration 2800: Loss = -11016.721957199301
Iteration 2900: Loss = -11016.702550929986
Iteration 3000: Loss = -11016.691542709972
Iteration 3100: Loss = -11016.685635249481
Iteration 3200: Loss = -11016.692454376212
1
Iteration 3300: Loss = -11016.677891775078
Iteration 3400: Loss = -11016.67603183396
Iteration 3500: Loss = -11016.67409258838
Iteration 3600: Loss = -11016.673532299461
Iteration 3700: Loss = -11016.676854203453
1
Iteration 3800: Loss = -11016.673438750982
Iteration 3900: Loss = -11016.668281951914
Iteration 4000: Loss = -11016.667024137781
Iteration 4100: Loss = -11016.671678175257
1
Iteration 4200: Loss = -11016.665042075681
Iteration 4300: Loss = -11016.664177036886
Iteration 4400: Loss = -11016.66326488013
Iteration 4500: Loss = -11016.66204682812
Iteration 4600: Loss = -11016.6589296078
Iteration 4700: Loss = -11016.653127425217
Iteration 4800: Loss = -11016.652567177647
Iteration 4900: Loss = -11016.652055401546
Iteration 5000: Loss = -11016.651598192324
Iteration 5100: Loss = -11016.654554270723
1
Iteration 5200: Loss = -11016.65032646307
Iteration 5300: Loss = -11016.65029666266
Iteration 5400: Loss = -11016.647638925886
Iteration 5500: Loss = -11016.647242846819
Iteration 5600: Loss = -11016.64698230636
Iteration 5700: Loss = -11016.646721897388
Iteration 5800: Loss = -11016.64644238297
Iteration 5900: Loss = -11016.646284687895
Iteration 6000: Loss = -11016.64606240067
Iteration 6100: Loss = -11016.645910699763
Iteration 6200: Loss = -11016.645743217068
Iteration 6300: Loss = -11016.645735513292
Iteration 6400: Loss = -11016.64545161136
Iteration 6500: Loss = -11016.655785217185
1
Iteration 6600: Loss = -11016.645200237681
Iteration 6700: Loss = -11016.64509976084
Iteration 6800: Loss = -11016.645000203685
Iteration 6900: Loss = -11016.644865611564
Iteration 7000: Loss = -11016.645303359837
1
Iteration 7100: Loss = -11016.644669761467
Iteration 7200: Loss = -11016.654544555044
1
Iteration 7300: Loss = -11016.644538594544
Iteration 7400: Loss = -11016.650365299836
1
Iteration 7500: Loss = -11016.644363306264
Iteration 7600: Loss = -11016.644214619322
Iteration 7700: Loss = -11016.645822631024
1
Iteration 7800: Loss = -11016.643516167516
Iteration 7900: Loss = -11016.643021757105
Iteration 8000: Loss = -11016.642831501751
Iteration 8100: Loss = -11016.642749068396
Iteration 8200: Loss = -11016.642482064226
Iteration 8300: Loss = -11016.648886720379
1
Iteration 8400: Loss = -11016.636172756866
Iteration 8500: Loss = -11016.636102636721
Iteration 8600: Loss = -11016.642179882267
1
Iteration 8700: Loss = -11016.635853753663
Iteration 8800: Loss = -11016.63568676441
Iteration 8900: Loss = -11016.640666611733
1
Iteration 9000: Loss = -11016.635534569872
Iteration 9100: Loss = -11016.635481989508
Iteration 9200: Loss = -11016.635472023112
Iteration 9300: Loss = -11016.63545430685
Iteration 9400: Loss = -11016.63539381628
Iteration 9500: Loss = -11016.635352400093
Iteration 9600: Loss = -11016.635518729649
1
Iteration 9700: Loss = -11016.635281184945
Iteration 9800: Loss = -11016.635247042143
Iteration 9900: Loss = -11016.635523851779
1
Iteration 10000: Loss = -11016.635152523118
Iteration 10100: Loss = -11016.655908755461
1
Iteration 10200: Loss = -11016.634954420151
Iteration 10300: Loss = -11016.634631350047
Iteration 10400: Loss = -11016.890316496168
1
Iteration 10500: Loss = -11016.55378992866
Iteration 10600: Loss = -11016.487412723814
Iteration 10700: Loss = -11016.472238864379
Iteration 10800: Loss = -11016.467005911843
Iteration 10900: Loss = -11016.472738131693
1
Iteration 11000: Loss = -11016.465633200542
Iteration 11100: Loss = -11016.326823508445
Iteration 11200: Loss = -11016.050983644143
Iteration 11300: Loss = -11015.982919598346
Iteration 11400: Loss = -11016.042080300318
1
Iteration 11500: Loss = -11015.983869161708
2
Iteration 11600: Loss = -11015.974069970336
Iteration 11700: Loss = -11015.97912791405
1
Iteration 11800: Loss = -11015.969338733288
Iteration 11900: Loss = -11015.963760239067
Iteration 12000: Loss = -11015.962258961263
Iteration 12100: Loss = -11015.992375283391
1
Iteration 12200: Loss = -11015.957869431793
Iteration 12300: Loss = -11015.96725827684
1
Iteration 12400: Loss = -11015.957911169162
Iteration 12500: Loss = -11015.957732829693
Iteration 12600: Loss = -11016.05938411153
1
Iteration 12700: Loss = -11015.931250259197
Iteration 12800: Loss = -11015.929873973959
Iteration 12900: Loss = -11015.943733632836
1
Iteration 13000: Loss = -11015.91281379025
Iteration 13100: Loss = -11015.927031163335
1
Iteration 13200: Loss = -11016.025113143814
2
Iteration 13300: Loss = -11015.909861408782
Iteration 13400: Loss = -11015.912215397317
1
Iteration 13500: Loss = -11016.60270373768
2
Iteration 13600: Loss = -10984.445472211262
Iteration 13700: Loss = -10984.38672602138
Iteration 13800: Loss = -10974.219061974618
Iteration 13900: Loss = -10974.212072553235
Iteration 14000: Loss = -10974.131017116006
Iteration 14100: Loss = -10974.08765765226
Iteration 14200: Loss = -10974.088274036121
1
Iteration 14300: Loss = -10974.09251779196
2
Iteration 14400: Loss = -10974.084815489705
Iteration 14500: Loss = -10974.08405419314
Iteration 14600: Loss = -10974.0836399651
Iteration 14700: Loss = -10974.069625098928
Iteration 14800: Loss = -10974.094829080355
1
Iteration 14900: Loss = -10974.035125173015
Iteration 15000: Loss = -10974.029413072254
Iteration 15100: Loss = -10974.028095369516
Iteration 15200: Loss = -10974.028236351
1
Iteration 15300: Loss = -10974.027919601078
Iteration 15400: Loss = -10974.027774215463
Iteration 15500: Loss = -10974.033801221321
1
Iteration 15600: Loss = -10974.02667358586
Iteration 15700: Loss = -10974.025123269
Iteration 15800: Loss = -10974.022359726338
Iteration 15900: Loss = -10974.024497420547
1
Iteration 16000: Loss = -10974.021994108989
Iteration 16100: Loss = -10974.022044120757
Iteration 16200: Loss = -10974.101261898142
1
Iteration 16300: Loss = -10974.00574943155
Iteration 16400: Loss = -10974.004265031537
Iteration 16500: Loss = -10974.008136294453
1
Iteration 16600: Loss = -10974.002129158393
Iteration 16700: Loss = -10974.002245931826
1
Iteration 16800: Loss = -10974.00035031451
Iteration 16900: Loss = -10974.001056350162
1
Iteration 17000: Loss = -10973.998240821114
Iteration 17100: Loss = -10973.999123110794
1
Iteration 17200: Loss = -10973.996984917292
Iteration 17300: Loss = -10974.155067966474
1
Iteration 17400: Loss = -10973.997140720558
2
Iteration 17500: Loss = -10973.993350409697
Iteration 17600: Loss = -10974.01637457111
1
Iteration 17700: Loss = -10974.085363613382
2
Iteration 17800: Loss = -10973.991113541844
Iteration 17900: Loss = -10973.991115768416
Iteration 18000: Loss = -10974.004169087191
1
Iteration 18100: Loss = -10973.997153013597
2
Iteration 18200: Loss = -10973.989844555574
Iteration 18300: Loss = -10973.991699863758
1
Iteration 18400: Loss = -10974.056994250175
2
Iteration 18500: Loss = -10973.989806316842
Iteration 18600: Loss = -10973.99217822553
1
Iteration 18700: Loss = -10973.989762798303
Iteration 18800: Loss = -10973.989910873941
1
Iteration 18900: Loss = -10973.989664687453
Iteration 19000: Loss = -10973.992768313945
1
Iteration 19100: Loss = -10974.010444092091
2
Iteration 19200: Loss = -10973.98932675995
Iteration 19300: Loss = -10973.988892772306
Iteration 19400: Loss = -10973.988147978542
Iteration 19500: Loss = -10973.98992758257
1
Iteration 19600: Loss = -10973.997731498222
2
Iteration 19700: Loss = -10973.99068967122
3
Iteration 19800: Loss = -10973.988011468644
Iteration 19900: Loss = -10973.987596108636
pi: tensor([[0.8166, 0.1834],
        [0.2559, 0.7441]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5009, 0.4991], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2580, 0.1061],
         [0.6175, 0.1943]],

        [[0.6374, 0.0939],
         [0.6674, 0.6636]],

        [[0.5360, 0.0911],
         [0.5829, 0.5427]],

        [[0.6503, 0.1074],
         [0.5796, 0.6367]],

        [[0.6657, 0.1038],
         [0.6443, 0.6644]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8075424217019954
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721167107229054
Global Adjusted Rand Index: 0.8683472525728395
Average Adjusted Rand Index: 0.8685760052774019
11019.609312180504
[0.8683472525728395, 0.8683472525728395] [0.8685760052774019, 0.8685760052774019] [10973.98898727137, 10974.037632910951]
-------------------------------------
This iteration is 26
True Objective function: Loss = -10801.392494781672
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23506.352017806388
Iteration 100: Loss = -10894.936845362145
Iteration 200: Loss = -10893.825244568567
Iteration 300: Loss = -10893.50495078357
Iteration 400: Loss = -10893.396282626398
Iteration 500: Loss = -10893.3450291162
Iteration 600: Loss = -10893.311006610898
Iteration 700: Loss = -10893.281137914431
Iteration 800: Loss = -10893.250084137966
Iteration 900: Loss = -10893.215873994814
Iteration 1000: Loss = -10893.178263467134
Iteration 1100: Loss = -10893.138003913302
Iteration 1200: Loss = -10893.095819518812
Iteration 1300: Loss = -10893.05196558745
Iteration 1400: Loss = -10893.005913201689
Iteration 1500: Loss = -10892.955751144651
Iteration 1600: Loss = -10892.896232072913
Iteration 1700: Loss = -10892.81417130243
Iteration 1800: Loss = -10892.715905127812
Iteration 1900: Loss = -10892.641299467763
Iteration 2000: Loss = -10892.568219389037
Iteration 2100: Loss = -10892.484478800643
Iteration 2200: Loss = -10892.372484503305
Iteration 2300: Loss = -10892.19997707862
Iteration 2400: Loss = -10891.937665015324
Iteration 2500: Loss = -10891.57655676778
Iteration 2600: Loss = -10891.083325682965
Iteration 2700: Loss = -10815.655545855467
Iteration 2800: Loss = -10811.190821655053
Iteration 2900: Loss = -10811.074284443264
Iteration 3000: Loss = -10811.028845993935
Iteration 3100: Loss = -10811.00170197263
Iteration 3200: Loss = -10810.959727585394
Iteration 3300: Loss = -10810.799015423425
Iteration 3400: Loss = -10810.239334272652
Iteration 3500: Loss = -10809.165421320615
Iteration 3600: Loss = -10808.99784397377
Iteration 3700: Loss = -10808.984714630129
Iteration 3800: Loss = -10808.94105838286
Iteration 3900: Loss = -10808.477691193064
Iteration 4000: Loss = -10808.47408573784
Iteration 4100: Loss = -10808.474079461976
Iteration 4200: Loss = -10808.47075116684
Iteration 4300: Loss = -10808.469019959557
Iteration 4400: Loss = -10808.466509336688
Iteration 4500: Loss = -10808.458681849026
Iteration 4600: Loss = -10808.444675139717
Iteration 4700: Loss = -10808.314524960253
Iteration 4800: Loss = -10808.107407918798
Iteration 4900: Loss = -10808.097253784663
Iteration 5000: Loss = -10808.094562576607
Iteration 5100: Loss = -10808.084053764296
Iteration 5200: Loss = -10808.067322848365
Iteration 5300: Loss = -10808.060091921883
Iteration 5400: Loss = -10808.059527246542
Iteration 5500: Loss = -10808.06371994533
1
Iteration 5600: Loss = -10808.058996824095
Iteration 5700: Loss = -10808.059170612732
1
Iteration 5800: Loss = -10808.064721985364
2
Iteration 5900: Loss = -10808.058595449886
Iteration 6000: Loss = -10808.058405015843
Iteration 6100: Loss = -10808.058713382798
1
Iteration 6200: Loss = -10808.05816471333
Iteration 6300: Loss = -10808.05795578462
Iteration 6400: Loss = -10808.057702890464
Iteration 6500: Loss = -10808.056369356109
Iteration 6600: Loss = -10808.044031093503
Iteration 6700: Loss = -10808.040013029626
Iteration 6800: Loss = -10808.041126962438
1
Iteration 6900: Loss = -10808.042660411222
2
Iteration 7000: Loss = -10808.03966183818
Iteration 7100: Loss = -10808.03974844473
Iteration 7200: Loss = -10808.039473621884
Iteration 7300: Loss = -10808.039431772533
Iteration 7400: Loss = -10808.039304696495
Iteration 7500: Loss = -10808.039344184672
Iteration 7600: Loss = -10808.039225084662
Iteration 7700: Loss = -10808.084514233768
1
Iteration 7800: Loss = -10808.041745970017
2
Iteration 7900: Loss = -10808.039054563485
Iteration 8000: Loss = -10808.039987596869
1
Iteration 8100: Loss = -10808.039076416359
Iteration 8200: Loss = -10808.039052511178
Iteration 8300: Loss = -10808.045346905694
1
Iteration 8400: Loss = -10808.06907999575
2
Iteration 8500: Loss = -10808.038968926872
Iteration 8600: Loss = -10808.0389909464
Iteration 8700: Loss = -10808.07339710831
1
Iteration 8800: Loss = -10808.038904081355
Iteration 8900: Loss = -10808.039030973641
1
Iteration 9000: Loss = -10808.170622045394
2
Iteration 9100: Loss = -10808.038883533382
Iteration 9200: Loss = -10808.050576827478
1
Iteration 9300: Loss = -10808.038881636096
Iteration 9400: Loss = -10808.0392991116
1
Iteration 9500: Loss = -10808.038846730436
Iteration 9600: Loss = -10808.039090848712
1
Iteration 9700: Loss = -10808.03885011602
Iteration 9800: Loss = -10808.0393882977
1
Iteration 9900: Loss = -10808.039988810231
2
Iteration 10000: Loss = -10808.039805714428
3
Iteration 10100: Loss = -10808.039019717333
4
Iteration 10200: Loss = -10808.0413912338
5
Iteration 10300: Loss = -10808.052866517528
6
Iteration 10400: Loss = -10808.041099818563
7
Iteration 10500: Loss = -10808.038791029661
Iteration 10600: Loss = -10808.04033913462
1
Iteration 10700: Loss = -10808.039073839995
2
Iteration 10800: Loss = -10808.038859437536
Iteration 10900: Loss = -10808.068989688494
1
Iteration 11000: Loss = -10808.038798694983
Iteration 11100: Loss = -10808.039051805988
1
Iteration 11200: Loss = -10808.055654751832
2
Iteration 11300: Loss = -10808.049077048861
3
Iteration 11400: Loss = -10808.051945582649
4
Iteration 11500: Loss = -10808.073848392001
5
Iteration 11600: Loss = -10808.039722013162
6
Iteration 11700: Loss = -10808.038792788535
Iteration 11800: Loss = -10808.047786048492
1
Iteration 11900: Loss = -10808.038756109974
Iteration 12000: Loss = -10808.038898941451
1
Iteration 12100: Loss = -10808.267331032057
2
Iteration 12200: Loss = -10808.038756667496
Iteration 12300: Loss = -10808.055881709484
1
Iteration 12400: Loss = -10808.038773275835
Iteration 12500: Loss = -10808.038817701297
Iteration 12600: Loss = -10808.08900980402
1
Iteration 12700: Loss = -10808.038737736093
Iteration 12800: Loss = -10808.167398040005
1
Iteration 12900: Loss = -10808.03873202202
Iteration 13000: Loss = -10808.038721979034
Iteration 13100: Loss = -10808.039141767491
1
Iteration 13200: Loss = -10808.038734166128
Iteration 13300: Loss = -10808.077159078579
1
Iteration 13400: Loss = -10808.038696275718
Iteration 13500: Loss = -10808.038692769614
Iteration 13600: Loss = -10808.03883117823
1
Iteration 13700: Loss = -10808.045342949585
2
Iteration 13800: Loss = -10808.038668968346
Iteration 13900: Loss = -10808.038674048083
Iteration 14000: Loss = -10808.194915328742
1
Iteration 14100: Loss = -10808.038665010401
Iteration 14200: Loss = -10808.052889046905
1
Iteration 14300: Loss = -10808.038654710734
Iteration 14400: Loss = -10808.044275837377
1
Iteration 14500: Loss = -10808.038630363948
Iteration 14600: Loss = -10808.039182997052
1
Iteration 14700: Loss = -10808.038682212878
Iteration 14800: Loss = -10808.038735986333
Iteration 14900: Loss = -10808.040862674943
1
Iteration 15000: Loss = -10808.03862044724
Iteration 15100: Loss = -10808.041568642126
1
Iteration 15200: Loss = -10808.04540453013
2
Iteration 15300: Loss = -10808.039174545
3
Iteration 15400: Loss = -10808.03865563614
Iteration 15500: Loss = -10808.038755813786
1
Iteration 15600: Loss = -10808.041854231926
2
Iteration 15700: Loss = -10808.043170227933
3
Iteration 15800: Loss = -10808.038600966502
Iteration 15900: Loss = -10808.069203208575
1
Iteration 16000: Loss = -10808.038595087206
Iteration 16100: Loss = -10808.038883454761
1
Iteration 16200: Loss = -10808.038626206213
Iteration 16300: Loss = -10808.110887464265
1
Iteration 16400: Loss = -10808.038585910614
Iteration 16500: Loss = -10808.039738107735
1
Iteration 16600: Loss = -10808.038623065055
Iteration 16700: Loss = -10808.03992430362
1
Iteration 16800: Loss = -10808.038599125579
Iteration 16900: Loss = -10808.138228073027
1
Iteration 17000: Loss = -10808.038598215697
Iteration 17100: Loss = -10808.177779387846
1
Iteration 17200: Loss = -10808.038599906302
Iteration 17300: Loss = -10808.039627252998
1
Iteration 17400: Loss = -10808.038624493343
Iteration 17500: Loss = -10808.03866660874
Iteration 17600: Loss = -10808.112441758969
1
Iteration 17700: Loss = -10808.03858872432
Iteration 17800: Loss = -10808.039270902504
1
Iteration 17900: Loss = -10808.03860089208
Iteration 18000: Loss = -10808.038725170805
1
Iteration 18100: Loss = -10808.038683489618
Iteration 18200: Loss = -10808.038637486967
Iteration 18300: Loss = -10808.038773651431
1
Iteration 18400: Loss = -10808.045477888267
2
Iteration 18500: Loss = -10808.069768724292
3
Iteration 18600: Loss = -10808.03863462758
Iteration 18700: Loss = -10808.038841440206
1
Iteration 18800: Loss = -10808.291345069474
2
Iteration 18900: Loss = -10808.03860908084
Iteration 19000: Loss = -10808.04768741045
1
Iteration 19100: Loss = -10808.047094540747
2
Iteration 19200: Loss = -10808.038638085156
Iteration 19300: Loss = -10808.038666734701
Iteration 19400: Loss = -10808.038686755866
Iteration 19500: Loss = -10808.038662653977
Iteration 19600: Loss = -10808.038661830531
Iteration 19700: Loss = -10808.03868185062
Iteration 19800: Loss = -10808.038599100835
Iteration 19900: Loss = -10808.039138074135
1
pi: tensor([[0.7950, 0.2050],
        [0.3142, 0.6858]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0809, 0.9191], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2468, 0.0989],
         [0.5768, 0.1843]],

        [[0.5522, 0.0961],
         [0.7270, 0.5257]],

        [[0.6598, 0.0998],
         [0.6700, 0.6018]],

        [[0.5634, 0.0956],
         [0.7171, 0.5597]],

        [[0.6923, 0.1097],
         [0.5914, 0.6460]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 12
Adjusted Rand Index: 0.5733333333333334
Global Adjusted Rand Index: 0.4946387764678061
Average Adjusted Rand Index: 0.628808854491597
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24431.576535989196
Iteration 100: Loss = -10893.831267622778
Iteration 200: Loss = -10893.105581281789
Iteration 300: Loss = -10892.915118254761
Iteration 400: Loss = -10892.703870522644
Iteration 500: Loss = -10892.460302701895
Iteration 600: Loss = -10892.257972201327
Iteration 700: Loss = -10892.117040802497
Iteration 800: Loss = -10892.021805162018
Iteration 900: Loss = -10891.972546252191
Iteration 1000: Loss = -10891.939705487466
Iteration 1100: Loss = -10891.911924404396
Iteration 1200: Loss = -10891.885417790478
Iteration 1300: Loss = -10891.858294754142
Iteration 1400: Loss = -10891.829220831147
Iteration 1500: Loss = -10891.794939551268
Iteration 1600: Loss = -10891.74423346171
Iteration 1700: Loss = -10891.60432214997
Iteration 1800: Loss = -10890.6600623917
Iteration 1900: Loss = -10889.678331350378
Iteration 2000: Loss = -10889.449332359898
Iteration 2100: Loss = -10889.372172929297
Iteration 2200: Loss = -10889.329952104588
Iteration 2300: Loss = -10889.304261340649
Iteration 2400: Loss = -10889.287234960157
Iteration 2500: Loss = -10889.275239277433
Iteration 2600: Loss = -10889.26642244183
Iteration 2700: Loss = -10889.259435917573
Iteration 2800: Loss = -10889.253601143522
Iteration 2900: Loss = -10889.248400683915
Iteration 3000: Loss = -10889.244080266679
Iteration 3100: Loss = -10889.240727984576
Iteration 3200: Loss = -10889.23791787984
Iteration 3300: Loss = -10889.23553446694
Iteration 3400: Loss = -10889.233431719284
Iteration 3500: Loss = -10889.231577113764
Iteration 3600: Loss = -10889.229830215374
Iteration 3700: Loss = -10889.228143959219
Iteration 3800: Loss = -10889.226314980602
Iteration 3900: Loss = -10889.224477788122
Iteration 4000: Loss = -10889.223186628355
Iteration 4100: Loss = -10889.22202645667
Iteration 4200: Loss = -10889.220834965548
Iteration 4300: Loss = -10889.219543097233
Iteration 4400: Loss = -10889.218179819462
Iteration 4500: Loss = -10889.216819751668
Iteration 4600: Loss = -10889.215994922879
Iteration 4700: Loss = -10889.215330392532
Iteration 4800: Loss = -10889.21476410085
Iteration 4900: Loss = -10889.214234144121
Iteration 5000: Loss = -10889.213754134555
Iteration 5100: Loss = -10889.21331389643
Iteration 5200: Loss = -10889.212842414567
Iteration 5300: Loss = -10889.21246172173
Iteration 5400: Loss = -10889.212077950311
Iteration 5500: Loss = -10889.211761129158
Iteration 5600: Loss = -10889.211443939643
Iteration 5700: Loss = -10889.211111246937
Iteration 5800: Loss = -10889.210793130998
Iteration 5900: Loss = -10889.210515947161
Iteration 6000: Loss = -10889.210197462819
Iteration 6100: Loss = -10889.209955673896
Iteration 6200: Loss = -10889.209653725542
Iteration 6300: Loss = -10889.209320309517
Iteration 6400: Loss = -10889.209017376308
Iteration 6500: Loss = -10889.208834529201
Iteration 6600: Loss = -10889.208711683115
Iteration 6700: Loss = -10889.209914110088
1
Iteration 6800: Loss = -10889.208415084737
Iteration 6900: Loss = -10889.208414510837
Iteration 7000: Loss = -10889.208154653985
Iteration 7100: Loss = -10889.208080561393
Iteration 7200: Loss = -10889.208001756522
Iteration 7300: Loss = -10889.207889632677
Iteration 7400: Loss = -10889.207818607994
Iteration 7500: Loss = -10889.207697592692
Iteration 7600: Loss = -10889.207584855758
Iteration 7700: Loss = -10889.209872918322
1
Iteration 7800: Loss = -10889.207516445751
Iteration 7900: Loss = -10889.210072177933
1
Iteration 8000: Loss = -10889.207394625204
Iteration 8100: Loss = -10889.207425839182
Iteration 8200: Loss = -10889.207253697
Iteration 8300: Loss = -10889.207448174082
1
Iteration 8400: Loss = -10889.207372402616
2
Iteration 8500: Loss = -10889.207175895608
Iteration 8600: Loss = -10889.212386713563
1
Iteration 8700: Loss = -10889.207062982237
Iteration 8800: Loss = -10889.206987191943
Iteration 8900: Loss = -10889.206930224773
Iteration 9000: Loss = -10889.206725348806
Iteration 9100: Loss = -10889.218061498199
1
Iteration 9200: Loss = -10889.206597990946
Iteration 9300: Loss = -10889.206574725216
Iteration 9400: Loss = -10889.211126711543
1
Iteration 9500: Loss = -10889.206502679526
Iteration 9600: Loss = -10889.206481638086
Iteration 9700: Loss = -10889.213186437464
1
Iteration 9800: Loss = -10889.206286192579
Iteration 9900: Loss = -10889.206166502117
Iteration 10000: Loss = -10889.206520090434
1
Iteration 10100: Loss = -10889.206092653794
Iteration 10200: Loss = -10889.20605435368
Iteration 10300: Loss = -10889.206056581756
Iteration 10400: Loss = -10889.206126425084
Iteration 10500: Loss = -10889.205960537833
Iteration 10600: Loss = -10889.205924132562
Iteration 10700: Loss = -10889.214379156498
1
Iteration 10800: Loss = -10889.205854699567
Iteration 10900: Loss = -10889.205841177421
Iteration 11000: Loss = -10889.210018526606
1
Iteration 11100: Loss = -10889.205825385037
Iteration 11200: Loss = -10889.205806039176
Iteration 11300: Loss = -10889.215196117213
1
Iteration 11400: Loss = -10889.20575172118
Iteration 11500: Loss = -10889.205697497713
Iteration 11600: Loss = -10889.20752421276
1
Iteration 11700: Loss = -10889.205662306955
Iteration 11800: Loss = -10889.205672929013
Iteration 11900: Loss = -10889.205642399995
Iteration 12000: Loss = -10889.205890125699
1
Iteration 12100: Loss = -10889.204781895303
Iteration 12200: Loss = -10889.20454338987
Iteration 12300: Loss = -10889.221341680804
1
Iteration 12400: Loss = -10889.204524807632
Iteration 12500: Loss = -10889.204552977666
Iteration 12600: Loss = -10889.204671262942
1
Iteration 12700: Loss = -10889.20452762326
Iteration 12800: Loss = -10889.205032308526
1
Iteration 12900: Loss = -10889.21627835562
2
Iteration 13000: Loss = -10889.204519845942
Iteration 13100: Loss = -10889.25926674149
1
Iteration 13200: Loss = -10889.204448010265
Iteration 13300: Loss = -10889.479764209118
1
Iteration 13400: Loss = -10889.204407164036
Iteration 13500: Loss = -10889.204373454213
Iteration 13600: Loss = -10889.28245802097
1
Iteration 13700: Loss = -10889.204261656809
Iteration 13800: Loss = -10889.204287477623
Iteration 13900: Loss = -10889.204234316234
Iteration 14000: Loss = -10889.204237126352
Iteration 14100: Loss = -10889.204653346467
1
Iteration 14200: Loss = -10889.204152908236
Iteration 14300: Loss = -10889.204315763367
1
Iteration 14400: Loss = -10889.204139877103
Iteration 14500: Loss = -10889.204209417841
Iteration 14600: Loss = -10889.204164209588
Iteration 14700: Loss = -10889.204272174757
1
Iteration 14800: Loss = -10889.204148737257
Iteration 14900: Loss = -10889.204568702004
1
Iteration 15000: Loss = -10889.204136140244
Iteration 15100: Loss = -10889.211203546512
1
Iteration 15200: Loss = -10889.204121789176
Iteration 15300: Loss = -10889.204121529128
Iteration 15400: Loss = -10889.206461351105
1
Iteration 15500: Loss = -10889.204272510191
2
Iteration 15600: Loss = -10889.204116160956
Iteration 15700: Loss = -10889.20831110269
1
Iteration 15800: Loss = -10889.214774953805
2
Iteration 15900: Loss = -10889.20412891451
Iteration 16000: Loss = -10889.204555047352
1
Iteration 16100: Loss = -10889.424323739586
2
Iteration 16200: Loss = -10889.20412506913
Iteration 16300: Loss = -10889.205735276904
1
Iteration 16400: Loss = -10889.254468162097
2
Iteration 16500: Loss = -10889.20497762652
3
Iteration 16600: Loss = -10889.204445814805
4
Iteration 16700: Loss = -10889.213060469112
5
Iteration 16800: Loss = -10889.205230690393
6
Iteration 16900: Loss = -10889.20434733909
7
Iteration 17000: Loss = -10889.211867047426
8
Iteration 17100: Loss = -10889.20526184312
9
Iteration 17200: Loss = -10889.205553692573
10
Iteration 17300: Loss = -10889.207979580488
11
Iteration 17400: Loss = -10889.20449078835
12
Iteration 17500: Loss = -10889.204172226673
Iteration 17600: Loss = -10889.252177274582
1
Iteration 17700: Loss = -10889.204128593941
Iteration 17800: Loss = -10889.22476047815
1
Iteration 17900: Loss = -10889.204110760596
Iteration 18000: Loss = -10889.227923703986
1
Iteration 18100: Loss = -10889.204156190115
Iteration 18200: Loss = -10889.212093072969
1
Iteration 18300: Loss = -10889.240383982036
2
Iteration 18400: Loss = -10889.20414818768
Iteration 18500: Loss = -10889.214940201698
1
Iteration 18600: Loss = -10889.204123286392
Iteration 18700: Loss = -10889.211521323748
1
Iteration 18800: Loss = -10889.20413323021
Iteration 18900: Loss = -10889.221013634748
1
Iteration 19000: Loss = -10889.204171902305
Iteration 19100: Loss = -10889.204133651263
Iteration 19200: Loss = -10889.205120921202
1
Iteration 19300: Loss = -10889.204157464428
Iteration 19400: Loss = -10889.204130797301
Iteration 19500: Loss = -10889.204620744958
1
Iteration 19600: Loss = -10889.204141208329
Iteration 19700: Loss = -10889.225199484183
1
Iteration 19800: Loss = -10889.23304614704
2
Iteration 19900: Loss = -10889.214126102446
3
pi: tensor([[1.0000e+00, 2.6723e-07],
        [1.9252e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0444, 0.9556], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1115, 0.1950],
         [0.6800, 0.1624]],

        [[0.7213, 0.0882],
         [0.6402, 0.7038]],

        [[0.7249, 0.1238],
         [0.6550, 0.6993]],

        [[0.5382, 0.1677],
         [0.6707, 0.5190]],

        [[0.5736, 0.1200],
         [0.5082, 0.5362]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0014922741295917668
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.011530202595462608
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.022626262626262626
Global Adjusted Rand Index: 0.004304030007199621
Average Adjusted Rand Index: 0.004418422411369864
10801.392494781672
[0.4946387764678061, 0.004304030007199621] [0.628808854491597, 0.004418422411369864] [10808.187028273573, 10889.204117929496]
-------------------------------------
This iteration is 27
True Objective function: Loss = -10645.842443808191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23239.964090092835
Iteration 100: Loss = -10646.343005632047
Iteration 200: Loss = -10645.47243858759
Iteration 300: Loss = -10645.177003397144
Iteration 400: Loss = -10645.011861478475
Iteration 500: Loss = -10644.89238900983
Iteration 600: Loss = -10644.785673307077
Iteration 700: Loss = -10644.671800431584
Iteration 800: Loss = -10644.531674844879
Iteration 900: Loss = -10644.297917717415
Iteration 1000: Loss = -10642.400355800739
Iteration 1100: Loss = -10640.589318379061
Iteration 1200: Loss = -10640.275805085947
Iteration 1300: Loss = -10640.092040729243
Iteration 1400: Loss = -10639.952042901918
Iteration 1500: Loss = -10639.823441908906
Iteration 1600: Loss = -10639.736393338053
Iteration 1700: Loss = -10639.67064068406
Iteration 1800: Loss = -10639.619066147327
Iteration 1900: Loss = -10639.575799136544
Iteration 2000: Loss = -10639.535869546175
Iteration 2100: Loss = -10639.495240473965
Iteration 2200: Loss = -10639.446092433076
Iteration 2300: Loss = -10639.364113298685
Iteration 2400: Loss = -10639.216023088185
Iteration 2500: Loss = -10639.062749183076
Iteration 2600: Loss = -10638.957565182882
Iteration 2700: Loss = -10638.89116018403
Iteration 2800: Loss = -10638.847902903177
Iteration 2900: Loss = -10638.817998525235
Iteration 3000: Loss = -10638.796128823313
Iteration 3100: Loss = -10638.77950892686
Iteration 3200: Loss = -10638.76641994788
Iteration 3300: Loss = -10638.755845244646
Iteration 3400: Loss = -10638.747114457052
Iteration 3500: Loss = -10638.73976064522
Iteration 3600: Loss = -10638.73349428644
Iteration 3700: Loss = -10638.728157712645
Iteration 3800: Loss = -10638.723445939804
Iteration 3900: Loss = -10638.719276781101
Iteration 4000: Loss = -10638.715569247075
Iteration 4100: Loss = -10638.712156120571
Iteration 4200: Loss = -10638.709003332495
Iteration 4300: Loss = -10638.705869500842
Iteration 4400: Loss = -10638.702425426325
Iteration 4500: Loss = -10638.699193941708
Iteration 4600: Loss = -10638.697149021898
Iteration 4700: Loss = -10638.69562373466
Iteration 4800: Loss = -10638.694388347092
Iteration 4900: Loss = -10638.693231138404
Iteration 5000: Loss = -10638.692309223794
Iteration 5100: Loss = -10638.691417376433
Iteration 5200: Loss = -10638.690628573695
Iteration 5300: Loss = -10638.689890689768
Iteration 5400: Loss = -10638.689204931745
Iteration 5500: Loss = -10638.688545910418
Iteration 5600: Loss = -10638.687947433033
Iteration 5700: Loss = -10638.687415382321
Iteration 5800: Loss = -10638.68687154511
Iteration 5900: Loss = -10638.686362906414
Iteration 6000: Loss = -10638.685879635657
Iteration 6100: Loss = -10638.685414886806
Iteration 6200: Loss = -10638.68491688103
Iteration 6300: Loss = -10638.684942354152
Iteration 6400: Loss = -10638.683577339467
Iteration 6500: Loss = -10638.683168403788
Iteration 6600: Loss = -10638.682874733287
Iteration 6700: Loss = -10638.682534155167
Iteration 6800: Loss = -10638.682980532381
1
Iteration 6900: Loss = -10638.681889479283
Iteration 7000: Loss = -10638.681513808882
Iteration 7100: Loss = -10638.681233683148
Iteration 7200: Loss = -10638.680997656777
Iteration 7300: Loss = -10638.681484363256
1
Iteration 7400: Loss = -10638.680555499846
Iteration 7500: Loss = -10638.680382050452
Iteration 7600: Loss = -10638.680219881144
Iteration 7700: Loss = -10638.680048416174
Iteration 7800: Loss = -10638.680087779647
Iteration 7900: Loss = -10638.679731657203
Iteration 8000: Loss = -10638.680095037076
1
Iteration 8100: Loss = -10638.679365242166
Iteration 8200: Loss = -10638.678945399843
Iteration 8300: Loss = -10638.67867063044
Iteration 8400: Loss = -10638.678523880637
Iteration 8500: Loss = -10638.695267792456
1
Iteration 8600: Loss = -10638.678211463184
Iteration 8700: Loss = -10638.6782547502
Iteration 8800: Loss = -10638.679442713266
1
Iteration 8900: Loss = -10638.677624784114
Iteration 9000: Loss = -10638.682929326003
1
Iteration 9100: Loss = -10638.677262528081
Iteration 9200: Loss = -10638.847052585354
1
Iteration 9300: Loss = -10638.677142683691
Iteration 9400: Loss = -10638.704703358431
1
Iteration 9500: Loss = -10638.677010792439
Iteration 9600: Loss = -10638.677182251962
1
Iteration 9700: Loss = -10638.676911835952
Iteration 9800: Loss = -10638.680018214021
1
Iteration 9900: Loss = -10638.67678416535
Iteration 10000: Loss = -10638.676959607661
1
Iteration 10100: Loss = -10638.677527793121
2
Iteration 10200: Loss = -10638.676668445767
Iteration 10300: Loss = -10638.838899015205
1
Iteration 10400: Loss = -10638.676602114345
Iteration 10500: Loss = -10638.836424623874
1
Iteration 10600: Loss = -10638.676513574963
Iteration 10700: Loss = -10638.676551936953
Iteration 10800: Loss = -10638.676460016846
Iteration 10900: Loss = -10638.676407498882
Iteration 11000: Loss = -10638.676399840759
Iteration 11100: Loss = -10638.682426009029
1
Iteration 11200: Loss = -10638.676302603737
Iteration 11300: Loss = -10638.676585250645
1
Iteration 11400: Loss = -10638.676253846941
Iteration 11500: Loss = -10638.67700531756
1
Iteration 11600: Loss = -10638.676201466504
Iteration 11700: Loss = -10638.676186623514
Iteration 11800: Loss = -10638.676206233695
Iteration 11900: Loss = -10638.677106981946
1
Iteration 12000: Loss = -10638.763018807193
2
Iteration 12100: Loss = -10638.675950979783
Iteration 12200: Loss = -10638.675939894849
Iteration 12300: Loss = -10638.675867121958
Iteration 12400: Loss = -10638.67587908876
Iteration 12500: Loss = -10638.711544501335
1
Iteration 12600: Loss = -10638.675821026132
Iteration 12700: Loss = -10638.676287886856
1
Iteration 12800: Loss = -10638.675778899747
Iteration 12900: Loss = -10638.675816999888
Iteration 13000: Loss = -10638.682238500687
1
Iteration 13100: Loss = -10638.675754148968
Iteration 13200: Loss = -10638.676009652465
1
Iteration 13300: Loss = -10638.675808253915
Iteration 13400: Loss = -10638.675725358038
Iteration 13500: Loss = -10638.69056684965
1
Iteration 13600: Loss = -10638.675694849706
Iteration 13700: Loss = -10638.788115987913
1
Iteration 13800: Loss = -10638.675611108034
Iteration 13900: Loss = -10638.675519603397
Iteration 14000: Loss = -10638.678201383624
1
Iteration 14100: Loss = -10638.675231124762
Iteration 14200: Loss = -10638.67808172887
1
Iteration 14300: Loss = -10638.67522267907
Iteration 14400: Loss = -10638.677594343231
1
Iteration 14500: Loss = -10638.675192561182
Iteration 14600: Loss = -10638.776220204567
1
Iteration 14700: Loss = -10638.675241362762
Iteration 14800: Loss = -10638.675240223485
Iteration 14900: Loss = -10638.676349546171
1
Iteration 15000: Loss = -10638.67521292406
Iteration 15100: Loss = -10638.710888934753
1
Iteration 15200: Loss = -10638.6751775172
Iteration 15300: Loss = -10638.67683050724
1
Iteration 15400: Loss = -10638.675199443396
Iteration 15500: Loss = -10638.675383359416
1
Iteration 15600: Loss = -10638.675112498511
Iteration 15700: Loss = -10638.677006983859
1
Iteration 15800: Loss = -10638.675111298573
Iteration 15900: Loss = -10638.675106751012
Iteration 16000: Loss = -10638.675444193354
1
Iteration 16100: Loss = -10638.675002633161
Iteration 16200: Loss = -10638.67532332529
1
Iteration 16300: Loss = -10638.67509851604
Iteration 16400: Loss = -10638.674942633157
Iteration 16500: Loss = -10638.67510597126
1
Iteration 16600: Loss = -10638.675015439416
Iteration 16700: Loss = -10638.675000400919
Iteration 16800: Loss = -10638.67493294427
Iteration 16900: Loss = -10638.67507891356
1
Iteration 17000: Loss = -10638.67472928203
Iteration 17100: Loss = -10638.721754805405
1
Iteration 17200: Loss = -10638.674740142775
Iteration 17300: Loss = -10638.675902933608
1
Iteration 17400: Loss = -10638.674684765267
Iteration 17500: Loss = -10638.675502709864
1
Iteration 17600: Loss = -10638.674627627226
Iteration 17700: Loss = -10638.687384998037
1
Iteration 17800: Loss = -10638.674611623535
Iteration 17900: Loss = -10638.679002876057
1
Iteration 18000: Loss = -10638.674716294563
2
Iteration 18100: Loss = -10638.674708958994
Iteration 18200: Loss = -10638.716810493579
1
Iteration 18300: Loss = -10638.674720359868
Iteration 18400: Loss = -10638.67471888976
Iteration 18500: Loss = -10638.692092417623
1
Iteration 18600: Loss = -10638.674702213733
Iteration 18700: Loss = -10638.674669203538
Iteration 18800: Loss = -10638.69483521342
1
Iteration 18900: Loss = -10638.674688108973
Iteration 19000: Loss = -10638.674672849196
Iteration 19100: Loss = -10638.67501312516
1
Iteration 19200: Loss = -10638.674682559222
Iteration 19300: Loss = -10638.674645066996
Iteration 19400: Loss = -10638.675239711058
1
Iteration 19500: Loss = -10638.674644477614
Iteration 19600: Loss = -10638.674634011435
Iteration 19700: Loss = -10638.680961407968
1
Iteration 19800: Loss = -10638.674631283246
Iteration 19900: Loss = -10638.674628641567
pi: tensor([[9.8070e-01, 1.9301e-02],
        [5.6483e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9933e-01, 6.7369e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1547, 0.1485],
         [0.6940, 0.2438]],

        [[0.6282, 0.2082],
         [0.5814, 0.5180]],

        [[0.6059, 0.0580],
         [0.5746, 0.5643]],

        [[0.5572, 0.1833],
         [0.5952, 0.5899]],

        [[0.6494, 0.1492],
         [0.6981, 0.5587]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.005431979218977636
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.021712907117008445
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.026272897264404446
Global Adjusted Rand Index: -0.003159406385520095
Average Adjusted Rand Index: 0.0015443228819187929
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21628.482240072593
Iteration 100: Loss = -10645.743281919027
Iteration 200: Loss = -10645.235276153755
Iteration 300: Loss = -10645.098632156383
Iteration 400: Loss = -10644.996333954781
Iteration 500: Loss = -10644.897284538329
Iteration 600: Loss = -10644.772003068023
Iteration 700: Loss = -10644.594864679053
Iteration 800: Loss = -10644.415383263393
Iteration 900: Loss = -10644.278309873196
Iteration 1000: Loss = -10644.09622293133
Iteration 1100: Loss = -10642.608700972018
Iteration 1200: Loss = -10640.15673502636
Iteration 1300: Loss = -10639.864615773102
Iteration 1400: Loss = -10639.755192721906
Iteration 1500: Loss = -10639.694565309568
Iteration 1600: Loss = -10639.652063001417
Iteration 1700: Loss = -10639.616104194425
Iteration 1800: Loss = -10639.580239328101
Iteration 1900: Loss = -10639.54620311248
Iteration 2000: Loss = -10639.514375448487
Iteration 2100: Loss = -10639.480979253114
Iteration 2200: Loss = -10639.439392745895
Iteration 2300: Loss = -10639.367365716478
Iteration 2400: Loss = -10639.220902605397
Iteration 2500: Loss = -10639.05319224844
Iteration 2600: Loss = -10638.941100531722
Iteration 2700: Loss = -10638.873347571545
Iteration 2800: Loss = -10638.830086154161
Iteration 2900: Loss = -10638.800655474883
Iteration 3000: Loss = -10638.779478531858
Iteration 3100: Loss = -10638.76349871799
Iteration 3200: Loss = -10638.751196140885
Iteration 3300: Loss = -10638.741387355407
Iteration 3400: Loss = -10638.733419361144
Iteration 3500: Loss = -10638.726832732773
Iteration 3600: Loss = -10638.721257359435
Iteration 3700: Loss = -10638.716496768866
Iteration 3800: Loss = -10638.712416527647
Iteration 3900: Loss = -10638.70892239609
Iteration 4000: Loss = -10638.705837625494
Iteration 4100: Loss = -10638.703204429796
Iteration 4200: Loss = -10638.700845579533
Iteration 4300: Loss = -10638.698746888589
Iteration 4400: Loss = -10638.69686710774
Iteration 4500: Loss = -10638.695176230072
Iteration 4600: Loss = -10638.693641340942
Iteration 4700: Loss = -10638.692320467599
Iteration 4800: Loss = -10638.691087614356
Iteration 4900: Loss = -10638.689978549239
Iteration 5000: Loss = -10638.688962675915
Iteration 5100: Loss = -10638.688050506598
Iteration 5200: Loss = -10638.68722553648
Iteration 5300: Loss = -10638.6864424534
Iteration 5400: Loss = -10638.685755606934
Iteration 5500: Loss = -10638.685133490993
Iteration 5600: Loss = -10638.684501080066
Iteration 5700: Loss = -10638.68398570664
Iteration 5800: Loss = -10638.68346169913
Iteration 5900: Loss = -10638.683013758027
Iteration 6000: Loss = -10638.682504901297
Iteration 6100: Loss = -10638.6820712961
Iteration 6200: Loss = -10638.681693551478
Iteration 6300: Loss = -10638.681294966229
Iteration 6400: Loss = -10638.680881288865
Iteration 6500: Loss = -10638.680549651814
Iteration 6600: Loss = -10638.680257909142
Iteration 6700: Loss = -10638.68111148419
1
Iteration 6800: Loss = -10638.679727272114
Iteration 6900: Loss = -10638.679456153553
Iteration 7000: Loss = -10638.67925138801
Iteration 7100: Loss = -10638.679021046553
Iteration 7200: Loss = -10638.678947327799
Iteration 7300: Loss = -10638.678609204422
Iteration 7400: Loss = -10638.678458387827
Iteration 7500: Loss = -10638.678232019207
Iteration 7600: Loss = -10638.678065273923
Iteration 7700: Loss = -10638.67792875423
Iteration 7800: Loss = -10638.67777351314
Iteration 7900: Loss = -10638.698754027719
1
Iteration 8000: Loss = -10638.67757761426
Iteration 8100: Loss = -10638.67738578564
Iteration 8200: Loss = -10638.715996388366
1
Iteration 8300: Loss = -10638.677166130932
Iteration 8400: Loss = -10638.693229257093
1
Iteration 8500: Loss = -10638.676747799767
Iteration 8600: Loss = -10638.676905326656
1
Iteration 8700: Loss = -10638.676565573174
Iteration 8800: Loss = -10638.67646291223
Iteration 8900: Loss = -10638.684752644916
1
Iteration 9000: Loss = -10638.67639956132
Iteration 9100: Loss = -10638.676304949315
Iteration 9200: Loss = -10638.676793609151
1
Iteration 9300: Loss = -10638.676235906789
Iteration 9400: Loss = -10638.676107408583
Iteration 9500: Loss = -10638.675991965101
Iteration 9600: Loss = -10638.675963885365
Iteration 9700: Loss = -10638.675845799147
Iteration 9800: Loss = -10638.677118656962
1
Iteration 9900: Loss = -10638.676019687273
2
Iteration 10000: Loss = -10638.711636791875
3
Iteration 10100: Loss = -10638.675552792938
Iteration 10200: Loss = -10638.773103823898
1
Iteration 10300: Loss = -10638.675498578623
Iteration 10400: Loss = -10638.675568279486
Iteration 10500: Loss = -10638.675432770417
Iteration 10600: Loss = -10638.676589047289
1
Iteration 10700: Loss = -10638.675390732324
Iteration 10800: Loss = -10638.69077348474
1
Iteration 10900: Loss = -10638.675298671034
Iteration 11000: Loss = -10638.675219125646
Iteration 11100: Loss = -10638.703586691103
1
Iteration 11200: Loss = -10638.675082152784
Iteration 11300: Loss = -10638.675026216395
Iteration 11400: Loss = -10638.675084897945
Iteration 11500: Loss = -10638.674957201782
Iteration 11600: Loss = -10638.680258936833
1
Iteration 11700: Loss = -10638.674958496333
Iteration 11800: Loss = -10638.674909970681
Iteration 11900: Loss = -10638.678515415711
1
Iteration 12000: Loss = -10638.674875023291
Iteration 12100: Loss = -10638.67503701781
1
Iteration 12200: Loss = -10638.81227458068
2
Iteration 12300: Loss = -10638.674853174585
Iteration 12400: Loss = -10638.674853070208
Iteration 12500: Loss = -10638.731560839164
1
Iteration 12600: Loss = -10638.674823763962
Iteration 12700: Loss = -10638.791852155966
1
Iteration 12800: Loss = -10638.67479408701
Iteration 12900: Loss = -10638.854062651491
1
Iteration 13000: Loss = -10638.674763539575
Iteration 13100: Loss = -10638.676942000366
1
Iteration 13200: Loss = -10638.674684388225
Iteration 13300: Loss = -10638.67502404602
1
Iteration 13400: Loss = -10638.674682933175
Iteration 13500: Loss = -10638.674885596825
1
Iteration 13600: Loss = -10638.674690624155
Iteration 13700: Loss = -10638.698775864488
1
Iteration 13800: Loss = -10638.674649786668
Iteration 13900: Loss = -10638.681024609148
1
Iteration 14000: Loss = -10638.674632796408
Iteration 14100: Loss = -10638.822371738199
1
Iteration 14200: Loss = -10638.67465524088
Iteration 14300: Loss = -10638.674662445146
Iteration 14400: Loss = -10638.674699030422
Iteration 14500: Loss = -10638.674610331484
Iteration 14600: Loss = -10638.675063611629
1
Iteration 14700: Loss = -10638.674632719421
Iteration 14800: Loss = -10638.674730483557
Iteration 14900: Loss = -10638.674584608465
Iteration 15000: Loss = -10638.79914935443
1
Iteration 15100: Loss = -10638.674548044135
Iteration 15200: Loss = -10638.674514921382
Iteration 15300: Loss = -10638.674735378652
1
Iteration 15400: Loss = -10638.674507121901
Iteration 15500: Loss = -10638.684069756888
1
Iteration 15600: Loss = -10638.674551746708
Iteration 15700: Loss = -10638.675132843458
1
Iteration 15800: Loss = -10638.674533126192
Iteration 15900: Loss = -10638.683055571051
1
Iteration 16000: Loss = -10638.674518458185
Iteration 16100: Loss = -10638.674261749784
Iteration 16200: Loss = -10638.674480939499
1
Iteration 16300: Loss = -10638.674188857385
Iteration 16400: Loss = -10638.674158006444
Iteration 16500: Loss = -10638.674172080653
Iteration 16600: Loss = -10638.67415744944
Iteration 16700: Loss = -10638.674270670781
1
Iteration 16800: Loss = -10638.674121156899
Iteration 16900: Loss = -10638.67406921315
Iteration 17000: Loss = -10638.67976974454
1
Iteration 17100: Loss = -10638.674053036548
Iteration 17200: Loss = -10638.740557842259
1
Iteration 17300: Loss = -10638.674078459813
Iteration 17400: Loss = -10638.674064008228
Iteration 17500: Loss = -10638.673958549905
Iteration 17600: Loss = -10638.673976461143
Iteration 17700: Loss = -10638.697370097523
1
Iteration 17800: Loss = -10638.674026670424
Iteration 17900: Loss = -10638.674035408436
Iteration 18000: Loss = -10638.694563048213
1
Iteration 18100: Loss = -10638.673891578957
Iteration 18200: Loss = -10638.673878592768
Iteration 18300: Loss = -10638.681292378235
1
Iteration 18400: Loss = -10638.67388776953
Iteration 18500: Loss = -10638.676437373924
1
Iteration 18600: Loss = -10638.67451699704
2
Iteration 18700: Loss = -10638.72752949046
3
Iteration 18800: Loss = -10638.673840438041
Iteration 18900: Loss = -10638.821205077586
1
Iteration 19000: Loss = -10638.673719643524
Iteration 19100: Loss = -10638.67369055545
Iteration 19200: Loss = -10638.67447173013
1
Iteration 19300: Loss = -10638.67369858746
Iteration 19400: Loss = -10638.675526540854
1
Iteration 19500: Loss = -10638.679723847205
2
Iteration 19600: Loss = -10638.674264125066
3
Iteration 19700: Loss = -10638.673736915958
Iteration 19800: Loss = -10638.692898197765
1
Iteration 19900: Loss = -10638.673607845727
pi: tensor([[9.8068e-01, 1.9318e-02],
        [4.9061e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9933e-01, 6.7072e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1549, 0.1484],
         [0.5285, 0.2436]],

        [[0.6496, 0.2079],
         [0.6361, 0.7025]],

        [[0.5198, 0.0581],
         [0.6594, 0.6622]],

        [[0.5432, 0.1833],
         [0.7036, 0.6165]],

        [[0.5183, 0.1489],
         [0.6676, 0.7018]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.005431979218977636
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.021712907117008445
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.026272897264404446
Global Adjusted Rand Index: -0.003159406385520095
Average Adjusted Rand Index: 0.0015443228819187929
10645.842443808191
[-0.003159406385520095, -0.003159406385520095] [0.0015443228819187929, 0.0015443228819187929] [10638.678189526567, 10638.67506501873]
-------------------------------------
This iteration is 28
True Objective function: Loss = -10789.897841060876
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25687.537798878297
Iteration 100: Loss = -10901.561665614867
Iteration 200: Loss = -10899.444929374342
Iteration 300: Loss = -10897.59854074505
Iteration 400: Loss = -10896.648695906992
Iteration 500: Loss = -10895.549470078491
Iteration 600: Loss = -10894.947886094078
Iteration 700: Loss = -10894.752487489664
Iteration 800: Loss = -10894.672611459982
Iteration 900: Loss = -10894.62876570889
Iteration 1000: Loss = -10894.599048245658
Iteration 1100: Loss = -10894.576394728887
Iteration 1200: Loss = -10894.55808378007
Iteration 1300: Loss = -10894.542876746122
Iteration 1400: Loss = -10894.530011186482
Iteration 1500: Loss = -10894.519068299624
Iteration 1600: Loss = -10894.509761010824
Iteration 1700: Loss = -10894.501868296213
Iteration 1800: Loss = -10894.495062444212
Iteration 1900: Loss = -10894.489140051373
Iteration 2000: Loss = -10894.483857434225
Iteration 2100: Loss = -10894.478912506793
Iteration 2200: Loss = -10894.474134903581
Iteration 2300: Loss = -10894.469112179091
Iteration 2400: Loss = -10894.463279466987
Iteration 2500: Loss = -10894.455751950762
Iteration 2600: Loss = -10894.444253788235
Iteration 2700: Loss = -10894.42462333315
Iteration 2800: Loss = -10894.391646020893
Iteration 2900: Loss = -10894.346514686149
Iteration 3000: Loss = -10894.303090867075
Iteration 3100: Loss = -10894.27547433139
Iteration 3200: Loss = -10894.260923570964
Iteration 3300: Loss = -10894.253075726598
Iteration 3400: Loss = -10894.248708378951
Iteration 3500: Loss = -10894.246130854981
Iteration 3600: Loss = -10894.244495359848
Iteration 3700: Loss = -10894.243390136258
Iteration 3800: Loss = -10894.243018057141
Iteration 3900: Loss = -10894.241931026929
Iteration 4000: Loss = -10894.241537151054
Iteration 4100: Loss = -10894.241029667794
Iteration 4200: Loss = -10894.241794110972
1
Iteration 4300: Loss = -10894.240382086904
Iteration 4400: Loss = -10894.240109584693
Iteration 4500: Loss = -10894.240032179654
Iteration 4600: Loss = -10894.239659773732
Iteration 4700: Loss = -10894.239475012648
Iteration 4800: Loss = -10894.23928811746
Iteration 4900: Loss = -10894.239144834975
Iteration 5000: Loss = -10894.239889885943
1
Iteration 5100: Loss = -10894.238872774593
Iteration 5200: Loss = -10894.238867348167
Iteration 5300: Loss = -10894.238704393354
Iteration 5400: Loss = -10894.238638155617
Iteration 5500: Loss = -10894.23849318577
Iteration 5600: Loss = -10894.238410855756
Iteration 5700: Loss = -10894.23840325959
Iteration 5800: Loss = -10894.2382802541
Iteration 5900: Loss = -10894.239657539798
1
Iteration 6000: Loss = -10894.238153075303
Iteration 6100: Loss = -10894.23808337529
Iteration 6200: Loss = -10894.238112600766
Iteration 6300: Loss = -10894.23797350585
Iteration 6400: Loss = -10894.237990316942
Iteration 6500: Loss = -10894.237864728484
Iteration 6600: Loss = -10894.237892197765
Iteration 6700: Loss = -10894.237828360392
Iteration 6800: Loss = -10894.237754545435
Iteration 6900: Loss = -10894.238767700457
1
Iteration 7000: Loss = -10894.237700299678
Iteration 7100: Loss = -10894.240808663364
1
Iteration 7200: Loss = -10894.23766156587
Iteration 7300: Loss = -10894.237634401943
Iteration 7400: Loss = -10894.237613867937
Iteration 7500: Loss = -10894.237589256296
Iteration 7600: Loss = -10894.242320956739
1
Iteration 7700: Loss = -10894.237528619326
Iteration 7800: Loss = -10894.2375248151
Iteration 7900: Loss = -10894.237507886848
Iteration 8000: Loss = -10894.237467305404
Iteration 8100: Loss = -10894.23743932466
Iteration 8200: Loss = -10894.238096202702
1
Iteration 8300: Loss = -10894.257600955956
2
Iteration 8400: Loss = -10894.237393211655
Iteration 8500: Loss = -10894.237434466328
Iteration 8600: Loss = -10894.237378458847
Iteration 8700: Loss = -10894.237514906803
1
Iteration 8800: Loss = -10894.237362737555
Iteration 8900: Loss = -10894.23796295042
1
Iteration 9000: Loss = -10894.237350947787
Iteration 9100: Loss = -10894.237318362453
Iteration 9200: Loss = -10894.237256878685
Iteration 9300: Loss = -10894.237388407639
1
Iteration 9400: Loss = -10894.23731592739
Iteration 9500: Loss = -10894.237252880175
Iteration 9600: Loss = -10894.34381321787
1
Iteration 9700: Loss = -10894.237274120627
Iteration 9800: Loss = -10894.237255508986
Iteration 9900: Loss = -10894.357356652998
1
Iteration 10000: Loss = -10894.237220279609
Iteration 10100: Loss = -10894.237251348826
Iteration 10200: Loss = -10894.350506511737
1
Iteration 10300: Loss = -10894.237217827116
Iteration 10400: Loss = -10894.237275261963
Iteration 10500: Loss = -10894.257797981172
1
Iteration 10600: Loss = -10894.237240665225
Iteration 10700: Loss = -10894.237194786858
Iteration 10800: Loss = -10894.2620629128
1
Iteration 10900: Loss = -10894.237233388341
Iteration 11000: Loss = -10894.237392625962
1
Iteration 11100: Loss = -10894.23779201248
2
Iteration 11200: Loss = -10894.237214383747
Iteration 11300: Loss = -10894.237625996937
1
Iteration 11400: Loss = -10894.23744325735
2
Iteration 11500: Loss = -10894.237158896345
Iteration 11600: Loss = -10894.237196043434
Iteration 11700: Loss = -10894.237210582101
Iteration 11800: Loss = -10894.23717105416
Iteration 11900: Loss = -10894.238032683663
1
Iteration 12000: Loss = -10894.23730382106
2
Iteration 12100: Loss = -10894.238112702475
3
Iteration 12200: Loss = -10894.256145563982
4
Iteration 12300: Loss = -10894.335714980012
5
Iteration 12400: Loss = -10894.23719406734
Iteration 12500: Loss = -10894.238123086718
1
Iteration 12600: Loss = -10894.23715716355
Iteration 12700: Loss = -10894.237548121377
1
Iteration 12800: Loss = -10894.238360729774
2
Iteration 12900: Loss = -10894.272645736452
3
Iteration 13000: Loss = -10894.239862073811
4
Iteration 13100: Loss = -10894.237171274477
Iteration 13200: Loss = -10894.239387724941
1
Iteration 13300: Loss = -10894.238148687047
2
Iteration 13400: Loss = -10894.237181153028
Iteration 13500: Loss = -10894.257319637707
1
Iteration 13600: Loss = -10894.237142284082
Iteration 13700: Loss = -10894.237168880889
Iteration 13800: Loss = -10894.237145581648
Iteration 13900: Loss = -10894.243040386405
1
Iteration 14000: Loss = -10894.238313953862
2
Iteration 14100: Loss = -10894.237140699572
Iteration 14200: Loss = -10894.241873515037
1
Iteration 14300: Loss = -10894.237168229505
Iteration 14400: Loss = -10894.237916003929
1
Iteration 14500: Loss = -10894.237395527565
2
Iteration 14600: Loss = -10894.254538139608
3
Iteration 14700: Loss = -10894.237179502277
Iteration 14800: Loss = -10894.238045126636
1
Iteration 14900: Loss = -10894.237137150463
Iteration 15000: Loss = -10894.23737457491
1
Iteration 15100: Loss = -10894.237122457664
Iteration 15200: Loss = -10894.237276770195
1
Iteration 15300: Loss = -10894.252560139439
2
Iteration 15400: Loss = -10894.237257815108
3
Iteration 15500: Loss = -10894.243432672029
4
Iteration 15600: Loss = -10894.237202401164
Iteration 15700: Loss = -10894.287078063155
1
Iteration 15800: Loss = -10894.24704037803
2
Iteration 15900: Loss = -10894.238034371461
3
Iteration 16000: Loss = -10894.237379059301
4
Iteration 16100: Loss = -10894.240813698352
5
Iteration 16200: Loss = -10894.28851145454
6
Iteration 16300: Loss = -10894.237119303836
Iteration 16400: Loss = -10894.239066162256
1
Iteration 16500: Loss = -10894.257345248834
2
Iteration 16600: Loss = -10894.23753720394
3
Iteration 16700: Loss = -10894.237272972325
4
Iteration 16800: Loss = -10894.268287996205
5
Iteration 16900: Loss = -10894.239474307977
6
Iteration 17000: Loss = -10894.237161034293
Iteration 17100: Loss = -10894.239085188261
1
Iteration 17200: Loss = -10894.237229679604
Iteration 17300: Loss = -10894.237129279732
Iteration 17400: Loss = -10894.289318320134
1
Iteration 17500: Loss = -10894.24016450673
2
Iteration 17600: Loss = -10894.23770814868
3
Iteration 17700: Loss = -10894.237167966252
Iteration 17800: Loss = -10894.237370975095
1
Iteration 17900: Loss = -10894.237391635248
2
Iteration 18000: Loss = -10894.23867788439
3
Iteration 18100: Loss = -10894.239870378566
4
Iteration 18200: Loss = -10894.237586675277
5
Iteration 18300: Loss = -10894.2500025897
6
Iteration 18400: Loss = -10894.237110452965
Iteration 18500: Loss = -10894.268083013836
1
Iteration 18600: Loss = -10894.237141454336
Iteration 18700: Loss = -10894.529205641491
1
Iteration 18800: Loss = -10894.237284433704
2
Iteration 18900: Loss = -10894.23709937022
Iteration 19000: Loss = -10894.237278340224
1
Iteration 19100: Loss = -10894.237479145477
2
Iteration 19200: Loss = -10894.23715806093
Iteration 19300: Loss = -10894.23735940937
1
Iteration 19400: Loss = -10894.237301688216
2
Iteration 19500: Loss = -10894.245867198766
3
Iteration 19600: Loss = -10894.238011307816
4
Iteration 19700: Loss = -10894.237721439096
5
Iteration 19800: Loss = -10894.237406803533
6
Iteration 19900: Loss = -10894.237129852365
pi: tensor([[9.5170e-01, 4.8296e-02],
        [1.0000e+00, 2.5991e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9860, 0.0140], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1604, 0.2600],
         [0.6506, 0.1212]],

        [[0.5699, 0.0791],
         [0.6620, 0.5458]],

        [[0.5591, 0.1157],
         [0.7221, 0.5692]],

        [[0.6977, 0.2647],
         [0.6657, 0.6341]],

        [[0.6473, 0.1500],
         [0.6675, 0.6481]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007766707522784365
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.020905964803731424
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.002185903119278217
Average Adjusted Rand Index: -0.004493450138855653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22450.885415566605
Iteration 100: Loss = -10902.642337354959
Iteration 200: Loss = -10901.40623260698
Iteration 300: Loss = -10899.177963222715
Iteration 400: Loss = -10896.239912189842
Iteration 500: Loss = -10894.227117523405
Iteration 600: Loss = -10893.393966342375
Iteration 700: Loss = -10892.904634505652
Iteration 800: Loss = -10892.589554725098
Iteration 900: Loss = -10892.380999741172
Iteration 1000: Loss = -10892.256443650063
Iteration 1100: Loss = -10892.171034753092
Iteration 1200: Loss = -10892.10424058607
Iteration 1300: Loss = -10892.044082732822
Iteration 1400: Loss = -10891.982111755671
Iteration 1500: Loss = -10891.90445209429
Iteration 1600: Loss = -10891.753107543715
Iteration 1700: Loss = -10887.71735063902
Iteration 1800: Loss = -10818.266392979898
Iteration 1900: Loss = -10810.282195881106
Iteration 2000: Loss = -10809.896722606676
Iteration 2100: Loss = -10809.223814344226
Iteration 2200: Loss = -10809.000478755961
Iteration 2300: Loss = -10808.48824494366
Iteration 2400: Loss = -10802.429275911201
Iteration 2500: Loss = -10790.973897542906
Iteration 2600: Loss = -10789.313724128735
Iteration 2700: Loss = -10789.023573314338
Iteration 2800: Loss = -10785.627325153404
Iteration 2900: Loss = -10782.516893372805
Iteration 3000: Loss = -10781.321262391062
Iteration 3100: Loss = -10781.302621162476
Iteration 3200: Loss = -10781.293731221018
Iteration 3300: Loss = -10781.287686063308
Iteration 3400: Loss = -10781.283391019613
Iteration 3500: Loss = -10781.280912030496
Iteration 3600: Loss = -10781.277572491925
Iteration 3700: Loss = -10781.277924664626
1
Iteration 3800: Loss = -10781.27487922251
Iteration 3900: Loss = -10781.273637221215
Iteration 4000: Loss = -10781.272627787748
Iteration 4100: Loss = -10781.27131959612
Iteration 4200: Loss = -10781.270694654335
Iteration 4300: Loss = -10781.26749397621
Iteration 4400: Loss = -10781.266588324419
Iteration 4500: Loss = -10781.264539411577
Iteration 4600: Loss = -10781.262860081713
Iteration 4700: Loss = -10781.261029947562
Iteration 4800: Loss = -10781.260123954085
Iteration 4900: Loss = -10781.25987598594
Iteration 5000: Loss = -10781.26109362628
1
Iteration 5100: Loss = -10781.260330253028
2
Iteration 5200: Loss = -10781.263188923254
3
Iteration 5300: Loss = -10781.259212549066
Iteration 5400: Loss = -10781.259055875942
Iteration 5500: Loss = -10781.259151307018
Iteration 5600: Loss = -10781.258560803784
Iteration 5700: Loss = -10781.265765335322
1
Iteration 5800: Loss = -10781.25824651593
Iteration 5900: Loss = -10781.259028354652
1
Iteration 6000: Loss = -10781.259098265786
2
Iteration 6100: Loss = -10781.258103407881
Iteration 6200: Loss = -10781.258715352551
1
Iteration 6300: Loss = -10781.25884615793
2
Iteration 6400: Loss = -10781.257607621961
Iteration 6500: Loss = -10781.257523659306
Iteration 6600: Loss = -10781.257433605471
Iteration 6700: Loss = -10781.258362967987
1
Iteration 6800: Loss = -10781.257291311696
Iteration 6900: Loss = -10781.257585259322
1
Iteration 7000: Loss = -10781.261998012793
2
Iteration 7100: Loss = -10781.257782425448
3
Iteration 7200: Loss = -10781.256999525594
Iteration 7300: Loss = -10781.257124043379
1
Iteration 7400: Loss = -10781.256919572812
Iteration 7500: Loss = -10781.256995654841
Iteration 7600: Loss = -10781.256861583264
Iteration 7700: Loss = -10781.268388862947
1
Iteration 7800: Loss = -10781.256798100017
Iteration 7900: Loss = -10781.25679046685
Iteration 8000: Loss = -10781.256877689008
Iteration 8100: Loss = -10781.25670493614
Iteration 8200: Loss = -10781.299173348996
1
Iteration 8300: Loss = -10781.25667240271
Iteration 8400: Loss = -10781.256620594657
Iteration 8500: Loss = -10781.339586935006
1
Iteration 8600: Loss = -10781.256618953765
Iteration 8700: Loss = -10781.256597987924
Iteration 8800: Loss = -10781.256590791547
Iteration 8900: Loss = -10781.256570270112
Iteration 9000: Loss = -10781.256576201926
Iteration 9100: Loss = -10781.342147015903
1
Iteration 9200: Loss = -10781.256515713594
Iteration 9300: Loss = -10781.256508023016
Iteration 9400: Loss = -10781.269480568026
1
Iteration 9500: Loss = -10781.256484020876
Iteration 9600: Loss = -10781.299509747238
1
Iteration 9700: Loss = -10781.25646496587
Iteration 9800: Loss = -10781.381784917223
1
Iteration 9900: Loss = -10781.256427756656
Iteration 10000: Loss = -10781.262472480066
1
Iteration 10100: Loss = -10781.256404825568
Iteration 10200: Loss = -10781.315880902866
1
Iteration 10300: Loss = -10781.256359445926
Iteration 10400: Loss = -10781.256359970183
Iteration 10500: Loss = -10781.256428432876
Iteration 10600: Loss = -10781.256308347805
Iteration 10700: Loss = -10781.264453081782
1
Iteration 10800: Loss = -10781.256240366904
Iteration 10900: Loss = -10781.257712862167
1
Iteration 11000: Loss = -10781.256252318748
Iteration 11100: Loss = -10781.256223509054
Iteration 11200: Loss = -10781.265304218658
1
Iteration 11300: Loss = -10781.25617040485
Iteration 11400: Loss = -10781.256197285227
Iteration 11500: Loss = -10781.256837854335
1
Iteration 11600: Loss = -10781.256148141272
Iteration 11700: Loss = -10781.292632227918
1
Iteration 11800: Loss = -10781.255980168424
Iteration 11900: Loss = -10781.25593085515
Iteration 12000: Loss = -10781.306535284219
1
Iteration 12100: Loss = -10781.255907353157
Iteration 12200: Loss = -10781.255919123963
Iteration 12300: Loss = -10781.255994098601
Iteration 12400: Loss = -10781.447986032777
1
Iteration 12500: Loss = -10781.255919141699
Iteration 12600: Loss = -10781.256231028907
1
Iteration 12700: Loss = -10781.256472268946
2
Iteration 12800: Loss = -10781.45399068239
3
Iteration 12900: Loss = -10781.255736018018
Iteration 13000: Loss = -10781.330273968832
1
Iteration 13100: Loss = -10781.255631781241
Iteration 13200: Loss = -10781.255572782182
Iteration 13300: Loss = -10781.255693547657
1
Iteration 13400: Loss = -10781.255557104902
Iteration 13500: Loss = -10781.261162436496
1
Iteration 13600: Loss = -10781.255547932577
Iteration 13700: Loss = -10781.255556221982
Iteration 13800: Loss = -10781.255551302365
Iteration 13900: Loss = -10781.255545273658
Iteration 14000: Loss = -10781.27274001248
1
Iteration 14100: Loss = -10781.255513405467
Iteration 14200: Loss = -10781.255515299224
Iteration 14300: Loss = -10781.26103917874
1
Iteration 14400: Loss = -10781.255512013455
Iteration 14500: Loss = -10781.255513665206
Iteration 14600: Loss = -10781.266425529935
1
Iteration 14700: Loss = -10781.255511902595
Iteration 14800: Loss = -10781.255511877043
Iteration 14900: Loss = -10781.256615618224
1
Iteration 15000: Loss = -10781.25553632059
Iteration 15100: Loss = -10781.293281322567
1
Iteration 15200: Loss = -10781.25553817349
Iteration 15300: Loss = -10781.256810280282
1
Iteration 15400: Loss = -10781.25739496391
2
Iteration 15500: Loss = -10781.260775471543
3
Iteration 15600: Loss = -10781.256914174537
4
Iteration 15700: Loss = -10781.255597192454
Iteration 15800: Loss = -10781.255748233732
1
Iteration 15900: Loss = -10781.31364990606
2
Iteration 16000: Loss = -10781.255537931931
Iteration 16100: Loss = -10781.485587449368
1
Iteration 16200: Loss = -10781.255503271754
Iteration 16300: Loss = -10781.281231835837
1
Iteration 16400: Loss = -10781.255392878385
Iteration 16500: Loss = -10781.296661659297
1
Iteration 16600: Loss = -10781.255396249187
Iteration 16700: Loss = -10781.256473639465
1
Iteration 16800: Loss = -10781.255553081366
2
Iteration 16900: Loss = -10781.255564463441
3
Iteration 17000: Loss = -10781.264134321995
4
Iteration 17100: Loss = -10781.283643772738
5
Iteration 17200: Loss = -10781.388465756294
6
Iteration 17300: Loss = -10781.267143337189
7
Iteration 17400: Loss = -10781.25539714214
Iteration 17500: Loss = -10781.256370601892
1
Iteration 17600: Loss = -10781.255345256652
Iteration 17700: Loss = -10781.25993737596
1
Iteration 17800: Loss = -10781.255390773227
Iteration 17900: Loss = -10781.257777516079
1
Iteration 18000: Loss = -10781.255378560061
Iteration 18100: Loss = -10781.260891706244
1
Iteration 18200: Loss = -10781.255382708547
Iteration 18300: Loss = -10781.342499742283
1
Iteration 18400: Loss = -10781.255389256052
Iteration 18500: Loss = -10781.25541513299
Iteration 18600: Loss = -10781.265027139518
1
Iteration 18700: Loss = -10781.255379378077
Iteration 18800: Loss = -10781.255401148492
Iteration 18900: Loss = -10781.255504228493
1
Iteration 19000: Loss = -10781.255389407033
Iteration 19100: Loss = -10781.389927582997
1
Iteration 19200: Loss = -10781.255369481292
Iteration 19300: Loss = -10781.274217434993
1
Iteration 19400: Loss = -10781.291382721349
2
Iteration 19500: Loss = -10781.282182809651
3
Iteration 19600: Loss = -10781.255776632142
4
Iteration 19700: Loss = -10781.255457721954
Iteration 19800: Loss = -10781.279660503164
1
Iteration 19900: Loss = -10781.255918278002
2
pi: tensor([[0.6006, 0.3994],
        [0.4923, 0.5077]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3778, 0.6222], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2390, 0.0919],
         [0.6692, 0.2110]],

        [[0.5569, 0.0925],
         [0.6640, 0.5717]],

        [[0.6179, 0.0907],
         [0.6641, 0.6813]],

        [[0.5092, 0.1024],
         [0.7234, 0.5423]],

        [[0.5965, 0.0915],
         [0.6903, 0.6449]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.573573917525319
time is 1
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080682750429576
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720646154931399
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.3304381816734408
Average Adjusted Rand Index: 0.7765189299541169
10789.897841060876
[-0.002185903119278217, 0.3304381816734408] [-0.004493450138855653, 0.7765189299541169] [10894.237418000785, 10781.255440869945]
-------------------------------------
This iteration is 29
True Objective function: Loss = -10748.725338403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21053.89961843176
Iteration 100: Loss = -10845.804018336976
Iteration 200: Loss = -10845.208944959359
Iteration 300: Loss = -10845.073004450085
Iteration 400: Loss = -10844.972394960965
Iteration 500: Loss = -10844.869039454154
Iteration 600: Loss = -10844.789143350023
Iteration 700: Loss = -10844.740878414204
Iteration 800: Loss = -10844.707282925723
Iteration 900: Loss = -10844.684365724057
Iteration 1000: Loss = -10844.669221461996
Iteration 1100: Loss = -10844.658983962907
Iteration 1200: Loss = -10844.6511957089
Iteration 1300: Loss = -10844.644744317617
Iteration 1400: Loss = -10844.638847028276
Iteration 1500: Loss = -10844.633239410192
Iteration 1600: Loss = -10844.627609358147
Iteration 1700: Loss = -10844.62180243374
Iteration 1800: Loss = -10844.615497571573
Iteration 1900: Loss = -10844.608563881206
Iteration 2000: Loss = -10844.601084468675
Iteration 2100: Loss = -10844.594000706951
Iteration 2200: Loss = -10844.588072327268
Iteration 2300: Loss = -10844.583455328877
Iteration 2400: Loss = -10844.579973951833
Iteration 2500: Loss = -10844.577315910627
Iteration 2600: Loss = -10844.575306223927
Iteration 2700: Loss = -10844.573809089477
Iteration 2800: Loss = -10844.572667791517
Iteration 2900: Loss = -10844.571722508508
Iteration 3000: Loss = -10844.571009298357
Iteration 3100: Loss = -10844.570369939076
Iteration 3200: Loss = -10844.569813953609
Iteration 3300: Loss = -10844.56935843558
Iteration 3400: Loss = -10844.568941749878
Iteration 3500: Loss = -10844.568591889378
Iteration 3600: Loss = -10844.56826406531
Iteration 3700: Loss = -10844.567954471362
Iteration 3800: Loss = -10844.567702280181
Iteration 3900: Loss = -10844.56743956903
Iteration 4000: Loss = -10844.567148290931
Iteration 4100: Loss = -10844.566782757824
Iteration 4200: Loss = -10844.566301239507
Iteration 4300: Loss = -10844.565445100943
Iteration 4400: Loss = -10844.56395920411
Iteration 4500: Loss = -10844.56149092496
Iteration 4600: Loss = -10844.560245406286
Iteration 4700: Loss = -10844.55476424247
Iteration 4800: Loss = -10844.552047241432
Iteration 4900: Loss = -10844.549896832697
Iteration 5000: Loss = -10844.548307706078
Iteration 5100: Loss = -10844.557986788757
1
Iteration 5200: Loss = -10844.54612005481
Iteration 5300: Loss = -10844.545405615432
Iteration 5400: Loss = -10844.544788591764
Iteration 5500: Loss = -10844.544387904154
Iteration 5600: Loss = -10844.543981952673
Iteration 5700: Loss = -10844.54372873184
Iteration 5800: Loss = -10844.544857508963
1
Iteration 5900: Loss = -10844.543223862109
Iteration 6000: Loss = -10844.549505063662
1
Iteration 6100: Loss = -10844.542862364558
Iteration 6200: Loss = -10844.54273309456
Iteration 6300: Loss = -10844.542612679385
Iteration 6400: Loss = -10844.54249725945
Iteration 6500: Loss = -10844.54284286934
1
Iteration 6600: Loss = -10844.542312170715
Iteration 6700: Loss = -10844.542467292447
1
Iteration 6800: Loss = -10844.54216083216
Iteration 6900: Loss = -10844.542194202488
Iteration 7000: Loss = -10844.542071552054
Iteration 7100: Loss = -10844.542014229371
Iteration 7200: Loss = -10844.55792061652
1
Iteration 7300: Loss = -10844.541912514442
Iteration 7400: Loss = -10844.543754444567
1
Iteration 7500: Loss = -10844.541808750942
Iteration 7600: Loss = -10844.565685618674
1
Iteration 7700: Loss = -10844.542593964044
2
Iteration 7800: Loss = -10844.541833254227
Iteration 7900: Loss = -10844.768544332434
1
Iteration 8000: Loss = -10844.541959239425
2
Iteration 8100: Loss = -10844.552491081864
3
Iteration 8200: Loss = -10844.541631343038
Iteration 8300: Loss = -10844.541654627146
Iteration 8400: Loss = -10844.541680410754
Iteration 8500: Loss = -10844.54160148507
Iteration 8600: Loss = -10844.541634982157
Iteration 8700: Loss = -10844.654268107277
1
Iteration 8800: Loss = -10844.541537039682
Iteration 8900: Loss = -10844.812059816906
1
Iteration 9000: Loss = -10844.541558728864
Iteration 9100: Loss = -10844.547289473196
1
Iteration 9200: Loss = -10844.5415646441
Iteration 9300: Loss = -10844.78314149105
1
Iteration 9400: Loss = -10844.541444366007
Iteration 9500: Loss = -10844.541765086748
1
Iteration 9600: Loss = -10844.54153078228
Iteration 9700: Loss = -10844.541428844623
Iteration 9800: Loss = -10844.54144090552
Iteration 9900: Loss = -10844.541462645584
Iteration 10000: Loss = -10844.541448628828
Iteration 10100: Loss = -10844.541492292612
Iteration 10200: Loss = -10844.54142161102
Iteration 10300: Loss = -10844.541675488266
1
Iteration 10400: Loss = -10844.541473139072
Iteration 10500: Loss = -10844.5413790487
Iteration 10600: Loss = -10844.541644903684
1
Iteration 10700: Loss = -10844.541406462695
Iteration 10800: Loss = -10844.564100983898
1
Iteration 10900: Loss = -10844.541401472092
Iteration 11000: Loss = -10844.543711372185
1
Iteration 11100: Loss = -10844.541359560859
Iteration 11200: Loss = -10844.541374393153
Iteration 11300: Loss = -10844.541484840332
1
Iteration 11400: Loss = -10844.54134628726
Iteration 11500: Loss = -10844.541895524713
1
Iteration 11600: Loss = -10844.54137267989
Iteration 11700: Loss = -10844.569388024449
1
Iteration 11800: Loss = -10844.541389711743
Iteration 11900: Loss = -10844.541373249114
Iteration 12000: Loss = -10844.553647583063
1
Iteration 12100: Loss = -10844.541350328294
Iteration 12200: Loss = -10844.541358485947
Iteration 12300: Loss = -10844.542099859045
1
Iteration 12400: Loss = -10844.541345431478
Iteration 12500: Loss = -10844.541444166065
Iteration 12600: Loss = -10844.541377075615
Iteration 12700: Loss = -10844.54132824278
Iteration 12800: Loss = -10844.607994917016
1
Iteration 12900: Loss = -10844.541354472607
Iteration 13000: Loss = -10844.541342430937
Iteration 13100: Loss = -10844.544545639908
1
Iteration 13200: Loss = -10844.541309514285
Iteration 13300: Loss = -10844.541314485125
Iteration 13400: Loss = -10844.542753142583
1
Iteration 13500: Loss = -10844.54129968775
Iteration 13600: Loss = -10844.541801853002
1
Iteration 13700: Loss = -10844.541308922395
Iteration 13800: Loss = -10844.54185007792
1
Iteration 13900: Loss = -10844.542974811387
2
Iteration 14000: Loss = -10844.541456785026
3
Iteration 14100: Loss = -10844.554250001887
4
Iteration 14200: Loss = -10844.541515839519
5
Iteration 14300: Loss = -10844.541532906009
6
Iteration 14400: Loss = -10844.543354538691
7
Iteration 14500: Loss = -10844.547459749025
8
Iteration 14600: Loss = -10844.541464185028
9
Iteration 14700: Loss = -10844.541386366283
Iteration 14800: Loss = -10844.575366101439
1
Iteration 14900: Loss = -10844.541335363743
Iteration 15000: Loss = -10844.604749871136
1
Iteration 15100: Loss = -10844.541311008863
Iteration 15200: Loss = -10844.544820724108
1
Iteration 15300: Loss = -10844.541989457057
2
Iteration 15400: Loss = -10844.546126111294
3
Iteration 15500: Loss = -10844.547850066947
4
Iteration 15600: Loss = -10844.596547291485
5
Iteration 15700: Loss = -10844.54130514278
Iteration 15800: Loss = -10844.543917577135
1
Iteration 15900: Loss = -10844.546620704801
2
Iteration 16000: Loss = -10844.541344294114
Iteration 16100: Loss = -10844.562148053807
1
Iteration 16200: Loss = -10844.541360146919
Iteration 16300: Loss = -10844.639978844476
1
Iteration 16400: Loss = -10844.54133132507
Iteration 16500: Loss = -10844.544430153228
1
Iteration 16600: Loss = -10844.553801253627
2
Iteration 16700: Loss = -10844.54131980948
Iteration 16800: Loss = -10844.551957463043
1
Iteration 16900: Loss = -10844.54130876393
Iteration 17000: Loss = -10844.585449219736
1
Iteration 17100: Loss = -10844.541322077002
Iteration 17200: Loss = -10844.541429105771
1
Iteration 17300: Loss = -10844.541446248868
2
Iteration 17400: Loss = -10844.541406744242
Iteration 17500: Loss = -10844.558657249181
1
Iteration 17600: Loss = -10844.54215298795
2
Iteration 17700: Loss = -10844.541361197042
Iteration 17800: Loss = -10844.558902634852
1
Iteration 17900: Loss = -10844.543090358993
2
Iteration 18000: Loss = -10844.542340369166
3
Iteration 18100: Loss = -10844.71894271012
4
Iteration 18200: Loss = -10844.543744080822
5
Iteration 18300: Loss = -10844.54218489206
6
Iteration 18400: Loss = -10844.543075137031
7
Iteration 18500: Loss = -10844.541502450986
8
Iteration 18600: Loss = -10844.802888018103
9
Iteration 18700: Loss = -10844.54145365805
Iteration 18800: Loss = -10844.548296450672
1
Iteration 18900: Loss = -10844.547621003228
2
Iteration 19000: Loss = -10844.54134204982
Iteration 19100: Loss = -10844.541517222906
1
Iteration 19200: Loss = -10844.571850231907
2
Iteration 19300: Loss = -10844.541279803776
Iteration 19400: Loss = -10844.541794483946
1
Iteration 19500: Loss = -10844.541514329609
2
Iteration 19600: Loss = -10844.541356706526
Iteration 19700: Loss = -10844.633568085532
1
Iteration 19800: Loss = -10844.543125163276
2
Iteration 19900: Loss = -10844.541329062251
pi: tensor([[1.0000e+00, 2.9600e-08],
        [8.2679e-01, 1.7321e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0254, 0.9746], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1605, 0.2309],
         [0.6191, 0.1533]],

        [[0.6244, 0.1532],
         [0.6992, 0.7000]],

        [[0.7226, 0.1001],
         [0.6672, 0.5913]],

        [[0.6511, 0.1709],
         [0.7293, 0.5490]],

        [[0.5124, 0.2099],
         [0.6410, 0.6561]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017241414101251092
Average Adjusted Rand Index: 0.00023166188598777618
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24327.531475635584
Iteration 100: Loss = -10847.05623380287
Iteration 200: Loss = -10845.780057574135
Iteration 300: Loss = -10845.527608325416
Iteration 400: Loss = -10845.418407547058
Iteration 500: Loss = -10845.358536390002
Iteration 600: Loss = -10845.319475412249
Iteration 700: Loss = -10845.290301249732
Iteration 800: Loss = -10845.265187072997
Iteration 900: Loss = -10845.238925308748
Iteration 1000: Loss = -10845.202883666147
Iteration 1100: Loss = -10845.13361031331
Iteration 1200: Loss = -10845.007737524107
Iteration 1300: Loss = -10844.910765050952
Iteration 1400: Loss = -10844.83323555114
Iteration 1500: Loss = -10844.757701692997
Iteration 1600: Loss = -10844.684029524578
Iteration 1700: Loss = -10844.611144830726
Iteration 1800: Loss = -10844.535071921207
Iteration 1900: Loss = -10844.44948929633
Iteration 2000: Loss = -10844.354311123581
Iteration 2100: Loss = -10844.268154324696
Iteration 2200: Loss = -10844.201636248166
Iteration 2300: Loss = -10844.15056444362
Iteration 2400: Loss = -10844.113282721124
Iteration 2500: Loss = -10844.084587496774
Iteration 2600: Loss = -10844.061681719939
Iteration 2700: Loss = -10844.043081037427
Iteration 2800: Loss = -10844.02787992095
Iteration 2900: Loss = -10844.015277638407
Iteration 3000: Loss = -10844.00456650951
Iteration 3100: Loss = -10843.995372556195
Iteration 3200: Loss = -10843.987429864665
Iteration 3300: Loss = -10843.98039143266
Iteration 3400: Loss = -10843.974181028747
Iteration 3500: Loss = -10843.96874979407
Iteration 3600: Loss = -10843.963828187621
Iteration 3700: Loss = -10843.959372580006
Iteration 3800: Loss = -10843.955375295267
Iteration 3900: Loss = -10843.95165133294
Iteration 4000: Loss = -10843.948304426742
Iteration 4100: Loss = -10843.945120944138
Iteration 4200: Loss = -10843.942179957936
Iteration 4300: Loss = -10843.939390241827
Iteration 4400: Loss = -10843.93674266217
Iteration 4500: Loss = -10843.934319036498
Iteration 4600: Loss = -10843.931915096
Iteration 4700: Loss = -10843.92963529069
Iteration 4800: Loss = -10843.927384067427
Iteration 4900: Loss = -10843.92516779207
Iteration 5000: Loss = -10843.92304751405
Iteration 5100: Loss = -10843.920937758965
Iteration 5200: Loss = -10843.918787571653
Iteration 5300: Loss = -10843.916649403673
Iteration 5400: Loss = -10843.914541781236
Iteration 5500: Loss = -10843.912392822494
Iteration 5600: Loss = -10843.910204294692
Iteration 5700: Loss = -10843.907967272171
Iteration 5800: Loss = -10843.905684295321
Iteration 5900: Loss = -10843.903381239197
Iteration 6000: Loss = -10843.900948288487
Iteration 6100: Loss = -10843.898387288576
Iteration 6200: Loss = -10843.89577179643
Iteration 6300: Loss = -10843.892982760644
Iteration 6400: Loss = -10843.889975633805
Iteration 6500: Loss = -10843.886718257183
Iteration 6600: Loss = -10843.883122573725
Iteration 6700: Loss = -10843.879060975582
Iteration 6800: Loss = -10843.874197342855
Iteration 6900: Loss = -10843.867722512072
Iteration 7000: Loss = -10843.857274590457
Iteration 7100: Loss = -10843.838810982343
Iteration 7200: Loss = -10843.817985704383
Iteration 7300: Loss = -10843.799929189761
Iteration 7400: Loss = -10843.785163749088
Iteration 7500: Loss = -10843.673397895749
Iteration 7600: Loss = -10843.665176294115
Iteration 7700: Loss = -10843.662752281183
Iteration 7800: Loss = -10843.66157613794
Iteration 7900: Loss = -10843.660706695628
Iteration 8000: Loss = -10843.660054263928
Iteration 8100: Loss = -10843.659587579436
Iteration 8200: Loss = -10843.659174593986
Iteration 8300: Loss = -10843.658673937172
Iteration 8400: Loss = -10843.651084660967
Iteration 8500: Loss = -10843.660817813083
1
Iteration 8600: Loss = -10843.64654628506
Iteration 8700: Loss = -10843.697328046648
1
Iteration 8800: Loss = -10843.643911922843
Iteration 8900: Loss = -10843.644283831329
1
Iteration 9000: Loss = -10843.643041886422
Iteration 9100: Loss = -10843.642909609347
Iteration 9200: Loss = -10843.642928231327
Iteration 9300: Loss = -10843.64236207027
Iteration 9400: Loss = -10843.642283839565
Iteration 9500: Loss = -10843.64427982765
1
Iteration 9600: Loss = -10843.642077514392
Iteration 9700: Loss = -10843.642010630694
Iteration 9800: Loss = -10843.667775872691
1
Iteration 9900: Loss = -10843.641873707664
Iteration 10000: Loss = -10843.641776359626
Iteration 10100: Loss = -10843.641795128493
Iteration 10200: Loss = -10843.641997495062
1
Iteration 10300: Loss = -10843.641657269582
Iteration 10400: Loss = -10843.64162010892
Iteration 10500: Loss = -10843.669924295582
1
Iteration 10600: Loss = -10843.641607091082
Iteration 10700: Loss = -10843.64150720953
Iteration 10800: Loss = -10843.651275476552
1
Iteration 10900: Loss = -10843.641482927242
Iteration 11000: Loss = -10843.64143531872
Iteration 11100: Loss = -10843.641424050064
Iteration 11200: Loss = -10843.641444068884
Iteration 11300: Loss = -10843.64137879756
Iteration 11400: Loss = -10843.641352688883
Iteration 11500: Loss = -10843.659995931519
1
Iteration 11600: Loss = -10843.641211363982
Iteration 11700: Loss = -10843.639947898891
Iteration 11800: Loss = -10843.70613868129
1
Iteration 11900: Loss = -10843.639837822684
Iteration 12000: Loss = -10843.639809966206
Iteration 12100: Loss = -10843.980485245615
1
Iteration 12200: Loss = -10843.639780410156
Iteration 12300: Loss = -10843.639741640693
Iteration 12400: Loss = -10843.639573237198
Iteration 12500: Loss = -10843.63897798499
Iteration 12600: Loss = -10843.638620585973
Iteration 12700: Loss = -10843.638581242427
Iteration 12800: Loss = -10843.639087411193
1
Iteration 12900: Loss = -10843.63852854929
Iteration 13000: Loss = -10843.63884139432
1
Iteration 13100: Loss = -10843.638263939158
Iteration 13200: Loss = -10843.638175468777
Iteration 13300: Loss = -10843.638198162322
Iteration 13400: Loss = -10843.638230574541
Iteration 13500: Loss = -10843.638237722562
Iteration 13600: Loss = -10843.833932331654
1
Iteration 13700: Loss = -10843.638130004338
Iteration 13800: Loss = -10843.642958979759
1
Iteration 13900: Loss = -10843.638121527376
Iteration 14000: Loss = -10843.6388825139
1
Iteration 14100: Loss = -10843.638458677864
2
Iteration 14200: Loss = -10843.643111587131
3
Iteration 14300: Loss = -10843.638158464355
Iteration 14400: Loss = -10843.65449880124
1
Iteration 14500: Loss = -10843.638080697201
Iteration 14600: Loss = -10843.7055903426
1
Iteration 14700: Loss = -10843.638044312629
Iteration 14800: Loss = -10843.638137750786
Iteration 14900: Loss = -10843.63646533848
Iteration 15000: Loss = -10843.636403963066
Iteration 15100: Loss = -10843.637323433675
1
Iteration 15200: Loss = -10843.63958043443
2
Iteration 15300: Loss = -10843.636414263143
Iteration 15400: Loss = -10843.757517639797
1
Iteration 15500: Loss = -10843.636386489954
Iteration 15600: Loss = -10843.658837005276
1
Iteration 15700: Loss = -10843.636419694658
Iteration 15800: Loss = -10843.821758109214
1
Iteration 15900: Loss = -10843.635619480568
Iteration 16000: Loss = -10843.716010411403
1
Iteration 16100: Loss = -10843.63846333968
2
Iteration 16200: Loss = -10843.635620344823
Iteration 16300: Loss = -10843.636139182758
1
Iteration 16400: Loss = -10843.654689770765
2
Iteration 16500: Loss = -10843.635383055534
Iteration 16600: Loss = -10843.641655859796
1
Iteration 16700: Loss = -10843.63546091007
Iteration 16800: Loss = -10843.63542741805
Iteration 16900: Loss = -10843.699841175829
1
Iteration 17000: Loss = -10843.635400076131
Iteration 17100: Loss = -10843.63634597932
1
Iteration 17200: Loss = -10843.63534301879
Iteration 17300: Loss = -10843.671303814344
1
Iteration 17400: Loss = -10843.635395876163
Iteration 17500: Loss = -10843.635374764735
Iteration 17600: Loss = -10843.635565905726
1
Iteration 17700: Loss = -10843.635775637653
2
Iteration 17800: Loss = -10843.726396495504
3
Iteration 17900: Loss = -10843.635749448278
4
Iteration 18000: Loss = -10843.638799614002
5
Iteration 18100: Loss = -10843.635419735065
Iteration 18200: Loss = -10843.636808249235
1
Iteration 18300: Loss = -10843.635346403225
Iteration 18400: Loss = -10843.636128034113
1
Iteration 18500: Loss = -10843.635359002828
Iteration 18600: Loss = -10843.635340329849
Iteration 18700: Loss = -10843.635355932121
Iteration 18800: Loss = -10843.635322995675
Iteration 18900: Loss = -10843.636255794512
1
Iteration 19000: Loss = -10843.635340963543
Iteration 19100: Loss = -10843.636869007043
1
Iteration 19200: Loss = -10843.691445407256
2
Iteration 19300: Loss = -10843.63607032588
3
Iteration 19400: Loss = -10843.635495264367
4
Iteration 19500: Loss = -10843.646008703936
5
Iteration 19600: Loss = -10843.635314433892
Iteration 19700: Loss = -10843.636094940532
1
Iteration 19800: Loss = -10843.64056850308
2
Iteration 19900: Loss = -10843.635355801518
pi: tensor([[1.0000e+00, 1.8164e-08],
        [5.8921e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9673, 0.0327], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1575, 0.2002],
         [0.6339, 0.0324]],

        [[0.5006, 0.1385],
         [0.6885, 0.5587]],

        [[0.5965, 0.2200],
         [0.6086, 0.6777]],

        [[0.6419, 0.2159],
         [0.7062, 0.7180]],

        [[0.6969, 0.1981],
         [0.5210, 0.5540]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007766707522784365
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0003612375232548945
Average Adjusted Rand Index: -0.0016372487988051514
10748.725338403518
[-0.0017241414101251092, 0.0003612375232548945] [0.00023166188598777618, -0.0016372487988051514] [10844.553764700817, 10843.777747342981]
-------------------------------------
This iteration is 30
True Objective function: Loss = -10761.120887263682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20509.078542800038
Iteration 100: Loss = -10858.714750414434
Iteration 200: Loss = -10858.011635980036
Iteration 300: Loss = -10857.384771885163
Iteration 400: Loss = -10856.753808370644
Iteration 500: Loss = -10856.413212883428
Iteration 600: Loss = -10856.196332709938
Iteration 700: Loss = -10856.018127954878
Iteration 800: Loss = -10855.862152566599
Iteration 900: Loss = -10855.644049283816
Iteration 1000: Loss = -10854.897902978591
Iteration 1100: Loss = -10853.210456976984
Iteration 1200: Loss = -10852.797810265525
Iteration 1300: Loss = -10852.636583499132
Iteration 1400: Loss = -10852.552190904793
Iteration 1500: Loss = -10852.50019508131
Iteration 1600: Loss = -10852.464204374464
Iteration 1700: Loss = -10852.439804611848
Iteration 1800: Loss = -10852.420921708193
Iteration 1900: Loss = -10852.405854034685
Iteration 2000: Loss = -10852.393658722282
Iteration 2100: Loss = -10852.38362370958
Iteration 2200: Loss = -10852.375111662364
Iteration 2300: Loss = -10852.367903953927
Iteration 2400: Loss = -10852.3618733252
Iteration 2500: Loss = -10852.35669969272
Iteration 2600: Loss = -10852.352239648595
Iteration 2700: Loss = -10852.348334454022
Iteration 2800: Loss = -10852.34486958116
Iteration 2900: Loss = -10852.34175426469
Iteration 3000: Loss = -10852.33902826541
Iteration 3100: Loss = -10852.336547743318
Iteration 3200: Loss = -10852.334314035767
Iteration 3300: Loss = -10852.332312040715
Iteration 3400: Loss = -10852.330490933537
Iteration 3500: Loss = -10852.328842089462
Iteration 3600: Loss = -10852.327286600772
Iteration 3700: Loss = -10852.325952686939
Iteration 3800: Loss = -10852.324681968434
Iteration 3900: Loss = -10852.323521432914
Iteration 4000: Loss = -10852.322463413098
Iteration 4100: Loss = -10852.321450698499
Iteration 4200: Loss = -10852.320567934603
Iteration 4300: Loss = -10852.319709729094
Iteration 4400: Loss = -10852.318929114166
Iteration 4500: Loss = -10852.318244378008
Iteration 4600: Loss = -10852.317726161458
Iteration 4700: Loss = -10852.316888925177
Iteration 4800: Loss = -10852.31633157414
Iteration 4900: Loss = -10852.315804778415
Iteration 5000: Loss = -10852.315286398578
Iteration 5100: Loss = -10852.31542371907
1
Iteration 5200: Loss = -10852.314355964701
Iteration 5300: Loss = -10852.313915745757
Iteration 5400: Loss = -10852.313533123
Iteration 5500: Loss = -10852.313137534442
Iteration 5600: Loss = -10852.312859666748
Iteration 5700: Loss = -10852.312470095418
Iteration 5800: Loss = -10852.313043832468
1
Iteration 5900: Loss = -10852.311894848282
Iteration 6000: Loss = -10852.312650751239
1
Iteration 6100: Loss = -10852.311382633698
Iteration 6200: Loss = -10852.312032795373
1
Iteration 6300: Loss = -10852.310908108806
Iteration 6400: Loss = -10852.310715156733
Iteration 6500: Loss = -10852.310481449249
Iteration 6600: Loss = -10852.310370994654
Iteration 6700: Loss = -10852.310524235449
1
Iteration 6800: Loss = -10852.309977357461
Iteration 6900: Loss = -10852.310054415064
Iteration 7000: Loss = -10852.309661271218
Iteration 7100: Loss = -10852.309903572153
1
Iteration 7200: Loss = -10852.309372649508
Iteration 7300: Loss = -10852.30950655682
1
Iteration 7400: Loss = -10852.309093116775
Iteration 7500: Loss = -10852.309048704052
Iteration 7600: Loss = -10852.308989260173
Iteration 7700: Loss = -10852.30880155566
Iteration 7800: Loss = -10852.308836592523
Iteration 7900: Loss = -10852.308596269613
Iteration 8000: Loss = -10852.308556464852
Iteration 8100: Loss = -10852.311592083362
1
Iteration 8200: Loss = -10852.31245011269
2
Iteration 8300: Loss = -10852.308349661269
Iteration 8400: Loss = -10852.327227306641
1
Iteration 8500: Loss = -10852.308138357546
Iteration 8600: Loss = -10852.313838264537
1
Iteration 8700: Loss = -10852.308025161927
Iteration 8800: Loss = -10852.323990245508
1
Iteration 8900: Loss = -10852.307912419763
Iteration 9000: Loss = -10852.308548485096
1
Iteration 9100: Loss = -10852.307871167732
Iteration 9200: Loss = -10852.307769047395
Iteration 9300: Loss = -10852.340335174142
1
Iteration 9400: Loss = -10852.307694172761
Iteration 9500: Loss = -10852.307639715133
Iteration 9600: Loss = -10852.332896461903
1
Iteration 9700: Loss = -10852.324828408558
2
Iteration 9800: Loss = -10852.308546352155
3
Iteration 9900: Loss = -10852.308899673219
4
Iteration 10000: Loss = -10852.338316895013
5
Iteration 10100: Loss = -10852.308253787325
6
Iteration 10200: Loss = -10852.307683503577
Iteration 10300: Loss = -10852.307383132907
Iteration 10400: Loss = -10852.308583521959
1
Iteration 10500: Loss = -10852.385763604198
2
Iteration 10600: Loss = -10852.316645111236
3
Iteration 10700: Loss = -10852.307296090517
Iteration 10800: Loss = -10852.309942220205
1
Iteration 10900: Loss = -10852.307230925506
Iteration 11000: Loss = -10852.308146759024
1
Iteration 11100: Loss = -10852.313047015723
2
Iteration 11200: Loss = -10852.308384693442
3
Iteration 11300: Loss = -10852.307304913984
Iteration 11400: Loss = -10852.30759549761
1
Iteration 11500: Loss = -10852.311614587059
2
Iteration 11600: Loss = -10852.354332036632
3
Iteration 11700: Loss = -10852.317645617366
4
Iteration 11800: Loss = -10852.307109134303
Iteration 11900: Loss = -10852.577877595359
1
Iteration 12000: Loss = -10852.30709507577
Iteration 12100: Loss = -10852.308949669345
1
Iteration 12200: Loss = -10852.30707019192
Iteration 12300: Loss = -10852.322472155414
1
Iteration 12400: Loss = -10852.307080442999
Iteration 12500: Loss = -10852.308175399005
1
Iteration 12600: Loss = -10852.307623958439
2
Iteration 12700: Loss = -10852.313868430569
3
Iteration 12800: Loss = -10852.310661582598
4
Iteration 12900: Loss = -10852.307128332139
Iteration 13000: Loss = -10852.307996973665
1
Iteration 13100: Loss = -10852.307994726445
2
Iteration 13200: Loss = -10852.307290752502
3
Iteration 13300: Loss = -10852.307241884653
4
Iteration 13400: Loss = -10852.309477939374
5
Iteration 13500: Loss = -10852.313944984657
6
Iteration 13600: Loss = -10852.306987520087
Iteration 13700: Loss = -10852.374213651152
1
Iteration 13800: Loss = -10852.326664852351
2
Iteration 13900: Loss = -10852.306987557386
Iteration 14000: Loss = -10852.30737234108
1
Iteration 14100: Loss = -10852.32845558226
2
Iteration 14200: Loss = -10852.306973170658
Iteration 14300: Loss = -10852.314932022762
1
Iteration 14400: Loss = -10852.306961632996
Iteration 14500: Loss = -10852.307085849976
1
Iteration 14600: Loss = -10852.31177079675
2
Iteration 14700: Loss = -10852.33322600782
3
Iteration 14800: Loss = -10852.310557108827
4
Iteration 14900: Loss = -10852.307866029578
5
Iteration 15000: Loss = -10852.306890271784
Iteration 15100: Loss = -10852.307039023277
1
Iteration 15200: Loss = -10852.31627978699
2
Iteration 15300: Loss = -10852.30702461217
3
Iteration 15400: Loss = -10852.306943364587
Iteration 15500: Loss = -10852.338053217794
1
Iteration 15600: Loss = -10852.502498284977
2
Iteration 15700: Loss = -10852.306890882895
Iteration 15800: Loss = -10852.30886645843
1
Iteration 15900: Loss = -10852.320929037262
2
Iteration 16000: Loss = -10852.307861834615
3
Iteration 16100: Loss = -10852.306940694547
Iteration 16200: Loss = -10852.306942065381
Iteration 16300: Loss = -10852.307025441009
Iteration 16400: Loss = -10852.306988483144
Iteration 16500: Loss = -10852.314087257982
1
Iteration 16600: Loss = -10852.306930608529
Iteration 16700: Loss = -10852.307276659072
1
Iteration 16800: Loss = -10852.3712031907
2
Iteration 16900: Loss = -10852.306921015974
Iteration 17000: Loss = -10852.323116488484
1
Iteration 17100: Loss = -10852.306935611281
Iteration 17200: Loss = -10852.307419381708
1
Iteration 17300: Loss = -10852.3154143102
2
Iteration 17400: Loss = -10852.30797983051
3
Iteration 17500: Loss = -10852.306975687263
Iteration 17600: Loss = -10852.307272824037
1
Iteration 17700: Loss = -10852.324144111797
2
Iteration 17800: Loss = -10852.306909976258
Iteration 17900: Loss = -10852.307487032695
1
Iteration 18000: Loss = -10852.319742027248
2
Iteration 18100: Loss = -10852.306952643197
Iteration 18200: Loss = -10852.307058012655
1
Iteration 18300: Loss = -10852.339372296596
2
Iteration 18400: Loss = -10852.306883401121
Iteration 18500: Loss = -10852.309209373574
1
Iteration 18600: Loss = -10852.314383619332
2
Iteration 18700: Loss = -10852.307604154334
3
Iteration 18800: Loss = -10852.306919511031
Iteration 18900: Loss = -10852.460856629747
1
Iteration 19000: Loss = -10852.306928611077
Iteration 19100: Loss = -10852.306985006655
Iteration 19200: Loss = -10852.373720454381
1
Iteration 19300: Loss = -10852.341140736344
2
Iteration 19400: Loss = -10852.463776732146
3
Iteration 19500: Loss = -10852.312590484646
4
Iteration 19600: Loss = -10852.30740757654
5
Iteration 19700: Loss = -10852.399519321467
6
Iteration 19800: Loss = -10852.306930003966
Iteration 19900: Loss = -10852.313024497653
1
pi: tensor([[6.2689e-01, 3.7311e-01],
        [1.7209e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1553, 0.8447], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2697, 0.1869],
         [0.7132, 0.1548]],

        [[0.6216, 0.1914],
         [0.6903, 0.6689]],

        [[0.5506, 0.1818],
         [0.6933, 0.6411]],

        [[0.5228, 0.1291],
         [0.7205, 0.7133]],

        [[0.6866, 0.2687],
         [0.6991, 0.7140]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0056450870649491815
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.012148896880163282
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: 0.008886975850123834
Average Adjusted Rand Index: 0.0071244928176981085
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22620.697546546406
Iteration 100: Loss = -10860.254960700186
Iteration 200: Loss = -10859.011830405609
Iteration 300: Loss = -10858.684816478866
Iteration 400: Loss = -10858.554274988852
Iteration 500: Loss = -10858.450925688807
Iteration 600: Loss = -10858.336005561354
Iteration 700: Loss = -10858.2397840077
Iteration 800: Loss = -10858.165703376006
Iteration 900: Loss = -10858.085125924892
Iteration 1000: Loss = -10857.981929110074
Iteration 1100: Loss = -10857.841426494339
Iteration 1200: Loss = -10857.61926207807
Iteration 1300: Loss = -10856.71788816771
Iteration 1400: Loss = -10855.986848066756
Iteration 1500: Loss = -10855.138001243718
Iteration 1600: Loss = -10853.374765286764
Iteration 1700: Loss = -10852.899715694755
Iteration 1800: Loss = -10852.713144098789
Iteration 1900: Loss = -10852.612673915835
Iteration 2000: Loss = -10852.547059225446
Iteration 2100: Loss = -10852.50207811094
Iteration 2200: Loss = -10852.471383928478
Iteration 2300: Loss = -10852.448759622637
Iteration 2400: Loss = -10852.430644525542
Iteration 2500: Loss = -10852.415991425125
Iteration 2600: Loss = -10852.403810883927
Iteration 2700: Loss = -10852.39365036952
Iteration 2800: Loss = -10852.384926307886
Iteration 2900: Loss = -10852.377414843182
Iteration 3000: Loss = -10852.370941138788
Iteration 3100: Loss = -10852.365312022886
Iteration 3200: Loss = -10852.360411429594
Iteration 3300: Loss = -10852.356076910082
Iteration 3400: Loss = -10852.352224975306
Iteration 3500: Loss = -10852.348765959638
Iteration 3600: Loss = -10852.345651828886
Iteration 3700: Loss = -10852.34280724612
Iteration 3800: Loss = -10852.340285630955
Iteration 3900: Loss = -10852.33793411791
Iteration 4000: Loss = -10852.335859963736
Iteration 4100: Loss = -10852.333891843598
Iteration 4200: Loss = -10852.33213568573
Iteration 4300: Loss = -10852.330485275967
Iteration 4400: Loss = -10852.328964757042
Iteration 4500: Loss = -10852.327590711275
Iteration 4600: Loss = -10852.326298269367
Iteration 4700: Loss = -10852.325142147376
Iteration 4800: Loss = -10852.323994232325
Iteration 4900: Loss = -10852.322968545353
Iteration 5000: Loss = -10852.322029270335
Iteration 5100: Loss = -10852.321138695306
Iteration 5200: Loss = -10852.320326306874
Iteration 5300: Loss = -10852.319560209104
Iteration 5400: Loss = -10852.318825654993
Iteration 5500: Loss = -10852.318145583864
Iteration 5600: Loss = -10852.317517551368
Iteration 5700: Loss = -10852.316953554915
Iteration 5800: Loss = -10852.318549063257
1
Iteration 5900: Loss = -10852.315842491202
Iteration 6000: Loss = -10852.31536757465
Iteration 6100: Loss = -10852.314923010854
Iteration 6200: Loss = -10852.314458641331
Iteration 6300: Loss = -10852.314145835324
Iteration 6400: Loss = -10852.313668460394
Iteration 6500: Loss = -10852.317499141283
1
Iteration 6600: Loss = -10852.31296705892
Iteration 6700: Loss = -10852.312642560182
Iteration 6800: Loss = -10852.312375830186
Iteration 6900: Loss = -10852.31205899132
Iteration 7000: Loss = -10852.311916372628
Iteration 7100: Loss = -10852.311565950167
Iteration 7200: Loss = -10852.321061492592
1
Iteration 7300: Loss = -10852.311085784168
Iteration 7400: Loss = -10852.310876148355
Iteration 7500: Loss = -10852.310686224306
Iteration 7600: Loss = -10852.31046331227
Iteration 7700: Loss = -10852.310411015671
Iteration 7800: Loss = -10852.310135865018
Iteration 7900: Loss = -10852.31054421231
1
Iteration 8000: Loss = -10852.309791126088
Iteration 8100: Loss = -10852.30963415008
Iteration 8200: Loss = -10852.309478771262
Iteration 8300: Loss = -10852.30959630012
1
Iteration 8400: Loss = -10852.309245801405
Iteration 8500: Loss = -10852.310046252496
1
Iteration 8600: Loss = -10852.309238806667
Iteration 8700: Loss = -10852.308942828433
Iteration 8800: Loss = -10852.566686110218
1
Iteration 8900: Loss = -10852.30871927678
Iteration 9000: Loss = -10852.371372158917
1
Iteration 9100: Loss = -10852.308536863746
Iteration 9200: Loss = -10852.30841292561
Iteration 9300: Loss = -10852.309914255038
1
Iteration 9400: Loss = -10852.30828574847
Iteration 9500: Loss = -10852.313006607443
1
Iteration 9600: Loss = -10852.308202126573
Iteration 9700: Loss = -10852.30811417864
Iteration 9800: Loss = -10852.424417766748
1
Iteration 9900: Loss = -10852.307993173701
Iteration 10000: Loss = -10852.307954995686
Iteration 10100: Loss = -10852.430726868877
1
Iteration 10200: Loss = -10852.307822120369
Iteration 10300: Loss = -10852.307741190587
Iteration 10400: Loss = -10852.307754003092
Iteration 10500: Loss = -10852.310793222974
1
Iteration 10600: Loss = -10852.307649171988
Iteration 10700: Loss = -10852.307615965798
Iteration 10800: Loss = -10852.307643574739
Iteration 10900: Loss = -10852.311205795373
1
Iteration 11000: Loss = -10852.318142702266
2
Iteration 11100: Loss = -10852.307481806522
Iteration 11200: Loss = -10852.30821059841
1
Iteration 11300: Loss = -10852.308564951203
2
Iteration 11400: Loss = -10852.316992769505
3
Iteration 11500: Loss = -10852.307385000482
Iteration 11600: Loss = -10852.307722483667
1
Iteration 11700: Loss = -10852.307471205859
Iteration 11800: Loss = -10852.307331263279
Iteration 11900: Loss = -10852.3185193821
1
Iteration 12000: Loss = -10852.321295996684
2
Iteration 12100: Loss = -10852.308040040289
3
Iteration 12200: Loss = -10852.311830970948
4
Iteration 12300: Loss = -10852.346391357425
5
Iteration 12400: Loss = -10852.308166438703
6
Iteration 12500: Loss = -10852.370240815886
7
Iteration 12600: Loss = -10852.32020320188
8
Iteration 12700: Loss = -10852.311367818776
9
Iteration 12800: Loss = -10852.307134826235
Iteration 12900: Loss = -10852.326832421826
1
Iteration 13000: Loss = -10852.307094551219
Iteration 13100: Loss = -10852.328289535115
1
Iteration 13200: Loss = -10852.307086239989
Iteration 13300: Loss = -10852.317897945934
1
Iteration 13400: Loss = -10852.319982135838
2
Iteration 13500: Loss = -10852.309678518719
3
Iteration 13600: Loss = -10852.307083467273
Iteration 13700: Loss = -10852.314621039235
1
Iteration 13800: Loss = -10852.398607448138
2
Iteration 13900: Loss = -10852.307029929205
Iteration 14000: Loss = -10852.309766944321
1
Iteration 14100: Loss = -10852.349042042666
2
Iteration 14200: Loss = -10852.31209416288
3
Iteration 14300: Loss = -10852.307040370091
Iteration 14400: Loss = -10852.307582592735
1
Iteration 14500: Loss = -10852.307110582307
Iteration 14600: Loss = -10852.310159424842
1
Iteration 14700: Loss = -10852.306968372555
Iteration 14800: Loss = -10852.307320656155
1
Iteration 14900: Loss = -10852.307678376214
2
Iteration 15000: Loss = -10852.307080050125
3
Iteration 15100: Loss = -10852.306998833878
Iteration 15200: Loss = -10852.30723238061
1
Iteration 15300: Loss = -10852.30729677319
2
Iteration 15400: Loss = -10852.306994636181
Iteration 15500: Loss = -10852.308629644464
1
Iteration 15600: Loss = -10852.30694531589
Iteration 15700: Loss = -10852.307036825854
Iteration 15800: Loss = -10852.306968946195
Iteration 15900: Loss = -10852.306984700324
Iteration 16000: Loss = -10852.32973163487
1
Iteration 16100: Loss = -10852.308130253248
2
Iteration 16200: Loss = -10852.306996327066
Iteration 16300: Loss = -10852.306909950617
Iteration 16400: Loss = -10852.35050656634
1
Iteration 16500: Loss = -10852.308077081814
2
Iteration 16600: Loss = -10852.307044569661
3
Iteration 16700: Loss = -10852.30762694205
4
Iteration 16800: Loss = -10852.307909126108
5
Iteration 16900: Loss = -10852.306988374568
Iteration 17000: Loss = -10852.306940574548
Iteration 17100: Loss = -10852.30761272225
1
Iteration 17200: Loss = -10852.317341207006
2
Iteration 17300: Loss = -10852.306911409905
Iteration 17400: Loss = -10852.307503951106
1
Iteration 17500: Loss = -10852.322500016764
2
Iteration 17600: Loss = -10852.306943128138
Iteration 17700: Loss = -10852.306959113053
Iteration 17800: Loss = -10852.306916179836
Iteration 17900: Loss = -10852.307083357557
1
Iteration 18000: Loss = -10852.308157340814
2
Iteration 18100: Loss = -10852.306932188592
Iteration 18200: Loss = -10852.306950406773
Iteration 18300: Loss = -10852.31362064104
1
Iteration 18400: Loss = -10852.466901673884
2
Iteration 18500: Loss = -10852.306914689761
Iteration 18600: Loss = -10852.307522190336
1
Iteration 18700: Loss = -10852.30870373284
2
Iteration 18800: Loss = -10852.307162689736
3
Iteration 18900: Loss = -10852.306979524355
Iteration 19000: Loss = -10852.318094005903
1
Iteration 19100: Loss = -10852.307012894395
Iteration 19200: Loss = -10852.306947167257
Iteration 19300: Loss = -10852.331859513246
1
Iteration 19400: Loss = -10852.306895221362
Iteration 19500: Loss = -10852.311445008163
1
Iteration 19600: Loss = -10852.306893372386
Iteration 19700: Loss = -10852.311234752768
1
Iteration 19800: Loss = -10852.310205396801
2
Iteration 19900: Loss = -10852.309671913355
3
pi: tensor([[6.2765e-01, 3.7235e-01],
        [3.2735e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1543, 0.8457], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2702, 0.1871],
         [0.5775, 0.1547]],

        [[0.5659, 0.1926],
         [0.5167, 0.6561]],

        [[0.5512, 0.1828],
         [0.7094, 0.6072]],

        [[0.6613, 0.1291],
         [0.5091, 0.5008]],

        [[0.5633, 0.2688],
         [0.5269, 0.7096]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0056450870649491815
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.008096346523631212
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
Global Adjusted Rand Index: 0.008069852623335318
Average Adjusted Rand Index: 0.006313982746391694
10761.120887263682
[0.008886975850123834, 0.008069852623335318] [0.0071244928176981085, 0.006313982746391694] [10852.328291626736, 10852.424400008307]
-------------------------------------
This iteration is 31
True Objective function: Loss = -10913.50172611485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21920.191161541978
Iteration 100: Loss = -10982.996085534794
Iteration 200: Loss = -10982.594648122898
Iteration 300: Loss = -10982.487908517476
Iteration 400: Loss = -10982.415888519548
Iteration 500: Loss = -10982.357149373158
Iteration 600: Loss = -10982.297806779305
Iteration 700: Loss = -10982.22473288348
Iteration 800: Loss = -10982.125975336188
Iteration 900: Loss = -10982.004318602429
Iteration 1000: Loss = -10981.88269767866
Iteration 1100: Loss = -10981.771137890435
Iteration 1200: Loss = -10981.658110543163
Iteration 1300: Loss = -10981.529334375966
Iteration 1400: Loss = -10981.371534755412
Iteration 1500: Loss = -10981.175191575247
Iteration 1600: Loss = -10980.94247095976
Iteration 1700: Loss = -10980.701438864247
Iteration 1800: Loss = -10980.50441638884
Iteration 1900: Loss = -10980.393371414075
Iteration 2000: Loss = -10980.351467801936
Iteration 2100: Loss = -10980.33633832557
Iteration 2200: Loss = -10980.327999042243
Iteration 2300: Loss = -10980.321229540017
Iteration 2400: Loss = -10980.314887783234
Iteration 2500: Loss = -10980.30857171409
Iteration 2600: Loss = -10980.301703767567
Iteration 2700: Loss = -10980.294086252039
Iteration 2800: Loss = -10980.285084300938
Iteration 2900: Loss = -10980.273916171915
Iteration 3000: Loss = -10980.259227308981
Iteration 3100: Loss = -10980.238531509898
Iteration 3200: Loss = -10980.205597782595
Iteration 3300: Loss = -10980.140960073966
Iteration 3400: Loss = -10979.908814820768
Iteration 3500: Loss = -10977.836751722514
Iteration 3600: Loss = -10895.093796742658
Iteration 3700: Loss = -10856.930868399118
Iteration 3800: Loss = -10856.690566100056
Iteration 3900: Loss = -10856.5957778375
Iteration 4000: Loss = -10856.531157852216
Iteration 4100: Loss = -10856.52225071244
Iteration 4200: Loss = -10856.51718232953
Iteration 4300: Loss = -10856.514445631305
Iteration 4400: Loss = -10856.51108384187
Iteration 4500: Loss = -10856.509275108934
Iteration 4600: Loss = -10856.510166777694
1
Iteration 4700: Loss = -10856.506660472842
Iteration 4800: Loss = -10856.505724552118
Iteration 4900: Loss = -10856.504952887486
Iteration 5000: Loss = -10856.504252474288
Iteration 5100: Loss = -10856.50343264959
Iteration 5200: Loss = -10856.50265599448
Iteration 5300: Loss = -10856.501827397025
Iteration 5400: Loss = -10856.500613404256
Iteration 5500: Loss = -10856.503016817558
1
Iteration 5600: Loss = -10856.49872204921
Iteration 5700: Loss = -10856.525899139564
1
Iteration 5800: Loss = -10856.496711689846
Iteration 5900: Loss = -10856.49667138226
Iteration 6000: Loss = -10856.496237120979
Iteration 6100: Loss = -10856.49629709556
Iteration 6200: Loss = -10856.499723597526
1
Iteration 6300: Loss = -10856.461863844064
Iteration 6400: Loss = -10856.46163094398
Iteration 6500: Loss = -10856.461565184949
Iteration 6600: Loss = -10856.462201134893
1
Iteration 6700: Loss = -10856.461342210845
Iteration 6800: Loss = -10856.462925370564
1
Iteration 6900: Loss = -10856.461154226152
Iteration 7000: Loss = -10856.461688916152
1
Iteration 7100: Loss = -10856.46094731699
Iteration 7200: Loss = -10856.461781503805
1
Iteration 7300: Loss = -10856.462147182921
2
Iteration 7400: Loss = -10856.469162964044
3
Iteration 7500: Loss = -10856.460610709899
Iteration 7600: Loss = -10856.460595061168
Iteration 7700: Loss = -10856.460743412465
1
Iteration 7800: Loss = -10856.46047261396
Iteration 7900: Loss = -10856.46046863456
Iteration 8000: Loss = -10856.460880377483
1
Iteration 8100: Loss = -10856.46088345202
2
Iteration 8200: Loss = -10856.460346328658
Iteration 8300: Loss = -10856.459999333427
Iteration 8400: Loss = -10856.459736196508
Iteration 8500: Loss = -10856.460113141968
1
Iteration 8600: Loss = -10856.460454016911
2
Iteration 8700: Loss = -10856.45913855646
Iteration 8800: Loss = -10856.459066976047
Iteration 8900: Loss = -10856.459044111874
Iteration 9000: Loss = -10856.458869309272
Iteration 9100: Loss = -10856.464358616062
1
Iteration 9200: Loss = -10856.45882529686
Iteration 9300: Loss = -10856.718469290845
1
Iteration 9400: Loss = -10856.458841994709
Iteration 9500: Loss = -10856.460540794287
1
Iteration 9600: Loss = -10856.458952735156
2
Iteration 9700: Loss = -10856.463480529605
3
Iteration 9800: Loss = -10856.45880380238
Iteration 9900: Loss = -10856.458798308415
Iteration 10000: Loss = -10856.512506444056
1
Iteration 10100: Loss = -10856.458660038194
Iteration 10200: Loss = -10856.63375305779
1
Iteration 10300: Loss = -10856.458213444872
Iteration 10400: Loss = -10856.463175630197
1
Iteration 10500: Loss = -10856.457798214029
Iteration 10600: Loss = -10856.45922049266
1
Iteration 10700: Loss = -10856.568495762369
2
Iteration 10800: Loss = -10856.457618319851
Iteration 10900: Loss = -10856.594773496927
1
Iteration 11000: Loss = -10856.457791588406
2
Iteration 11100: Loss = -10856.467501238809
3
Iteration 11200: Loss = -10856.545918697551
4
Iteration 11300: Loss = -10856.457520414468
Iteration 11400: Loss = -10856.462398390457
1
Iteration 11500: Loss = -10856.508131187402
2
Iteration 11600: Loss = -10856.456825997939
Iteration 11700: Loss = -10856.46096924019
1
Iteration 11800: Loss = -10856.464517831437
2
Iteration 11900: Loss = -10856.472030921264
3
Iteration 12000: Loss = -10856.457442665049
4
Iteration 12100: Loss = -10856.45792191073
5
Iteration 12200: Loss = -10856.466021794402
6
Iteration 12300: Loss = -10856.61006407623
7
Iteration 12400: Loss = -10856.456840119425
Iteration 12500: Loss = -10856.457317736184
1
Iteration 12600: Loss = -10856.56126733358
2
Iteration 12700: Loss = -10856.456829355197
Iteration 12800: Loss = -10856.456960435462
1
Iteration 12900: Loss = -10856.660080400266
2
Iteration 13000: Loss = -10856.45671104342
Iteration 13100: Loss = -10856.456463454911
Iteration 13200: Loss = -10856.489260460177
1
Iteration 13300: Loss = -10856.457351010857
2
Iteration 13400: Loss = -10856.462689860306
3
Iteration 13500: Loss = -10856.456646725104
4
Iteration 13600: Loss = -10856.50173284949
5
Iteration 13700: Loss = -10856.456270523082
Iteration 13800: Loss = -10856.458973108563
1
Iteration 13900: Loss = -10856.461401547598
2
Iteration 14000: Loss = -10856.458013593385
3
Iteration 14100: Loss = -10856.459370755863
4
Iteration 14200: Loss = -10856.46786162186
5
Iteration 14300: Loss = -10856.522125973112
6
Iteration 14400: Loss = -10856.461417084409
7
Iteration 14500: Loss = -10856.45606609541
Iteration 14600: Loss = -10856.457089747606
1
Iteration 14700: Loss = -10856.483398698081
2
Iteration 14800: Loss = -10856.456558919785
3
Iteration 14900: Loss = -10856.456103653476
Iteration 15000: Loss = -10856.457403443332
1
Iteration 15100: Loss = -10856.48362150027
2
Iteration 15200: Loss = -10856.458068218613
3
Iteration 15300: Loss = -10856.459903647954
4
Iteration 15400: Loss = -10856.45693621472
5
Iteration 15500: Loss = -10856.459053613306
6
Iteration 15600: Loss = -10856.456753337614
7
Iteration 15700: Loss = -10856.458680171712
8
Iteration 15800: Loss = -10856.456916047273
9
Iteration 15900: Loss = -10856.456114775505
Iteration 16000: Loss = -10856.456322016978
1
Iteration 16100: Loss = -10856.456867206316
2
Iteration 16200: Loss = -10856.457646893557
3
Iteration 16300: Loss = -10856.465875249301
4
Iteration 16400: Loss = -10856.470169114193
5
Iteration 16500: Loss = -10856.456349191305
6
Iteration 16600: Loss = -10856.45607321474
Iteration 16700: Loss = -10856.457987231448
1
Iteration 16800: Loss = -10856.459529314634
2
Iteration 16900: Loss = -10856.456499929145
3
Iteration 17000: Loss = -10856.485828726407
4
Iteration 17100: Loss = -10856.459059971085
5
Iteration 17200: Loss = -10856.455702137979
Iteration 17300: Loss = -10856.461135570757
1
Iteration 17400: Loss = -10856.45657813596
2
Iteration 17500: Loss = -10856.46419213757
3
Iteration 17600: Loss = -10856.575602214765
4
Iteration 17700: Loss = -10856.45566209737
Iteration 17800: Loss = -10856.455679617071
Iteration 17900: Loss = -10856.457851263836
1
Iteration 18000: Loss = -10856.455864244745
2
Iteration 18100: Loss = -10856.455566529263
Iteration 18200: Loss = -10856.468705449442
1
Iteration 18300: Loss = -10856.455638760515
Iteration 18400: Loss = -10856.455795379441
1
Iteration 18500: Loss = -10856.456337094105
2
Iteration 18600: Loss = -10856.45621667067
3
Iteration 18700: Loss = -10856.464916796996
4
Iteration 18800: Loss = -10856.506765632734
5
Iteration 18900: Loss = -10856.460051805465
6
Iteration 19000: Loss = -10856.47219300681
7
Iteration 19100: Loss = -10856.511236776165
8
Iteration 19200: Loss = -10856.455278677731
Iteration 19300: Loss = -10856.455490559578
1
Iteration 19400: Loss = -10856.455808720557
2
Iteration 19500: Loss = -10856.455450381774
3
Iteration 19600: Loss = -10856.45665836797
4
Iteration 19700: Loss = -10856.459238724005
5
Iteration 19800: Loss = -10856.479784518331
6
Iteration 19900: Loss = -10856.456608973216
7
pi: tensor([[0.7738, 0.2262],
        [0.2437, 0.7563]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4635, 0.5365], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2509, 0.0957],
         [0.5586, 0.1982]],

        [[0.5913, 0.0975],
         [0.6364, 0.6767]],

        [[0.6481, 0.1042],
         [0.5737, 0.6640]],

        [[0.5074, 0.0994],
         [0.5120, 0.5830]],

        [[0.6480, 0.1128],
         [0.5774, 0.6276]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 8
Adjusted Rand Index: 0.7026076198471212
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.604435145582853
Global Adjusted Rand Index: 0.7881196087565815
Average Adjusted Rand Index: 0.7915079199765296
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20080.17789297015
Iteration 100: Loss = -10982.663760854213
Iteration 200: Loss = -10982.48519060455
Iteration 300: Loss = -10982.446142475972
Iteration 400: Loss = -10982.428702444327
Iteration 500: Loss = -10982.419205614297
Iteration 600: Loss = -10982.413205925564
Iteration 700: Loss = -10982.408944503777
Iteration 800: Loss = -10982.405635753745
Iteration 900: Loss = -10982.402997713965
Iteration 1000: Loss = -10982.400647147035
Iteration 1100: Loss = -10982.398488309296
Iteration 1200: Loss = -10982.396421208528
Iteration 1300: Loss = -10982.394333577404
Iteration 1400: Loss = -10982.392074609183
Iteration 1500: Loss = -10982.389614560085
Iteration 1600: Loss = -10982.38680485177
Iteration 1700: Loss = -10982.38344776684
Iteration 1800: Loss = -10982.379248677398
Iteration 1900: Loss = -10982.373762353556
Iteration 2000: Loss = -10982.366124767464
Iteration 2100: Loss = -10982.354245683784
Iteration 2200: Loss = -10982.33256118764
Iteration 2300: Loss = -10982.274906793298
Iteration 2400: Loss = -10981.809529319858
Iteration 2500: Loss = -10980.460758392373
Iteration 2600: Loss = -10980.341272365777
Iteration 2700: Loss = -10980.327026723453
Iteration 2800: Loss = -10980.301054076568
Iteration 2900: Loss = -10980.154191205937
Iteration 3000: Loss = -10862.477833193043
Iteration 3100: Loss = -10859.886042696506
Iteration 3200: Loss = -10859.640796462585
Iteration 3300: Loss = -10859.379213296428
Iteration 3400: Loss = -10859.364601131923
Iteration 3500: Loss = -10859.359379102745
Iteration 3600: Loss = -10859.28595138249
Iteration 3700: Loss = -10859.282166081379
Iteration 3800: Loss = -10859.256808672284
Iteration 3900: Loss = -10859.237619906755
Iteration 4000: Loss = -10859.235991007088
Iteration 4100: Loss = -10859.231811424881
Iteration 4200: Loss = -10859.227015413646
Iteration 4300: Loss = -10859.219349565421
Iteration 4400: Loss = -10859.199071194094
Iteration 4500: Loss = -10859.197652223356
Iteration 4600: Loss = -10859.192308230853
Iteration 4700: Loss = -10859.183796864192
Iteration 4800: Loss = -10859.177738135428
Iteration 4900: Loss = -10859.170388049328
Iteration 5000: Loss = -10859.168925928567
Iteration 5100: Loss = -10859.140639247747
Iteration 5200: Loss = -10859.141223145867
1
Iteration 5300: Loss = -10859.151795658865
2
Iteration 5400: Loss = -10859.13859406555
Iteration 5500: Loss = -10859.13289402036
Iteration 5600: Loss = -10859.132028942271
Iteration 5700: Loss = -10859.129059016359
Iteration 5800: Loss = -10859.080047283094
Iteration 5900: Loss = -10859.091793008987
1
Iteration 6000: Loss = -10859.073749767866
Iteration 6100: Loss = -10859.037347826072
Iteration 6200: Loss = -10859.033372954667
Iteration 6300: Loss = -10859.032096113286
Iteration 6400: Loss = -10859.031836762408
Iteration 6500: Loss = -10859.032065657853
1
Iteration 6600: Loss = -10859.02853043707
Iteration 6700: Loss = -10859.028410584939
Iteration 6800: Loss = -10859.030060633457
1
Iteration 6900: Loss = -10859.02753369885
Iteration 7000: Loss = -10859.0247237526
Iteration 7100: Loss = -10859.024515629328
Iteration 7200: Loss = -10859.023967462443
Iteration 7300: Loss = -10856.539984245142
Iteration 7400: Loss = -10856.529234531106
Iteration 7500: Loss = -10856.529034217558
Iteration 7600: Loss = -10856.528952627476
Iteration 7700: Loss = -10856.527868357469
Iteration 7800: Loss = -10856.541436847694
1
Iteration 7900: Loss = -10856.525929306104
Iteration 8000: Loss = -10856.53638816459
1
Iteration 8100: Loss = -10856.52542260366
Iteration 8200: Loss = -10856.524211366916
Iteration 8300: Loss = -10856.540499299092
1
Iteration 8400: Loss = -10856.518083761152
Iteration 8500: Loss = -10856.518196758485
1
Iteration 8600: Loss = -10856.517915520611
Iteration 8700: Loss = -10856.518264506525
1
Iteration 8800: Loss = -10856.51973334811
2
Iteration 8900: Loss = -10856.545056367988
3
Iteration 9000: Loss = -10856.526261572066
4
Iteration 9100: Loss = -10856.532648508253
5
Iteration 9200: Loss = -10856.519262994721
6
Iteration 9300: Loss = -10856.518675137322
7
Iteration 9400: Loss = -10856.527168568284
8
Iteration 9500: Loss = -10856.517914806456
Iteration 9600: Loss = -10856.518309748371
1
Iteration 9700: Loss = -10856.531411420774
2
Iteration 9800: Loss = -10856.515064001916
Iteration 9900: Loss = -10856.463332092488
Iteration 10000: Loss = -10856.46186397966
Iteration 10100: Loss = -10856.463679622351
1
Iteration 10200: Loss = -10856.469007658032
2
Iteration 10300: Loss = -10856.469452470996
3
Iteration 10400: Loss = -10856.465148622492
4
Iteration 10500: Loss = -10856.474630580416
5
Iteration 10600: Loss = -10856.46108101378
Iteration 10700: Loss = -10856.460922787694
Iteration 10800: Loss = -10856.54347139495
1
Iteration 10900: Loss = -10856.460350071711
Iteration 11000: Loss = -10856.461923737283
1
Iteration 11100: Loss = -10856.471180217928
2
Iteration 11200: Loss = -10856.47629716951
3
Iteration 11300: Loss = -10856.459922054168
Iteration 11400: Loss = -10856.460009535494
Iteration 11500: Loss = -10856.464057538013
1
Iteration 11600: Loss = -10856.461852119499
2
Iteration 11700: Loss = -10856.46199621598
3
Iteration 11800: Loss = -10856.460260319911
4
Iteration 11900: Loss = -10856.45858315795
Iteration 12000: Loss = -10856.458820300877
1
Iteration 12100: Loss = -10856.462574533407
2
Iteration 12200: Loss = -10856.50111700539
3
Iteration 12300: Loss = -10856.519368744277
4
Iteration 12400: Loss = -10856.4863594248
5
Iteration 12500: Loss = -10856.550615344357
6
Iteration 12600: Loss = -10856.522991868274
7
Iteration 12700: Loss = -10856.474656989598
8
Iteration 12800: Loss = -10856.473100977755
9
Iteration 12900: Loss = -10856.460051249076
10
Iteration 13000: Loss = -10856.456921660034
Iteration 13100: Loss = -10856.456630193023
Iteration 13200: Loss = -10856.457077234729
1
Iteration 13300: Loss = -10856.457353409412
2
Iteration 13400: Loss = -10856.4576315501
3
Iteration 13500: Loss = -10856.457940229506
4
Iteration 13600: Loss = -10856.457284570759
5
Iteration 13700: Loss = -10856.477730793846
6
Iteration 13800: Loss = -10856.462376751617
7
Iteration 13900: Loss = -10856.459857610205
8
Iteration 14000: Loss = -10856.457492773692
9
Iteration 14100: Loss = -10856.456731063961
10
Iteration 14200: Loss = -10856.459317379233
11
Iteration 14300: Loss = -10856.456132158162
Iteration 14400: Loss = -10856.45583064722
Iteration 14500: Loss = -10856.51104893524
1
Iteration 14600: Loss = -10856.455763834365
Iteration 14700: Loss = -10856.459285630854
1
Iteration 14800: Loss = -10856.455819067098
Iteration 14900: Loss = -10856.45587629013
Iteration 15000: Loss = -10856.458625351846
1
Iteration 15100: Loss = -10856.477791842244
2
Iteration 15200: Loss = -10856.455777356787
Iteration 15300: Loss = -10856.455853827061
Iteration 15400: Loss = -10856.457904387107
1
Iteration 15500: Loss = -10856.457698176358
2
Iteration 15600: Loss = -10856.460267316534
3
Iteration 15700: Loss = -10856.492282385483
4
Iteration 15800: Loss = -10856.455769244367
Iteration 15900: Loss = -10856.45589369864
1
Iteration 16000: Loss = -10856.456037371796
2
Iteration 16100: Loss = -10856.46279547997
3
Iteration 16200: Loss = -10856.487616536098
4
Iteration 16300: Loss = -10856.45999614894
5
Iteration 16400: Loss = -10856.456187650963
6
Iteration 16500: Loss = -10856.45583756628
Iteration 16600: Loss = -10856.456728048981
1
Iteration 16700: Loss = -10856.4594141218
2
Iteration 16800: Loss = -10856.455842160236
Iteration 16900: Loss = -10856.459142848858
1
Iteration 17000: Loss = -10856.455743208384
Iteration 17100: Loss = -10856.461101660452
1
Iteration 17200: Loss = -10856.455936379485
2
Iteration 17300: Loss = -10856.475745926906
3
Iteration 17400: Loss = -10856.455391070123
Iteration 17500: Loss = -10856.45562871994
1
Iteration 17600: Loss = -10856.461668994052
2
Iteration 17700: Loss = -10856.457428417021
3
Iteration 17800: Loss = -10856.455286555065
Iteration 17900: Loss = -10856.45949940814
1
Iteration 18000: Loss = -10856.462166851756
2
Iteration 18100: Loss = -10856.4876883747
3
Iteration 18200: Loss = -10856.524723489943
4
Iteration 18300: Loss = -10856.456380562946
5
Iteration 18400: Loss = -10856.456196954325
6
Iteration 18500: Loss = -10856.45654892687
7
Iteration 18600: Loss = -10856.457355942195
8
Iteration 18700: Loss = -10856.698993960386
9
Iteration 18800: Loss = -10856.456242415672
10
Iteration 18900: Loss = -10856.45625954822
11
Iteration 19000: Loss = -10856.463035037496
12
Iteration 19100: Loss = -10856.523368571046
13
Iteration 19200: Loss = -10856.467634530209
14
Iteration 19300: Loss = -10856.471778362951
15
Stopping early at iteration 19300 due to no improvement.
pi: tensor([[0.7575, 0.2425],
        [0.2254, 0.7746]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5351, 0.4649], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.0959],
         [0.6597, 0.2504]],

        [[0.6927, 0.0961],
         [0.7121, 0.5179]],

        [[0.6799, 0.1044],
         [0.7012, 0.5487]],

        [[0.6659, 0.0997],
         [0.5394, 0.6672]],

        [[0.6778, 0.1130],
         [0.6183, 0.7086]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026076198471212
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.604435145582853
Global Adjusted Rand Index: 0.7881196087565815
Average Adjusted Rand Index: 0.7915079199765296
10913.50172611485
[0.7881196087565815, 0.7881196087565815] [0.7915079199765296, 0.7915079199765296] [10856.45854347716, 10856.471778362951]
-------------------------------------
This iteration is 32
True Objective function: Loss = -10691.75465580235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22325.27744676746
Iteration 100: Loss = -10788.761338824597
Iteration 200: Loss = -10787.420298498215
Iteration 300: Loss = -10787.013545126714
Iteration 400: Loss = -10786.86714011075
Iteration 500: Loss = -10786.795433453668
Iteration 600: Loss = -10786.743883834877
Iteration 700: Loss = -10786.698389127756
Iteration 800: Loss = -10786.655246281487
Iteration 900: Loss = -10786.613066937372
Iteration 1000: Loss = -10786.570261793131
Iteration 1100: Loss = -10786.525994498697
Iteration 1200: Loss = -10786.48047981525
Iteration 1300: Loss = -10786.434401966386
Iteration 1400: Loss = -10786.386898082652
Iteration 1500: Loss = -10786.327048014731
Iteration 1600: Loss = -10786.13050856763
Iteration 1700: Loss = -10785.604831431212
Iteration 1800: Loss = -10785.177754679013
Iteration 1900: Loss = -10784.86865100657
Iteration 2000: Loss = -10784.615693349484
Iteration 2100: Loss = -10784.408450753412
Iteration 2200: Loss = -10784.267914628297
Iteration 2300: Loss = -10784.143908463771
Iteration 2400: Loss = -10783.853144471741
Iteration 2500: Loss = -10782.966417722011
Iteration 2600: Loss = -10782.583388545412
Iteration 2700: Loss = -10782.396992131902
Iteration 2800: Loss = -10782.309290098037
Iteration 2900: Loss = -10782.252020387541
Iteration 3000: Loss = -10782.211579028792
Iteration 3100: Loss = -10782.181793180676
Iteration 3200: Loss = -10782.158368177015
Iteration 3300: Loss = -10782.138958709163
Iteration 3400: Loss = -10782.121624976728
Iteration 3500: Loss = -10782.106519132056
Iteration 3600: Loss = -10782.094722925438
Iteration 3700: Loss = -10782.085062585487
Iteration 3800: Loss = -10782.07652967516
Iteration 3900: Loss = -10782.068834731981
Iteration 4000: Loss = -10782.061849438
Iteration 4100: Loss = -10782.055452471252
Iteration 4200: Loss = -10782.049595392016
Iteration 4300: Loss = -10782.044232423254
Iteration 4400: Loss = -10782.039209711065
Iteration 4500: Loss = -10782.03496863433
Iteration 4600: Loss = -10782.031199773364
Iteration 4700: Loss = -10782.02784837673
Iteration 4800: Loss = -10782.024926907654
Iteration 4900: Loss = -10782.022381159728
Iteration 5000: Loss = -10782.020162053483
Iteration 5100: Loss = -10782.018184133241
Iteration 5200: Loss = -10782.016410887662
Iteration 5300: Loss = -10782.014718769846
Iteration 5400: Loss = -10782.013145753075
Iteration 5500: Loss = -10782.011630838348
Iteration 5600: Loss = -10782.01018492319
Iteration 5700: Loss = -10782.008628552316
Iteration 5800: Loss = -10782.00710906939
Iteration 5900: Loss = -10782.005629483754
Iteration 6000: Loss = -10782.00448651147
Iteration 6100: Loss = -10782.003504594548
Iteration 6200: Loss = -10782.002588785312
Iteration 6300: Loss = -10782.001743478511
Iteration 6400: Loss = -10782.001010356616
Iteration 6500: Loss = -10782.00084385919
Iteration 6600: Loss = -10781.999569092575
Iteration 6700: Loss = -10781.998949722027
Iteration 6800: Loss = -10781.99837190757
Iteration 6900: Loss = -10781.997772501287
Iteration 7000: Loss = -10781.997235250765
Iteration 7100: Loss = -10781.996694903117
Iteration 7200: Loss = -10781.99615623563
Iteration 7300: Loss = -10781.995642364378
Iteration 7400: Loss = -10781.995092477451
Iteration 7500: Loss = -10781.99463153045
Iteration 7600: Loss = -10781.993963933723
Iteration 7700: Loss = -10781.993226292203
Iteration 7800: Loss = -10781.994783290555
1
Iteration 7900: Loss = -10782.057248552524
2
Iteration 8000: Loss = -10781.991155137192
Iteration 8100: Loss = -10782.004458506191
1
Iteration 8200: Loss = -10781.990088289285
Iteration 8300: Loss = -10781.989616470042
Iteration 8400: Loss = -10782.022807605596
1
Iteration 8500: Loss = -10781.988922884324
Iteration 8600: Loss = -10781.988578870769
Iteration 8700: Loss = -10782.010817554808
1
Iteration 8800: Loss = -10781.98794876016
Iteration 8900: Loss = -10781.987576205996
Iteration 9000: Loss = -10782.399610318835
1
Iteration 9100: Loss = -10781.986714319193
Iteration 9200: Loss = -10781.986385668568
Iteration 9300: Loss = -10781.98611430119
Iteration 9400: Loss = -10781.985800891425
Iteration 9500: Loss = -10781.985575673807
Iteration 9600: Loss = -10781.985382864716
Iteration 9700: Loss = -10781.986500834533
1
Iteration 9800: Loss = -10781.984938452559
Iteration 9900: Loss = -10781.984694764715
Iteration 10000: Loss = -10781.984447591736
Iteration 10100: Loss = -10781.984078566813
Iteration 10200: Loss = -10781.983430069917
Iteration 10300: Loss = -10781.982936613185
Iteration 10400: Loss = -10781.98257389224
Iteration 10500: Loss = -10781.982072044282
Iteration 10600: Loss = -10781.981845094459
Iteration 10700: Loss = -10781.981661917182
Iteration 10800: Loss = -10781.981446218933
Iteration 10900: Loss = -10781.981182758009
Iteration 11000: Loss = -10781.981088463644
Iteration 11100: Loss = -10781.980616658702
Iteration 11200: Loss = -10781.979613453124
Iteration 11300: Loss = -10781.979154435396
Iteration 11400: Loss = -10781.978534661206
Iteration 11500: Loss = -10781.978346370994
Iteration 11600: Loss = -10781.986065496742
1
Iteration 11700: Loss = -10781.977850537462
Iteration 11800: Loss = -10781.97735748505
Iteration 11900: Loss = -10781.978205460618
1
Iteration 12000: Loss = -10781.976988486109
Iteration 12100: Loss = -10781.976840429048
Iteration 12200: Loss = -10781.980710476479
1
Iteration 12300: Loss = -10781.976173270627
Iteration 12400: Loss = -10781.975943298638
Iteration 12500: Loss = -10781.983233970283
1
Iteration 12600: Loss = -10781.975818952915
Iteration 12700: Loss = -10781.975800558785
Iteration 12800: Loss = -10782.232234796773
1
Iteration 12900: Loss = -10781.975656800782
Iteration 13000: Loss = -10781.975555573736
Iteration 13100: Loss = -10781.97565789761
1
Iteration 13200: Loss = -10781.9756319295
Iteration 13300: Loss = -10781.975490720986
Iteration 13400: Loss = -10781.975484426559
Iteration 13500: Loss = -10781.97883654574
1
Iteration 13600: Loss = -10781.975446658349
Iteration 13700: Loss = -10781.975463596835
Iteration 13800: Loss = -10782.00078049068
1
Iteration 13900: Loss = -10781.975389916423
Iteration 14000: Loss = -10781.975426313444
Iteration 14100: Loss = -10781.975897700473
1
Iteration 14200: Loss = -10781.975409655917
Iteration 14300: Loss = -10781.975344584698
Iteration 14400: Loss = -10781.97538229832
Iteration 14500: Loss = -10781.975276626772
Iteration 14600: Loss = -10781.975231531844
Iteration 14700: Loss = -10781.975236006554
Iteration 14800: Loss = -10781.975466021053
1
Iteration 14900: Loss = -10781.975226117578
Iteration 15000: Loss = -10781.975205090193
Iteration 15100: Loss = -10781.975252679402
Iteration 15200: Loss = -10781.975120504523
Iteration 15300: Loss = -10781.97800924732
1
Iteration 15400: Loss = -10781.975033352048
Iteration 15500: Loss = -10781.974986121426
Iteration 15600: Loss = -10782.226588490223
1
Iteration 15700: Loss = -10781.974945640164
Iteration 15800: Loss = -10781.974868405823
Iteration 15900: Loss = -10781.974846087078
Iteration 16000: Loss = -10781.975108950905
1
Iteration 16100: Loss = -10781.974877802735
Iteration 16200: Loss = -10781.97483338959
Iteration 16300: Loss = -10782.023495830384
1
Iteration 16400: Loss = -10781.974819041121
Iteration 16500: Loss = -10781.975329090228
1
Iteration 16600: Loss = -10781.975905426489
2
Iteration 16700: Loss = -10781.974805974023
Iteration 16800: Loss = -10781.984046448986
1
Iteration 16900: Loss = -10781.97477113003
Iteration 17000: Loss = -10781.97475537517
Iteration 17100: Loss = -10781.976225475053
1
Iteration 17200: Loss = -10781.974653537789
Iteration 17300: Loss = -10781.99450514372
1
Iteration 17400: Loss = -10781.974631100955
Iteration 17500: Loss = -10781.997047928824
1
Iteration 17600: Loss = -10781.974630527178
Iteration 17700: Loss = -10781.974647279458
Iteration 17800: Loss = -10781.974670520929
Iteration 17900: Loss = -10781.974572403491
Iteration 18000: Loss = -10781.97680444778
1
Iteration 18100: Loss = -10781.974494028194
Iteration 18200: Loss = -10781.974510643904
Iteration 18300: Loss = -10781.974523176119
Iteration 18400: Loss = -10781.974511761326
Iteration 18500: Loss = -10781.974508419426
Iteration 18600: Loss = -10781.974582358482
Iteration 18700: Loss = -10781.974476604586
Iteration 18800: Loss = -10781.98709992327
1
Iteration 18900: Loss = -10781.974482684347
Iteration 19000: Loss = -10781.974462386152
Iteration 19100: Loss = -10781.978563262608
1
Iteration 19200: Loss = -10781.974450219204
Iteration 19300: Loss = -10781.97477966908
1
Iteration 19400: Loss = -10781.97451658849
Iteration 19500: Loss = -10781.97448144022
Iteration 19600: Loss = -10781.974763725828
1
Iteration 19700: Loss = -10781.974479224562
Iteration 19800: Loss = -10782.20368967564
1
Iteration 19900: Loss = -10781.974488062
pi: tensor([[1.0000e+00, 4.3711e-07],
        [1.8443e-03, 9.9816e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0497, 0.9503], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3364, 0.1242],
         [0.7260, 0.1549]],

        [[0.5929, 0.1806],
         [0.5666, 0.6172]],

        [[0.5863, 0.2132],
         [0.7170, 0.5656]],

        [[0.7142, 0.1473],
         [0.6443, 0.7303]],

        [[0.6585, 0.2175],
         [0.5016, 0.5274]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.007984204356142407
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.01599776304986733
Global Adjusted Rand Index: -0.00306642100257056
Average Adjusted Rand Index: -0.0035835354287238087
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23162.10908164646
Iteration 100: Loss = -10787.450498178801
Iteration 200: Loss = -10787.045022283735
Iteration 300: Loss = -10786.894032353779
Iteration 400: Loss = -10786.797088071504
Iteration 500: Loss = -10786.7271882536
Iteration 600: Loss = -10786.674155969373
Iteration 700: Loss = -10786.631709347688
Iteration 800: Loss = -10786.595011101192
Iteration 900: Loss = -10786.559988775776
Iteration 1000: Loss = -10786.522219457043
Iteration 1100: Loss = -10786.475643277867
Iteration 1200: Loss = -10786.411589983069
Iteration 1300: Loss = -10786.316883304477
Iteration 1400: Loss = -10786.06362520854
Iteration 1500: Loss = -10785.387231471415
Iteration 1600: Loss = -10784.906262989178
Iteration 1700: Loss = -10784.572755498284
Iteration 1800: Loss = -10784.315100938877
Iteration 1900: Loss = -10783.929321382027
Iteration 2000: Loss = -10783.034358428382
Iteration 2100: Loss = -10782.615177847112
Iteration 2200: Loss = -10782.422506276338
Iteration 2300: Loss = -10782.313440254178
Iteration 2400: Loss = -10782.245736278606
Iteration 2500: Loss = -10782.200019937827
Iteration 2600: Loss = -10782.166818107315
Iteration 2700: Loss = -10782.141675785515
Iteration 2800: Loss = -10782.121928552944
Iteration 2900: Loss = -10782.105823015698
Iteration 3000: Loss = -10782.092412486152
Iteration 3100: Loss = -10782.081032774984
Iteration 3200: Loss = -10782.071358230532
Iteration 3300: Loss = -10782.063032381853
Iteration 3400: Loss = -10782.055785407594
Iteration 3500: Loss = -10782.049393366162
Iteration 3600: Loss = -10782.043782681045
Iteration 3700: Loss = -10782.038789343831
Iteration 3800: Loss = -10782.034315911791
Iteration 3900: Loss = -10782.030207121305
Iteration 4000: Loss = -10782.026436853668
Iteration 4100: Loss = -10782.023063014283
Iteration 4200: Loss = -10782.019975999605
Iteration 4300: Loss = -10782.017111580635
Iteration 4400: Loss = -10782.01454839057
Iteration 4500: Loss = -10782.012242785917
Iteration 4600: Loss = -10782.010188892758
Iteration 4700: Loss = -10782.008306926718
Iteration 4800: Loss = -10782.006495438605
Iteration 4900: Loss = -10782.004852572554
Iteration 5000: Loss = -10782.00323698164
Iteration 5100: Loss = -10782.001629977332
Iteration 5200: Loss = -10782.000055392713
Iteration 5300: Loss = -10781.998657457649
Iteration 5400: Loss = -10781.997404093001
Iteration 5500: Loss = -10781.996317303083
Iteration 5600: Loss = -10781.995340041753
Iteration 5700: Loss = -10781.994451909362
Iteration 5800: Loss = -10781.99362072147
Iteration 5900: Loss = -10781.99286383547
Iteration 6000: Loss = -10781.992141120014
Iteration 6100: Loss = -10781.991464727735
Iteration 6200: Loss = -10781.990777204028
Iteration 6300: Loss = -10781.990176247478
Iteration 6400: Loss = -10781.989615448407
Iteration 6500: Loss = -10781.989023990249
Iteration 6600: Loss = -10781.98848113981
Iteration 6700: Loss = -10781.98792071062
Iteration 6800: Loss = -10781.987398899917
Iteration 6900: Loss = -10781.9868468925
Iteration 7000: Loss = -10781.986279828565
Iteration 7100: Loss = -10781.985731221044
Iteration 7200: Loss = -10781.98519174354
Iteration 7300: Loss = -10781.984751978798
Iteration 7400: Loss = -10781.984342113068
Iteration 7500: Loss = -10781.983925696626
Iteration 7600: Loss = -10781.983548488392
Iteration 7700: Loss = -10781.983196372974
Iteration 7800: Loss = -10781.982841809902
Iteration 7900: Loss = -10781.98260640917
Iteration 8000: Loss = -10781.982377639652
Iteration 8100: Loss = -10781.982175316336
Iteration 8200: Loss = -10781.984380762913
1
Iteration 8300: Loss = -10781.982394570468
2
Iteration 8400: Loss = -10782.006343825107
3
Iteration 8500: Loss = -10781.981377552207
Iteration 8600: Loss = -10781.98130790915
Iteration 8700: Loss = -10781.981042547048
Iteration 8800: Loss = -10781.981187769654
1
Iteration 8900: Loss = -10781.980708624316
Iteration 9000: Loss = -10781.984014044227
1
Iteration 9100: Loss = -10781.980364486624
Iteration 9200: Loss = -10781.981896459043
1
Iteration 9300: Loss = -10781.979966358987
Iteration 9400: Loss = -10781.97968325349
Iteration 9500: Loss = -10781.982738902057
1
Iteration 9600: Loss = -10781.979271860046
Iteration 9700: Loss = -10781.979083191116
Iteration 9800: Loss = -10781.978892328952
Iteration 9900: Loss = -10781.979131167935
1
Iteration 10000: Loss = -10781.978572007285
Iteration 10100: Loss = -10781.978467104516
Iteration 10200: Loss = -10781.979807457872
1
Iteration 10300: Loss = -10781.978249084685
Iteration 10400: Loss = -10781.978097493888
Iteration 10500: Loss = -10781.989286835309
1
Iteration 10600: Loss = -10781.977746795897
Iteration 10700: Loss = -10781.977520159662
Iteration 10800: Loss = -10782.122704210658
1
Iteration 10900: Loss = -10781.977241991071
Iteration 11000: Loss = -10781.977059067458
Iteration 11100: Loss = -10781.993549020315
1
Iteration 11200: Loss = -10781.976904512827
Iteration 11300: Loss = -10781.976859139213
Iteration 11400: Loss = -10782.014710172243
1
Iteration 11500: Loss = -10781.97678172523
Iteration 11600: Loss = -10781.97670801033
Iteration 11700: Loss = -10782.164826455919
1
Iteration 11800: Loss = -10781.976619470246
Iteration 11900: Loss = -10781.976579748327
Iteration 12000: Loss = -10782.029603363959
1
Iteration 12100: Loss = -10781.97641109179
Iteration 12200: Loss = -10781.976327551247
Iteration 12300: Loss = -10781.978565407388
1
Iteration 12400: Loss = -10781.976162356548
Iteration 12500: Loss = -10781.976041448921
Iteration 12600: Loss = -10781.997821802226
1
Iteration 12700: Loss = -10781.97589783976
Iteration 12800: Loss = -10781.975788947524
Iteration 12900: Loss = -10781.983512265391
1
Iteration 13000: Loss = -10781.975692871561
Iteration 13100: Loss = -10781.975652887986
Iteration 13200: Loss = -10781.984216878092
1
Iteration 13300: Loss = -10781.975438603251
Iteration 13400: Loss = -10781.975384540156
Iteration 13500: Loss = -10782.015482803035
1
Iteration 13600: Loss = -10781.97532978206
Iteration 13700: Loss = -10781.97529254654
Iteration 13800: Loss = -10781.994759949699
1
Iteration 13900: Loss = -10781.975271768024
Iteration 14000: Loss = -10781.975289105983
Iteration 14100: Loss = -10781.97735992177
1
Iteration 14200: Loss = -10781.975269730725
Iteration 14300: Loss = -10781.975269631326
Iteration 14400: Loss = -10781.987379190186
1
Iteration 14500: Loss = -10781.97512044435
Iteration 14600: Loss = -10781.975162333843
Iteration 14700: Loss = -10782.024949282613
1
Iteration 14800: Loss = -10781.975103650553
Iteration 14900: Loss = -10781.975079286423
Iteration 15000: Loss = -10782.073036168666
1
Iteration 15100: Loss = -10781.975036967164
Iteration 15200: Loss = -10781.975028541396
Iteration 15300: Loss = -10782.0436104164
1
Iteration 15400: Loss = -10781.974930927385
Iteration 15500: Loss = -10781.974932962847
Iteration 15600: Loss = -10782.000465315252
1
Iteration 15700: Loss = -10781.974840848034
Iteration 15800: Loss = -10781.974833013503
Iteration 15900: Loss = -10782.031593531969
1
Iteration 16000: Loss = -10781.974791506458
Iteration 16100: Loss = -10781.974765305427
Iteration 16200: Loss = -10782.140665393015
1
Iteration 16300: Loss = -10781.97478969114
Iteration 16400: Loss = -10781.974781118413
Iteration 16500: Loss = -10781.974790666756
Iteration 16600: Loss = -10781.97522556788
1
Iteration 16700: Loss = -10781.974739004569
Iteration 16800: Loss = -10781.97474489401
Iteration 16900: Loss = -10781.976385247026
1
Iteration 17000: Loss = -10781.974725047272
Iteration 17100: Loss = -10781.97471130462
Iteration 17200: Loss = -10781.974716700088
Iteration 17300: Loss = -10781.974916782292
1
Iteration 17400: Loss = -10781.974706386352
Iteration 17500: Loss = -10781.97795252425
1
Iteration 17600: Loss = -10781.9747231132
Iteration 17700: Loss = -10781.975982573704
1
Iteration 17800: Loss = -10781.974727293167
Iteration 17900: Loss = -10782.210776490136
1
Iteration 18000: Loss = -10781.974683728931
Iteration 18100: Loss = -10781.98552658025
1
Iteration 18200: Loss = -10781.974710160304
Iteration 18300: Loss = -10782.075068542337
1
Iteration 18400: Loss = -10781.974724867898
Iteration 18500: Loss = -10781.974711146906
Iteration 18600: Loss = -10781.974800530932
Iteration 18700: Loss = -10781.974736515278
Iteration 18800: Loss = -10781.97478533977
Iteration 18900: Loss = -10781.974719262325
Iteration 19000: Loss = -10781.974691082174
Iteration 19100: Loss = -10781.981434512021
1
Iteration 19200: Loss = -10781.974673780462
Iteration 19300: Loss = -10782.033162783559
1
Iteration 19400: Loss = -10781.974637469943
Iteration 19500: Loss = -10782.161677826074
1
Iteration 19600: Loss = -10781.974628241423
Iteration 19700: Loss = -10782.005568153432
1
Iteration 19800: Loss = -10781.974591528726
Iteration 19900: Loss = -10781.974588212726
pi: tensor([[9.9816e-01, 1.8434e-03],
        [3.6789e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9503, 0.0497], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1549, 0.1243],
         [0.6193, 0.3365]],

        [[0.7210, 0.1806],
         [0.7299, 0.5462]],

        [[0.5636, 0.2132],
         [0.5134, 0.5994]],

        [[0.7042, 0.1475],
         [0.7142, 0.5016]],

        [[0.7248, 0.2175],
         [0.6646, 0.7243]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.007984204356142407
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0019209323236274677
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.01599776304986733
Global Adjusted Rand Index: -0.00306642100257056
Average Adjusted Rand Index: -0.0035835354287238087
10691.75465580235
[-0.00306642100257056, -0.00306642100257056] [-0.0035835354287238087, -0.0035835354287238087] [10781.974477625685, 10781.974719802634]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11065.480520492065
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19710.52474610996
Iteration 100: Loss = -11087.264068422206
Iteration 200: Loss = -11086.231587360297
Iteration 300: Loss = -11085.089755205729
Iteration 400: Loss = -11084.871209871038
Iteration 500: Loss = -11084.72259073513
Iteration 600: Loss = -11082.421636572499
Iteration 700: Loss = -11081.743664883872
Iteration 800: Loss = -11081.599200646073
Iteration 900: Loss = -11081.53972867752
Iteration 1000: Loss = -11081.503024080514
Iteration 1100: Loss = -11081.473582853827
Iteration 1200: Loss = -11081.447682149272
Iteration 1300: Loss = -11081.423063673821
Iteration 1400: Loss = -11081.395495589433
Iteration 1500: Loss = -11081.356172613745
Iteration 1600: Loss = -11081.295421797555
Iteration 1700: Loss = -11081.230796453057
Iteration 1800: Loss = -11081.184126505332
Iteration 1900: Loss = -11081.15179117013
Iteration 2000: Loss = -11081.12770038933
Iteration 2100: Loss = -11081.109650702176
Iteration 2200: Loss = -11081.095599260738
Iteration 2300: Loss = -11081.084034652295
Iteration 2400: Loss = -11081.074163924693
Iteration 2500: Loss = -11081.065905520787
Iteration 2600: Loss = -11081.05920846569
Iteration 2700: Loss = -11081.053609758897
Iteration 2800: Loss = -11081.048765385543
Iteration 2900: Loss = -11081.044571869807
Iteration 3000: Loss = -11081.04094953446
Iteration 3100: Loss = -11081.0377406112
Iteration 3200: Loss = -11081.034926726148
Iteration 3300: Loss = -11081.032416670401
Iteration 3400: Loss = -11081.030246798799
Iteration 3500: Loss = -11081.028312516217
Iteration 3600: Loss = -11081.026588509923
Iteration 3700: Loss = -11081.02501503911
Iteration 3800: Loss = -11081.023594561504
Iteration 3900: Loss = -11081.022325545391
Iteration 4000: Loss = -11081.021170719967
Iteration 4100: Loss = -11081.0201201287
Iteration 4200: Loss = -11081.019122134649
Iteration 4300: Loss = -11081.01823263874
Iteration 4400: Loss = -11081.017439820656
Iteration 4500: Loss = -11081.016694223943
Iteration 4600: Loss = -11081.016237319154
Iteration 4700: Loss = -11081.015432118546
Iteration 4800: Loss = -11081.032461796509
1
Iteration 4900: Loss = -11081.014355347352
Iteration 5000: Loss = -11081.01388118674
Iteration 5100: Loss = -11081.014246843122
1
Iteration 5200: Loss = -11081.01300247488
Iteration 5300: Loss = -11081.01266607987
Iteration 5400: Loss = -11081.012256095113
Iteration 5500: Loss = -11081.012069074266
Iteration 5600: Loss = -11081.011595528202
Iteration 5700: Loss = -11081.0113192972
Iteration 5800: Loss = -11081.011050220437
Iteration 5900: Loss = -11081.010792390229
Iteration 6000: Loss = -11081.0105107179
Iteration 6100: Loss = -11081.010307452822
Iteration 6200: Loss = -11081.01011352098
Iteration 6300: Loss = -11081.00992740418
Iteration 6400: Loss = -11081.009715342534
Iteration 6500: Loss = -11081.009626155197
Iteration 6600: Loss = -11081.009406929394
Iteration 6700: Loss = -11081.010693919232
1
Iteration 6800: Loss = -11081.00910400867
Iteration 6900: Loss = -11081.009115525758
Iteration 7000: Loss = -11081.008848892374
Iteration 7100: Loss = -11081.008717550556
Iteration 7200: Loss = -11081.00865941085
Iteration 7300: Loss = -11081.00847936729
Iteration 7400: Loss = -11081.008785205742
1
Iteration 7500: Loss = -11081.008328859745
Iteration 7600: Loss = -11081.009184721328
1
Iteration 7700: Loss = -11081.06346248554
2
Iteration 7800: Loss = -11081.008051709518
Iteration 7900: Loss = -11081.00807939332
Iteration 8000: Loss = -11081.00787581572
Iteration 8100: Loss = -11081.007916012988
Iteration 8200: Loss = -11081.007790229796
Iteration 8300: Loss = -11081.012266921336
1
Iteration 8400: Loss = -11081.007627160156
Iteration 8500: Loss = -11081.10215732306
1
Iteration 8600: Loss = -11081.007579282601
Iteration 8700: Loss = -11081.144158881756
1
Iteration 8800: Loss = -11081.00744587986
Iteration 8900: Loss = -11081.007441395674
Iteration 9000: Loss = -11081.00746451067
Iteration 9100: Loss = -11081.010148215144
1
Iteration 9200: Loss = -11081.007555221606
Iteration 9300: Loss = -11081.228088047066
1
Iteration 9400: Loss = -11081.00722669893
Iteration 9500: Loss = -11081.007231717304
Iteration 9600: Loss = -11081.007213571984
Iteration 9700: Loss = -11081.009113961509
1
Iteration 9800: Loss = -11081.141530107036
2
Iteration 9900: Loss = -11081.007076362916
Iteration 10000: Loss = -11081.02763668482
1
Iteration 10100: Loss = -11081.00701694912
Iteration 10200: Loss = -11081.36257575681
1
Iteration 10300: Loss = -11081.007050727605
Iteration 10400: Loss = -11081.006979255644
Iteration 10500: Loss = -11081.007475614653
1
Iteration 10600: Loss = -11081.007171848558
2
Iteration 10700: Loss = -11081.006935491678
Iteration 10800: Loss = -11081.00688950398
Iteration 10900: Loss = -11081.054037586495
1
Iteration 11000: Loss = -11081.009650317506
2
Iteration 11100: Loss = -11081.008090884843
3
Iteration 11200: Loss = -11081.030069507367
4
Iteration 11300: Loss = -11081.006915136004
Iteration 11400: Loss = -11081.010274345577
1
Iteration 11500: Loss = -11081.013096365015
2
Iteration 11600: Loss = -11081.007588802793
3
Iteration 11700: Loss = -11081.007274297517
4
Iteration 11800: Loss = -11081.006745429077
Iteration 11900: Loss = -11081.008492647272
1
Iteration 12000: Loss = -11081.014535798531
2
Iteration 12100: Loss = -11081.007509888308
3
Iteration 12200: Loss = -11081.006739545915
Iteration 12300: Loss = -11081.009779265118
1
Iteration 12400: Loss = -11081.093575168705
2
Iteration 12500: Loss = -11081.00772889571
3
Iteration 12600: Loss = -11081.008250919172
4
Iteration 12700: Loss = -11081.04165749072
5
Iteration 12800: Loss = -11081.006676191733
Iteration 12900: Loss = -11081.008718057024
1
Iteration 13000: Loss = -11081.006712704722
Iteration 13100: Loss = -11081.006695598568
Iteration 13200: Loss = -11081.00677081506
Iteration 13300: Loss = -11081.006657173817
Iteration 13400: Loss = -11081.067213230628
1
Iteration 13500: Loss = -11081.006653553528
Iteration 13600: Loss = -11081.01289969005
1
Iteration 13700: Loss = -11081.006687282277
Iteration 13800: Loss = -11081.00667195876
Iteration 13900: Loss = -11081.008499234315
1
Iteration 14000: Loss = -11081.006636633105
Iteration 14100: Loss = -11081.006936410655
1
Iteration 14200: Loss = -11081.006701398144
Iteration 14300: Loss = -11081.00670059675
Iteration 14400: Loss = -11081.007249251328
1
Iteration 14500: Loss = -11081.006644844618
Iteration 14600: Loss = -11081.046664692089
1
Iteration 14700: Loss = -11081.013774234161
2
Iteration 14800: Loss = -11081.016769865379
3
Iteration 14900: Loss = -11081.006664858907
Iteration 15000: Loss = -11081.008789217298
1
Iteration 15100: Loss = -11081.01958779642
2
Iteration 15200: Loss = -11081.008212563658
3
Iteration 15300: Loss = -11081.006685614586
Iteration 15400: Loss = -11081.057205307963
1
Iteration 15500: Loss = -11081.00661930614
Iteration 15600: Loss = -11081.00703550218
1
Iteration 15700: Loss = -11081.006828006499
2
Iteration 15800: Loss = -11081.012133945513
3
Iteration 15900: Loss = -11081.027046126514
4
Iteration 16000: Loss = -11081.006669764134
Iteration 16100: Loss = -11081.006808528637
1
Iteration 16200: Loss = -11081.28308617859
2
Iteration 16300: Loss = -11081.006621964925
Iteration 16400: Loss = -11081.006887058169
1
Iteration 16500: Loss = -11081.160340626884
2
Iteration 16600: Loss = -11081.0066343235
Iteration 16700: Loss = -11081.006929061354
1
Iteration 16800: Loss = -11081.006605830937
Iteration 16900: Loss = -11081.006638309653
Iteration 17000: Loss = -11081.018968229295
1
Iteration 17100: Loss = -11081.006624420916
Iteration 17200: Loss = -11081.051416587916
1
Iteration 17300: Loss = -11081.006641709426
Iteration 17400: Loss = -11081.00702921895
1
Iteration 17500: Loss = -11081.008791577906
2
Iteration 17600: Loss = -11081.006863031149
3
Iteration 17700: Loss = -11081.013832125898
4
Iteration 17800: Loss = -11081.006679452097
Iteration 17900: Loss = -11081.00671710774
Iteration 18000: Loss = -11081.032213240842
1
Iteration 18100: Loss = -11081.006583225193
Iteration 18200: Loss = -11081.044189921617
1
Iteration 18300: Loss = -11081.028311149159
2
Iteration 18400: Loss = -11081.006614909948
Iteration 18500: Loss = -11081.009927911035
1
Iteration 18600: Loss = -11081.007450805962
2
Iteration 18700: Loss = -11081.024564599184
3
Iteration 18800: Loss = -11081.007439318877
4
Iteration 18900: Loss = -11081.00664370249
Iteration 19000: Loss = -11081.00695507586
1
Iteration 19100: Loss = -11081.258418245088
2
Iteration 19200: Loss = -11081.006597104371
Iteration 19300: Loss = -11081.009690285297
1
Iteration 19400: Loss = -11081.008266031495
2
Iteration 19500: Loss = -11081.006654888457
Iteration 19600: Loss = -11081.007051372379
1
Iteration 19700: Loss = -11081.01461511213
2
Iteration 19800: Loss = -11081.008314750832
3
Iteration 19900: Loss = -11081.007564347288
4
pi: tensor([[1.0000e+00, 2.7464e-08],
        [7.5092e-01, 2.4908e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8176, 0.1824], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1606, 0.1957],
         [0.5051, 0.2391]],

        [[0.6409, 0.2657],
         [0.6202, 0.5041]],

        [[0.5145, 0.0984],
         [0.6029, 0.5778]],

        [[0.5380, 0.1701],
         [0.6177, 0.5557]],

        [[0.5840, 0.1329],
         [0.5268, 0.7060]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.018289900270691713
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003841451570715221
Average Adjusted Rand Index: -0.004172213163243583
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23524.906056738044
Iteration 100: Loss = -11087.295395399682
Iteration 200: Loss = -11085.721501188375
Iteration 300: Loss = -11085.170305545389
Iteration 400: Loss = -11084.994242326855
Iteration 500: Loss = -11084.703845660397
Iteration 600: Loss = -11083.76157908094
Iteration 700: Loss = -11082.974044681174
Iteration 800: Loss = -11082.391831209241
Iteration 900: Loss = -11081.947155779477
Iteration 1000: Loss = -11081.634499562862
Iteration 1100: Loss = -11081.422182865128
Iteration 1200: Loss = -11081.276178350166
Iteration 1300: Loss = -11081.181224977845
Iteration 1400: Loss = -11081.125873157795
Iteration 1500: Loss = -11081.093515240218
Iteration 1600: Loss = -11081.072841267158
Iteration 1700: Loss = -11081.058559698378
Iteration 1800: Loss = -11081.048202235024
Iteration 1900: Loss = -11081.040382759958
Iteration 2000: Loss = -11081.034254017468
Iteration 2100: Loss = -11081.029282646105
Iteration 2200: Loss = -11081.025207360693
Iteration 2300: Loss = -11081.021738371217
Iteration 2400: Loss = -11081.018843294087
Iteration 2500: Loss = -11081.016309110468
Iteration 2600: Loss = -11081.014176732797
Iteration 2700: Loss = -11081.012263296678
Iteration 2800: Loss = -11081.010590081793
Iteration 2900: Loss = -11081.009077173507
Iteration 3000: Loss = -11081.007798465069
Iteration 3100: Loss = -11081.00664080984
Iteration 3200: Loss = -11081.00559390739
Iteration 3300: Loss = -11081.004626122467
Iteration 3400: Loss = -11081.003750271431
Iteration 3500: Loss = -11081.002979511904
Iteration 3600: Loss = -11081.002246378883
Iteration 3700: Loss = -11081.001599053337
Iteration 3800: Loss = -11081.001015215852
Iteration 3900: Loss = -11081.000454152367
Iteration 4000: Loss = -11080.999919729431
Iteration 4100: Loss = -11080.999470143372
Iteration 4200: Loss = -11080.999011257285
Iteration 4300: Loss = -11080.998628019543
Iteration 4400: Loss = -11080.998261302646
Iteration 4500: Loss = -11080.997917646568
Iteration 4600: Loss = -11080.997575300817
Iteration 4700: Loss = -11080.997233825807
Iteration 4800: Loss = -11080.996962405961
Iteration 4900: Loss = -11080.996681752167
Iteration 5000: Loss = -11080.996388980071
Iteration 5100: Loss = -11080.996175782006
Iteration 5200: Loss = -11080.99588777102
Iteration 5300: Loss = -11080.995693868188
Iteration 5400: Loss = -11080.995470763355
Iteration 5500: Loss = -11080.995299170712
Iteration 5600: Loss = -11080.995058740245
Iteration 5700: Loss = -11080.99487282666
Iteration 5800: Loss = -11080.994689696592
Iteration 5900: Loss = -11080.997215388323
1
Iteration 6000: Loss = -11080.994357567484
Iteration 6100: Loss = -11080.994191152966
Iteration 6200: Loss = -11080.99407245068
Iteration 6300: Loss = -11080.993875568993
Iteration 6400: Loss = -11080.994771738762
1
Iteration 6500: Loss = -11080.99365940813
Iteration 6600: Loss = -11080.99352401461
Iteration 6700: Loss = -11080.99343048633
Iteration 6800: Loss = -11080.993319091016
Iteration 6900: Loss = -11080.993249014467
Iteration 7000: Loss = -11080.993140912657
Iteration 7100: Loss = -11080.993054726385
Iteration 7200: Loss = -11080.992998782673
Iteration 7300: Loss = -11080.993125685465
1
Iteration 7400: Loss = -11080.992820663541
Iteration 7500: Loss = -11080.995603972506
1
Iteration 7600: Loss = -11080.99270133931
Iteration 7700: Loss = -11080.99267404588
Iteration 7800: Loss = -11080.992606973976
Iteration 7900: Loss = -11080.992543441138
Iteration 8000: Loss = -11080.992542959306
Iteration 8100: Loss = -11080.992482517995
Iteration 8200: Loss = -11080.992614980767
1
Iteration 8300: Loss = -11080.992452678325
Iteration 8400: Loss = -11080.992869701804
1
Iteration 8500: Loss = -11080.992375501024
Iteration 8600: Loss = -11081.003664932978
1
Iteration 8700: Loss = -11080.99228355426
Iteration 8800: Loss = -11081.128304769174
1
Iteration 8900: Loss = -11080.9922464923
Iteration 9000: Loss = -11080.992188178478
Iteration 9100: Loss = -11080.992414269234
1
Iteration 9200: Loss = -11080.99218435107
Iteration 9300: Loss = -11080.992198385846
Iteration 9400: Loss = -11080.992128729747
Iteration 9500: Loss = -11080.992242028706
1
Iteration 9600: Loss = -11080.992059577722
Iteration 9700: Loss = -11081.131930723619
1
Iteration 9800: Loss = -11080.992049557504
Iteration 9900: Loss = -11080.99255152505
1
Iteration 10000: Loss = -11080.992059247428
Iteration 10100: Loss = -11080.992198199654
1
Iteration 10200: Loss = -11080.992030003656
Iteration 10300: Loss = -11080.996770103797
1
Iteration 10400: Loss = -11080.991979142385
Iteration 10500: Loss = -11080.994490009425
1
Iteration 10600: Loss = -11080.991968544364
Iteration 10700: Loss = -11080.991985267472
Iteration 10800: Loss = -11080.99254014756
1
Iteration 10900: Loss = -11080.991916399344
Iteration 11000: Loss = -11081.051438017239
1
Iteration 11100: Loss = -11080.991940946249
Iteration 11200: Loss = -11081.01530251846
1
Iteration 11300: Loss = -11080.99401221219
2
Iteration 11400: Loss = -11081.022210613328
3
Iteration 11500: Loss = -11080.991869311063
Iteration 11600: Loss = -11081.003847206981
1
Iteration 11700: Loss = -11080.991850497438
Iteration 11800: Loss = -11081.312888080432
1
Iteration 11900: Loss = -11080.991878878329
Iteration 12000: Loss = -11080.991889128527
Iteration 12100: Loss = -11080.99185373178
Iteration 12200: Loss = -11080.991845739552
Iteration 12300: Loss = -11080.991923221001
Iteration 12400: Loss = -11080.991794728361
Iteration 12500: Loss = -11080.992986898467
1
Iteration 12600: Loss = -11080.991827826956
Iteration 12700: Loss = -11080.994639691377
1
Iteration 12800: Loss = -11080.992723269987
2
Iteration 12900: Loss = -11080.992303451842
3
Iteration 13000: Loss = -11080.9919765097
4
Iteration 13100: Loss = -11080.991858327054
Iteration 13200: Loss = -11080.99191570911
Iteration 13300: Loss = -11080.991860178825
Iteration 13400: Loss = -11080.991782209016
Iteration 13500: Loss = -11081.127087739356
1
Iteration 13600: Loss = -11080.991791816912
Iteration 13700: Loss = -11080.991789252055
Iteration 13800: Loss = -11080.99240968676
1
Iteration 13900: Loss = -11080.991811622607
Iteration 14000: Loss = -11080.993645170107
1
Iteration 14100: Loss = -11080.991897967157
Iteration 14200: Loss = -11080.99254593364
1
Iteration 14300: Loss = -11080.992974732726
2
Iteration 14400: Loss = -11080.991853340898
Iteration 14500: Loss = -11080.994596209077
1
Iteration 14600: Loss = -11080.991790637985
Iteration 14700: Loss = -11080.992040805666
1
Iteration 14800: Loss = -11081.070451745132
2
Iteration 14900: Loss = -11080.991784216734
Iteration 15000: Loss = -11081.051509527644
1
Iteration 15100: Loss = -11080.991797069022
Iteration 15200: Loss = -11081.02770161773
1
Iteration 15300: Loss = -11080.991876750139
Iteration 15400: Loss = -11081.004348257518
1
Iteration 15500: Loss = -11081.055742481796
2
Iteration 15600: Loss = -11080.992289047403
3
Iteration 15700: Loss = -11080.992462655262
4
Iteration 15800: Loss = -11080.999673530048
5
Iteration 15900: Loss = -11080.991800094456
Iteration 16000: Loss = -11080.995672571822
1
Iteration 16100: Loss = -11080.99182390856
Iteration 16200: Loss = -11080.9978623689
1
Iteration 16300: Loss = -11080.991782338353
Iteration 16400: Loss = -11080.99216003383
1
Iteration 16500: Loss = -11081.0534825745
2
Iteration 16600: Loss = -11080.991788837124
Iteration 16700: Loss = -11080.99217598718
1
Iteration 16800: Loss = -11080.996330076403
2
Iteration 16900: Loss = -11080.991766710127
Iteration 17000: Loss = -11080.99356826062
1
Iteration 17100: Loss = -11081.096437425003
2
Iteration 17200: Loss = -11080.991794415064
Iteration 17300: Loss = -11080.99204104631
1
Iteration 17400: Loss = -11080.993618627288
2
Iteration 17500: Loss = -11081.027574261949
3
Iteration 17600: Loss = -11080.994433938127
4
Iteration 17700: Loss = -11080.991819840705
Iteration 17800: Loss = -11080.997870606274
1
Iteration 17900: Loss = -11081.029847045927
2
Iteration 18000: Loss = -11080.991982667827
3
Iteration 18100: Loss = -11080.991868827894
Iteration 18200: Loss = -11080.993664088104
1
Iteration 18300: Loss = -11080.991750250088
Iteration 18400: Loss = -11080.992393227316
1
Iteration 18500: Loss = -11080.992304601345
2
Iteration 18600: Loss = -11081.048278816334
3
Iteration 18700: Loss = -11080.991744211173
Iteration 18800: Loss = -11080.991848847856
1
Iteration 18900: Loss = -11081.00901386118
2
Iteration 19000: Loss = -11080.992255144005
3
Iteration 19100: Loss = -11080.991843666467
Iteration 19200: Loss = -11081.006255943637
1
Iteration 19300: Loss = -11080.99176276468
Iteration 19400: Loss = -11081.015869442059
1
Iteration 19500: Loss = -11080.991762388856
Iteration 19600: Loss = -11081.012663388976
1
Iteration 19700: Loss = -11080.9921131019
2
Iteration 19800: Loss = -11080.991880565804
3
Iteration 19900: Loss = -11081.00452494217
4
pi: tensor([[9.8931e-01, 1.0687e-02],
        [1.0000e+00, 6.6419e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9146, 0.0854], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1638, 0.2140],
         [0.6127, 0.3085]],

        [[0.5234, 0.0308],
         [0.7165, 0.5758]],

        [[0.5813, 0.1916],
         [0.6858, 0.6618]],

        [[0.6090, 0.1287],
         [0.6677, 0.5617]],

        [[0.5390, 0.1723],
         [0.7080, 0.6553]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0007674516613332494
Average Adjusted Rand Index: -0.0026568596553989444
11065.480520492065
[0.003841451570715221, 0.0007674516613332494] [-0.004172213163243583, -0.0026568596553989444] [11081.013965342876, 11080.99187510946]
-------------------------------------
This iteration is 34
True Objective function: Loss = -10712.712643091018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23522.29426026138
Iteration 100: Loss = -10896.33305923558
Iteration 200: Loss = -10894.71217043968
Iteration 300: Loss = -10893.201429268349
Iteration 400: Loss = -10892.12592168966
Iteration 500: Loss = -10891.705774435472
Iteration 600: Loss = -10891.480186539635
Iteration 700: Loss = -10891.336136488564
Iteration 800: Loss = -10891.233633473223
Iteration 900: Loss = -10891.144364097765
Iteration 1000: Loss = -10891.051804648932
Iteration 1100: Loss = -10890.978043057163
Iteration 1200: Loss = -10890.893306609178
Iteration 1300: Loss = -10890.744536439783
Iteration 1400: Loss = -10890.23370607359
Iteration 1500: Loss = -10888.761677148734
Iteration 1600: Loss = -10887.002529192945
Iteration 1700: Loss = -10849.971618148787
Iteration 1800: Loss = -10764.767712758689
Iteration 1900: Loss = -10735.532932364851
Iteration 2000: Loss = -10732.166627657973
Iteration 2100: Loss = -10731.859266677391
Iteration 2200: Loss = -10726.437825735635
Iteration 2300: Loss = -10725.927786773367
Iteration 2400: Loss = -10725.831357132576
Iteration 2500: Loss = -10725.814777342122
Iteration 2600: Loss = -10725.80238935588
Iteration 2700: Loss = -10725.791354957155
Iteration 2800: Loss = -10725.780861294712
Iteration 2900: Loss = -10725.759447291346
Iteration 3000: Loss = -10725.69444532219
Iteration 3100: Loss = -10725.687321663321
Iteration 3200: Loss = -10725.640947130401
Iteration 3300: Loss = -10725.636000495137
Iteration 3400: Loss = -10725.62847547178
Iteration 3500: Loss = -10724.677109557391
Iteration 3600: Loss = -10724.671037581602
Iteration 3700: Loss = -10724.6688113353
Iteration 3800: Loss = -10724.666478931898
Iteration 3900: Loss = -10724.664578666836
Iteration 4000: Loss = -10724.66284161138
Iteration 4100: Loss = -10724.661191101119
Iteration 4200: Loss = -10724.659483140565
Iteration 4300: Loss = -10724.657807648244
Iteration 4400: Loss = -10724.65598046808
Iteration 4500: Loss = -10724.653827172822
Iteration 4600: Loss = -10724.650881367259
Iteration 4700: Loss = -10724.648550379403
Iteration 4800: Loss = -10724.674919539413
1
Iteration 4900: Loss = -10724.642694914992
Iteration 5000: Loss = -10724.638697413602
Iteration 5100: Loss = -10724.63360000754
Iteration 5200: Loss = -10724.62625363117
Iteration 5300: Loss = -10724.62057670307
Iteration 5400: Loss = -10724.600036064101
Iteration 5500: Loss = -10724.578922159271
Iteration 5600: Loss = -10724.547888518924
Iteration 5700: Loss = -10723.921803653093
Iteration 5800: Loss = -10723.35182348903
Iteration 5900: Loss = -10722.640507039518
Iteration 6000: Loss = -10722.636150725026
Iteration 6100: Loss = -10722.633684972217
Iteration 6200: Loss = -10722.494943294883
Iteration 6300: Loss = -10722.485441777833
Iteration 6400: Loss = -10722.46973128407
Iteration 6500: Loss = -10722.457088020159
Iteration 6600: Loss = -10720.189326102516
Iteration 6700: Loss = -10720.150083827573
Iteration 6800: Loss = -10720.149772953951
Iteration 6900: Loss = -10720.148682065135
Iteration 7000: Loss = -10720.143933802829
Iteration 7100: Loss = -10719.69851249235
Iteration 7200: Loss = -10719.694629014637
Iteration 7300: Loss = -10719.67596918949
Iteration 7400: Loss = -10719.675667548536
Iteration 7500: Loss = -10719.67285948864
Iteration 7600: Loss = -10719.626621992162
Iteration 7700: Loss = -10719.625682022986
Iteration 7800: Loss = -10719.615359008507
Iteration 7900: Loss = -10719.615215541427
Iteration 8000: Loss = -10719.615146189797
Iteration 8100: Loss = -10719.619225567045
1
Iteration 8200: Loss = -10719.614986823699
Iteration 8300: Loss = -10719.614950753934
Iteration 8400: Loss = -10719.614888585893
Iteration 8500: Loss = -10719.614763223966
Iteration 8600: Loss = -10719.618403155571
1
Iteration 8700: Loss = -10719.615017553739
2
Iteration 8800: Loss = -10719.614621473156
Iteration 8900: Loss = -10719.614551471246
Iteration 9000: Loss = -10719.615175641049
1
Iteration 9100: Loss = -10719.615677642607
2
Iteration 9200: Loss = -10719.62055082581
3
Iteration 9300: Loss = -10719.614558531006
Iteration 9400: Loss = -10719.657080854586
1
Iteration 9500: Loss = -10719.625874603977
2
Iteration 9600: Loss = -10719.613477012828
Iteration 9700: Loss = -10719.580924764883
Iteration 9800: Loss = -10719.588192875648
1
Iteration 9900: Loss = -10719.571678066523
Iteration 10000: Loss = -10719.564789902588
Iteration 10100: Loss = -10719.566368562952
1
Iteration 10200: Loss = -10719.680741552591
2
Iteration 10300: Loss = -10719.564688428165
Iteration 10400: Loss = -10719.565416243102
1
Iteration 10500: Loss = -10719.564663363648
Iteration 10600: Loss = -10719.554321992284
Iteration 10700: Loss = -10719.55769304986
1
Iteration 10800: Loss = -10719.557658423986
2
Iteration 10900: Loss = -10719.553566246279
Iteration 11000: Loss = -10719.563745655312
1
Iteration 11100: Loss = -10719.55384298807
2
Iteration 11200: Loss = -10719.505924154659
Iteration 11300: Loss = -10719.509306173359
1
Iteration 11400: Loss = -10719.502701308636
Iteration 11500: Loss = -10719.50548484217
1
Iteration 11600: Loss = -10719.512747725155
2
Iteration 11700: Loss = -10719.502784967499
Iteration 11800: Loss = -10719.644741971963
1
Iteration 11900: Loss = -10719.502163260939
Iteration 12000: Loss = -10719.504212626485
1
Iteration 12100: Loss = -10719.430695000414
Iteration 12200: Loss = -10719.430636748997
Iteration 12300: Loss = -10719.52468947154
1
Iteration 12400: Loss = -10719.435748091646
2
Iteration 12500: Loss = -10719.431057230157
3
Iteration 12600: Loss = -10719.44396450092
4
Iteration 12700: Loss = -10719.430340702118
Iteration 12800: Loss = -10719.463755201956
1
Iteration 12900: Loss = -10719.430246388158
Iteration 13000: Loss = -10719.430307262835
Iteration 13100: Loss = -10719.429960293179
Iteration 13200: Loss = -10719.429906546236
Iteration 13300: Loss = -10719.430021480552
1
Iteration 13400: Loss = -10719.429905501407
Iteration 13500: Loss = -10719.432334317904
1
Iteration 13600: Loss = -10719.429876320135
Iteration 13700: Loss = -10719.438686147163
1
Iteration 13800: Loss = -10719.429875483323
Iteration 13900: Loss = -10719.446713466052
1
Iteration 14000: Loss = -10719.429821362432
Iteration 14100: Loss = -10719.432689395457
1
Iteration 14200: Loss = -10719.429804662579
Iteration 14300: Loss = -10719.432356773756
1
Iteration 14400: Loss = -10719.431188237208
2
Iteration 14500: Loss = -10719.430785783961
3
Iteration 14600: Loss = -10719.389583883738
Iteration 14700: Loss = -10719.448120832243
1
Iteration 14800: Loss = -10719.380702597304
Iteration 14900: Loss = -10719.37896209049
Iteration 15000: Loss = -10719.391830729255
1
Iteration 15100: Loss = -10719.378896498689
Iteration 15200: Loss = -10719.380139734261
1
Iteration 15300: Loss = -10719.378892113356
Iteration 15400: Loss = -10719.379425777684
1
Iteration 15500: Loss = -10719.379952281717
2
Iteration 15600: Loss = -10719.379619762018
3
Iteration 15700: Loss = -10719.378867333642
Iteration 15800: Loss = -10719.379372486323
1
Iteration 15900: Loss = -10719.378861971718
Iteration 16000: Loss = -10719.432584066195
1
Iteration 16100: Loss = -10719.378850554212
Iteration 16200: Loss = -10719.707760180328
1
Iteration 16300: Loss = -10719.378771591433
Iteration 16400: Loss = -10719.40212267623
1
Iteration 16500: Loss = -10719.380125394908
2
Iteration 16600: Loss = -10719.3740488717
Iteration 16700: Loss = -10719.376226052065
1
Iteration 16800: Loss = -10719.373729519762
Iteration 16900: Loss = -10719.374383851344
1
Iteration 17000: Loss = -10719.366620403975
Iteration 17100: Loss = -10719.40410596754
1
Iteration 17200: Loss = -10719.370826805365
2
Iteration 17300: Loss = -10719.387900775693
3
Iteration 17400: Loss = -10719.373088861172
4
Iteration 17500: Loss = -10719.366426968678
Iteration 17600: Loss = -10719.36761034985
1
Iteration 17700: Loss = -10719.366354626056
Iteration 17800: Loss = -10719.366957694034
1
Iteration 17900: Loss = -10719.366366378277
Iteration 18000: Loss = -10719.366646733986
1
Iteration 18100: Loss = -10719.366851063398
2
Iteration 18200: Loss = -10719.36963370849
3
Iteration 18300: Loss = -10719.370726923718
4
Iteration 18400: Loss = -10719.373023701544
5
Iteration 18500: Loss = -10719.369122644883
6
Iteration 18600: Loss = -10719.370276912769
7
Iteration 18700: Loss = -10719.366355891427
Iteration 18800: Loss = -10719.371654441704
1
Iteration 18900: Loss = -10719.36916178649
2
Iteration 19000: Loss = -10719.365240598074
Iteration 19100: Loss = -10719.37463926511
1
Iteration 19200: Loss = -10719.376851320447
2
Iteration 19300: Loss = -10719.365327516569
Iteration 19400: Loss = -10719.365336852588
Iteration 19500: Loss = -10719.369503857195
1
Iteration 19600: Loss = -10719.365555132132
2
Iteration 19700: Loss = -10719.381048471605
3
Iteration 19800: Loss = -10719.365327100622
Iteration 19900: Loss = -10719.420946929527
1
pi: tensor([[0.6895, 0.3105],
        [0.2287, 0.7713]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9049, 0.0951], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1875, 0.1108],
         [0.6957, 0.2637]],

        [[0.5170, 0.0960],
         [0.7034, 0.6685]],

        [[0.5174, 0.0892],
         [0.5342, 0.5184]],

        [[0.5782, 0.0909],
         [0.5402, 0.5440]],

        [[0.6373, 0.0858],
         [0.6064, 0.6432]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0029298663248489287
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.5467187733329368
Average Adjusted Rand Index: 0.7231667085374951
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25393.091358374233
Iteration 100: Loss = -10890.7524339796
Iteration 200: Loss = -10678.91338988376
Iteration 300: Loss = -10672.252528597757
Iteration 400: Loss = -10671.845471556915
Iteration 500: Loss = -10671.737138825663
Iteration 600: Loss = -10671.688792978171
Iteration 700: Loss = -10671.649499879113
Iteration 800: Loss = -10671.630454361793
Iteration 900: Loss = -10671.617269925315
Iteration 1000: Loss = -10671.605182078023
Iteration 1100: Loss = -10671.582521324079
Iteration 1200: Loss = -10671.576794186753
Iteration 1300: Loss = -10671.572089938445
Iteration 1400: Loss = -10671.56872453798
Iteration 1500: Loss = -10671.566232657522
Iteration 1600: Loss = -10671.564970636544
Iteration 1700: Loss = -10671.567166288438
1
Iteration 1800: Loss = -10671.563875224037
Iteration 1900: Loss = -10671.55844520287
Iteration 2000: Loss = -10671.556935614406
Iteration 2100: Loss = -10671.551942670529
Iteration 2200: Loss = -10671.549436881976
Iteration 2300: Loss = -10671.531274169229
Iteration 2400: Loss = -10671.5272268858
Iteration 2500: Loss = -10671.5264383797
Iteration 2600: Loss = -10671.52610872703
Iteration 2700: Loss = -10671.525603288637
Iteration 2800: Loss = -10671.528345166475
1
Iteration 2900: Loss = -10671.52491564377
Iteration 3000: Loss = -10671.52457427656
Iteration 3100: Loss = -10671.524821989406
1
Iteration 3200: Loss = -10671.535229566856
2
Iteration 3300: Loss = -10671.523312682335
Iteration 3400: Loss = -10671.522092017187
Iteration 3500: Loss = -10671.52186783968
Iteration 3600: Loss = -10671.522610184742
1
Iteration 3700: Loss = -10671.521387000588
Iteration 3800: Loss = -10671.521668405137
1
Iteration 3900: Loss = -10671.520439090704
Iteration 4000: Loss = -10671.520282119393
Iteration 4100: Loss = -10671.526770748009
1
Iteration 4200: Loss = -10671.520044684014
Iteration 4300: Loss = -10671.521468050276
1
Iteration 4400: Loss = -10671.519821845768
Iteration 4500: Loss = -10671.522950087947
1
Iteration 4600: Loss = -10671.519390570507
Iteration 4700: Loss = -10671.518856888688
Iteration 4800: Loss = -10671.52066019282
1
Iteration 4900: Loss = -10671.51861776287
Iteration 5000: Loss = -10671.51858192115
Iteration 5100: Loss = -10671.519756503092
1
Iteration 5200: Loss = -10671.51871020256
2
Iteration 5300: Loss = -10671.518438538364
Iteration 5400: Loss = -10671.518329364451
Iteration 5500: Loss = -10671.519599388126
1
Iteration 5600: Loss = -10671.518641473258
2
Iteration 5700: Loss = -10671.518234646584
Iteration 5800: Loss = -10671.524326792924
1
Iteration 5900: Loss = -10671.520291843794
2
Iteration 6000: Loss = -10671.532481103073
3
Iteration 6100: Loss = -10671.517795794873
Iteration 6200: Loss = -10671.517857302424
Iteration 6300: Loss = -10671.517289291129
Iteration 6400: Loss = -10671.517274597181
Iteration 6500: Loss = -10671.517291628317
Iteration 6600: Loss = -10671.517209917874
Iteration 6700: Loss = -10671.517137305518
Iteration 6800: Loss = -10671.517055419932
Iteration 6900: Loss = -10671.517428583677
1
Iteration 7000: Loss = -10671.517057802292
Iteration 7100: Loss = -10671.521218280555
1
Iteration 7200: Loss = -10671.517428477277
2
Iteration 7300: Loss = -10671.523607786019
3
Iteration 7400: Loss = -10671.517571476632
4
Iteration 7500: Loss = -10671.522619286925
5
Iteration 7600: Loss = -10671.518619981101
6
Iteration 7700: Loss = -10671.517019812778
Iteration 7800: Loss = -10671.515281328055
Iteration 7900: Loss = -10671.520130070845
1
Iteration 8000: Loss = -10671.523993831142
2
Iteration 8100: Loss = -10671.515839622367
3
Iteration 8200: Loss = -10671.5151047261
Iteration 8300: Loss = -10671.517383111923
1
Iteration 8400: Loss = -10671.56668458392
2
Iteration 8500: Loss = -10671.514671459072
Iteration 8600: Loss = -10671.51496225853
1
Iteration 8700: Loss = -10671.514711292333
Iteration 8800: Loss = -10671.51468798379
Iteration 8900: Loss = -10671.514693182688
Iteration 9000: Loss = -10671.515011985315
1
Iteration 9100: Loss = -10671.514660178504
Iteration 9200: Loss = -10671.514858779132
1
Iteration 9300: Loss = -10671.514661029194
Iteration 9400: Loss = -10671.51465889715
Iteration 9500: Loss = -10671.514624471902
Iteration 9600: Loss = -10671.514734010354
1
Iteration 9700: Loss = -10671.514665959427
Iteration 9800: Loss = -10671.51473814275
Iteration 9900: Loss = -10671.514684122978
Iteration 10000: Loss = -10671.544991782426
1
Iteration 10100: Loss = -10671.514625491818
Iteration 10200: Loss = -10671.519705671844
1
Iteration 10300: Loss = -10671.514642699198
Iteration 10400: Loss = -10671.589539474902
1
Iteration 10500: Loss = -10671.514663124643
Iteration 10600: Loss = -10671.51460652291
Iteration 10700: Loss = -10671.514830453772
1
Iteration 10800: Loss = -10671.514639440682
Iteration 10900: Loss = -10671.514697575758
Iteration 11000: Loss = -10671.534998329875
1
Iteration 11100: Loss = -10671.516815962
2
Iteration 11200: Loss = -10671.51486487707
3
Iteration 11300: Loss = -10671.51524888888
4
Iteration 11400: Loss = -10671.519699705947
5
Iteration 11500: Loss = -10671.533156179648
6
Iteration 11600: Loss = -10671.514593917203
Iteration 11700: Loss = -10671.514747715053
1
Iteration 11800: Loss = -10671.514601258737
Iteration 11900: Loss = -10671.514687772797
Iteration 12000: Loss = -10671.51441834718
Iteration 12100: Loss = -10671.937284777629
1
Iteration 12200: Loss = -10671.514438790167
Iteration 12300: Loss = -10671.514441611367
Iteration 12400: Loss = -10671.540829368467
1
Iteration 12500: Loss = -10671.514416925793
Iteration 12600: Loss = -10671.514412268165
Iteration 12700: Loss = -10671.515183502379
1
Iteration 12800: Loss = -10671.513005968382
Iteration 12900: Loss = -10671.51298398082
Iteration 13000: Loss = -10671.513053395049
Iteration 13100: Loss = -10671.513448461981
1
Iteration 13200: Loss = -10671.513074942191
Iteration 13300: Loss = -10671.51327321797
1
Iteration 13400: Loss = -10671.540643683456
2
Iteration 13500: Loss = -10671.51297168271
Iteration 13600: Loss = -10671.579646754773
1
Iteration 13700: Loss = -10671.512993602932
Iteration 13800: Loss = -10671.513948089461
1
Iteration 13900: Loss = -10671.528686386344
2
Iteration 14000: Loss = -10671.512466612261
Iteration 14100: Loss = -10671.512576335326
1
Iteration 14200: Loss = -10671.560472386027
2
Iteration 14300: Loss = -10671.513134312703
3
Iteration 14400: Loss = -10671.512469480356
Iteration 14500: Loss = -10671.514660755402
1
Iteration 14600: Loss = -10671.516763872094
2
Iteration 14700: Loss = -10671.52286886217
3
Iteration 14800: Loss = -10671.512954389318
4
Iteration 14900: Loss = -10671.512527414057
Iteration 15000: Loss = -10671.514729159002
1
Iteration 15100: Loss = -10671.532249713739
2
Iteration 15200: Loss = -10671.512507628462
Iteration 15300: Loss = -10671.512489360206
Iteration 15400: Loss = -10671.59950978154
1
Iteration 15500: Loss = -10671.512511346053
Iteration 15600: Loss = -10671.513166634537
1
Iteration 15700: Loss = -10671.520657817857
2
Iteration 15800: Loss = -10671.890842593475
3
Iteration 15900: Loss = -10671.512535148993
Iteration 16000: Loss = -10671.51245136526
Iteration 16100: Loss = -10671.512994255496
1
Iteration 16200: Loss = -10671.512501930798
Iteration 16300: Loss = -10671.512887825367
1
Iteration 16400: Loss = -10671.515755441658
2
Iteration 16500: Loss = -10671.605250269831
3
Iteration 16600: Loss = -10671.512391879449
Iteration 16700: Loss = -10671.513253899382
1
Iteration 16800: Loss = -10671.515757690951
2
Iteration 16900: Loss = -10671.539594402588
3
Iteration 17000: Loss = -10671.5139469526
4
Iteration 17100: Loss = -10671.514827685289
5
Iteration 17200: Loss = -10671.52328747789
6
Iteration 17300: Loss = -10671.512657941597
7
Iteration 17400: Loss = -10671.51181315326
Iteration 17500: Loss = -10671.511520962771
Iteration 17600: Loss = -10671.512527590256
1
Iteration 17700: Loss = -10671.517784675843
2
Iteration 17800: Loss = -10671.514729593213
3
Iteration 17900: Loss = -10671.511584110372
Iteration 18000: Loss = -10671.511417541918
Iteration 18100: Loss = -10671.515254952928
1
Iteration 18200: Loss = -10671.564193728838
2
Iteration 18300: Loss = -10671.517529333507
3
Iteration 18400: Loss = -10671.5114154209
Iteration 18500: Loss = -10671.51757846792
1
Iteration 18600: Loss = -10671.511349195924
Iteration 18700: Loss = -10671.511632953418
1
Iteration 18800: Loss = -10671.511343210344
Iteration 18900: Loss = -10671.515681153029
1
Iteration 19000: Loss = -10671.511352141355
Iteration 19100: Loss = -10671.511343671244
Iteration 19200: Loss = -10671.515738092072
1
Iteration 19300: Loss = -10671.511341180978
Iteration 19400: Loss = -10671.511341167954
Iteration 19500: Loss = -10671.511683525503
1
Iteration 19600: Loss = -10671.511749210556
2
Iteration 19700: Loss = -10671.511752783008
3
Iteration 19800: Loss = -10671.515555733342
4
Iteration 19900: Loss = -10671.513912172946
5
pi: tensor([[0.7499, 0.2501],
        [0.2448, 0.7552]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5169, 0.4831], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2578, 0.0943],
         [0.6617, 0.2040]],

        [[0.6055, 0.0935],
         [0.6388, 0.5717]],

        [[0.7146, 0.0890],
         [0.6604, 0.6883]],

        [[0.5603, 0.0904],
         [0.5702, 0.5634]],

        [[0.6829, 0.0858],
         [0.6010, 0.5664]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721141809334062
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080702804390127
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208
Global Adjusted Rand Index: 0.8833663707331195
Average Adjusted Rand Index: 0.8843571508825064
10712.712643091018
[0.5467187733329368, 0.8833663707331195] [0.7231667085374951, 0.8843571508825064] [10719.393518596096, 10671.51134518123]
-------------------------------------
This iteration is 35
True Objective function: Loss = -10919.182164415199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20542.94194412936
Iteration 100: Loss = -11012.09667222798
Iteration 200: Loss = -11010.563940706783
Iteration 300: Loss = -11008.713965812613
Iteration 400: Loss = -11007.857089256602
Iteration 500: Loss = -11007.2154999675
Iteration 600: Loss = -11006.583083783707
Iteration 700: Loss = -11005.16436299393
Iteration 800: Loss = -11003.883023891565
Iteration 900: Loss = -10978.554540345294
Iteration 1000: Loss = -10913.13653486862
Iteration 1100: Loss = -10911.687403238615
Iteration 1200: Loss = -10911.34812295669
Iteration 1300: Loss = -10911.300680189812
Iteration 1400: Loss = -10911.279994181295
Iteration 1500: Loss = -10911.252524258163
Iteration 1600: Loss = -10911.236952555508
Iteration 1700: Loss = -10911.218991574153
Iteration 1800: Loss = -10911.179361760785
Iteration 1900: Loss = -10911.173192439484
Iteration 2000: Loss = -10911.169235981042
Iteration 2100: Loss = -10911.166781425365
Iteration 2200: Loss = -10911.164516932266
Iteration 2300: Loss = -10911.162208436801
Iteration 2400: Loss = -10911.144387011691
Iteration 2500: Loss = -10911.127045851676
Iteration 2600: Loss = -10911.125487883208
Iteration 2700: Loss = -10911.121421976773
Iteration 2800: Loss = -10911.100330140027
Iteration 2900: Loss = -10911.08915803608
Iteration 3000: Loss = -10911.088360280022
Iteration 3100: Loss = -10911.08748127654
Iteration 3200: Loss = -10911.086383915228
Iteration 3300: Loss = -10911.08489704048
Iteration 3400: Loss = -10911.08887408795
1
Iteration 3500: Loss = -10911.084629340769
Iteration 3600: Loss = -10911.085862427799
1
Iteration 3700: Loss = -10911.081615270607
Iteration 3800: Loss = -10911.076356641724
Iteration 3900: Loss = -10911.05560672975
Iteration 4000: Loss = -10911.054831422445
Iteration 4100: Loss = -10911.054017333878
Iteration 4200: Loss = -10911.052983316378
Iteration 4300: Loss = -10911.051494005937
Iteration 4400: Loss = -10911.046943669036
Iteration 4500: Loss = -10911.032290431329
Iteration 4600: Loss = -10910.976819098185
Iteration 4700: Loss = -10910.531604347585
Iteration 4800: Loss = -10910.320187879648
Iteration 4900: Loss = -10910.31209541213
Iteration 5000: Loss = -10910.30901557833
Iteration 5100: Loss = -10910.309315381619
1
Iteration 5200: Loss = -10910.308473620868
Iteration 5300: Loss = -10910.308269854668
Iteration 5400: Loss = -10910.308140711531
Iteration 5500: Loss = -10910.30798592557
Iteration 5600: Loss = -10910.31054552082
1
Iteration 5700: Loss = -10910.307718714123
Iteration 5800: Loss = -10910.307595158509
Iteration 5900: Loss = -10910.307492013813
Iteration 6000: Loss = -10910.307418354563
Iteration 6100: Loss = -10910.306188748931
Iteration 6200: Loss = -10909.980620363489
Iteration 6300: Loss = -10909.980575327629
Iteration 6400: Loss = -10909.982113077822
1
Iteration 6500: Loss = -10909.980571354296
Iteration 6600: Loss = -10909.988577914868
1
Iteration 6700: Loss = -10909.980569144529
Iteration 6800: Loss = -10909.980564499774
Iteration 6900: Loss = -10909.980511625345
Iteration 7000: Loss = -10909.980497048147
Iteration 7100: Loss = -10909.980556876206
Iteration 7200: Loss = -10909.980564378733
Iteration 7300: Loss = -10909.98799320905
1
Iteration 7400: Loss = -10909.980535984621
Iteration 7500: Loss = -10909.980470501645
Iteration 7600: Loss = -10909.985194300854
1
Iteration 7700: Loss = -10909.980793168568
2
Iteration 7800: Loss = -10909.980571534408
3
Iteration 7900: Loss = -10909.995382165831
4
Iteration 8000: Loss = -10909.980439358344
Iteration 8100: Loss = -10909.980422760178
Iteration 8200: Loss = -10909.980437253562
Iteration 8300: Loss = -10909.98633540282
1
Iteration 8400: Loss = -10909.980405952254
Iteration 8500: Loss = -10909.980420371992
Iteration 8600: Loss = -10909.98043563236
Iteration 8700: Loss = -10909.98044038437
Iteration 8800: Loss = -10909.980441723555
Iteration 8900: Loss = -10909.981327409861
1
Iteration 9000: Loss = -10909.98143072292
2
Iteration 9100: Loss = -10909.981673353635
3
Iteration 9200: Loss = -10909.990682948122
4
Iteration 9300: Loss = -10909.98105491927
5
Iteration 9400: Loss = -10909.98226030919
6
Iteration 9500: Loss = -10910.05437443736
7
Iteration 9600: Loss = -10909.986202329244
8
Iteration 9700: Loss = -10910.007339079586
9
Iteration 9800: Loss = -10909.983833241944
10
Iteration 9900: Loss = -10909.980470627073
Iteration 10000: Loss = -10909.98743754023
1
Iteration 10100: Loss = -10910.022151272311
2
Iteration 10200: Loss = -10909.98044040214
Iteration 10300: Loss = -10909.981227393746
1
Iteration 10400: Loss = -10909.98044221567
Iteration 10500: Loss = -10909.980990712678
1
Iteration 10600: Loss = -10909.981470638202
2
Iteration 10700: Loss = -10909.982892725237
3
Iteration 10800: Loss = -10909.981183628832
4
Iteration 10900: Loss = -10909.98062017531
5
Iteration 11000: Loss = -10909.98285070911
6
Iteration 11100: Loss = -10910.009998927668
7
Iteration 11200: Loss = -10909.995484073288
8
Iteration 11300: Loss = -10909.983804411682
9
Iteration 11400: Loss = -10909.980574069083
10
Iteration 11500: Loss = -10909.980440043793
Iteration 11600: Loss = -10909.997690146873
1
Iteration 11700: Loss = -10909.996727778278
2
Iteration 11800: Loss = -10909.980614347303
3
Iteration 11900: Loss = -10909.982025378002
4
Iteration 12000: Loss = -10909.980804784831
5
Iteration 12100: Loss = -10909.98090437379
6
Iteration 12200: Loss = -10909.989736335845
7
Iteration 12300: Loss = -10909.985446653833
8
Iteration 12400: Loss = -10910.004336702064
9
Iteration 12500: Loss = -10909.980275273108
Iteration 12600: Loss = -10909.980527770635
1
Iteration 12700: Loss = -10909.989396919354
2
Iteration 12800: Loss = -10909.999229224484
3
Iteration 12900: Loss = -10909.981388089633
4
Iteration 13000: Loss = -10909.980778778916
5
Iteration 13100: Loss = -10909.980341166261
Iteration 13200: Loss = -10910.038724494112
1
Iteration 13300: Loss = -10909.98021041575
Iteration 13400: Loss = -10909.981004346986
1
Iteration 13500: Loss = -10909.981339495827
2
Iteration 13600: Loss = -10909.9803009837
Iteration 13700: Loss = -10909.980614830309
1
Iteration 13800: Loss = -10909.981207821826
2
Iteration 13900: Loss = -10910.100226221313
3
Iteration 14000: Loss = -10909.981919626094
4
Iteration 14100: Loss = -10909.980367993046
Iteration 14200: Loss = -10909.98078926524
1
Iteration 14300: Loss = -10909.999047888996
2
Iteration 14400: Loss = -10909.980248156124
Iteration 14500: Loss = -10909.980451168542
1
Iteration 14600: Loss = -10910.014190126507
2
Iteration 14700: Loss = -10909.989496523689
3
Iteration 14800: Loss = -10909.999612871667
4
Iteration 14900: Loss = -10909.980526822097
5
Iteration 15000: Loss = -10909.980341667255
Iteration 15100: Loss = -10909.980316722453
Iteration 15200: Loss = -10909.982025586338
1
Iteration 15300: Loss = -10909.98434016321
2
Iteration 15400: Loss = -10910.081434939488
3
Iteration 15500: Loss = -10909.980257765646
Iteration 15600: Loss = -10909.980362275639
1
Iteration 15700: Loss = -10910.019956413515
2
Iteration 15800: Loss = -10909.980367230533
3
Iteration 15900: Loss = -10909.991848579157
4
Iteration 16000: Loss = -10910.04140944744
5
Iteration 16100: Loss = -10909.98022120696
Iteration 16200: Loss = -10909.982098479257
1
Iteration 16300: Loss = -10909.980244041471
Iteration 16400: Loss = -10910.118972588198
1
Iteration 16500: Loss = -10909.981633059542
2
Iteration 16600: Loss = -10909.980738754697
3
Iteration 16700: Loss = -10909.980219821764
Iteration 16800: Loss = -10909.987024897342
1
Iteration 16900: Loss = -10909.980269866322
Iteration 17000: Loss = -10909.980272426574
Iteration 17100: Loss = -10909.999229128736
1
Iteration 17200: Loss = -10909.980231897773
Iteration 17300: Loss = -10909.981330643328
1
Iteration 17400: Loss = -10909.981616546527
2
Iteration 17500: Loss = -10909.983816070682
3
Iteration 17600: Loss = -10910.014822924286
4
Iteration 17700: Loss = -10909.988264387668
5
Iteration 17800: Loss = -10910.029672914336
6
Iteration 17900: Loss = -10909.984606058635
7
Iteration 18000: Loss = -10909.980561201846
8
Iteration 18100: Loss = -10910.02175298359
9
Iteration 18200: Loss = -10909.981027141515
10
Iteration 18300: Loss = -10909.980538623648
11
Iteration 18400: Loss = -10909.980286843478
Iteration 18500: Loss = -10909.98091218086
1
Iteration 18600: Loss = -10909.980276644692
Iteration 18700: Loss = -10910.013781097963
1
Iteration 18800: Loss = -10909.980208646406
Iteration 18900: Loss = -10909.997790745001
1
Iteration 19000: Loss = -10910.002152202616
2
Iteration 19100: Loss = -10910.002423260476
3
Iteration 19200: Loss = -10909.980832325067
4
Iteration 19300: Loss = -10909.980203408606
Iteration 19400: Loss = -10910.000209083883
1
Iteration 19500: Loss = -10909.990439090165
2
Iteration 19600: Loss = -10909.980206850067
Iteration 19700: Loss = -10909.990369593304
1
Iteration 19800: Loss = -10909.981068070518
2
Iteration 19900: Loss = -10909.996545921555
3
pi: tensor([[0.6723, 0.3277],
        [0.1963, 0.8037]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9305, 0.0695], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1754, 0.1421],
         [0.6857, 0.2580]],

        [[0.7253, 0.1056],
         [0.6086, 0.6732]],

        [[0.5725, 0.1000],
         [0.6833, 0.5678]],

        [[0.6692, 0.1061],
         [0.6816, 0.6916]],

        [[0.6538, 0.0988],
         [0.5704, 0.5769]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 90
Adjusted Rand Index: 0.6364168179624292
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.5466841024351917
Average Adjusted Rand Index: 0.6875944374990066
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20086.130574102328
Iteration 100: Loss = -11012.408638464301
Iteration 200: Loss = -11011.760087885126
Iteration 300: Loss = -11011.502901911821
Iteration 400: Loss = -11009.785488106128
Iteration 500: Loss = -11007.370301416464
Iteration 600: Loss = -11006.575696094995
Iteration 700: Loss = -11005.576319089649
Iteration 800: Loss = -11001.660451050297
Iteration 900: Loss = -10931.100590677514
Iteration 1000: Loss = -10911.651938878447
Iteration 1100: Loss = -10911.373636778477
Iteration 1200: Loss = -10911.281626232703
Iteration 1300: Loss = -10911.234634504439
Iteration 1400: Loss = -10911.200740388482
Iteration 1500: Loss = -10911.181379608517
Iteration 1600: Loss = -10911.164250512646
Iteration 1700: Loss = -10911.14728551942
Iteration 1800: Loss = -10911.132014533221
Iteration 1900: Loss = -10911.122846786591
Iteration 2000: Loss = -10911.113223001556
Iteration 2100: Loss = -10911.104040411952
Iteration 2200: Loss = -10911.092236240696
Iteration 2300: Loss = -10911.074498378692
Iteration 2400: Loss = -10911.042089930714
Iteration 2500: Loss = -10910.93494918569
Iteration 2600: Loss = -10910.789789079943
Iteration 2700: Loss = -10910.74369386587
Iteration 2800: Loss = -10910.708326542384
Iteration 2900: Loss = -10910.664320658352
Iteration 3000: Loss = -10910.616222352974
Iteration 3100: Loss = -10910.606593330273
Iteration 3200: Loss = -10910.252944194652
Iteration 3300: Loss = -10910.236304397922
Iteration 3400: Loss = -10910.230256983983
Iteration 3500: Loss = -10910.225299572672
Iteration 3600: Loss = -10910.23940668395
1
Iteration 3700: Loss = -10910.222095466073
Iteration 3800: Loss = -10910.217358859834
Iteration 3900: Loss = -10910.020007528432
Iteration 4000: Loss = -10910.018363890615
Iteration 4100: Loss = -10910.01791572633
Iteration 4200: Loss = -10910.019219793629
1
Iteration 4300: Loss = -10910.017238608289
Iteration 4400: Loss = -10910.016911812012
Iteration 4500: Loss = -10910.043871595828
1
Iteration 4600: Loss = -10910.015652767892
Iteration 4700: Loss = -10910.029625332578
1
Iteration 4800: Loss = -10910.015268427185
Iteration 4900: Loss = -10910.015193572184
Iteration 5000: Loss = -10910.01506814849
Iteration 5100: Loss = -10910.015003761442
Iteration 5200: Loss = -10910.015050029817
Iteration 5300: Loss = -10910.014765880976
Iteration 5400: Loss = -10910.019224658023
1
Iteration 5500: Loss = -10909.982019329209
Iteration 5600: Loss = -10909.981519983061
Iteration 5700: Loss = -10909.981437198483
Iteration 5800: Loss = -10909.981346852413
Iteration 5900: Loss = -10909.98142986309
Iteration 6000: Loss = -10909.981204162188
Iteration 6100: Loss = -10909.98113131125
Iteration 6200: Loss = -10909.981130489829
Iteration 6300: Loss = -10909.98102181218
Iteration 6400: Loss = -10909.980962133031
Iteration 6500: Loss = -10909.980945142286
Iteration 6600: Loss = -10909.980917694085
Iteration 6700: Loss = -10909.989805753197
1
Iteration 6800: Loss = -10909.980848939498
Iteration 6900: Loss = -10909.98083247495
Iteration 7000: Loss = -10909.980878358554
Iteration 7100: Loss = -10909.980770074932
Iteration 7200: Loss = -10909.983150060832
1
Iteration 7300: Loss = -10909.981120479742
2
Iteration 7400: Loss = -10909.981553966363
3
Iteration 7500: Loss = -10909.980735546367
Iteration 7600: Loss = -10909.981450742765
1
Iteration 7700: Loss = -10909.980713962937
Iteration 7800: Loss = -10909.981321045056
1
Iteration 7900: Loss = -10909.9806720764
Iteration 8000: Loss = -10909.981747837042
1
Iteration 8100: Loss = -10909.980770216576
Iteration 8200: Loss = -10909.981598184408
1
Iteration 8300: Loss = -10909.980643413313
Iteration 8400: Loss = -10909.981496606386
1
Iteration 8500: Loss = -10909.981923788779
2
Iteration 8600: Loss = -10909.980666415982
Iteration 8700: Loss = -10909.98404665868
1
Iteration 8800: Loss = -10909.981292775123
2
Iteration 8900: Loss = -10909.980578221739
Iteration 9000: Loss = -10909.98061530177
Iteration 9100: Loss = -10909.980524913057
Iteration 9200: Loss = -10909.981221139322
1
Iteration 9300: Loss = -10909.984601703869
2
Iteration 9400: Loss = -10909.984988639806
3
Iteration 9500: Loss = -10909.996282358898
4
Iteration 9600: Loss = -10909.98053491196
Iteration 9700: Loss = -10909.982878524981
1
Iteration 9800: Loss = -10909.980565145293
Iteration 9900: Loss = -10909.980522758708
Iteration 10000: Loss = -10910.132772375471
1
Iteration 10100: Loss = -10909.98114994017
2
Iteration 10200: Loss = -10909.997057680914
3
Iteration 10300: Loss = -10909.981462702386
4
Iteration 10400: Loss = -10909.980405632998
Iteration 10500: Loss = -10909.981632407029
1
Iteration 10600: Loss = -10909.984697340838
2
Iteration 10700: Loss = -10909.980987604418
3
Iteration 10800: Loss = -10909.980908077334
4
Iteration 10900: Loss = -10909.98525470679
5
Iteration 11000: Loss = -10910.177137809778
6
Iteration 11100: Loss = -10909.980669929408
7
Iteration 11200: Loss = -10909.980998669449
8
Iteration 11300: Loss = -10910.113499549396
9
Iteration 11400: Loss = -10909.980582261602
10
Iteration 11500: Loss = -10909.991933645088
11
Iteration 11600: Loss = -10910.274997426524
12
Iteration 11700: Loss = -10909.980371344445
Iteration 11800: Loss = -10910.045541245398
1
Iteration 11900: Loss = -10909.980301052208
Iteration 12000: Loss = -10910.131997120418
1
Iteration 12100: Loss = -10909.980283257495
Iteration 12200: Loss = -10909.980980211501
1
Iteration 12300: Loss = -10909.980692621577
2
Iteration 12400: Loss = -10909.980323709244
Iteration 12500: Loss = -10909.98166762558
1
Iteration 12600: Loss = -10909.981707074012
2
Iteration 12700: Loss = -10909.980632910077
3
Iteration 12800: Loss = -10909.980430795194
4
Iteration 12900: Loss = -10909.984956972688
5
Iteration 13000: Loss = -10909.980827465888
6
Iteration 13100: Loss = -10909.980296388507
Iteration 13200: Loss = -10909.980426778488
1
Iteration 13300: Loss = -10909.982864969023
2
Iteration 13400: Loss = -10910.024046903378
3
Iteration 13500: Loss = -10909.981067623961
4
Iteration 13600: Loss = -10909.98071222672
5
Iteration 13700: Loss = -10909.984891250291
6
Iteration 13800: Loss = -10909.981258039075
7
Iteration 13900: Loss = -10909.980743275388
8
Iteration 14000: Loss = -10909.983785605424
9
Iteration 14100: Loss = -10910.011119388988
10
Iteration 14200: Loss = -10909.994245671242
11
Iteration 14300: Loss = -10909.980466304103
12
Iteration 14400: Loss = -10909.985872485748
13
Iteration 14500: Loss = -10909.98088953666
14
Iteration 14600: Loss = -10909.98035997148
Iteration 14700: Loss = -10909.980395254875
Iteration 14800: Loss = -10909.98649312853
1
Iteration 14900: Loss = -10909.980777674573
2
Iteration 15000: Loss = -10909.981453369766
3
Iteration 15100: Loss = -10910.007220741345
4
Iteration 15200: Loss = -10909.993439093934
5
Iteration 15300: Loss = -10909.992361939647
6
Iteration 15400: Loss = -10909.988278443665
7
Iteration 15500: Loss = -10909.981655957408
8
Iteration 15600: Loss = -10909.980227292206
Iteration 15700: Loss = -10909.98226412676
1
Iteration 15800: Loss = -10909.986640042716
2
Iteration 15900: Loss = -10909.980897225838
3
Iteration 16000: Loss = -10909.98033074647
4
Iteration 16100: Loss = -10910.146719982262
5
Iteration 16200: Loss = -10909.98017248855
Iteration 16300: Loss = -10909.984085768689
1
Iteration 16400: Loss = -10909.980515779242
2
Iteration 16500: Loss = -10909.980713914194
3
Iteration 16600: Loss = -10909.980510666282
4
Iteration 16700: Loss = -10909.982300637155
5
Iteration 16800: Loss = -10909.987500224688
6
Iteration 16900: Loss = -10909.980197581
Iteration 17000: Loss = -10910.098285424025
1
Iteration 17100: Loss = -10909.980187050853
Iteration 17200: Loss = -10909.982251985875
1
Iteration 17300: Loss = -10909.98025824761
Iteration 17400: Loss = -10909.980233127639
Iteration 17500: Loss = -10910.053349958403
1
Iteration 17600: Loss = -10909.980207955463
Iteration 17700: Loss = -10909.98029048574
Iteration 17800: Loss = -10910.030889661719
1
Iteration 17900: Loss = -10909.982154941315
2
Iteration 18000: Loss = -10909.981052609708
3
Iteration 18100: Loss = -10909.981146425365
4
Iteration 18200: Loss = -10910.012723317985
5
Iteration 18300: Loss = -10909.980370502155
Iteration 18400: Loss = -10909.980457001817
Iteration 18500: Loss = -10910.022469719503
1
Iteration 18600: Loss = -10909.987341812144
2
Iteration 18700: Loss = -10909.997148217382
3
Iteration 18800: Loss = -10909.98133573222
4
Iteration 18900: Loss = -10909.980875354851
5
Iteration 19000: Loss = -10909.981192424673
6
Iteration 19100: Loss = -10909.980350457945
Iteration 19200: Loss = -10909.981770404167
1
Iteration 19300: Loss = -10909.98083584651
2
Iteration 19400: Loss = -10910.050361993182
3
Iteration 19500: Loss = -10909.993838512233
4
Iteration 19600: Loss = -10909.980504413976
5
Iteration 19700: Loss = -10909.98195219376
6
Iteration 19800: Loss = -10909.988559511645
7
Iteration 19900: Loss = -10909.980189783104
pi: tensor([[0.6690, 0.3310],
        [0.1947, 0.8053]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9299, 0.0701], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1766, 0.1411],
         [0.6899, 0.2565]],

        [[0.7007, 0.1048],
         [0.5813, 0.5845]],

        [[0.6150, 0.0992],
         [0.6394, 0.5199]],

        [[0.6913, 0.1054],
         [0.5839, 0.5597]],

        [[0.6622, 0.0981],
         [0.7134, 0.7008]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 90
Adjusted Rand Index: 0.6364168179624292
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
Global Adjusted Rand Index: 0.5466841024351917
Average Adjusted Rand Index: 0.6875944374990066
10919.182164415199
[0.5466841024351917, 0.5466841024351917] [0.6875944374990066, 0.6875944374990066] [10910.080436927108, 10909.980260118678]
-------------------------------------
This iteration is 36
True Objective function: Loss = -10799.410926326182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21104.894109213343
Iteration 100: Loss = -10890.904544418028
Iteration 200: Loss = -10890.148352763706
Iteration 300: Loss = -10889.968811338636
Iteration 400: Loss = -10889.83268359104
Iteration 500: Loss = -10889.725754770605
Iteration 600: Loss = -10889.642117673422
Iteration 700: Loss = -10889.563663534225
Iteration 800: Loss = -10889.462567131204
Iteration 900: Loss = -10889.26951783869
Iteration 1000: Loss = -10888.834948911828
Iteration 1100: Loss = -10888.334594535365
Iteration 1200: Loss = -10887.978933221146
Iteration 1300: Loss = -10887.738723123732
Iteration 1400: Loss = -10887.551944848383
Iteration 1500: Loss = -10887.400194800492
Iteration 1600: Loss = -10887.283049125626
Iteration 1700: Loss = -10887.194003514256
Iteration 1800: Loss = -10887.126174729721
Iteration 1900: Loss = -10887.073874647864
Iteration 2000: Loss = -10887.032445831865
Iteration 2100: Loss = -10887.000080954556
Iteration 2200: Loss = -10886.975559770046
Iteration 2300: Loss = -10886.956758578403
Iteration 2400: Loss = -10886.941926368843
Iteration 2500: Loss = -10886.929827803053
Iteration 2600: Loss = -10886.919742543701
Iteration 2700: Loss = -10886.911075347249
Iteration 2800: Loss = -10886.90361854046
Iteration 2900: Loss = -10886.897027472593
Iteration 3000: Loss = -10886.891220158896
Iteration 3100: Loss = -10886.886050026043
Iteration 3200: Loss = -10886.881431401032
Iteration 3300: Loss = -10886.877216647825
Iteration 3400: Loss = -10886.8733910212
Iteration 3500: Loss = -10886.869907745147
Iteration 3600: Loss = -10886.866716842584
Iteration 3700: Loss = -10886.863797524084
Iteration 3800: Loss = -10886.861091971019
Iteration 3900: Loss = -10886.85867599546
Iteration 4000: Loss = -10886.856480381255
Iteration 4100: Loss = -10886.854409309151
Iteration 4200: Loss = -10886.852591775005
Iteration 4300: Loss = -10886.850831404374
Iteration 4400: Loss = -10886.849232223929
Iteration 4500: Loss = -10886.847674753713
Iteration 4600: Loss = -10886.846277352
Iteration 4700: Loss = -10886.844890428632
Iteration 4800: Loss = -10886.843548009627
Iteration 4900: Loss = -10886.84226261205
Iteration 5000: Loss = -10886.841029201152
Iteration 5100: Loss = -10886.839836148923
Iteration 5200: Loss = -10886.838665605908
Iteration 5300: Loss = -10886.837335168153
Iteration 5400: Loss = -10886.83604935689
Iteration 5500: Loss = -10886.834759095771
Iteration 5600: Loss = -10886.833843023518
Iteration 5700: Loss = -10886.833088565745
Iteration 5800: Loss = -10886.832455371014
Iteration 5900: Loss = -10886.831825844552
Iteration 6000: Loss = -10886.831326976659
Iteration 6100: Loss = -10886.83079621017
Iteration 6200: Loss = -10886.83029363157
Iteration 6300: Loss = -10886.829813762735
Iteration 6400: Loss = -10886.829370036237
Iteration 6500: Loss = -10886.828963211206
Iteration 6600: Loss = -10886.82856658606
Iteration 6700: Loss = -10886.828164316119
Iteration 6800: Loss = -10886.827794046987
Iteration 6900: Loss = -10886.827436543383
Iteration 7000: Loss = -10886.82710342998
Iteration 7100: Loss = -10886.82675012289
Iteration 7200: Loss = -10886.826482735865
Iteration 7300: Loss = -10886.826195608975
Iteration 7400: Loss = -10886.825915450036
Iteration 7500: Loss = -10886.825682901432
Iteration 7600: Loss = -10886.825408943987
Iteration 7700: Loss = -10886.825165721033
Iteration 7800: Loss = -10886.8249437748
Iteration 7900: Loss = -10886.848067418612
1
Iteration 8000: Loss = -10886.824566126237
Iteration 8100: Loss = -10886.824402257716
Iteration 8200: Loss = -10887.3316519369
1
Iteration 8300: Loss = -10886.824098024423
Iteration 8400: Loss = -10886.823941389757
Iteration 8500: Loss = -10886.823808081159
Iteration 8600: Loss = -10886.82373279801
Iteration 8700: Loss = -10886.823412331996
Iteration 8800: Loss = -10886.82322624915
Iteration 8900: Loss = -10886.823418153
1
Iteration 9000: Loss = -10886.822796502442
Iteration 9100: Loss = -10886.822656504843
Iteration 9200: Loss = -10886.977842586635
1
Iteration 9300: Loss = -10886.822478127422
Iteration 9400: Loss = -10886.822397810514
Iteration 9500: Loss = -10886.822318202398
Iteration 9600: Loss = -10886.82229436298
Iteration 9700: Loss = -10886.822184556437
Iteration 9800: Loss = -10886.822126058678
Iteration 9900: Loss = -10886.833160292692
1
Iteration 10000: Loss = -10886.821996811479
Iteration 10100: Loss = -10886.821947984576
Iteration 10200: Loss = -10887.27600380647
1
Iteration 10300: Loss = -10886.821871625945
Iteration 10400: Loss = -10886.821785226855
Iteration 10500: Loss = -10886.821754621678
Iteration 10600: Loss = -10886.82175099868
Iteration 10700: Loss = -10886.821616724716
Iteration 10800: Loss = -10886.821484333497
Iteration 10900: Loss = -10886.822280436836
1
Iteration 11000: Loss = -10886.821314879504
Iteration 11100: Loss = -10886.821271493942
Iteration 11200: Loss = -10886.82286389878
1
Iteration 11300: Loss = -10886.821177962458
Iteration 11400: Loss = -10886.821104043176
Iteration 11500: Loss = -10886.927205369513
1
Iteration 11600: Loss = -10886.820841923338
Iteration 11700: Loss = -10886.82069929002
Iteration 11800: Loss = -10886.93052727038
1
Iteration 11900: Loss = -10886.820535834027
Iteration 12000: Loss = -10886.820456220375
Iteration 12100: Loss = -10886.865513610393
1
Iteration 12200: Loss = -10886.820388901442
Iteration 12300: Loss = -10886.82033746332
Iteration 12400: Loss = -10887.02389464893
1
Iteration 12500: Loss = -10886.820289604742
Iteration 12600: Loss = -10886.820243493503
Iteration 12700: Loss = -10886.829131519515
1
Iteration 12800: Loss = -10886.82020309357
Iteration 12900: Loss = -10886.820178595033
Iteration 13000: Loss = -10886.852918274812
1
Iteration 13100: Loss = -10886.820113298561
Iteration 13200: Loss = -10886.82008221954
Iteration 13300: Loss = -10887.370062736423
1
Iteration 13400: Loss = -10886.820018925517
Iteration 13500: Loss = -10886.819998591067
Iteration 13600: Loss = -10886.820026256664
Iteration 13700: Loss = -10886.820012816996
Iteration 13800: Loss = -10886.819938396151
Iteration 13900: Loss = -10886.819893287113
Iteration 14000: Loss = -10886.820498320254
1
Iteration 14100: Loss = -10886.819866089201
Iteration 14200: Loss = -10886.819869768939
Iteration 14300: Loss = -10886.827740983412
1
Iteration 14400: Loss = -10886.819844560043
Iteration 14500: Loss = -10886.906996469204
1
Iteration 14600: Loss = -10886.819828015152
Iteration 14700: Loss = -10886.83144113468
1
Iteration 14800: Loss = -10886.819812195636
Iteration 14900: Loss = -10886.819806987536
Iteration 15000: Loss = -10886.820906555204
1
Iteration 15100: Loss = -10886.81973630262
Iteration 15200: Loss = -10886.82136871565
1
Iteration 15300: Loss = -10886.819726957321
Iteration 15400: Loss = -10886.819721005137
Iteration 15500: Loss = -10886.949440977123
1
Iteration 15600: Loss = -10886.819698818923
Iteration 15700: Loss = -10886.819655902542
Iteration 15800: Loss = -10886.819645787258
Iteration 15900: Loss = -10886.819733695149
Iteration 16000: Loss = -10886.819547376344
Iteration 16100: Loss = -10886.819562563946
Iteration 16200: Loss = -10886.823238454745
1
Iteration 16300: Loss = -10886.819560547634
Iteration 16400: Loss = -10886.819517992713
Iteration 16500: Loss = -10886.819535841856
Iteration 16600: Loss = -10886.819657590915
1
Iteration 16700: Loss = -10886.819529306904
Iteration 16800: Loss = -10886.819505970965
Iteration 16900: Loss = -10886.826114894446
1
Iteration 17000: Loss = -10886.819553135836
Iteration 17100: Loss = -10886.819508463432
Iteration 17200: Loss = -10887.40621199272
1
Iteration 17300: Loss = -10886.819528430193
Iteration 17400: Loss = -10886.81952826976
Iteration 17500: Loss = -10886.819517261882
Iteration 17600: Loss = -10886.820706230197
1
Iteration 17700: Loss = -10886.819477511985
Iteration 17800: Loss = -10886.819484401316
Iteration 17900: Loss = -10886.828362982516
1
Iteration 18000: Loss = -10886.819515621666
Iteration 18100: Loss = -10886.819489308302
Iteration 18200: Loss = -10887.023427105869
1
Iteration 18300: Loss = -10886.819496275595
Iteration 18400: Loss = -10886.81946841678
Iteration 18500: Loss = -10886.819480141616
Iteration 18600: Loss = -10886.819887937823
1
Iteration 18700: Loss = -10886.819474885207
Iteration 18800: Loss = -10886.81945984949
Iteration 18900: Loss = -10886.819472999097
Iteration 19000: Loss = -10886.81973665539
1
Iteration 19100: Loss = -10886.819490962771
Iteration 19200: Loss = -10886.819523518745
Iteration 19300: Loss = -10886.819508228326
Iteration 19400: Loss = -10886.819513279943
Iteration 19500: Loss = -10886.819506320171
Iteration 19600: Loss = -10886.81947079278
Iteration 19700: Loss = -10886.826876215231
1
Iteration 19800: Loss = -10886.819474223195
Iteration 19900: Loss = -10886.819494294356
pi: tensor([[1.0000e+00, 3.6245e-08],
        [1.5535e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9604, 0.0396], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1579, 0.1812],
         [0.5685, 0.3053]],

        [[0.6750, 0.2490],
         [0.5148, 0.6784]],

        [[0.5896, 0.1608],
         [0.6719, 0.6457]],

        [[0.6472, 0.1921],
         [0.5852, 0.5280]],

        [[0.6428, 0.1376],
         [0.6788, 0.6347]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.007431593898976406
Global Adjusted Rand Index: -0.0021293740646396173
Average Adjusted Rand Index: -0.0024918136196764367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22167.66681961006
Iteration 100: Loss = -10891.378324135687
Iteration 200: Loss = -10890.487993422197
Iteration 300: Loss = -10890.278207598181
Iteration 400: Loss = -10890.164108087576
Iteration 500: Loss = -10890.081812051858
Iteration 600: Loss = -10890.0101450717
Iteration 700: Loss = -10889.944912076073
Iteration 800: Loss = -10889.887682622046
Iteration 900: Loss = -10889.835185106625
Iteration 1000: Loss = -10889.784719580513
Iteration 1100: Loss = -10889.73386761101
Iteration 1200: Loss = -10889.67879838814
Iteration 1300: Loss = -10889.610710869509
Iteration 1400: Loss = -10889.503660653327
Iteration 1500: Loss = -10889.262615122367
Iteration 1600: Loss = -10888.76357837594
Iteration 1700: Loss = -10888.299938423053
Iteration 1800: Loss = -10887.976325265112
Iteration 1900: Loss = -10887.7388671883
Iteration 2000: Loss = -10887.550070951653
Iteration 2100: Loss = -10887.399112914836
Iteration 2200: Loss = -10887.2829825319
Iteration 2300: Loss = -10887.197168934534
Iteration 2400: Loss = -10887.134859453452
Iteration 2500: Loss = -10887.088831535566
Iteration 2600: Loss = -10887.053755639618
Iteration 2700: Loss = -10887.026078928415
Iteration 2800: Loss = -10887.003606967395
Iteration 2900: Loss = -10886.984989104163
Iteration 3000: Loss = -10886.96919922993
Iteration 3100: Loss = -10886.955616218287
Iteration 3200: Loss = -10886.943823301788
Iteration 3300: Loss = -10886.933431682364
Iteration 3400: Loss = -10886.924362766582
Iteration 3500: Loss = -10886.916335104741
Iteration 3600: Loss = -10886.909287074664
Iteration 3700: Loss = -10886.90308452598
Iteration 3800: Loss = -10886.8975656862
Iteration 3900: Loss = -10886.892584099216
Iteration 4000: Loss = -10886.88807443374
Iteration 4100: Loss = -10886.883985345192
Iteration 4200: Loss = -10886.880219784834
Iteration 4300: Loss = -10886.876790038783
Iteration 4400: Loss = -10886.873651824419
Iteration 4500: Loss = -10886.870701998412
Iteration 4600: Loss = -10886.867981897018
Iteration 4700: Loss = -10886.865410155502
Iteration 4800: Loss = -10886.862951142577
Iteration 4900: Loss = -10886.86049774632
Iteration 5000: Loss = -10886.857960630367
Iteration 5100: Loss = -10886.85529198944
Iteration 5200: Loss = -10886.852835470369
Iteration 5300: Loss = -10886.850972112748
Iteration 5400: Loss = -10886.849336905572
Iteration 5500: Loss = -10886.847885411316
Iteration 5600: Loss = -10886.846482053706
Iteration 5700: Loss = -10886.845278418083
Iteration 5800: Loss = -10886.844102704605
Iteration 5900: Loss = -10886.843004887713
Iteration 6000: Loss = -10886.841981012582
Iteration 6100: Loss = -10886.840963811957
Iteration 6200: Loss = -10886.840052662443
Iteration 6300: Loss = -10886.83920645574
Iteration 6400: Loss = -10886.838351612276
Iteration 6500: Loss = -10886.83754831236
Iteration 6600: Loss = -10886.836708242956
Iteration 6700: Loss = -10886.835933186456
Iteration 6800: Loss = -10886.835116085962
Iteration 6900: Loss = -10886.834269081082
Iteration 7000: Loss = -10886.833514419937
Iteration 7100: Loss = -10886.833470433116
Iteration 7200: Loss = -10886.832195131414
Iteration 7300: Loss = -10886.831647761497
Iteration 7400: Loss = -10886.831148063085
Iteration 7500: Loss = -10886.830596623211
Iteration 7600: Loss = -10886.830164277157
Iteration 7700: Loss = -10886.829653377057
Iteration 7800: Loss = -10886.829223115526
Iteration 7900: Loss = -10886.82883256942
Iteration 8000: Loss = -10886.828600509216
Iteration 8100: Loss = -10886.828065174046
Iteration 8200: Loss = -10886.827693346306
Iteration 8300: Loss = -10886.827235997656
Iteration 8400: Loss = -10886.82680041084
Iteration 8500: Loss = -10886.82682061095
Iteration 8600: Loss = -10886.826124152956
Iteration 8700: Loss = -10886.82732654367
1
Iteration 8800: Loss = -10886.825562817095
Iteration 8900: Loss = -10886.980745534525
1
Iteration 9000: Loss = -10886.825036804074
Iteration 9100: Loss = -10886.824748559226
Iteration 9200: Loss = -10886.83086121088
1
Iteration 9300: Loss = -10886.824294571714
Iteration 9400: Loss = -10886.824134970051
Iteration 9500: Loss = -10886.824345759376
1
Iteration 9600: Loss = -10886.823776464333
Iteration 9700: Loss = -10886.823592730581
Iteration 9800: Loss = -10887.145906579615
1
Iteration 9900: Loss = -10886.823212947042
Iteration 10000: Loss = -10886.822943826113
Iteration 10100: Loss = -10886.822749615145
Iteration 10200: Loss = -10886.82258648654
Iteration 10300: Loss = -10886.822326373145
Iteration 10400: Loss = -10886.822183219583
Iteration 10500: Loss = -10886.822150726788
Iteration 10600: Loss = -10886.821750690131
Iteration 10700: Loss = -10886.821641205848
Iteration 10800: Loss = -10886.821590045318
Iteration 10900: Loss = -10886.8215275691
Iteration 11000: Loss = -10886.821395631177
Iteration 11100: Loss = -10886.821637682022
1
Iteration 11200: Loss = -10886.82125433522
Iteration 11300: Loss = -10886.821193917393
Iteration 11400: Loss = -10886.822262301534
1
Iteration 11500: Loss = -10886.821072884895
Iteration 11600: Loss = -10886.820956415137
Iteration 11700: Loss = -10886.821278310084
1
Iteration 11800: Loss = -10886.820592976508
Iteration 11900: Loss = -10886.82053653939
Iteration 12000: Loss = -10886.823657762787
1
Iteration 12100: Loss = -10886.820419206617
Iteration 12200: Loss = -10886.82041799444
Iteration 12300: Loss = -10886.821480170907
1
Iteration 12400: Loss = -10886.82029136457
Iteration 12500: Loss = -10886.820278390893
Iteration 12600: Loss = -10886.822112353586
1
Iteration 12700: Loss = -10886.820217526514
Iteration 12800: Loss = -10886.820162844568
Iteration 12900: Loss = -10886.858200677485
1
Iteration 13000: Loss = -10886.820112602034
Iteration 13100: Loss = -10886.820077665798
Iteration 13200: Loss = -10886.820040809265
Iteration 13300: Loss = -10886.820003312503
Iteration 13400: Loss = -10886.820246544801
1
Iteration 13500: Loss = -10886.819966243744
Iteration 13600: Loss = -10886.819964916202
Iteration 13700: Loss = -10886.82043135853
1
Iteration 13800: Loss = -10886.819926034994
Iteration 13900: Loss = -10886.819878401842
Iteration 14000: Loss = -10886.824857218588
1
Iteration 14100: Loss = -10886.819811703193
Iteration 14200: Loss = -10886.81978864875
Iteration 14300: Loss = -10886.819786619533
Iteration 14400: Loss = -10886.819773770418
Iteration 14500: Loss = -10886.819782248347
Iteration 14600: Loss = -10886.819725589645
Iteration 14700: Loss = -10886.824451772527
1
Iteration 14800: Loss = -10886.81975427054
Iteration 14900: Loss = -10886.819717103159
Iteration 15000: Loss = -10886.820044418371
1
Iteration 15100: Loss = -10886.819657269703
Iteration 15200: Loss = -10886.819670022074
Iteration 15300: Loss = -10886.820996690432
1
Iteration 15400: Loss = -10886.819612223559
Iteration 15500: Loss = -10886.819658815431
Iteration 15600: Loss = -10886.821011883312
1
Iteration 15700: Loss = -10886.81962891893
Iteration 15800: Loss = -10886.819610171191
Iteration 15900: Loss = -10886.819701039762
Iteration 16000: Loss = -10886.819588561313
Iteration 16100: Loss = -10886.81960562063
Iteration 16200: Loss = -10886.820107596392
1
Iteration 16300: Loss = -10886.819567810813
Iteration 16400: Loss = -10886.8195509883
Iteration 16500: Loss = -10886.819523638389
Iteration 16600: Loss = -10886.819575389776
Iteration 16700: Loss = -10886.819529657472
Iteration 16800: Loss = -10886.819525253479
Iteration 16900: Loss = -10886.821141559862
1
Iteration 17000: Loss = -10886.819513456041
Iteration 17100: Loss = -10886.819546292125
Iteration 17200: Loss = -10886.826459426406
1
Iteration 17300: Loss = -10886.819526641879
Iteration 17400: Loss = -10886.819513372213
Iteration 17500: Loss = -10887.358371309956
1
Iteration 17600: Loss = -10886.819538255415
Iteration 17700: Loss = -10886.819490220187
Iteration 17800: Loss = -10886.819539431774
Iteration 17900: Loss = -10886.820456307958
1
Iteration 18000: Loss = -10886.819510798638
Iteration 18100: Loss = -10886.819468453421
Iteration 18200: Loss = -10887.067670724875
1
Iteration 18300: Loss = -10886.819453858532
Iteration 18400: Loss = -10886.819453381115
Iteration 18500: Loss = -10886.819469018928
Iteration 18600: Loss = -10886.819513769813
Iteration 18700: Loss = -10886.81948872996
Iteration 18800: Loss = -10886.81947754516
Iteration 18900: Loss = -10886.81982038622
1
Iteration 19000: Loss = -10886.819484728656
Iteration 19100: Loss = -10886.819449920622
Iteration 19200: Loss = -10886.822711524397
1
Iteration 19300: Loss = -10886.819487374043
Iteration 19400: Loss = -10886.819466918814
Iteration 19500: Loss = -10886.852613045054
1
Iteration 19600: Loss = -10886.81944719319
Iteration 19700: Loss = -10886.81946161696
Iteration 19800: Loss = -10886.819540588916
Iteration 19900: Loss = -10886.819550262573
pi: tensor([[1.0000e+00, 2.1528e-06],
        [8.0894e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0395, 0.9605], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3056, 0.1813],
         [0.6095, 0.1580]],

        [[0.6990, 0.2491],
         [0.5931, 0.5591]],

        [[0.5888, 0.1608],
         [0.6865, 0.6336]],

        [[0.5207, 0.1922],
         [0.6283, 0.5183]],

        [[0.6289, 0.1375],
         [0.6491, 0.7137]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.006467401572035518
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
Global Adjusted Rand Index: -0.0021293740646396173
Average Adjusted Rand Index: -0.0024918136196764367
10799.410926326182
[-0.0021293740646396173, -0.0021293740646396173] [-0.0024918136196764367, -0.0024918136196764367] [10886.81986842162, 10886.819466368841]
-------------------------------------
This iteration is 37
True Objective function: Loss = -10879.600986584708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23617.851189126468
Iteration 100: Loss = -10950.21554633616
Iteration 200: Loss = -10948.872295882948
Iteration 300: Loss = -10948.292169731954
Iteration 400: Loss = -10947.790418769657
Iteration 500: Loss = -10947.07291512476
Iteration 600: Loss = -10946.789770233465
Iteration 700: Loss = -10946.680974373237
Iteration 800: Loss = -10946.59370682498
Iteration 900: Loss = -10946.49920568629
Iteration 1000: Loss = -10946.390337538658
Iteration 1100: Loss = -10946.272372950463
Iteration 1200: Loss = -10946.146611487635
Iteration 1300: Loss = -10946.02632942687
Iteration 1400: Loss = -10945.93332757826
Iteration 1500: Loss = -10945.871216921209
Iteration 1600: Loss = -10945.830555116449
Iteration 1700: Loss = -10945.803009826792
Iteration 1800: Loss = -10945.783847797113
Iteration 1900: Loss = -10945.770466616857
Iteration 2000: Loss = -10945.761019549744
Iteration 2100: Loss = -10945.754310052354
Iteration 2200: Loss = -10945.749518760322
Iteration 2300: Loss = -10945.745942454227
Iteration 2400: Loss = -10945.743160497494
Iteration 2500: Loss = -10945.741008295794
Iteration 2600: Loss = -10945.739358201556
Iteration 2700: Loss = -10945.737961097695
Iteration 2800: Loss = -10945.736853404407
Iteration 2900: Loss = -10945.735982744958
Iteration 3000: Loss = -10945.735227151286
Iteration 3100: Loss = -10945.734622933063
Iteration 3200: Loss = -10945.73404752698
Iteration 3300: Loss = -10945.73361825449
Iteration 3400: Loss = -10945.733178596318
Iteration 3500: Loss = -10945.732891726286
Iteration 3600: Loss = -10945.732563518457
Iteration 3700: Loss = -10945.732266353773
Iteration 3800: Loss = -10945.732071936915
Iteration 3900: Loss = -10945.731860450027
Iteration 4000: Loss = -10945.731633632735
Iteration 4100: Loss = -10945.73148255708
Iteration 4200: Loss = -10945.731317671329
Iteration 4300: Loss = -10945.731207327686
Iteration 4400: Loss = -10945.731067392957
Iteration 4500: Loss = -10945.731006943137
Iteration 4600: Loss = -10945.730848473606
Iteration 4700: Loss = -10945.730763247419
Iteration 4800: Loss = -10945.730686988109
Iteration 4900: Loss = -10945.73061110414
Iteration 5000: Loss = -10945.730583182878
Iteration 5100: Loss = -10945.730502114035
Iteration 5200: Loss = -10945.73045719296
Iteration 5300: Loss = -10945.730408973646
Iteration 5400: Loss = -10945.730395795295
Iteration 5500: Loss = -10945.73034561466
Iteration 5600: Loss = -10945.730324870952
Iteration 5700: Loss = -10945.73026469537
Iteration 5800: Loss = -10945.730242029662
Iteration 5900: Loss = -10945.730201386463
Iteration 6000: Loss = -10945.730198254405
Iteration 6100: Loss = -10945.730167529844
Iteration 6200: Loss = -10945.730159753954
Iteration 6300: Loss = -10945.730142249811
Iteration 6400: Loss = -10945.730105353045
Iteration 6500: Loss = -10945.730156201686
Iteration 6600: Loss = -10945.730096203904
Iteration 6700: Loss = -10945.73008687868
Iteration 6800: Loss = -10945.730113600817
Iteration 6900: Loss = -10945.730014168348
Iteration 7000: Loss = -10945.730021967265
Iteration 7100: Loss = -10945.731524511291
1
Iteration 7200: Loss = -10945.730134135385
2
Iteration 7300: Loss = -10945.730013792274
Iteration 7400: Loss = -10945.732869328245
1
Iteration 7500: Loss = -10945.7299768555
Iteration 7600: Loss = -10945.73159982115
1
Iteration 7700: Loss = -10945.729953110571
Iteration 7800: Loss = -10945.730509839455
1
Iteration 7900: Loss = -10945.729966752095
Iteration 8000: Loss = -10945.870843642006
1
Iteration 8100: Loss = -10945.729950218038
Iteration 8200: Loss = -10945.729948943408
Iteration 8300: Loss = -10946.044007949831
1
Iteration 8400: Loss = -10945.729963551215
Iteration 8500: Loss = -10945.729935103576
Iteration 8600: Loss = -10946.166624771146
1
Iteration 8700: Loss = -10945.729896346327
Iteration 8800: Loss = -10945.729974554199
Iteration 8900: Loss = -10946.173763592224
1
Iteration 9000: Loss = -10945.72995727655
Iteration 9100: Loss = -10945.72990686601
Iteration 9200: Loss = -10945.729947223208
Iteration 9300: Loss = -10945.73055025649
1
Iteration 9400: Loss = -10945.729915166308
Iteration 9500: Loss = -10945.72993354775
Iteration 9600: Loss = -10945.731029974271
1
Iteration 9700: Loss = -10945.729918271998
Iteration 9800: Loss = -10945.729935066423
Iteration 9900: Loss = -10945.736474406143
1
Iteration 10000: Loss = -10945.729973758655
Iteration 10100: Loss = -10945.729927676037
Iteration 10200: Loss = -10945.730465310717
1
Iteration 10300: Loss = -10945.729925446047
Iteration 10400: Loss = -10945.729919698133
Iteration 10500: Loss = -10945.729928974613
Iteration 10600: Loss = -10945.730096761601
1
Iteration 10700: Loss = -10945.729927095272
Iteration 10800: Loss = -10945.729932243747
Iteration 10900: Loss = -10945.730149561474
1
Iteration 11000: Loss = -10945.729920521915
Iteration 11100: Loss = -10945.729915704538
Iteration 11200: Loss = -10945.733981306377
1
Iteration 11300: Loss = -10945.7299163244
Iteration 11400: Loss = -10945.72992664201
Iteration 11500: Loss = -10945.73100895339
1
Iteration 11600: Loss = -10945.729910287553
Iteration 11700: Loss = -10945.729905831688
Iteration 11800: Loss = -10945.734016519873
1
Iteration 11900: Loss = -10945.81614353054
2
Iteration 12000: Loss = -10945.729938137827
Iteration 12100: Loss = -10945.741441957212
1
Iteration 12200: Loss = -10945.72990897994
Iteration 12300: Loss = -10945.740564191268
1
Iteration 12400: Loss = -10945.729926375
Iteration 12500: Loss = -10945.735074461309
1
Iteration 12600: Loss = -10945.73305717673
2
Iteration 12700: Loss = -10945.88568706841
3
Iteration 12800: Loss = -10945.730404860877
4
Iteration 12900: Loss = -10945.751759789173
5
Iteration 13000: Loss = -10945.729934316581
Iteration 13100: Loss = -10945.756738448319
1
Iteration 13200: Loss = -10945.73002948428
Iteration 13300: Loss = -10945.731162841268
1
Iteration 13400: Loss = -10945.74680846034
2
Iteration 13500: Loss = -10945.731902729001
3
Iteration 13600: Loss = -10945.789524121805
4
Iteration 13700: Loss = -10945.730986040806
5
Iteration 13800: Loss = -10945.730020476805
Iteration 13900: Loss = -10945.734936867437
1
Iteration 14000: Loss = -10945.72995431554
Iteration 14100: Loss = -10945.730063191402
1
Iteration 14200: Loss = -10945.72989928872
Iteration 14300: Loss = -10945.736374029533
1
Iteration 14400: Loss = -10945.729920235028
Iteration 14500: Loss = -10945.730151225538
1
Iteration 14600: Loss = -10945.732068952448
2
Iteration 14700: Loss = -10945.729968770964
Iteration 14800: Loss = -10945.733291393311
1
Iteration 14900: Loss = -10945.729939573064
Iteration 15000: Loss = -10945.730077694216
1
Iteration 15100: Loss = -10946.044183157332
2
Iteration 15200: Loss = -10945.729926883134
Iteration 15300: Loss = -10945.73399535606
1
Iteration 15400: Loss = -10945.729915946786
Iteration 15500: Loss = -10945.793742755293
1
Iteration 15600: Loss = -10945.75219292914
2
Iteration 15700: Loss = -10945.751317931026
3
Iteration 15800: Loss = -10945.730095460427
4
Iteration 15900: Loss = -10945.730007179096
Iteration 16000: Loss = -10945.954886767175
1
Iteration 16100: Loss = -10945.729948408145
Iteration 16200: Loss = -10945.730843992667
1
Iteration 16300: Loss = -10945.731169900873
2
Iteration 16400: Loss = -10945.741040626226
3
Iteration 16500: Loss = -10945.730723450872
4
Iteration 16600: Loss = -10945.751178077086
5
Iteration 16700: Loss = -10945.729934640412
Iteration 16800: Loss = -10945.731110051585
1
Iteration 16900: Loss = -10945.729925421254
Iteration 17000: Loss = -10945.731233784927
1
Iteration 17100: Loss = -10945.729897583065
Iteration 17200: Loss = -10945.730272878553
1
Iteration 17300: Loss = -10945.729928718903
Iteration 17400: Loss = -10945.750856814953
1
Iteration 17500: Loss = -10945.755053528876
2
Iteration 17600: Loss = -10945.742865659138
3
Iteration 17700: Loss = -10945.781915837844
4
Iteration 17800: Loss = -10945.731536359614
5
Iteration 17900: Loss = -10945.729942378643
Iteration 18000: Loss = -10945.864450222381
1
Iteration 18100: Loss = -10945.729940679337
Iteration 18200: Loss = -10945.730087412829
1
Iteration 18300: Loss = -10945.729952356487
Iteration 18400: Loss = -10945.729928020914
Iteration 18500: Loss = -10945.741738774734
1
Iteration 18600: Loss = -10945.729933516977
Iteration 18700: Loss = -10945.72991800632
Iteration 18800: Loss = -10945.730080073732
1
Iteration 18900: Loss = -10945.739206905891
2
Iteration 19000: Loss = -10945.72992471824
Iteration 19100: Loss = -10945.735654903423
1
Iteration 19200: Loss = -10945.729941056343
Iteration 19300: Loss = -10945.731530432055
1
Iteration 19400: Loss = -10945.729905488675
Iteration 19500: Loss = -10945.73114560189
1
Iteration 19600: Loss = -10945.731427327251
2
Iteration 19700: Loss = -10945.730924801983
3
Iteration 19800: Loss = -10945.753757256494
4
Iteration 19900: Loss = -10945.730644502739
5
pi: tensor([[0.9831, 0.0169],
        [0.8740, 0.1260]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9418, 0.0582], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1590, 0.2243],
         [0.5962, 0.2811]],

        [[0.6486, 0.2208],
         [0.5078, 0.5569]],

        [[0.5324, 0.2442],
         [0.6781, 0.6667]],

        [[0.5865, 0.1486],
         [0.6791, 0.6676]],

        [[0.6784, 0.1659],
         [0.5255, 0.7196]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.005431979218977636
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016641857062426643
Average Adjusted Rand Index: 0.001583169443572027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21831.151100672098
Iteration 100: Loss = -10948.666764062158
Iteration 200: Loss = -10948.074530638698
Iteration 300: Loss = -10947.12454206184
Iteration 400: Loss = -10946.73081639949
Iteration 500: Loss = -10946.628900065933
Iteration 600: Loss = -10946.560204801432
Iteration 700: Loss = -10946.503787259426
Iteration 800: Loss = -10946.441052554994
Iteration 900: Loss = -10946.312997864368
Iteration 1000: Loss = -10945.97836871884
Iteration 1100: Loss = -10945.834678768626
Iteration 1200: Loss = -10945.79016052513
Iteration 1300: Loss = -10945.767842268988
Iteration 1400: Loss = -10945.75527493666
Iteration 1500: Loss = -10945.747729785095
Iteration 1600: Loss = -10945.74295358634
Iteration 1700: Loss = -10945.739788541572
Iteration 1800: Loss = -10945.73757804208
Iteration 1900: Loss = -10945.73598218284
Iteration 2000: Loss = -10945.734728890313
Iteration 2100: Loss = -10945.733846495295
Iteration 2200: Loss = -10945.733074502878
Iteration 2300: Loss = -10945.732535744311
Iteration 2400: Loss = -10945.7320692639
Iteration 2500: Loss = -10945.73176517295
Iteration 2600: Loss = -10945.731462774807
Iteration 2700: Loss = -10945.731239057399
Iteration 2800: Loss = -10945.73102698268
Iteration 2900: Loss = -10945.730872953993
Iteration 3000: Loss = -10945.730771176544
Iteration 3100: Loss = -10945.73062956151
Iteration 3200: Loss = -10945.730536856698
Iteration 3300: Loss = -10945.730487066729
Iteration 3400: Loss = -10945.730398988848
Iteration 3500: Loss = -10945.730347524035
Iteration 3600: Loss = -10945.730299099077
Iteration 3700: Loss = -10945.73029661648
Iteration 3800: Loss = -10945.730195175014
Iteration 3900: Loss = -10945.730177231331
Iteration 4000: Loss = -10945.73014561723
Iteration 4100: Loss = -10945.730147604143
Iteration 4200: Loss = -10945.730113100268
Iteration 4300: Loss = -10945.730101907451
Iteration 4400: Loss = -10945.730097131365
Iteration 4500: Loss = -10945.730045374487
Iteration 4600: Loss = -10945.730018370057
Iteration 4700: Loss = -10945.730030985518
Iteration 4800: Loss = -10945.730023794185
Iteration 4900: Loss = -10945.730002907225
Iteration 5000: Loss = -10945.72999757893
Iteration 5100: Loss = -10945.729978894651
Iteration 5200: Loss = -10945.729968779871
Iteration 5300: Loss = -10945.73000727791
Iteration 5400: Loss = -10945.72996330955
Iteration 5500: Loss = -10945.729973288293
Iteration 5600: Loss = -10945.729957882726
Iteration 5700: Loss = -10945.729951889714
Iteration 5800: Loss = -10945.729949938948
Iteration 5900: Loss = -10945.729963483906
Iteration 6000: Loss = -10945.729968487773
Iteration 6100: Loss = -10945.729977284851
Iteration 6200: Loss = -10945.729940782878
Iteration 6300: Loss = -10945.729933151619
Iteration 6400: Loss = -10945.729951045625
Iteration 6500: Loss = -10945.729941293694
Iteration 6600: Loss = -10945.729969126225
Iteration 6700: Loss = -10945.729936240585
Iteration 6800: Loss = -10945.729937568427
Iteration 6900: Loss = -10945.730006415324
Iteration 7000: Loss = -10945.729952261447
Iteration 7100: Loss = -10945.729918111903
Iteration 7200: Loss = -10945.729900788572
Iteration 7300: Loss = -10945.72990674039
Iteration 7400: Loss = -10945.729955893417
Iteration 7500: Loss = -10945.73229331563
1
Iteration 7600: Loss = -10945.731101852729
2
Iteration 7700: Loss = -10945.730085175353
3
Iteration 7800: Loss = -10945.729920887166
Iteration 7900: Loss = -10945.729924257863
Iteration 8000: Loss = -10945.729977605492
Iteration 8100: Loss = -10945.729911143928
Iteration 8200: Loss = -10945.730362787948
1
Iteration 8300: Loss = -10945.733827077176
2
Iteration 8400: Loss = -10945.730701056835
3
Iteration 8500: Loss = -10945.729949271295
Iteration 8600: Loss = -10945.73971005647
1
Iteration 8700: Loss = -10945.729933957478
Iteration 8800: Loss = -10945.737171594714
1
Iteration 8900: Loss = -10945.729914732117
Iteration 9000: Loss = -10945.729912047938
Iteration 9100: Loss = -10945.729934458348
Iteration 9200: Loss = -10945.729908736246
Iteration 9300: Loss = -10945.729940864954
Iteration 9400: Loss = -10945.73019142025
1
Iteration 9500: Loss = -10945.729914084426
Iteration 9600: Loss = -10945.729888567123
Iteration 9700: Loss = -10945.730508746268
1
Iteration 9800: Loss = -10945.729922915778
Iteration 9900: Loss = -10945.729913305486
Iteration 10000: Loss = -10945.729996663113
Iteration 10100: Loss = -10945.729944210027
Iteration 10200: Loss = -10945.729934412691
Iteration 10300: Loss = -10945.731158041008
1
Iteration 10400: Loss = -10945.729886054518
Iteration 10500: Loss = -10945.729924182539
Iteration 10600: Loss = -10945.731744695964
1
Iteration 10700: Loss = -10945.729960881215
Iteration 10800: Loss = -10945.72991612179
Iteration 10900: Loss = -10945.733447954532
1
Iteration 11000: Loss = -10945.729914954212
Iteration 11100: Loss = -10945.729889168135
Iteration 11200: Loss = -10945.735678847943
1
Iteration 11300: Loss = -10945.730618772535
2
Iteration 11400: Loss = -10945.729999274417
3
Iteration 11500: Loss = -10945.729922336337
Iteration 11600: Loss = -10945.73019347436
1
Iteration 11700: Loss = -10945.729932872473
Iteration 11800: Loss = -10945.730057751878
1
Iteration 11900: Loss = -10945.729888922202
Iteration 12000: Loss = -10945.7318844653
1
Iteration 12100: Loss = -10945.733615897414
2
Iteration 12200: Loss = -10945.729925623902
Iteration 12300: Loss = -10945.73052572738
1
Iteration 12400: Loss = -10945.72991742241
Iteration 12500: Loss = -10945.730001612963
Iteration 12600: Loss = -10945.729915799187
Iteration 12700: Loss = -10945.73045654019
1
Iteration 12800: Loss = -10945.74401408786
2
Iteration 12900: Loss = -10945.729916639573
Iteration 13000: Loss = -10945.984908141601
1
Iteration 13100: Loss = -10945.729882046702
Iteration 13200: Loss = -10945.734768026954
1
Iteration 13300: Loss = -10945.86093525452
2
Iteration 13400: Loss = -10945.730088089997
3
Iteration 13500: Loss = -10945.729924155854
Iteration 13600: Loss = -10945.73964250799
1
Iteration 13700: Loss = -10945.72990771463
Iteration 13800: Loss = -10945.73007103811
1
Iteration 13900: Loss = -10945.829719262501
2
Iteration 14000: Loss = -10945.730272819568
3
Iteration 14100: Loss = -10945.729976949502
Iteration 14200: Loss = -10945.73265553895
1
Iteration 14300: Loss = -10945.830746215073
2
Iteration 14400: Loss = -10945.729924746809
Iteration 14500: Loss = -10945.736009990958
1
Iteration 14600: Loss = -10945.729916678685
Iteration 14700: Loss = -10945.736088280304
1
Iteration 14800: Loss = -10945.734997395457
2
Iteration 14900: Loss = -10945.783096429077
3
Iteration 15000: Loss = -10945.729963869078
Iteration 15100: Loss = -10945.730219357098
1
Iteration 15200: Loss = -10945.924764695319
2
Iteration 15300: Loss = -10945.729925325933
Iteration 15400: Loss = -10945.73048753445
1
Iteration 15500: Loss = -10945.735316115983
2
Iteration 15600: Loss = -10945.729986550825
Iteration 15700: Loss = -10945.747286541542
1
Iteration 15800: Loss = -10945.729969595044
Iteration 15900: Loss = -10945.73006860217
Iteration 16000: Loss = -10945.873748433874
1
Iteration 16100: Loss = -10945.729961440145
Iteration 16200: Loss = -10945.730536798887
1
Iteration 16300: Loss = -10945.733498148464
2
Iteration 16400: Loss = -10945.730983188705
3
Iteration 16500: Loss = -10945.751762872074
4
Iteration 16600: Loss = -10945.729917402241
Iteration 16700: Loss = -10945.783170412788
1
Iteration 16800: Loss = -10945.729946565718
Iteration 16900: Loss = -10945.730548668187
1
Iteration 17000: Loss = -10945.730116483179
2
Iteration 17100: Loss = -10945.732016269869
3
Iteration 17200: Loss = -10945.729906767507
Iteration 17300: Loss = -10945.730240486591
1
Iteration 17400: Loss = -10945.729944112565
Iteration 17500: Loss = -10945.730085795582
1
Iteration 17600: Loss = -10945.741271285413
2
Iteration 17700: Loss = -10945.72989224918
Iteration 17800: Loss = -10945.865535687666
1
Iteration 17900: Loss = -10945.729950593906
Iteration 18000: Loss = -10945.75211778988
1
Iteration 18100: Loss = -10945.730270418522
2
Iteration 18200: Loss = -10945.769642549065
3
Iteration 18300: Loss = -10945.734224956168
4
Iteration 18400: Loss = -10945.731621637968
5
Iteration 18500: Loss = -10945.730029625138
Iteration 18600: Loss = -10945.731866924749
1
Iteration 18700: Loss = -10945.737394903575
2
Iteration 18800: Loss = -10945.73008415806
Iteration 18900: Loss = -10945.729968845468
Iteration 19000: Loss = -10945.744487267546
1
Iteration 19100: Loss = -10945.729900533883
Iteration 19200: Loss = -10945.73409828775
1
Iteration 19300: Loss = -10945.734767799437
2
Iteration 19400: Loss = -10945.746239781174
3
Iteration 19500: Loss = -10945.731348821437
4
Iteration 19600: Loss = -10945.729955019633
Iteration 19700: Loss = -10945.731695100752
1
Iteration 19800: Loss = -10945.732508787416
2
Iteration 19900: Loss = -10945.732611881298
3
pi: tensor([[0.1290, 0.8710],
        [0.0169, 0.9831]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0594, 0.9406], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2802, 0.2228],
         [0.6702, 0.1588]],

        [[0.6575, 0.2207],
         [0.6094, 0.6674]],

        [[0.6376, 0.2441],
         [0.6509, 0.6996]],

        [[0.6064, 0.1490],
         [0.6379, 0.6316]],

        [[0.6181, 0.1663],
         [0.7245, 0.6689]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.005431979218977636
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016641857062426643
Average Adjusted Rand Index: 0.001583169443572027
10879.600986584708
[0.0016641857062426643, 0.0016641857062426643] [0.001583169443572027, 0.001583169443572027] [10945.751937429446, 10945.729962242316]
-------------------------------------
This iteration is 38
True Objective function: Loss = -10991.27300525153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23138.901859037673
Iteration 100: Loss = -11109.118741512648
Iteration 200: Loss = -11107.826530204176
Iteration 300: Loss = -11106.698273513153
Iteration 400: Loss = -11105.781124169289
Iteration 500: Loss = -11102.520239626403
Iteration 600: Loss = -11100.928090869233
Iteration 700: Loss = -11073.086486667136
Iteration 800: Loss = -11013.285304452533
Iteration 900: Loss = -11008.303993975967
Iteration 1000: Loss = -11007.746773066869
Iteration 1100: Loss = -11007.5225541992
Iteration 1200: Loss = -11007.39147576476
Iteration 1300: Loss = -11007.230015671015
Iteration 1400: Loss = -11007.189424822524
Iteration 1500: Loss = -11007.155070636494
Iteration 1600: Loss = -11007.120723229324
Iteration 1700: Loss = -11007.085020003218
Iteration 1800: Loss = -11007.065919071883
Iteration 1900: Loss = -11007.046150119908
Iteration 2000: Loss = -11007.018808660912
Iteration 2100: Loss = -11006.986699309784
Iteration 2200: Loss = -11006.880280974428
Iteration 2300: Loss = -11006.247189119178
Iteration 2400: Loss = -11005.784627644542
Iteration 2500: Loss = -11005.690030320364
Iteration 2600: Loss = -11005.617091262671
Iteration 2700: Loss = -11005.592168941592
Iteration 2800: Loss = -11005.492092437515
Iteration 2900: Loss = -11005.462907504936
Iteration 3000: Loss = -11005.452623343304
Iteration 3100: Loss = -11005.43115448836
Iteration 3200: Loss = -11005.395235389135
Iteration 3300: Loss = -11005.34150044935
Iteration 3400: Loss = -11005.271195073668
Iteration 3500: Loss = -11005.262829815747
Iteration 3600: Loss = -11005.259666051124
Iteration 3700: Loss = -11005.257151668344
Iteration 3800: Loss = -11005.251726186423
Iteration 3900: Loss = -11005.172983022345
Iteration 4000: Loss = -11005.159537466725
Iteration 4100: Loss = -11005.15789817077
Iteration 4200: Loss = -11005.155848363698
Iteration 4300: Loss = -11005.152244645848
Iteration 4400: Loss = -11005.151160156332
Iteration 4500: Loss = -11005.151119389351
Iteration 4600: Loss = -11005.150300263243
Iteration 4700: Loss = -11005.150085679323
Iteration 4800: Loss = -11005.149848657737
Iteration 4900: Loss = -11005.149778652449
Iteration 5000: Loss = -11005.149472479456
Iteration 5100: Loss = -11005.149197344093
Iteration 5200: Loss = -11005.149481474551
1
Iteration 5300: Loss = -11005.14892099437
Iteration 5400: Loss = -11005.14887016251
Iteration 5500: Loss = -11005.150178878997
1
Iteration 5600: Loss = -11005.151965532717
2
Iteration 5700: Loss = -11005.148342043594
Iteration 5800: Loss = -11005.148303297918
Iteration 5900: Loss = -11005.148172834753
Iteration 6000: Loss = -11005.14812983838
Iteration 6100: Loss = -11005.151375910467
1
Iteration 6200: Loss = -11005.14791685362
Iteration 6300: Loss = -11005.147820147944
Iteration 6400: Loss = -11005.154573817224
1
Iteration 6500: Loss = -11005.147696070488
Iteration 6600: Loss = -11005.148974535694
1
Iteration 6700: Loss = -11005.147594749693
Iteration 6800: Loss = -11005.147734429938
1
Iteration 6900: Loss = -11005.147560403482
Iteration 7000: Loss = -11005.147575958474
Iteration 7100: Loss = -11005.14743790723
Iteration 7200: Loss = -11005.15779160835
1
Iteration 7300: Loss = -11005.147342820397
Iteration 7400: Loss = -11005.147332076533
Iteration 7500: Loss = -11005.147256522358
Iteration 7600: Loss = -11005.148110482509
1
Iteration 7700: Loss = -11005.147229008597
Iteration 7800: Loss = -11005.147206054246
Iteration 7900: Loss = -11005.147189379686
Iteration 8000: Loss = -11005.147216615833
Iteration 8100: Loss = -11005.160763004893
1
Iteration 8200: Loss = -11005.147094617607
Iteration 8300: Loss = -11005.147539617852
1
Iteration 8400: Loss = -11005.147022697558
Iteration 8500: Loss = -11005.182677801828
1
Iteration 8600: Loss = -11005.146982029073
Iteration 8700: Loss = -11005.146959991613
Iteration 8800: Loss = -11005.146992629472
Iteration 8900: Loss = -11005.146856122452
Iteration 9000: Loss = -11005.15127833497
1
Iteration 9100: Loss = -11005.146827737597
Iteration 9200: Loss = -11005.146783121094
Iteration 9300: Loss = -11005.146801771163
Iteration 9400: Loss = -11005.146861259242
Iteration 9500: Loss = -11005.146787450933
Iteration 9600: Loss = -11005.146783962191
Iteration 9700: Loss = -11005.1473755032
1
Iteration 9800: Loss = -11005.146768332308
Iteration 9900: Loss = -11005.49298560349
1
Iteration 10000: Loss = -11005.146761482174
Iteration 10100: Loss = -11005.146783129836
Iteration 10200: Loss = -11005.146904472866
1
Iteration 10300: Loss = -11005.14684584862
Iteration 10400: Loss = -11005.146747047786
Iteration 10500: Loss = -11005.146728834268
Iteration 10600: Loss = -11005.146881458835
1
Iteration 10700: Loss = -11005.146739960492
Iteration 10800: Loss = -11005.147278787324
1
Iteration 10900: Loss = -11005.14671135807
Iteration 11000: Loss = -11005.14671855273
Iteration 11100: Loss = -11005.152101420252
1
Iteration 11200: Loss = -11005.146715339932
Iteration 11300: Loss = -11005.146738672664
Iteration 11400: Loss = -11005.148125904969
1
Iteration 11500: Loss = -11005.146717505992
Iteration 11600: Loss = -11005.14673282447
Iteration 11700: Loss = -11005.335718878989
1
Iteration 11800: Loss = -11005.146709395764
Iteration 11900: Loss = -11005.146706436035
Iteration 12000: Loss = -11005.146721146808
Iteration 12100: Loss = -11005.213938625284
1
Iteration 12200: Loss = -11005.146697325603
Iteration 12300: Loss = -11005.146696469476
Iteration 12400: Loss = -11005.14667656474
Iteration 12500: Loss = -11005.147148321565
1
Iteration 12600: Loss = -11005.146643071852
Iteration 12700: Loss = -11005.146666071232
Iteration 12800: Loss = -11005.149693800062
1
Iteration 12900: Loss = -11005.14695870579
2
Iteration 13000: Loss = -11005.146654054945
Iteration 13100: Loss = -11005.148118371202
1
Iteration 13200: Loss = -11005.146645500463
Iteration 13300: Loss = -11005.250137033512
1
Iteration 13400: Loss = -11005.146663943482
Iteration 13500: Loss = -11005.159546302897
1
Iteration 13600: Loss = -11005.146672351397
Iteration 13700: Loss = -11005.150621191187
1
Iteration 13800: Loss = -11005.148057296301
2
Iteration 13900: Loss = -11005.14671347765
Iteration 14000: Loss = -11005.173870429557
1
Iteration 14100: Loss = -11005.14667466539
Iteration 14200: Loss = -11005.147006789255
1
Iteration 14300: Loss = -11005.147132084201
2
Iteration 14400: Loss = -11005.146651765259
Iteration 14500: Loss = -11005.43183321366
1
Iteration 14600: Loss = -11005.146421350557
Iteration 14700: Loss = -11005.29099443547
1
Iteration 14800: Loss = -11005.146413645443
Iteration 14900: Loss = -11005.161951852508
1
Iteration 15000: Loss = -11005.146421558995
Iteration 15100: Loss = -11005.153458142802
1
Iteration 15200: Loss = -11005.146443679581
Iteration 15300: Loss = -11005.15801980975
1
Iteration 15400: Loss = -11005.146404591434
Iteration 15500: Loss = -11005.191708371254
1
Iteration 15600: Loss = -11005.147022319517
2
Iteration 15700: Loss = -11005.380741810995
3
Iteration 15800: Loss = -11005.146356909427
Iteration 15900: Loss = -11005.562534867493
1
Iteration 16000: Loss = -11005.146384381569
Iteration 16100: Loss = -11005.146802495166
1
Iteration 16200: Loss = -11005.146357552325
Iteration 16300: Loss = -11005.191278587597
1
Iteration 16400: Loss = -11005.146362533973
Iteration 16500: Loss = -11005.146699584262
1
Iteration 16600: Loss = -11005.401162227157
2
Iteration 16700: Loss = -11005.14636294988
Iteration 16800: Loss = -11005.14827384359
1
Iteration 16900: Loss = -11005.374276110715
2
Iteration 17000: Loss = -11005.146369738557
Iteration 17100: Loss = -11005.29854973182
1
Iteration 17200: Loss = -11005.146371744067
Iteration 17300: Loss = -11005.391462579559
1
Iteration 17400: Loss = -11005.146374897151
Iteration 17500: Loss = -11005.150607622843
1
Iteration 17600: Loss = -11005.146348115224
Iteration 17700: Loss = -11005.148432615148
1
Iteration 17800: Loss = -11005.146342869666
Iteration 17900: Loss = -11005.147725469802
1
Iteration 18000: Loss = -11005.146351172021
Iteration 18100: Loss = -11005.149366004403
1
Iteration 18200: Loss = -11005.146340918898
Iteration 18300: Loss = -11005.152327057578
1
Iteration 18400: Loss = -11005.146902484252
2
Iteration 18500: Loss = -11005.14651282921
3
Iteration 18600: Loss = -11005.149402465577
4
Iteration 18700: Loss = -11005.146416703023
Iteration 18800: Loss = -11005.146470519816
Iteration 18900: Loss = -11005.34095212854
1
Iteration 19000: Loss = -11005.14637070981
Iteration 19100: Loss = -11005.148996814472
1
Iteration 19200: Loss = -11005.146347191723
Iteration 19300: Loss = -11005.14647153038
1
Iteration 19400: Loss = -11005.146668583033
2
Iteration 19500: Loss = -11005.335678872498
3
Iteration 19600: Loss = -11005.147588837968
4
Iteration 19700: Loss = -11005.317801198344
5
Iteration 19800: Loss = -11005.146354158556
Iteration 19900: Loss = -11005.15413210767
1
pi: tensor([[0.6989, 0.3011],
        [0.3816, 0.6184]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0607, 0.9393], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2676, 0.1151],
         [0.7166, 0.1781]],

        [[0.7008, 0.1101],
         [0.6509, 0.6791]],

        [[0.5787, 0.0997],
         [0.6766, 0.7109]],

        [[0.5205, 0.1043],
         [0.6736, 0.6438]],

        [[0.6121, 0.1030],
         [0.6703, 0.6499]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.5348890540274414
Average Adjusted Rand Index: 0.6789468585318247
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24279.94691466641
Iteration 100: Loss = -11107.684967531191
Iteration 200: Loss = -11106.158236580506
Iteration 300: Loss = -11103.85264570516
Iteration 400: Loss = -11102.063865518023
Iteration 500: Loss = -11101.324256695036
Iteration 600: Loss = -11096.229229693588
Iteration 700: Loss = -11040.683551381595
Iteration 800: Loss = -11010.682138845086
Iteration 900: Loss = -11008.748659302504
Iteration 1000: Loss = -11007.72749268677
Iteration 1100: Loss = -11007.601945601584
Iteration 1200: Loss = -11007.533784456007
Iteration 1300: Loss = -11007.487242277235
Iteration 1400: Loss = -11007.45230214083
Iteration 1500: Loss = -11007.374289194973
Iteration 1600: Loss = -11007.274857528853
Iteration 1700: Loss = -11007.233006500299
Iteration 1800: Loss = -11007.148954408774
Iteration 1900: Loss = -11006.731694942224
Iteration 2000: Loss = -11005.921824660973
Iteration 2100: Loss = -11005.641556332112
Iteration 2200: Loss = -11005.488580235204
Iteration 2300: Loss = -11005.39264047293
Iteration 2400: Loss = -11005.373731558082
Iteration 2500: Loss = -11005.362122711504
Iteration 2600: Loss = -11005.354004459869
Iteration 2700: Loss = -11005.349749143765
Iteration 2800: Loss = -11005.347198548618
Iteration 2900: Loss = -11005.345332525922
Iteration 3000: Loss = -11005.343887880987
Iteration 3100: Loss = -11005.344448042197
1
Iteration 3200: Loss = -11005.342190211735
Iteration 3300: Loss = -11005.340986797963
Iteration 3400: Loss = -11005.341163600731
1
Iteration 3500: Loss = -11005.353693276063
2
Iteration 3600: Loss = -11005.336900695496
Iteration 3700: Loss = -11005.184423013394
Iteration 3800: Loss = -11005.182410768573
Iteration 3900: Loss = -11005.17658670863
Iteration 4000: Loss = -11005.175938159875
Iteration 4100: Loss = -11005.175307406167
Iteration 4200: Loss = -11005.174924788604
Iteration 4300: Loss = -11005.174296897143
Iteration 4400: Loss = -11005.173481808022
Iteration 4500: Loss = -11005.172357780719
Iteration 4600: Loss = -11005.168598200305
Iteration 4700: Loss = -11005.166500683046
Iteration 4800: Loss = -11005.165998539853
Iteration 4900: Loss = -11005.166086901152
Iteration 5000: Loss = -11005.164992776745
Iteration 5100: Loss = -11005.164053193808
Iteration 5200: Loss = -11005.160546992665
Iteration 5300: Loss = -11005.157565234334
Iteration 5400: Loss = -11005.161959527046
1
Iteration 5500: Loss = -11005.156390315353
Iteration 5600: Loss = -11005.15784934907
1
Iteration 5700: Loss = -11005.155820016322
Iteration 5800: Loss = -11005.16285864913
1
Iteration 5900: Loss = -11005.155447316543
Iteration 6000: Loss = -11005.155881340352
1
Iteration 6100: Loss = -11005.156658771302
2
Iteration 6200: Loss = -11005.155476748789
Iteration 6300: Loss = -11005.154197864764
Iteration 6400: Loss = -11005.156035367274
1
Iteration 6500: Loss = -11005.153473912304
Iteration 6600: Loss = -11005.153854278276
1
Iteration 6700: Loss = -11005.153377559165
Iteration 6800: Loss = -11005.155637443359
1
Iteration 6900: Loss = -11005.153296091814
Iteration 7000: Loss = -11005.15327507201
Iteration 7100: Loss = -11005.154209244818
1
Iteration 7200: Loss = -11005.15323435296
Iteration 7300: Loss = -11005.15410048587
1
Iteration 7400: Loss = -11005.153204724595
Iteration 7500: Loss = -11005.15317754114
Iteration 7600: Loss = -11005.15756702616
1
Iteration 7700: Loss = -11005.153053272796
Iteration 7800: Loss = -11005.154777808273
1
Iteration 7900: Loss = -11005.149067874529
Iteration 8000: Loss = -11005.148661736644
Iteration 8100: Loss = -11005.14865249644
Iteration 8200: Loss = -11005.15092961225
1
Iteration 8300: Loss = -11005.148328658284
Iteration 8400: Loss = -11005.150145560634
1
Iteration 8500: Loss = -11005.148111048993
Iteration 8600: Loss = -11005.164765224868
1
Iteration 8700: Loss = -11005.148081402624
Iteration 8800: Loss = -11005.148065494337
Iteration 8900: Loss = -11005.148444349401
1
Iteration 9000: Loss = -11005.147978587835
Iteration 9100: Loss = -11005.151012001217
1
Iteration 9200: Loss = -11005.147808520203
Iteration 9300: Loss = -11005.147734336342
Iteration 9400: Loss = -11005.148629747708
1
Iteration 9500: Loss = -11005.147762727518
Iteration 9600: Loss = -11005.14772907693
Iteration 9700: Loss = -11005.147749304402
Iteration 9800: Loss = -11005.147677430094
Iteration 9900: Loss = -11005.257113343743
1
Iteration 10000: Loss = -11005.147717274996
Iteration 10100: Loss = -11005.147689766325
Iteration 10200: Loss = -11005.149509387858
1
Iteration 10300: Loss = -11005.14766729154
Iteration 10400: Loss = -11005.147641080512
Iteration 10500: Loss = -11005.149211584669
1
Iteration 10600: Loss = -11005.147654243403
Iteration 10700: Loss = -11005.147708112274
Iteration 10800: Loss = -11005.148234192244
1
Iteration 10900: Loss = -11005.14767038685
Iteration 11000: Loss = -11005.381199339523
1
Iteration 11100: Loss = -11005.147654137962
Iteration 11200: Loss = -11005.147634153805
Iteration 11300: Loss = -11005.21917284833
1
Iteration 11400: Loss = -11005.147641266292
Iteration 11500: Loss = -11005.147654834209
Iteration 11600: Loss = -11005.150680295648
1
Iteration 11700: Loss = -11005.147618529689
Iteration 11800: Loss = -11005.147714987403
Iteration 11900: Loss = -11005.147625944743
Iteration 12000: Loss = -11005.14762377542
Iteration 12100: Loss = -11005.172851064992
1
Iteration 12200: Loss = -11005.147630292886
Iteration 12300: Loss = -11005.147597060415
Iteration 12400: Loss = -11005.148702284296
1
Iteration 12500: Loss = -11005.14760572901
Iteration 12600: Loss = -11005.1475109824
Iteration 12700: Loss = -11005.154465773267
1
Iteration 12800: Loss = -11005.14742856379
Iteration 12900: Loss = -11005.146808346368
Iteration 13000: Loss = -11005.147363292735
1
Iteration 13100: Loss = -11005.146707492142
Iteration 13200: Loss = -11005.314967550508
1
Iteration 13300: Loss = -11005.146685828975
Iteration 13400: Loss = -11005.147173117024
1
Iteration 13500: Loss = -11005.146715230616
Iteration 13600: Loss = -11005.14957501548
1
Iteration 13700: Loss = -11005.146701756876
Iteration 13800: Loss = -11005.211256332996
1
Iteration 13900: Loss = -11005.146710650592
Iteration 14000: Loss = -11005.146713076516
Iteration 14100: Loss = -11005.147701709155
1
Iteration 14200: Loss = -11005.146682048575
Iteration 14300: Loss = -11005.357921050183
1
Iteration 14400: Loss = -11005.146682721293
Iteration 14500: Loss = -11005.232276015051
1
Iteration 14600: Loss = -11005.1466456076
Iteration 14700: Loss = -11005.355457975895
1
Iteration 14800: Loss = -11005.146662490328
Iteration 14900: Loss = -11005.147331633647
1
Iteration 15000: Loss = -11005.146745328653
Iteration 15100: Loss = -11005.14684977902
1
Iteration 15200: Loss = -11005.202437382464
2
Iteration 15300: Loss = -11005.146663303694
Iteration 15400: Loss = -11005.4529794271
1
Iteration 15500: Loss = -11005.14642690945
Iteration 15600: Loss = -11005.14650610939
Iteration 15700: Loss = -11005.146460459438
Iteration 15800: Loss = -11005.146410964517
Iteration 15900: Loss = -11005.14889133243
1
Iteration 16000: Loss = -11005.150177655116
2
Iteration 16100: Loss = -11005.277835532013
3
Iteration 16200: Loss = -11005.146412389287
Iteration 16300: Loss = -11005.160546810082
1
Iteration 16400: Loss = -11005.251026330563
2
Iteration 16500: Loss = -11005.146457066623
Iteration 16600: Loss = -11005.14644223353
Iteration 16700: Loss = -11005.155325097408
1
Iteration 16800: Loss = -11005.146409093693
Iteration 16900: Loss = -11005.172260113703
1
Iteration 17000: Loss = -11005.146357454407
Iteration 17100: Loss = -11005.148730816356
1
Iteration 17200: Loss = -11005.146420140427
Iteration 17300: Loss = -11005.151951037536
1
Iteration 17400: Loss = -11005.146351065277
Iteration 17500: Loss = -11005.198096979606
1
Iteration 17600: Loss = -11005.146383339086
Iteration 17700: Loss = -11005.146361201148
Iteration 17800: Loss = -11005.147379417585
1
Iteration 17900: Loss = -11005.14634840989
Iteration 18000: Loss = -11005.224518696581
1
Iteration 18100: Loss = -11005.14633539732
Iteration 18200: Loss = -11005.146362390056
Iteration 18300: Loss = -11005.214409659482
1
Iteration 18400: Loss = -11005.146357819698
Iteration 18500: Loss = -11005.146353592972
Iteration 18600: Loss = -11005.147625857391
1
Iteration 18700: Loss = -11005.146390572247
Iteration 18800: Loss = -11005.14638023986
Iteration 18900: Loss = -11005.146369288044
Iteration 19000: Loss = -11005.146350816705
Iteration 19100: Loss = -11005.350003653852
1
Iteration 19200: Loss = -11005.146334436391
Iteration 19300: Loss = -11005.146392190329
Iteration 19400: Loss = -11005.154636904505
1
Iteration 19500: Loss = -11005.146390194246
Iteration 19600: Loss = -11005.146382088333
Iteration 19700: Loss = -11005.146736780587
1
Iteration 19800: Loss = -11005.146356301848
Iteration 19900: Loss = -11005.278821506587
1
pi: tensor([[0.6989, 0.3011],
        [0.3816, 0.6184]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0607, 0.9393], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2677, 0.1151],
         [0.6547, 0.1781]],

        [[0.5649, 0.1101],
         [0.7029, 0.6304]],

        [[0.7293, 0.0997],
         [0.5868, 0.5015]],

        [[0.7262, 0.1043],
         [0.6492, 0.6677]],

        [[0.6877, 0.1030],
         [0.7003, 0.5201]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 3
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 4
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.5348890540274414
Average Adjusted Rand Index: 0.6789468585318247
10991.27300525153
[0.5348890540274414, 0.5348890540274414] [0.6789468585318247, 0.6789468585318247] [11005.146369705435, 11005.146375128252]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11160.826880379685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21411.521588445215
Iteration 100: Loss = -11239.37297207767
Iteration 200: Loss = -11238.27451535815
Iteration 300: Loss = -11237.799255518288
Iteration 400: Loss = -11237.285535981806
Iteration 500: Loss = -11236.877885169688
Iteration 600: Loss = -11236.617946264212
Iteration 700: Loss = -11236.224737453658
Iteration 800: Loss = -11236.008195505885
Iteration 900: Loss = -11235.860504208287
Iteration 1000: Loss = -11235.774591112573
Iteration 1100: Loss = -11235.71248881909
Iteration 1200: Loss = -11235.657390949254
Iteration 1300: Loss = -11235.59964230709
Iteration 1400: Loss = -11235.517418856323
Iteration 1500: Loss = -11234.772733832127
Iteration 1600: Loss = -11234.086325138904
Iteration 1700: Loss = -11233.499406867979
Iteration 1800: Loss = -11233.387719651771
Iteration 1900: Loss = -11233.36067648536
Iteration 2000: Loss = -11233.34112773624
Iteration 2100: Loss = -11233.325897875327
Iteration 2200: Loss = -11233.300736098847
Iteration 2300: Loss = -11233.246362700982
Iteration 2400: Loss = -11233.146052821081
Iteration 2500: Loss = -11233.091709176317
Iteration 2600: Loss = -11233.075333674273
Iteration 2700: Loss = -11233.070203093355
Iteration 2800: Loss = -11233.066865394512
Iteration 2900: Loss = -11233.064717658766
Iteration 3000: Loss = -11233.063036257032
Iteration 3100: Loss = -11233.061790363712
Iteration 3200: Loss = -11233.060926370697
Iteration 3300: Loss = -11233.060507981794
Iteration 3400: Loss = -11233.059926932461
Iteration 3500: Loss = -11233.059594450924
Iteration 3600: Loss = -11233.059196876997
Iteration 3700: Loss = -11233.058885900158
Iteration 3800: Loss = -11233.058559854378
Iteration 3900: Loss = -11233.058340290447
Iteration 4000: Loss = -11233.058000281988
Iteration 4100: Loss = -11233.05760838281
Iteration 4200: Loss = -11233.056660942351
Iteration 4300: Loss = -11233.053617650565
Iteration 4400: Loss = -11233.052408333488
Iteration 4500: Loss = -11233.052222013761
Iteration 4600: Loss = -11233.05207059704
Iteration 4700: Loss = -11233.051929364765
Iteration 4800: Loss = -11233.051641453085
Iteration 4900: Loss = -11233.052870709085
1
Iteration 5000: Loss = -11233.051215327016
Iteration 5100: Loss = -11233.051101905641
Iteration 5200: Loss = -11233.050924344301
Iteration 5300: Loss = -11233.050854504083
Iteration 5400: Loss = -11233.050744072241
Iteration 5500: Loss = -11233.050604805903
Iteration 5600: Loss = -11233.050519681048
Iteration 5700: Loss = -11233.05050797736
Iteration 5800: Loss = -11233.050413545478
Iteration 5900: Loss = -11233.05034362699
Iteration 6000: Loss = -11233.052017111511
1
Iteration 6100: Loss = -11233.050202243488
Iteration 6200: Loss = -11233.050717367503
1
Iteration 6300: Loss = -11233.050127190874
Iteration 6400: Loss = -11233.050083020506
Iteration 6500: Loss = -11233.05192734593
1
Iteration 6600: Loss = -11233.049962103165
Iteration 6700: Loss = -11233.04999219446
Iteration 6800: Loss = -11233.049895710838
Iteration 6900: Loss = -11233.049857791133
Iteration 7000: Loss = -11233.049838424335
Iteration 7100: Loss = -11233.05222857795
1
Iteration 7200: Loss = -11233.049787571246
Iteration 7300: Loss = -11233.049734179032
Iteration 7400: Loss = -11233.049759102765
Iteration 7500: Loss = -11233.056115219431
1
Iteration 7600: Loss = -11233.049667353776
Iteration 7700: Loss = -11233.049646414669
Iteration 7800: Loss = -11233.049657591388
Iteration 7900: Loss = -11233.049612804094
Iteration 8000: Loss = -11233.051395979464
1
Iteration 8100: Loss = -11233.049772396613
2
Iteration 8200: Loss = -11233.050610754344
3
Iteration 8300: Loss = -11233.086797504688
4
Iteration 8400: Loss = -11233.049526538849
Iteration 8500: Loss = -11233.09915909882
1
Iteration 8600: Loss = -11233.049506072735
Iteration 8700: Loss = -11233.278548162894
1
Iteration 8800: Loss = -11233.04947227972
Iteration 8900: Loss = -11233.04942208765
Iteration 9000: Loss = -11233.049568459792
1
Iteration 9100: Loss = -11233.049453010486
Iteration 9200: Loss = -11233.08034263774
1
Iteration 9300: Loss = -11233.04941136594
Iteration 9400: Loss = -11233.049368096175
Iteration 9500: Loss = -11233.088243136131
1
Iteration 9600: Loss = -11233.049388863814
Iteration 9700: Loss = -11233.064079769378
1
Iteration 9800: Loss = -11233.049370453766
Iteration 9900: Loss = -11233.178345130944
1
Iteration 10000: Loss = -11233.049330028694
Iteration 10100: Loss = -11233.04934090757
Iteration 10200: Loss = -11233.050279674202
1
Iteration 10300: Loss = -11233.049310935174
Iteration 10400: Loss = -11233.06459672014
1
Iteration 10500: Loss = -11233.049328945308
Iteration 10600: Loss = -11233.053219940597
1
Iteration 10700: Loss = -11233.049276277454
Iteration 10800: Loss = -11233.04934016314
Iteration 10900: Loss = -11233.049281845853
Iteration 11000: Loss = -11233.049833483412
1
Iteration 11100: Loss = -11233.049922581553
2
Iteration 11200: Loss = -11233.050357702703
3
Iteration 11300: Loss = -11233.04932947229
Iteration 11400: Loss = -11233.050211658192
1
Iteration 11500: Loss = -11233.04925099942
Iteration 11600: Loss = -11233.050951095294
1
Iteration 11700: Loss = -11233.04923167446
Iteration 11800: Loss = -11233.050364376058
1
Iteration 11900: Loss = -11233.049237885687
Iteration 12000: Loss = -11233.050714197558
1
Iteration 12100: Loss = -11233.049230126857
Iteration 12200: Loss = -11233.0492758002
Iteration 12300: Loss = -11233.051753993099
1
Iteration 12400: Loss = -11233.061379179917
2
Iteration 12500: Loss = -11233.063036802268
3
Iteration 12600: Loss = -11233.062211537523
4
Iteration 12700: Loss = -11233.064647016501
5
Iteration 12800: Loss = -11233.05655628722
6
Iteration 12900: Loss = -11233.050505720294
7
Iteration 13000: Loss = -11233.050839170428
8
Iteration 13100: Loss = -11233.051596242838
9
Iteration 13200: Loss = -11233.059944742226
10
Iteration 13300: Loss = -11233.051909042859
11
Iteration 13400: Loss = -11233.053607088226
12
Iteration 13500: Loss = -11233.053604107687
13
Iteration 13600: Loss = -11233.10683831194
14
Iteration 13700: Loss = -11233.050532320867
15
Stopping early at iteration 13700 due to no improvement.
pi: tensor([[9.1792e-01, 8.2078e-02],
        [9.4070e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0929, 0.9071], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4140, 0.1663],
         [0.7139, 0.1681]],

        [[0.6839, 0.1582],
         [0.6855, 0.6952]],

        [[0.6628, 0.2156],
         [0.6913, 0.6867]],

        [[0.6713, 0.1442],
         [0.5917, 0.5923]],

        [[0.5249, 0.1297],
         [0.5124, 0.7179]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005914731726761669
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002508217918958077
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.016623577451856865
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.003702958125914224
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0029336721668764927
Global Adjusted Rand Index: -0.0031435669164542423
Average Adjusted Rand Index: -0.006336631478073465
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22755.01180717343
Iteration 100: Loss = -11241.613037276777
Iteration 200: Loss = -11238.930866154104
Iteration 300: Loss = -11238.148219328928
Iteration 400: Loss = -11237.009714661768
Iteration 500: Loss = -11234.08137471448
Iteration 600: Loss = -11232.652744317627
Iteration 700: Loss = -11231.731481341027
Iteration 800: Loss = -11231.278254861198
Iteration 900: Loss = -11230.98270245009
Iteration 1000: Loss = -11230.747565688473
Iteration 1100: Loss = -11230.533629996562
Iteration 1200: Loss = -11230.292085736044
Iteration 1300: Loss = -11229.993749358135
Iteration 1400: Loss = -11229.582698576054
Iteration 1500: Loss = -11228.505252220297
Iteration 1600: Loss = -11159.461910278702
Iteration 1700: Loss = -11113.812664474515
Iteration 1800: Loss = -11108.1771996067
Iteration 1900: Loss = -11107.91235309658
Iteration 2000: Loss = -11107.502551295194
Iteration 2100: Loss = -11104.918710178272
Iteration 2200: Loss = -11104.619611049138
Iteration 2300: Loss = -11104.597316618647
Iteration 2400: Loss = -11104.583744574824
Iteration 2500: Loss = -11104.573758404404
Iteration 2600: Loss = -11104.565620754172
Iteration 2700: Loss = -11104.558084680035
Iteration 2800: Loss = -11104.549748355721
Iteration 2900: Loss = -11104.3005854926
Iteration 3000: Loss = -11104.287355485492
Iteration 3100: Loss = -11104.280201661015
Iteration 3200: Loss = -11104.235316810797
Iteration 3300: Loss = -11104.163974134168
Iteration 3400: Loss = -11104.160481662746
Iteration 3500: Loss = -11104.188808708288
1
Iteration 3600: Loss = -11104.155651710538
Iteration 3700: Loss = -11104.150549040098
Iteration 3800: Loss = -11104.170531065814
1
Iteration 3900: Loss = -11104.146665933713
Iteration 4000: Loss = -11104.146009645649
Iteration 4100: Loss = -11104.14233672071
Iteration 4200: Loss = -11104.131839129888
Iteration 4300: Loss = -11104.128895695165
Iteration 4400: Loss = -11104.129399981632
1
Iteration 4500: Loss = -11104.127512505564
Iteration 4600: Loss = -11104.143088986673
1
Iteration 4700: Loss = -11104.12652457878
Iteration 4800: Loss = -11104.128261323609
1
Iteration 4900: Loss = -11104.125699952425
Iteration 5000: Loss = -11104.125268886686
Iteration 5100: Loss = -11104.12466939266
Iteration 5200: Loss = -11104.123377139113
Iteration 5300: Loss = -11104.125216278624
1
Iteration 5400: Loss = -11104.118183892198
Iteration 5500: Loss = -11104.117856758417
Iteration 5600: Loss = -11104.116967761256
Iteration 5700: Loss = -11104.115985260196
Iteration 5800: Loss = -11104.115811548761
Iteration 5900: Loss = -11104.114668609765
Iteration 6000: Loss = -11104.114318644604
Iteration 6100: Loss = -11104.113955330757
Iteration 6200: Loss = -11104.113650689696
Iteration 6300: Loss = -11104.113258468626
Iteration 6400: Loss = -11104.112347833407
Iteration 6500: Loss = -11104.094535315473
Iteration 6600: Loss = -11104.058607967925
Iteration 6700: Loss = -11104.058471446484
Iteration 6800: Loss = -11104.058310997652
Iteration 6900: Loss = -11104.05856138908
1
Iteration 7000: Loss = -11104.05903342546
2
Iteration 7100: Loss = -11104.057726010298
Iteration 7200: Loss = -11104.057706824748
Iteration 7300: Loss = -11104.057576057232
Iteration 7400: Loss = -11104.057676773524
1
Iteration 7500: Loss = -11104.05744543122
Iteration 7600: Loss = -11104.057388724903
Iteration 7700: Loss = -11104.057292612571
Iteration 7800: Loss = -11104.057237846213
Iteration 7900: Loss = -11104.05707297478
Iteration 8000: Loss = -11104.061751743156
1
Iteration 8100: Loss = -11104.055895542126
Iteration 8200: Loss = -11104.122489719204
1
Iteration 8300: Loss = -11104.056114755505
2
Iteration 8400: Loss = -11104.054871924256
Iteration 8500: Loss = -11104.047224781143
Iteration 8600: Loss = -11104.03624971292
Iteration 8700: Loss = -11104.036265452405
Iteration 8800: Loss = -11104.036072670691
Iteration 8900: Loss = -11104.080514164878
1
Iteration 9000: Loss = -11104.03594529326
Iteration 9100: Loss = -11104.036009645084
Iteration 9200: Loss = -11104.03519718898
Iteration 9300: Loss = -11104.221222775277
1
Iteration 9400: Loss = -11104.037371735338
2
Iteration 9500: Loss = -11104.052936403938
3
Iteration 9600: Loss = -11104.034933480347
Iteration 9700: Loss = -11104.034973276504
Iteration 9800: Loss = -11104.050287533319
1
Iteration 9900: Loss = -11104.035296701359
2
Iteration 10000: Loss = -11104.035194245655
3
Iteration 10100: Loss = -11104.126796205019
4
Iteration 10200: Loss = -11104.051871714693
5
Iteration 10300: Loss = -11104.053404097578
6
Iteration 10400: Loss = -11104.035667943503
7
Iteration 10500: Loss = -11104.034969127448
Iteration 10600: Loss = -11104.04082891003
1
Iteration 10700: Loss = -11104.03733531685
2
Iteration 10800: Loss = -11104.034008169398
Iteration 10900: Loss = -11104.03557034031
1
Iteration 11000: Loss = -11104.036192936317
2
Iteration 11100: Loss = -11104.033951039322
Iteration 11200: Loss = -11104.036873108758
1
Iteration 11300: Loss = -11104.078976089118
2
Iteration 11400: Loss = -11104.03923255489
3
Iteration 11500: Loss = -11104.037718187677
4
Iteration 11600: Loss = -11104.037001967694
5
Iteration 11700: Loss = -11104.03699882836
6
Iteration 11800: Loss = -11104.038148881013
7
Iteration 11900: Loss = -11104.043872874741
8
Iteration 12000: Loss = -11104.034934517562
9
Iteration 12100: Loss = -11104.034536934747
10
Iteration 12200: Loss = -11104.04036126198
11
Iteration 12300: Loss = -11104.03366039942
Iteration 12400: Loss = -11104.036095247146
1
Iteration 12500: Loss = -11104.043590681911
2
Iteration 12600: Loss = -11104.040887894495
3
Iteration 12700: Loss = -11104.033819919134
4
Iteration 12800: Loss = -11104.037452884062
5
Iteration 12900: Loss = -11104.033984926757
6
Iteration 13000: Loss = -11104.033895052478
7
Iteration 13100: Loss = -11104.034259836888
8
Iteration 13200: Loss = -11104.033294208313
Iteration 13300: Loss = -11104.269568655736
1
Iteration 13400: Loss = -11104.033724250023
2
Iteration 13500: Loss = -11104.035574385176
3
Iteration 13600: Loss = -11104.033991426744
4
Iteration 13700: Loss = -11104.05207437266
5
Iteration 13800: Loss = -11104.071096120115
6
Iteration 13900: Loss = -11104.163051482148
7
Iteration 14000: Loss = -11104.033318060789
Iteration 14100: Loss = -11104.034972263367
1
Iteration 14200: Loss = -11104.033881221752
2
Iteration 14300: Loss = -11104.043137690745
3
Iteration 14400: Loss = -11104.0443028635
4
Iteration 14500: Loss = -11104.033515934587
5
Iteration 14600: Loss = -11104.034061993258
6
Iteration 14700: Loss = -11104.036608649343
7
Iteration 14800: Loss = -11104.046328428047
8
Iteration 14900: Loss = -11104.091134253626
9
Iteration 15000: Loss = -11104.032679126669
Iteration 15100: Loss = -11104.032286088554
Iteration 15200: Loss = -11104.032252109346
Iteration 15300: Loss = -11104.032285398867
Iteration 15400: Loss = -11104.032189789457
Iteration 15500: Loss = -11104.035932554349
1
Iteration 15600: Loss = -11104.03882298786
2
Iteration 15700: Loss = -11104.032030186509
Iteration 15800: Loss = -11104.293265044482
1
Iteration 15900: Loss = -11104.032125530672
Iteration 16000: Loss = -11104.036102588294
1
Iteration 16100: Loss = -11104.034727186528
2
Iteration 16200: Loss = -11104.032382361172
3
Iteration 16300: Loss = -11104.03293048275
4
Iteration 16400: Loss = -11104.046443473928
5
Iteration 16500: Loss = -11104.03227657194
6
Iteration 16600: Loss = -11104.038111322268
7
Iteration 16700: Loss = -11104.039450906917
8
Iteration 16800: Loss = -11104.032085560371
Iteration 16900: Loss = -11104.033846509761
1
Iteration 17000: Loss = -11104.03512953333
2
Iteration 17100: Loss = -11104.032157206004
Iteration 17200: Loss = -11104.038899215811
1
Iteration 17300: Loss = -11104.032399511601
2
Iteration 17400: Loss = -11104.034236054851
3
Iteration 17500: Loss = -11104.075926635942
4
Iteration 17600: Loss = -11104.035348886302
5
Iteration 17700: Loss = -11104.15434352805
6
Iteration 17800: Loss = -11104.033282900547
7
Iteration 17900: Loss = -11104.0581572771
8
Iteration 18000: Loss = -11104.043745811308
9
Iteration 18100: Loss = -11104.032734063492
10
Iteration 18200: Loss = -11104.032360510271
11
Iteration 18300: Loss = -11104.032734027194
12
Iteration 18400: Loss = -11104.032107976811
Iteration 18500: Loss = -11104.042589167408
1
Iteration 18600: Loss = -11104.047219584843
2
Iteration 18700: Loss = -11104.128897885073
3
Iteration 18800: Loss = -11104.039049071027
4
Iteration 18900: Loss = -11104.046759480487
5
Iteration 19000: Loss = -11104.03439547522
6
Iteration 19100: Loss = -11104.032251332119
7
Iteration 19200: Loss = -11104.033171433439
8
Iteration 19300: Loss = -11104.033405752516
9
Iteration 19400: Loss = -11104.035894270968
10
Iteration 19500: Loss = -11104.033100167078
11
Iteration 19600: Loss = -11104.035452749118
12
Iteration 19700: Loss = -11104.032415264715
13
Iteration 19800: Loss = -11104.039958711688
14
Iteration 19900: Loss = -11104.043718500836
15
Stopping early at iteration 19900 due to no improvement.
pi: tensor([[0.7344, 0.2656],
        [0.2141, 0.7859]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4326, 0.5674], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.1083],
         [0.5780, 0.2463]],

        [[0.5324, 0.1005],
         [0.5410, 0.5767]],

        [[0.5935, 0.0996],
         [0.6560, 0.5499]],

        [[0.5108, 0.1132],
         [0.6842, 0.6441]],

        [[0.6063, 0.1012],
         [0.6398, 0.6605]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.772104805341358
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8447901065451248
time is 3
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.573395733075917
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
Global Adjusted Rand Index: 0.7739430384007802
Average Adjusted Rand Index: 0.7766413863395395
11160.826880379685
[-0.0031435669164542423, 0.7739430384007802] [-0.006336631478073465, 0.7766413863395395] [11233.050532320867, 11104.043718500836]
-------------------------------------
This iteration is 40
True Objective function: Loss = -10927.422521933191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22248.256330061402
Iteration 100: Loss = -11004.307088266443
Iteration 200: Loss = -11003.920255778898
Iteration 300: Loss = -11003.835161281972
Iteration 400: Loss = -11003.785097199576
Iteration 500: Loss = -11003.750735940996
Iteration 600: Loss = -11003.724677792377
Iteration 700: Loss = -11003.70295232383
Iteration 800: Loss = -11003.68283288283
Iteration 900: Loss = -11003.660460201298
Iteration 1000: Loss = -11003.62679964837
Iteration 1100: Loss = -11003.550078115079
Iteration 1200: Loss = -11003.297654551048
Iteration 1300: Loss = -11002.905243847868
Iteration 1400: Loss = -11002.778214962666
Iteration 1500: Loss = -11002.730611543833
Iteration 1600: Loss = -11002.697066401359
Iteration 1700: Loss = -11002.66276824924
Iteration 1800: Loss = -11002.619197049828
Iteration 1900: Loss = -11002.5481411961
Iteration 2000: Loss = -11002.388120752092
Iteration 2100: Loss = -11001.815806731596
Iteration 2200: Loss = -11000.672441475133
Iteration 2300: Loss = -10999.255461542913
Iteration 2400: Loss = -10998.82774249208
Iteration 2500: Loss = -10998.55256497358
Iteration 2600: Loss = -10998.433283771366
Iteration 2700: Loss = -10998.375618591817
Iteration 2800: Loss = -10998.342234582558
Iteration 2900: Loss = -10998.3172033204
Iteration 3000: Loss = -10998.298763253211
Iteration 3100: Loss = -10998.287152536723
Iteration 3200: Loss = -10998.278569059017
Iteration 3300: Loss = -10998.271554661045
Iteration 3400: Loss = -10998.265550186985
Iteration 3500: Loss = -10998.260725841135
Iteration 3600: Loss = -10998.256703835024
Iteration 3700: Loss = -10998.253268354103
Iteration 3800: Loss = -10998.250112375763
Iteration 3900: Loss = -10998.246997161925
Iteration 4000: Loss = -10998.243557922484
Iteration 4100: Loss = -10998.239172460379
Iteration 4200: Loss = -10998.235275182886
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [14:20:46<20:36:15, 1257.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [14:42:58<20:36:53, 1279.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [14:58:33<18:37:22, 1176.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [15:14:19<17:13:24, 1107.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [15:36:06<17:50:03, 1167.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [15:57:46<18:06:23, 1207.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 47%|████▋     | 47/100 [16:19:18<18:08:40, 1232.46s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [16:40:59<18:05:55, 1252.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [17:02:43<17:58:12, 1268.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [17:21:23<16:59:44, 1223.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [17:39:05<15:59:53, 1175.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [17:56:33<15:09:35, 1136.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [18:18:12<15:28:40, 1185.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [18:39:46<15:33:52, 1218.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [19:01:19<15:30:35, 1240.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [19:22:54<15:21:50, 1257.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [19:44:34<15:09:56, 1269.69s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [20:06:07<14:53:44, 1276.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [20:27:49<14:37:37, 1284.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [20:46:15<13:40:35, 1230.88s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [21:07:56<13:33:46, 1251.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [21:29:45<13:23:44, 1269.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [21:51:16<13:06:35, 1275.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [22:13:11<12:52:25, 1287.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [22:34:52<12:33:20, 1291.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [22:56:41<12:14:52, 1296.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [23:18:19<11:53:29, 1297.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [23:40:12<11:34:21, 1301.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [24:01:56<11:12:54, 1302.42s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [24:19:51<10:17:04, 1234.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [24:41:27<10:05:33, 1252.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [25:00:41<9:30:51, 1223.28s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [25:22:15<9:19:59, 1244.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [25:43:50<9:05:49, 1259.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [26:05:27<8:49:29, 1270.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [26:24:37<8:13:50, 1234.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [26:46:16<8:00:40, 1253.95s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [27:08:14<7:46:46, 1273.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [27:29:56<7:28:39, 1281.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [27:51:36<7:09:06, 1287.33s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 4300: Loss = -10998.231662372496
Iteration 4400: Loss = -10998.228769618798
Iteration 4500: Loss = -10998.225436842484
Iteration 4600: Loss = -10998.222537359972
Iteration 4700: Loss = -10998.22031680888
Iteration 4800: Loss = -10998.218393090117
Iteration 4900: Loss = -10998.216247143762
Iteration 5000: Loss = -10998.21440958784
Iteration 5100: Loss = -10998.213172285012
Iteration 5200: Loss = -10998.212192701663
Iteration 5300: Loss = -10998.211002786109
Iteration 5400: Loss = -10998.210113079498
Iteration 5500: Loss = -10998.20891490585
Iteration 5600: Loss = -10998.207725049406
Iteration 5700: Loss = -10998.207031188123
Iteration 5800: Loss = -10998.205940504937
Iteration 5900: Loss = -10998.205190433087
Iteration 6000: Loss = -10998.203921689628
Iteration 6100: Loss = -10998.202859349225
Iteration 6200: Loss = -10998.202281109767
Iteration 6300: Loss = -10998.201945673147
Iteration 6400: Loss = -10998.201582719912
Iteration 6500: Loss = -10998.201279130924
Iteration 6600: Loss = -10998.200980293534
Iteration 6700: Loss = -10998.200696923346
Iteration 6800: Loss = -10998.200372459854
Iteration 6900: Loss = -10998.199744255673
Iteration 7000: Loss = -10998.198711853916
Iteration 7100: Loss = -10998.197994187542
Iteration 7200: Loss = -10998.197758000992
Iteration 7300: Loss = -10998.197057074154
Iteration 7400: Loss = -10998.196810453841
Iteration 7500: Loss = -10998.194488162446
Iteration 7600: Loss = -10998.193450176994
Iteration 7700: Loss = -10998.191372044508
Iteration 7800: Loss = -10998.190725341972
Iteration 7900: Loss = -10998.1903464152
Iteration 8000: Loss = -10998.189729026504
Iteration 8100: Loss = -10998.241176673526
1
Iteration 8200: Loss = -10998.189102538292
Iteration 8300: Loss = -10998.218559160623
1
Iteration 8400: Loss = -10998.188098838882
Iteration 8500: Loss = -10998.187217909615
Iteration 8600: Loss = -10998.187106404283
Iteration 8700: Loss = -10998.186676609092
Iteration 8800: Loss = -10998.188568571051
1
Iteration 8900: Loss = -10998.186585304255
Iteration 9000: Loss = -10998.18645618962
Iteration 9100: Loss = -10998.411358554049
1
Iteration 9200: Loss = -10998.186224456776
Iteration 9300: Loss = -10998.18526262984
Iteration 9400: Loss = -10998.198599621684
1
Iteration 9500: Loss = -10998.183663886928
Iteration 9600: Loss = -10998.1833380831
Iteration 9700: Loss = -10998.181268566874
Iteration 9800: Loss = -10998.18082390144
Iteration 9900: Loss = -10998.180476680482
Iteration 10000: Loss = -10998.180216424656
Iteration 10100: Loss = -10998.182627753415
1
Iteration 10200: Loss = -10998.180083659056
Iteration 10300: Loss = -10998.180046022528
Iteration 10400: Loss = -10998.2924184804
1
Iteration 10500: Loss = -10998.180005245915
Iteration 10600: Loss = -10998.179962709248
Iteration 10700: Loss = -10998.179895650652
Iteration 10800: Loss = -10998.181887132609
1
Iteration 10900: Loss = -10998.17975631325
Iteration 11000: Loss = -10998.179737930861
Iteration 11100: Loss = -10998.181760409087
1
Iteration 11200: Loss = -10998.179566082083
Iteration 11300: Loss = -10998.179496507859
Iteration 11400: Loss = -10998.195415430277
1
Iteration 11500: Loss = -10998.178832372223
Iteration 11600: Loss = -10998.178406322435
Iteration 11700: Loss = -10998.178697714751
1
Iteration 11800: Loss = -10998.177967008265
Iteration 11900: Loss = -10998.177893155476
Iteration 12000: Loss = -10998.177830803701
Iteration 12100: Loss = -10998.177311891499
Iteration 12200: Loss = -10998.177252371323
Iteration 12300: Loss = -10998.17701577146
Iteration 12400: Loss = -10998.178108305552
1
Iteration 12500: Loss = -10998.1768969249
Iteration 12600: Loss = -10998.17689291604
Iteration 12700: Loss = -10998.17768162933
1
Iteration 12800: Loss = -10998.17685990283
Iteration 12900: Loss = -10998.180024106357
1
Iteration 13000: Loss = -10998.176712619288
Iteration 13100: Loss = -10998.17752128951
1
Iteration 13200: Loss = -10998.174688051362
Iteration 13300: Loss = -10998.177167791395
1
Iteration 13400: Loss = -10998.174859949026
2
Iteration 13500: Loss = -10998.225944436195
3
Iteration 13600: Loss = -10998.175050240396
4
Iteration 13700: Loss = -10998.17622422321
5
Iteration 13800: Loss = -10998.177357546812
6
Iteration 13900: Loss = -10998.174669901646
Iteration 14000: Loss = -10998.174427077041
Iteration 14100: Loss = -10998.427580461495
1
Iteration 14200: Loss = -10998.174239751383
Iteration 14300: Loss = -10998.184654127368
1
Iteration 14400: Loss = -10998.17441856628
2
Iteration 14500: Loss = -10998.175656045325
3
Iteration 14600: Loss = -10998.174085700417
Iteration 14700: Loss = -10998.17377571886
Iteration 14800: Loss = -10998.173400576845
Iteration 14900: Loss = -10998.174814920278
1
Iteration 15000: Loss = -10998.17602553617
2
Iteration 15100: Loss = -10998.17338848051
Iteration 15200: Loss = -10998.174237538555
1
Iteration 15300: Loss = -10998.17313671475
Iteration 15400: Loss = -10998.172653515318
Iteration 15500: Loss = -10998.173115376472
1
Iteration 15600: Loss = -10998.172582723215
Iteration 15700: Loss = -10998.172249574167
Iteration 15800: Loss = -10998.172086197648
Iteration 15900: Loss = -10998.1725837848
1
Iteration 16000: Loss = -10998.17209062149
Iteration 16100: Loss = -10998.171679777604
Iteration 16200: Loss = -10998.172210228315
1
Iteration 16300: Loss = -10998.170517000659
Iteration 16400: Loss = -10998.216324366007
1
Iteration 16500: Loss = -10998.169998575993
Iteration 16600: Loss = -10998.17189515832
1
Iteration 16700: Loss = -10998.17273749836
2
Iteration 16800: Loss = -10998.171051967534
3
Iteration 16900: Loss = -10998.451196488404
4
Iteration 17000: Loss = -10998.169577040238
Iteration 17100: Loss = -10998.188987353418
1
Iteration 17200: Loss = -10998.169545136736
Iteration 17300: Loss = -10998.17022840357
1
Iteration 17400: Loss = -10998.199176579174
2
Iteration 17500: Loss = -10998.16960967415
Iteration 17600: Loss = -10998.170951248554
1
Iteration 17700: Loss = -10998.170028268058
2
Iteration 17800: Loss = -10998.169614984805
Iteration 17900: Loss = -10998.172318369798
1
Iteration 18000: Loss = -10998.169596117225
Iteration 18100: Loss = -10998.171410106666
1
Iteration 18200: Loss = -10998.169885669608
2
Iteration 18300: Loss = -10998.16957663604
Iteration 18400: Loss = -10998.252586872042
1
Iteration 18500: Loss = -10998.169487146617
Iteration 18600: Loss = -10998.170151876702
1
Iteration 18700: Loss = -10998.169268365293
Iteration 18800: Loss = -10998.169385832141
1
Iteration 18900: Loss = -10998.177462195894
2
Iteration 19000: Loss = -10998.170469443436
3
Iteration 19100: Loss = -10998.17113787808
4
Iteration 19200: Loss = -10998.168826201892
Iteration 19300: Loss = -10998.17248963201
1
Iteration 19400: Loss = -10998.16894423807
2
Iteration 19500: Loss = -10998.17980605439
3
Iteration 19600: Loss = -10998.166939138202
Iteration 19700: Loss = -10998.168277066228
1
Iteration 19800: Loss = -10998.168450973153
2
Iteration 19900: Loss = -10998.169175806375
3
pi: tensor([[9.9084e-01, 9.1575e-03],
        [4.6336e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9802, 0.0198], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1651, 0.0587],
         [0.7101, 0.1122]],

        [[0.5178, 0.1550],
         [0.6919, 0.6925]],

        [[0.5255, 0.1984],
         [0.5909, 0.7194]],

        [[0.5918, 0.1317],
         [0.7210, 0.6965]],

        [[0.5329, 0.1095],
         [0.7215, 0.6720]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
Global Adjusted Rand Index: 0.0015976036136852137
Average Adjusted Rand Index: 0.0016514321874629052
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21266.9101684464
Iteration 100: Loss = -11004.200708796183
Iteration 200: Loss = -11003.90756778859
Iteration 300: Loss = -11003.841562535046
Iteration 400: Loss = -11003.80142168184
Iteration 500: Loss = -11003.76972652139
Iteration 600: Loss = -11003.743407065234
Iteration 700: Loss = -11003.720937708815
Iteration 800: Loss = -11003.701038331492
Iteration 900: Loss = -11003.682920225196
Iteration 1000: Loss = -11003.665575006578
Iteration 1100: Loss = -11003.647616544764
Iteration 1200: Loss = -11003.626011613991
Iteration 1300: Loss = -11003.591713414467
Iteration 1400: Loss = -11003.50695621704
Iteration 1500: Loss = -11003.220725005893
Iteration 1600: Loss = -11002.880285262085
Iteration 1700: Loss = -11002.753212442494
Iteration 1800: Loss = -11002.688506352564
Iteration 1900: Loss = -11002.633935925556
Iteration 2000: Loss = -11002.571229562716
Iteration 2100: Loss = -11002.487849683788
Iteration 2200: Loss = -11002.378945524348
Iteration 2300: Loss = -11002.250234550305
Iteration 2400: Loss = -11002.10713284837
Iteration 2500: Loss = -11001.937505399603
Iteration 2600: Loss = -11001.297368559424
Iteration 2700: Loss = -10999.837371512818
Iteration 2800: Loss = -10999.25433283906
Iteration 2900: Loss = -10998.96781026308
Iteration 3000: Loss = -10998.750506921486
Iteration 3100: Loss = -10998.581780339235
Iteration 3200: Loss = -10998.47319309766
Iteration 3300: Loss = -10998.405574919872
Iteration 3400: Loss = -10998.358688717455
Iteration 3500: Loss = -10998.324478194772
Iteration 3600: Loss = -10998.302091509431
Iteration 3700: Loss = -10998.286010034939
Iteration 3800: Loss = -10998.273592773647
Iteration 3900: Loss = -10998.263530127915
Iteration 4000: Loss = -10998.255071403548
Iteration 4100: Loss = -10998.24774291071
Iteration 4200: Loss = -10998.241370502978
Iteration 4300: Loss = -10998.235695923524
Iteration 4400: Loss = -10998.23083656386
Iteration 4500: Loss = -10998.226661935614
Iteration 4600: Loss = -10998.222964262295
Iteration 4700: Loss = -10998.219622819488
Iteration 4800: Loss = -10998.216696220325
Iteration 4900: Loss = -10998.213948513743
Iteration 5000: Loss = -10998.211424353694
Iteration 5100: Loss = -10998.209098626729
Iteration 5200: Loss = -10998.20687766537
Iteration 5300: Loss = -10998.204729252213
Iteration 5400: Loss = -10998.202716297277
Iteration 5500: Loss = -10998.200957067515
Iteration 5600: Loss = -10998.199376586235
Iteration 5700: Loss = -10998.19780544429
Iteration 5800: Loss = -10998.1962647834
Iteration 5900: Loss = -10998.19450951823
Iteration 6000: Loss = -10998.192673112371
Iteration 6100: Loss = -10998.191274750188
Iteration 6200: Loss = -10998.19043646278
Iteration 6300: Loss = -10998.18972831655
Iteration 6400: Loss = -10998.189149390399
Iteration 6500: Loss = -10998.188569739044
Iteration 6600: Loss = -10998.188064160848
Iteration 6700: Loss = -10998.187540042307
Iteration 6800: Loss = -10998.18710810277
Iteration 6900: Loss = -10998.186568388186
Iteration 7000: Loss = -10998.186019295434
Iteration 7100: Loss = -10998.18549358144
Iteration 7200: Loss = -10998.194973870879
1
Iteration 7300: Loss = -10998.18473594987
Iteration 7400: Loss = -10998.184519487928
Iteration 7500: Loss = -10998.184125619828
Iteration 7600: Loss = -10998.183777140484
Iteration 7700: Loss = -10998.195619785132
1
Iteration 7800: Loss = -10998.18291910363
Iteration 7900: Loss = -10998.18261242066
Iteration 8000: Loss = -10998.213711981512
1
Iteration 8100: Loss = -10998.182106589555
Iteration 8200: Loss = -10998.181863252737
Iteration 8300: Loss = -10998.18159219244
Iteration 8400: Loss = -10998.181453776806
Iteration 8500: Loss = -10998.180994443423
Iteration 8600: Loss = -10998.180721447034
Iteration 8700: Loss = -10998.18385671915
1
Iteration 8800: Loss = -10998.179691748188
Iteration 8900: Loss = -10998.178712176237
Iteration 9000: Loss = -10998.226613404426
1
Iteration 9100: Loss = -10998.177953555027
Iteration 9200: Loss = -10998.177778119898
Iteration 9300: Loss = -10998.588560440958
1
Iteration 9400: Loss = -10998.177050962693
Iteration 9500: Loss = -10998.176380468996
Iteration 9600: Loss = -10998.17601840078
Iteration 9700: Loss = -10998.176012271411
Iteration 9800: Loss = -10998.175794582346
Iteration 9900: Loss = -10998.175712850107
Iteration 10000: Loss = -10998.178145056674
1
Iteration 10100: Loss = -10998.175566110716
Iteration 10200: Loss = -10998.175511404319
Iteration 10300: Loss = -10998.175447125579
Iteration 10400: Loss = -10998.175388909684
Iteration 10500: Loss = -10998.175231315674
Iteration 10600: Loss = -10998.174593572461
Iteration 10700: Loss = -10998.181441885861
1
Iteration 10800: Loss = -10998.174418669942
Iteration 10900: Loss = -10998.174362231703
Iteration 11000: Loss = -10998.183432211192
1
Iteration 11100: Loss = -10998.174227837359
Iteration 11200: Loss = -10998.174020740547
Iteration 11300: Loss = -10998.17722467409
1
Iteration 11400: Loss = -10998.172986934622
Iteration 11500: Loss = -10998.1727163876
Iteration 11600: Loss = -10998.172129415147
Iteration 11700: Loss = -10998.175155524897
1
Iteration 11800: Loss = -10998.169790267355
Iteration 11900: Loss = -10998.169697497091
Iteration 12000: Loss = -10998.176439057175
1
Iteration 12100: Loss = -10998.169487468114
Iteration 12200: Loss = -10998.16944150205
Iteration 12300: Loss = -10998.172611065762
1
Iteration 12400: Loss = -10998.16933844075
Iteration 12500: Loss = -10998.170102947688
1
Iteration 12600: Loss = -10998.171081582259
2
Iteration 12700: Loss = -10998.16908727685
Iteration 12800: Loss = -10998.169051212122
Iteration 12900: Loss = -10998.168822050742
Iteration 13000: Loss = -10998.168976551442
1
Iteration 13100: Loss = -10998.177491368735
2
Iteration 13200: Loss = -10998.168253491003
Iteration 13300: Loss = -10998.167969403934
Iteration 13400: Loss = -10998.168572564715
1
Iteration 13500: Loss = -10998.167377006162
Iteration 13600: Loss = -10998.168558125926
1
Iteration 13700: Loss = -10998.167231141344
Iteration 13800: Loss = -10998.228654224667
1
Iteration 13900: Loss = -10998.167063765308
Iteration 14000: Loss = -10998.169714834678
1
Iteration 14100: Loss = -10998.167042102863
Iteration 14200: Loss = -10998.16788344973
1
Iteration 14300: Loss = -10998.166802160664
Iteration 14400: Loss = -10998.16690344918
1
Iteration 14500: Loss = -10998.166594121483
Iteration 14600: Loss = -10998.166554896614
Iteration 14700: Loss = -10998.168344628639
1
Iteration 14800: Loss = -10998.16636138691
Iteration 14900: Loss = -10998.166353174722
Iteration 15000: Loss = -10998.166953617378
1
Iteration 15100: Loss = -10998.16864926002
2
Iteration 15200: Loss = -10998.167787863895
3
Iteration 15300: Loss = -10998.1659543891
Iteration 15400: Loss = -10998.16593948809
Iteration 15500: Loss = -10998.167222626016
1
Iteration 15600: Loss = -10998.16600509855
Iteration 15700: Loss = -10998.167488452413
1
Iteration 15800: Loss = -10998.166086952839
Iteration 15900: Loss = -10998.166139934867
Iteration 16000: Loss = -10998.168689442222
1
Iteration 16100: Loss = -10998.165968118092
Iteration 16200: Loss = -10998.195643743298
1
Iteration 16300: Loss = -10998.165493278571
Iteration 16400: Loss = -10998.272034600215
1
Iteration 16500: Loss = -10998.165521606075
Iteration 16600: Loss = -10998.166781524242
1
Iteration 16700: Loss = -10998.165128229562
Iteration 16800: Loss = -10998.164953709927
Iteration 16900: Loss = -10998.16657020583
1
Iteration 17000: Loss = -10998.164844809757
Iteration 17100: Loss = -10998.164882047347
Iteration 17200: Loss = -10998.168228735369
1
Iteration 17300: Loss = -10998.164804117552
Iteration 17400: Loss = -10998.16586506244
1
Iteration 17500: Loss = -10998.164759584512
Iteration 17600: Loss = -10998.166617368463
1
Iteration 17700: Loss = -10998.164472959941
Iteration 17800: Loss = -10998.164547428783
Iteration 17900: Loss = -10998.16536213126
1
Iteration 18000: Loss = -10998.164419835373
Iteration 18100: Loss = -10998.165486615986
1
Iteration 18200: Loss = -10998.164610771946
2
Iteration 18300: Loss = -10998.164703992621
3
Iteration 18400: Loss = -10998.164495133557
Iteration 18500: Loss = -10998.17723522989
1
Iteration 18600: Loss = -10998.164270623733
Iteration 18700: Loss = -10998.260617558275
1
Iteration 18800: Loss = -10998.16416324852
Iteration 18900: Loss = -10998.176451025602
1
Iteration 19000: Loss = -10998.164581021005
2
Iteration 19100: Loss = -10998.164728302678
3
Iteration 19200: Loss = -10998.164004219685
Iteration 19300: Loss = -10998.215337204452
1
Iteration 19400: Loss = -10998.164204109957
2
Iteration 19500: Loss = -10998.166303437029
3
Iteration 19600: Loss = -10998.208920042154
4
Iteration 19700: Loss = -10998.163708095271
Iteration 19800: Loss = -10998.250447611523
1
Iteration 19900: Loss = -10998.163711672429
pi: tensor([[9.9065e-01, 9.3476e-03],
        [1.0395e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9801, 0.0199], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1654, 0.0589],
         [0.5747, 0.1118]],

        [[0.5316, 0.1547],
         [0.5922, 0.5220]],

        [[0.5017, 0.1977],
         [0.6896, 0.6172]],

        [[0.5752, 0.1316],
         [0.5337, 0.6218]],

        [[0.5216, 0.1101],
         [0.5046, 0.5021]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
Global Adjusted Rand Index: 0.0015976036136852137
Average Adjusted Rand Index: 0.0016514321874629052
10927.422521933191
[0.0015976036136852137, 0.0015976036136852137] [0.0016514321874629052, 0.0016514321874629052] [10998.168667281358, 10998.163871407145]
-------------------------------------
This iteration is 41
True Objective function: Loss = -10905.173457448027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23437.561022517177
Iteration 100: Loss = -11054.254059208146
Iteration 200: Loss = -11053.069036697067
Iteration 300: Loss = -11051.779071945908
Iteration 400: Loss = -11051.375720333686
Iteration 500: Loss = -11050.977383936637
Iteration 600: Loss = -11050.51192599379
Iteration 700: Loss = -11049.778648036832
Iteration 800: Loss = -11044.428082093054
Iteration 900: Loss = -11026.343286934225
Iteration 1000: Loss = -11024.807554560137
Iteration 1100: Loss = -11024.51882560115
Iteration 1200: Loss = -11024.305081827515
Iteration 1300: Loss = -11024.199069887092
Iteration 1400: Loss = -11024.151152560902
Iteration 1500: Loss = -11024.12689785422
Iteration 1600: Loss = -11024.11231878683
Iteration 1700: Loss = -11024.10164138225
Iteration 1800: Loss = -11024.091849012757
Iteration 1900: Loss = -11024.079155343028
Iteration 2000: Loss = -11024.054775190054
Iteration 2100: Loss = -11023.9898613919
Iteration 2200: Loss = -11023.862174573529
Iteration 2300: Loss = -11023.79601156746
Iteration 2400: Loss = -11023.783920457754
Iteration 2500: Loss = -11023.780300022063
Iteration 2600: Loss = -11023.778399647463
Iteration 2700: Loss = -11023.777165881578
Iteration 2800: Loss = -11023.776242531027
Iteration 2900: Loss = -11023.775575808051
Iteration 3000: Loss = -11023.775027118683
Iteration 3100: Loss = -11023.77454579304
Iteration 3200: Loss = -11023.774173656528
Iteration 3300: Loss = -11023.773867927994
Iteration 3400: Loss = -11023.773592723537
Iteration 3500: Loss = -11023.773375399314
Iteration 3600: Loss = -11023.773194885012
Iteration 3700: Loss = -11023.77299244379
Iteration 3800: Loss = -11023.77285773146
Iteration 3900: Loss = -11023.772696311275
Iteration 4000: Loss = -11023.77256719992
Iteration 4100: Loss = -11023.772464981006
Iteration 4200: Loss = -11023.772379610355
Iteration 4300: Loss = -11023.77226203565
Iteration 4400: Loss = -11023.772209675233
Iteration 4500: Loss = -11023.772132595555
Iteration 4600: Loss = -11023.772073903376
Iteration 4700: Loss = -11023.771965930986
Iteration 4800: Loss = -11023.771934689345
Iteration 4900: Loss = -11023.771880348759
Iteration 5000: Loss = -11023.771818351786
Iteration 5100: Loss = -11023.771809669359
Iteration 5200: Loss = -11023.77177200639
Iteration 5300: Loss = -11023.771722358932
Iteration 5400: Loss = -11023.771713807668
Iteration 5500: Loss = -11023.771652334008
Iteration 5600: Loss = -11023.771669728065
Iteration 5700: Loss = -11023.771605216383
Iteration 5800: Loss = -11023.771617501006
Iteration 5900: Loss = -11023.771576461872
Iteration 6000: Loss = -11023.771558312812
Iteration 6100: Loss = -11023.771550879526
Iteration 6200: Loss = -11023.771505664607
Iteration 6300: Loss = -11023.771684993051
1
Iteration 6400: Loss = -11023.771506424382
Iteration 6500: Loss = -11023.77181327188
1
Iteration 6600: Loss = -11023.771480269354
Iteration 6700: Loss = -11023.77147835697
Iteration 6800: Loss = -11023.771475318803
Iteration 6900: Loss = -11023.771506498924
Iteration 7000: Loss = -11023.771414491288
Iteration 7100: Loss = -11023.77456311438
1
Iteration 7200: Loss = -11023.771414695868
Iteration 7300: Loss = -11023.771400287906
Iteration 7400: Loss = -11023.771458774994
Iteration 7500: Loss = -11023.771489474682
Iteration 7600: Loss = -11023.771435354354
Iteration 7700: Loss = -11023.771384742944
Iteration 7800: Loss = -11023.771370907221
Iteration 7900: Loss = -11023.771450034548
Iteration 8000: Loss = -11023.772084490962
1
Iteration 8100: Loss = -11023.794446151713
2
Iteration 8200: Loss = -11023.771342180215
Iteration 8300: Loss = -11023.771488324519
1
Iteration 8400: Loss = -11023.771431965117
Iteration 8500: Loss = -11023.771376943756
Iteration 8600: Loss = -11023.77134708696
Iteration 8700: Loss = -11023.771388140538
Iteration 8800: Loss = -11023.77131801815
Iteration 8900: Loss = -11023.787128598007
1
Iteration 9000: Loss = -11023.771317937877
Iteration 9100: Loss = -11023.771356263647
Iteration 9200: Loss = -11023.772158613507
1
Iteration 9300: Loss = -11023.771297994284
Iteration 9400: Loss = -11023.771300148652
Iteration 9500: Loss = -11023.77183292169
1
Iteration 9600: Loss = -11023.771293905837
Iteration 9700: Loss = -11023.771285342056
Iteration 9800: Loss = -11023.77138646226
1
Iteration 9900: Loss = -11023.771318137875
Iteration 10000: Loss = -11023.771274398898
Iteration 10100: Loss = -11023.819884136456
1
Iteration 10200: Loss = -11023.7712901234
Iteration 10300: Loss = -11023.771300599983
Iteration 10400: Loss = -11023.771264253353
Iteration 10500: Loss = -11023.771455836515
1
Iteration 10600: Loss = -11023.771292097706
Iteration 10700: Loss = -11023.771288832071
Iteration 10800: Loss = -11023.775720462947
1
Iteration 10900: Loss = -11023.771294966886
Iteration 11000: Loss = -11023.771288350892
Iteration 11100: Loss = -11023.781946962496
1
Iteration 11200: Loss = -11023.771275981657
Iteration 11300: Loss = -11023.771284099734
Iteration 11400: Loss = -11023.799965900122
1
Iteration 11500: Loss = -11023.77129721505
Iteration 11600: Loss = -11023.771280697645
Iteration 11700: Loss = -11023.819793184919
1
Iteration 11800: Loss = -11023.77125798049
Iteration 11900: Loss = -11023.771284050785
Iteration 12000: Loss = -11023.965465393601
1
Iteration 12100: Loss = -11023.771281057363
Iteration 12200: Loss = -11023.771273596843
Iteration 12300: Loss = -11024.02403689729
1
Iteration 12400: Loss = -11023.771284353204
Iteration 12500: Loss = -11023.771258036923
Iteration 12600: Loss = -11023.781750380376
1
Iteration 12700: Loss = -11023.771326609241
Iteration 12800: Loss = -11023.771302821686
Iteration 12900: Loss = -11023.901687178211
1
Iteration 13000: Loss = -11023.771316226695
Iteration 13100: Loss = -11023.771272564894
Iteration 13200: Loss = -11023.816182308197
1
Iteration 13300: Loss = -11023.771305735663
Iteration 13400: Loss = -11023.771282765583
Iteration 13500: Loss = -11023.806364194377
1
Iteration 13600: Loss = -11023.771296584804
Iteration 13700: Loss = -11023.77126326043
Iteration 13800: Loss = -11023.77623267904
1
Iteration 13900: Loss = -11023.771291997624
Iteration 14000: Loss = -11023.771271200147
Iteration 14100: Loss = -11023.778862224997
1
Iteration 14200: Loss = -11023.771283715567
Iteration 14300: Loss = -11023.771307158491
Iteration 14400: Loss = -11023.790529764798
1
Iteration 14500: Loss = -11023.771281173586
Iteration 14600: Loss = -11023.771280254809
Iteration 14700: Loss = -11023.772021675808
1
Iteration 14800: Loss = -11023.771302805939
Iteration 14900: Loss = -11023.773080014722
1
Iteration 15000: Loss = -11023.771318035428
Iteration 15100: Loss = -11023.771284578752
Iteration 15200: Loss = -11023.77261845823
1
Iteration 15300: Loss = -11023.771312779516
Iteration 15400: Loss = -11023.772686934179
1
Iteration 15500: Loss = -11023.771417822754
2
Iteration 15600: Loss = -11023.77143214095
3
Iteration 15700: Loss = -11023.771272221637
Iteration 15800: Loss = -11024.035200276796
1
Iteration 15900: Loss = -11023.771574748233
2
Iteration 16000: Loss = -11023.826269522322
3
Iteration 16100: Loss = -11023.77145486035
4
Iteration 16200: Loss = -11024.1044226863
5
Iteration 16300: Loss = -11023.77135451464
Iteration 16400: Loss = -11023.943076602216
1
Iteration 16500: Loss = -11023.771703218394
2
Iteration 16600: Loss = -11023.77256170212
3
Iteration 16700: Loss = -11023.771327364417
Iteration 16800: Loss = -11023.800063782352
1
Iteration 16900: Loss = -11023.77139220621
Iteration 17000: Loss = -11023.791374100945
1
Iteration 17100: Loss = -11023.771277960652
Iteration 17200: Loss = -11023.781083497825
1
Iteration 17300: Loss = -11023.77211883907
2
Iteration 17400: Loss = -11023.824912302121
3
Iteration 17500: Loss = -11023.771634124716
4
Iteration 17600: Loss = -11023.771343854562
Iteration 17700: Loss = -11023.772798230479
1
Iteration 17800: Loss = -11023.772637502852
2
Iteration 17900: Loss = -11023.771292199828
Iteration 18000: Loss = -11023.773325091359
1
Iteration 18100: Loss = -11023.772726106452
2
Iteration 18200: Loss = -11023.787161783313
3
Iteration 18300: Loss = -11023.771896550814
4
Iteration 18400: Loss = -11023.795421243993
5
Iteration 18500: Loss = -11023.771501511437
6
Iteration 18600: Loss = -11023.776131145205
7
Iteration 18700: Loss = -11023.771320101132
Iteration 18800: Loss = -11023.771543374816
1
Iteration 18900: Loss = -11023.922752427858
2
Iteration 19000: Loss = -11023.771862691578
3
Iteration 19100: Loss = -11023.777220477912
4
Iteration 19200: Loss = -11023.772436163612
5
Iteration 19300: Loss = -11023.771324971129
Iteration 19400: Loss = -11023.814592026978
1
Iteration 19500: Loss = -11023.77129344914
Iteration 19600: Loss = -11023.771437779462
1
Iteration 19700: Loss = -11023.77146641806
2
Iteration 19800: Loss = -11023.782091799887
3
Iteration 19900: Loss = -11023.771303510435
pi: tensor([[0.9801, 0.0199],
        [0.9943, 0.0057]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4711, 0.5289], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1671, 0.1037],
         [0.6939, 0.2730]],

        [[0.5489, 0.1318],
         [0.5342, 0.6589]],

        [[0.5172, 0.0946],
         [0.5962, 0.5693]],

        [[0.5930, 0.0793],
         [0.6129, 0.7272]],

        [[0.5866, 0.0948],
         [0.6143, 0.6856]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 90
Adjusted Rand Index: 0.6364547952958964
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.018087416530256265
Average Adjusted Rand Index: 0.12881379336889884
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20958.697218955433
Iteration 100: Loss = -11054.137834122306
Iteration 200: Loss = -11051.298306099914
Iteration 300: Loss = -11050.101254967863
Iteration 400: Loss = -11025.56950669431
Iteration 500: Loss = -11025.114690302828
Iteration 600: Loss = -11025.065254596511
Iteration 700: Loss = -11025.02510535175
Iteration 800: Loss = -11024.985756315253
Iteration 900: Loss = -11024.946745466967
Iteration 1000: Loss = -11024.909556596427
Iteration 1100: Loss = -11024.875418600108
Iteration 1200: Loss = -11024.844577046342
Iteration 1300: Loss = -11024.816650054303
Iteration 1400: Loss = -11024.791912534723
Iteration 1500: Loss = -11024.770525930808
Iteration 1600: Loss = -11024.752509856358
Iteration 1700: Loss = -11024.73753271885
Iteration 1800: Loss = -11024.725219195388
Iteration 1900: Loss = -11024.715158052428
Iteration 2000: Loss = -11024.707030314683
Iteration 2100: Loss = -11024.700353069162
Iteration 2200: Loss = -11024.69496869146
Iteration 2300: Loss = -11024.690550035088
Iteration 2400: Loss = -11024.686793092302
Iteration 2500: Loss = -11024.683730956
Iteration 2600: Loss = -11024.68106789389
Iteration 2700: Loss = -11024.67879339717
Iteration 2800: Loss = -11024.676797888756
Iteration 2900: Loss = -11024.675074656121
Iteration 3000: Loss = -11024.673543648274
Iteration 3100: Loss = -11024.67220672261
Iteration 3200: Loss = -11024.670981363219
Iteration 3300: Loss = -11024.669887185199
Iteration 3400: Loss = -11024.66892628069
Iteration 3500: Loss = -11024.667982322015
Iteration 3600: Loss = -11024.6671865539
Iteration 3700: Loss = -11024.666428045739
Iteration 3800: Loss = -11024.665733776697
Iteration 3900: Loss = -11024.6650893577
Iteration 4000: Loss = -11024.664545767764
Iteration 4100: Loss = -11024.663982381791
Iteration 4200: Loss = -11024.66346039679
Iteration 4300: Loss = -11024.663014631737
Iteration 4400: Loss = -11024.662617040474
Iteration 4500: Loss = -11024.662269408744
Iteration 4600: Loss = -11024.661854547241
Iteration 4700: Loss = -11024.661450257074
Iteration 4800: Loss = -11024.661128907986
Iteration 4900: Loss = -11024.660840601682
Iteration 5000: Loss = -11024.660575022453
Iteration 5100: Loss = -11024.660342081412
Iteration 5200: Loss = -11024.660068832018
Iteration 5300: Loss = -11024.6693901082
1
Iteration 5400: Loss = -11024.659620879991
Iteration 5500: Loss = -11024.659426395403
Iteration 5600: Loss = -11024.665868500624
1
Iteration 5700: Loss = -11024.659076875463
Iteration 5800: Loss = -11024.658907600156
Iteration 5900: Loss = -11024.6587457388
Iteration 6000: Loss = -11024.658573483599
Iteration 6100: Loss = -11024.658452866342
Iteration 6200: Loss = -11024.658304719545
Iteration 6300: Loss = -11024.658160474486
Iteration 6400: Loss = -11024.658053512036
Iteration 6500: Loss = -11024.657932588367
Iteration 6600: Loss = -11024.657851183754
Iteration 6700: Loss = -11024.657729067123
Iteration 6800: Loss = -11024.659668646576
1
Iteration 6900: Loss = -11024.657549160695
Iteration 7000: Loss = -11024.65887647968
1
Iteration 7100: Loss = -11024.657390369886
Iteration 7200: Loss = -11024.657419684067
Iteration 7300: Loss = -11024.657282933504
Iteration 7400: Loss = -11024.65720609931
Iteration 7500: Loss = -11024.659886831108
1
Iteration 7600: Loss = -11024.657063666327
Iteration 7700: Loss = -11024.657106875988
Iteration 7800: Loss = -11024.657070314672
Iteration 7900: Loss = -11024.656947401238
Iteration 8000: Loss = -11024.656974169266
Iteration 8100: Loss = -11024.65699495816
Iteration 8200: Loss = -11024.656799880202
Iteration 8300: Loss = -11024.656985773152
1
Iteration 8400: Loss = -11024.656792978354
Iteration 8500: Loss = -11024.656656742909
Iteration 8600: Loss = -11024.657355488767
1
Iteration 8700: Loss = -11024.656573801713
Iteration 8800: Loss = -11024.725970982969
1
Iteration 8900: Loss = -11024.656499274546
Iteration 9000: Loss = -11024.65648003501
Iteration 9100: Loss = -11024.657868186523
1
Iteration 9200: Loss = -11024.656377732299
Iteration 9300: Loss = -11024.656376840776
Iteration 9400: Loss = -11024.657280500316
1
Iteration 9500: Loss = -11024.65635022886
Iteration 9600: Loss = -11024.656318144784
Iteration 9700: Loss = -11024.656975963568
1
Iteration 9800: Loss = -11024.656303459567
Iteration 9900: Loss = -11024.65627468975
Iteration 10000: Loss = -11024.656602224464
1
Iteration 10100: Loss = -11024.656227084432
Iteration 10200: Loss = -11024.656221418045
Iteration 10300: Loss = -11024.656801554516
1
Iteration 10400: Loss = -11024.656168540612
Iteration 10500: Loss = -11024.656204301222
Iteration 10600: Loss = -11024.656290161944
Iteration 10700: Loss = -11024.656170914448
Iteration 10800: Loss = -11024.65614426979
Iteration 10900: Loss = -11024.656184533324
Iteration 11000: Loss = -11024.656126883368
Iteration 11100: Loss = -11025.003043089919
1
Iteration 11200: Loss = -11024.656118115981
Iteration 11300: Loss = -11024.65609718867
Iteration 11400: Loss = -11024.726531105416
1
Iteration 11500: Loss = -11024.656122438671
Iteration 11600: Loss = -11024.65609122413
Iteration 11700: Loss = -11024.72773375696
1
Iteration 11800: Loss = -11024.656041604612
Iteration 11900: Loss = -11024.656026204346
Iteration 12000: Loss = -11024.671955556934
1
Iteration 12100: Loss = -11024.65603235464
Iteration 12200: Loss = -11024.65605108165
Iteration 12300: Loss = -11024.657299396018
1
Iteration 12400: Loss = -11024.656017067391
Iteration 12500: Loss = -11024.695341323462
1
Iteration 12600: Loss = -11024.710051761933
2
Iteration 12700: Loss = -11024.66060555861
3
Iteration 12800: Loss = -11024.656480594105
4
Iteration 12900: Loss = -11024.714825515575
5
Iteration 13000: Loss = -11024.656005464121
Iteration 13100: Loss = -11024.657416670154
1
Iteration 13200: Loss = -11024.655977967122
Iteration 13300: Loss = -11024.656092348141
1
Iteration 13400: Loss = -11024.656264021556
2
Iteration 13500: Loss = -11024.658052386054
3
Iteration 13600: Loss = -11024.655984239746
Iteration 13700: Loss = -11024.65627078885
1
Iteration 13800: Loss = -11024.786300432941
2
Iteration 13900: Loss = -11024.65598659238
Iteration 14000: Loss = -11024.66850961411
1
Iteration 14100: Loss = -11024.656483157109
2
Iteration 14200: Loss = -11024.656284334807
3
Iteration 14300: Loss = -11024.656639801453
4
Iteration 14400: Loss = -11024.656289820312
5
Iteration 14500: Loss = -11024.656218664564
6
Iteration 14600: Loss = -11024.656136840782
7
Iteration 14700: Loss = -11024.694162209804
8
Iteration 14800: Loss = -11024.666228565018
9
Iteration 14900: Loss = -11024.656403378958
10
Iteration 15000: Loss = -11024.65771674333
11
Iteration 15100: Loss = -11024.662074407595
12
Iteration 15200: Loss = -11024.74694572616
13
Iteration 15300: Loss = -11024.65595535207
Iteration 15400: Loss = -11024.656116056356
1
Iteration 15500: Loss = -11024.655946414892
Iteration 15600: Loss = -11024.656186234872
1
Iteration 15700: Loss = -11024.658491156797
2
Iteration 15800: Loss = -11024.656127146385
3
Iteration 15900: Loss = -11024.655988996214
Iteration 16000: Loss = -11024.907947714351
1
Iteration 16100: Loss = -11024.655982964645
Iteration 16200: Loss = -11024.664432542035
1
Iteration 16300: Loss = -11024.656168816235
2
Iteration 16400: Loss = -11024.656008179012
Iteration 16500: Loss = -11024.717032690836
1
Iteration 16600: Loss = -11024.655939689965
Iteration 16700: Loss = -11024.656209010252
1
Iteration 16800: Loss = -11024.657757956982
2
Iteration 16900: Loss = -11024.65643381563
3
Iteration 17000: Loss = -11024.661771676252
4
Iteration 17100: Loss = -11024.660000826287
5
Iteration 17200: Loss = -11024.656982278158
6
Iteration 17300: Loss = -11024.656009872964
Iteration 17400: Loss = -11024.695282697608
1
Iteration 17500: Loss = -11024.657965901515
2
Iteration 17600: Loss = -11024.65597020906
Iteration 17700: Loss = -11024.658198361029
1
Iteration 17800: Loss = -11024.685461479798
2
Iteration 17900: Loss = -11024.655953943813
Iteration 18000: Loss = -11024.65598363211
Iteration 18100: Loss = -11024.661112157037
1
Iteration 18200: Loss = -11024.655907647313
Iteration 18300: Loss = -11024.655983091248
Iteration 18400: Loss = -11024.655987206539
Iteration 18500: Loss = -11024.85112754486
1
Iteration 18600: Loss = -11024.655938534319
Iteration 18700: Loss = -11024.684193939882
1
Iteration 18800: Loss = -11024.655961246202
Iteration 18900: Loss = -11024.756040107906
1
Iteration 19000: Loss = -11024.655959461536
Iteration 19100: Loss = -11024.657211287913
1
Iteration 19200: Loss = -11024.662271662703
2
Iteration 19300: Loss = -11024.656505124265
3
Iteration 19400: Loss = -11024.656043703922
Iteration 19500: Loss = -11024.656852335906
1
Iteration 19600: Loss = -11024.683883597809
2
Iteration 19700: Loss = -11024.655928861339
Iteration 19800: Loss = -11024.655962981768
Iteration 19900: Loss = -11024.656319807453
1
pi: tensor([[3.7824e-07, 1.0000e+00],
        [3.3510e-02, 9.6649e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5265, 0.4735], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2750, 0.1040],
         [0.5549, 0.1619]],

        [[0.6632, 0.1406],
         [0.6141, 0.6997]],

        [[0.6072, 0.1869],
         [0.5610, 0.5716]],

        [[0.7198, 0.2148],
         [0.6521, 0.7309]],

        [[0.6343, 0.2155],
         [0.5781, 0.6944]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 10
Adjusted Rand Index: 0.6364547952958964
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.019251536905495945
Average Adjusted Rand Index: 0.12729095905917928
10905.173457448027
[0.018087416530256265, 0.019251536905495945] [0.12881379336889884, 0.12729095905917928] [11023.78537104433, 11024.66511153474]
-------------------------------------
This iteration is 42
True Objective function: Loss = -10929.033726915199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22029.681981256726
Iteration 100: Loss = -10983.247171831441
Iteration 200: Loss = -10982.780016183504
Iteration 300: Loss = -10982.612508765358
Iteration 400: Loss = -10982.534073215858
Iteration 500: Loss = -10982.492550709261
Iteration 600: Loss = -10982.468111177948
Iteration 700: Loss = -10982.452294545934
Iteration 800: Loss = -10982.441177601642
Iteration 900: Loss = -10982.43262631703
Iteration 1000: Loss = -10982.425661628993
Iteration 1100: Loss = -10982.419839612277
Iteration 1200: Loss = -10982.414688303526
Iteration 1300: Loss = -10982.410026241683
Iteration 1400: Loss = -10982.40566699993
Iteration 1500: Loss = -10982.401544745293
Iteration 1600: Loss = -10982.397501269237
Iteration 1700: Loss = -10982.39333558419
Iteration 1800: Loss = -10982.38897201131
Iteration 1900: Loss = -10982.38421745032
Iteration 2000: Loss = -10982.378713402475
Iteration 2100: Loss = -10982.372131808483
Iteration 2200: Loss = -10982.36386794202
Iteration 2300: Loss = -10982.352845654723
Iteration 2400: Loss = -10982.337101916666
Iteration 2500: Loss = -10982.313018249179
Iteration 2600: Loss = -10982.271291331936
Iteration 2700: Loss = -10982.1378516505
Iteration 2800: Loss = -10981.050812190024
Iteration 2900: Loss = -10980.103543958807
Iteration 3000: Loss = -10979.902175030009
Iteration 3100: Loss = -10979.819537293126
Iteration 3200: Loss = -10979.756445603207
Iteration 3300: Loss = -10979.707546594364
Iteration 3400: Loss = -10979.66880604821
Iteration 3500: Loss = -10979.627340259221
Iteration 3600: Loss = -10979.58913584812
Iteration 3700: Loss = -10979.556423569975
Iteration 3800: Loss = -10979.52682153424
Iteration 3900: Loss = -10979.503859519651
Iteration 4000: Loss = -10979.489578130799
Iteration 4100: Loss = -10979.480012893973
Iteration 4200: Loss = -10979.473457123366
Iteration 4300: Loss = -10979.468725092775
Iteration 4400: Loss = -10979.465151717017
Iteration 4500: Loss = -10979.462207884013
Iteration 4600: Loss = -10979.459791038164
Iteration 4700: Loss = -10979.45755993707
Iteration 4800: Loss = -10979.455470125187
Iteration 4900: Loss = -10979.453437810986
Iteration 5000: Loss = -10979.45168054417
Iteration 5100: Loss = -10979.450342654183
Iteration 5200: Loss = -10979.449305381324
Iteration 5300: Loss = -10979.448356178887
Iteration 5400: Loss = -10979.447534663279
Iteration 5500: Loss = -10979.446832791265
Iteration 5600: Loss = -10979.44617734374
Iteration 5700: Loss = -10979.445539082173
Iteration 5800: Loss = -10979.444932717823
Iteration 5900: Loss = -10979.444468280615
Iteration 6000: Loss = -10979.443959289798
Iteration 6100: Loss = -10979.443570171321
Iteration 6200: Loss = -10979.443149206834
Iteration 6300: Loss = -10979.442763815465
Iteration 6400: Loss = -10979.442450550308
Iteration 6500: Loss = -10979.442152288164
Iteration 6600: Loss = -10979.441856942465
Iteration 6700: Loss = -10979.441571016987
Iteration 6800: Loss = -10979.44131317259
Iteration 6900: Loss = -10979.441131788264
Iteration 7000: Loss = -10979.440942421697
Iteration 7100: Loss = -10979.44071298543
Iteration 7200: Loss = -10979.440577162262
Iteration 7300: Loss = -10979.440338855486
Iteration 7400: Loss = -10979.440192222968
Iteration 7500: Loss = -10979.440051730542
Iteration 7600: Loss = -10979.439907614918
Iteration 7700: Loss = -10979.43978123401
Iteration 7800: Loss = -10979.439659923659
Iteration 7900: Loss = -10979.439520182425
Iteration 8000: Loss = -10979.43944231327
Iteration 8100: Loss = -10979.439579400028
1
Iteration 8200: Loss = -10979.439223932377
Iteration 8300: Loss = -10979.439427953766
1
Iteration 8400: Loss = -10979.439074528887
Iteration 8500: Loss = -10979.468906870801
1
Iteration 8600: Loss = -10979.43891769431
Iteration 8700: Loss = -10979.438855560893
Iteration 8800: Loss = -10979.438742904551
Iteration 8900: Loss = -10979.438666910968
Iteration 9000: Loss = -10979.448205648805
1
Iteration 9100: Loss = -10979.439237569668
2
Iteration 9200: Loss = -10979.438485245604
Iteration 9300: Loss = -10979.438732772547
1
Iteration 9400: Loss = -10979.438432997773
Iteration 9500: Loss = -10979.463199721933
1
Iteration 9600: Loss = -10979.438326538595
Iteration 9700: Loss = -10979.438183591841
Iteration 9800: Loss = -10979.457370502347
1
Iteration 9900: Loss = -10979.438051041898
Iteration 10000: Loss = -10979.438024386742
Iteration 10100: Loss = -10979.437883011535
Iteration 10200: Loss = -10979.437813826598
Iteration 10300: Loss = -10979.563937199391
1
Iteration 10400: Loss = -10979.437746167998
Iteration 10500: Loss = -10979.437700534594
Iteration 10600: Loss = -10979.440497092266
1
Iteration 10700: Loss = -10979.437681183797
Iteration 10800: Loss = -10979.490506415177
1
Iteration 10900: Loss = -10979.437593381597
Iteration 11000: Loss = -10979.462599259772
1
Iteration 11100: Loss = -10979.437556924866
Iteration 11200: Loss = -10979.513957183679
1
Iteration 11300: Loss = -10979.437554223126
Iteration 11400: Loss = -10979.438055509914
1
Iteration 11500: Loss = -10979.437576543372
Iteration 11600: Loss = -10979.437500833275
Iteration 11700: Loss = -10979.437693933136
1
Iteration 11800: Loss = -10979.437454838444
Iteration 11900: Loss = -10979.438202097464
1
Iteration 12000: Loss = -10979.437396912459
Iteration 12100: Loss = -10979.437794126348
1
Iteration 12200: Loss = -10979.43741987244
Iteration 12300: Loss = -10979.43741096245
Iteration 12400: Loss = -10979.437594164376
1
Iteration 12500: Loss = -10979.437414452052
Iteration 12600: Loss = -10979.460328672749
1
Iteration 12700: Loss = -10979.43739848976
Iteration 12800: Loss = -10979.438837555912
1
Iteration 12900: Loss = -10979.437366247119
Iteration 13000: Loss = -10979.481159549225
1
Iteration 13100: Loss = -10979.43735916429
Iteration 13200: Loss = -10979.438514681313
1
Iteration 13300: Loss = -10979.437536288577
2
Iteration 13400: Loss = -10979.437351241171
Iteration 13500: Loss = -10979.505355626776
1
Iteration 13600: Loss = -10979.437349815858
Iteration 13700: Loss = -10979.441045258009
1
Iteration 13800: Loss = -10979.438726804152
2
Iteration 13900: Loss = -10979.488952323718
3
Iteration 14000: Loss = -10979.438501255087
4
Iteration 14100: Loss = -10979.559802103737
5
Iteration 14200: Loss = -10979.437646386157
6
Iteration 14300: Loss = -10979.437417426685
Iteration 14400: Loss = -10979.43730199076
Iteration 14500: Loss = -10979.453956279991
1
Iteration 14600: Loss = -10979.437282955094
Iteration 14700: Loss = -10979.437703765108
1
Iteration 14800: Loss = -10979.503352608586
2
Iteration 14900: Loss = -10979.437235960399
Iteration 15000: Loss = -10979.437516894517
1
Iteration 15100: Loss = -10979.441394305615
2
Iteration 15200: Loss = -10979.437582581868
3
Iteration 15300: Loss = -10979.438489603554
4
Iteration 15400: Loss = -10979.437231232505
Iteration 15500: Loss = -10979.439096124748
1
Iteration 15600: Loss = -10979.437259355973
Iteration 15700: Loss = -10979.449167716213
1
Iteration 15800: Loss = -10979.437294195614
Iteration 15900: Loss = -10979.437953076329
1
Iteration 16000: Loss = -10979.43747477172
2
Iteration 16100: Loss = -10979.437206163013
Iteration 16200: Loss = -10979.43737063259
1
Iteration 16300: Loss = -10979.643873769175
2
Iteration 16400: Loss = -10979.437488851432
3
Iteration 16500: Loss = -10979.437382526325
4
Iteration 16600: Loss = -10979.44464735296
5
Iteration 16700: Loss = -10979.437233234938
Iteration 16800: Loss = -10979.437751556163
1
Iteration 16900: Loss = -10979.437146795328
Iteration 17000: Loss = -10979.44074394133
1
Iteration 17100: Loss = -10979.482021435691
2
Iteration 17200: Loss = -10979.43831397523
3
Iteration 17300: Loss = -10979.440864948947
4
Iteration 17400: Loss = -10979.437245355884
Iteration 17500: Loss = -10979.438179026949
1
Iteration 17600: Loss = -10979.437177392934
Iteration 17700: Loss = -10979.443293178621
1
Iteration 17800: Loss = -10979.43718398773
Iteration 17900: Loss = -10979.442341950495
1
Iteration 18000: Loss = -10979.437152826733
Iteration 18100: Loss = -10979.437402262703
1
Iteration 18200: Loss = -10979.43714809585
Iteration 18300: Loss = -10979.437538866558
1
Iteration 18400: Loss = -10979.43719440166
Iteration 18500: Loss = -10979.437176672911
Iteration 18600: Loss = -10979.439366566516
1
Iteration 18700: Loss = -10979.437206881184
Iteration 18800: Loss = -10979.438399224027
1
Iteration 18900: Loss = -10979.438849655733
2
Iteration 19000: Loss = -10979.437207283112
Iteration 19100: Loss = -10979.437327014206
1
Iteration 19200: Loss = -10979.437281037033
Iteration 19300: Loss = -10979.437212188262
Iteration 19400: Loss = -10979.437212563114
Iteration 19500: Loss = -10979.437442473316
1
Iteration 19600: Loss = -10979.437230617152
Iteration 19700: Loss = -10979.438257214486
1
Iteration 19800: Loss = -10979.43728093948
Iteration 19900: Loss = -10979.495751327138
1
pi: tensor([[1.0000e+00, 1.6633e-08],
        [2.5123e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9744, 0.0256], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1628, 0.1385],
         [0.6869, 0.3593]],

        [[0.7236, 0.2275],
         [0.6558, 0.5519]],

        [[0.6719, 0.0843],
         [0.6671, 0.6021]],

        [[0.5434, 0.1844],
         [0.7027, 0.7006]],

        [[0.7058, 0.1388],
         [0.6331, 0.5795]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
Global Adjusted Rand Index: -0.0005985299320895325
Average Adjusted Rand Index: -0.0013297662255183063
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21103.546794731752
Iteration 100: Loss = -10982.802168618648
Iteration 200: Loss = -10982.549916166246
Iteration 300: Loss = -10982.484136665407
Iteration 400: Loss = -10982.450234046932
Iteration 500: Loss = -10982.429096463233
Iteration 600: Loss = -10982.414348835842
Iteration 700: Loss = -10982.403465536632
Iteration 800: Loss = -10982.395088307292
Iteration 900: Loss = -10982.388461696568
Iteration 1000: Loss = -10982.38297359544
Iteration 1100: Loss = -10982.378043599154
Iteration 1200: Loss = -10982.373167844662
Iteration 1300: Loss = -10982.367775156159
Iteration 1400: Loss = -10982.361439475815
Iteration 1500: Loss = -10982.35344996156
Iteration 1600: Loss = -10982.342856180785
Iteration 1700: Loss = -10982.328008793873
Iteration 1800: Loss = -10982.306056360236
Iteration 1900: Loss = -10982.272613490553
Iteration 2000: Loss = -10982.222889873008
Iteration 2100: Loss = -10982.153074023998
Iteration 2200: Loss = -10982.04984244449
Iteration 2300: Loss = -10981.91961318956
Iteration 2400: Loss = -10981.831450199781
Iteration 2500: Loss = -10981.78283239842
Iteration 2600: Loss = -10981.743392529765
Iteration 2700: Loss = -10981.714084962081
Iteration 2800: Loss = -10981.675391688208
Iteration 2900: Loss = -10981.645488963795
Iteration 3000: Loss = -10981.62400620809
Iteration 3100: Loss = -10981.612589365544
Iteration 3200: Loss = -10981.607389560182
Iteration 3300: Loss = -10981.60503117109
Iteration 3400: Loss = -10981.60376635154
Iteration 3500: Loss = -10981.602889440746
Iteration 3600: Loss = -10981.602201850279
Iteration 3700: Loss = -10981.601609547446
Iteration 3800: Loss = -10981.601042177666
Iteration 3900: Loss = -10981.600499596334
Iteration 4000: Loss = -10981.599970951407
Iteration 4100: Loss = -10981.599410957526
Iteration 4200: Loss = -10981.598831886118
Iteration 4300: Loss = -10981.598721914608
Iteration 4400: Loss = -10981.597548244548
Iteration 4500: Loss = -10981.596807269267
Iteration 4600: Loss = -10981.59605954626
Iteration 4700: Loss = -10981.595004210258
Iteration 4800: Loss = -10981.621532433552
1
Iteration 4900: Loss = -10981.59213785907
Iteration 5000: Loss = -10981.588437766688
Iteration 5100: Loss = -10981.579728026847
Iteration 5200: Loss = -10981.513535900072
Iteration 5300: Loss = -10981.509737672231
Iteration 5400: Loss = -10981.509516784561
Iteration 5500: Loss = -10981.508847492356
Iteration 5600: Loss = -10981.50875345785
Iteration 5700: Loss = -10981.508713059948
Iteration 5800: Loss = -10981.508571063297
Iteration 5900: Loss = -10981.508768821453
1
Iteration 6000: Loss = -10981.50849150099
Iteration 6100: Loss = -10981.508570913067
Iteration 6200: Loss = -10981.514729546609
1
Iteration 6300: Loss = -10981.508344888456
Iteration 6400: Loss = -10981.508377872751
Iteration 6500: Loss = -10981.510220494252
1
Iteration 6600: Loss = -10981.522343847428
2
Iteration 6700: Loss = -10981.508179089971
Iteration 6800: Loss = -10981.508234445155
Iteration 6900: Loss = -10981.5089191582
1
Iteration 7000: Loss = -10981.508094326695
Iteration 7100: Loss = -10981.508111376264
Iteration 7200: Loss = -10981.508006900807
Iteration 7300: Loss = -10981.50825482173
1
Iteration 7400: Loss = -10981.5079151812
Iteration 7500: Loss = -10981.508203377321
1
Iteration 7600: Loss = -10981.51189937553
2
Iteration 7700: Loss = -10981.521165924129
3
Iteration 7800: Loss = -10981.51221734213
4
Iteration 7900: Loss = -10981.508878864452
5
Iteration 8000: Loss = -10981.508775275968
6
Iteration 8100: Loss = -10981.50894296881
7
Iteration 8200: Loss = -10981.509302430999
8
Iteration 8300: Loss = -10981.544968996055
9
Iteration 8400: Loss = -10981.547176332979
10
Iteration 8500: Loss = -10981.522232568408
11
Iteration 8600: Loss = -10981.513904689918
12
Iteration 8700: Loss = -10981.546367538545
13
Iteration 8800: Loss = -10981.5132471946
14
Iteration 8900: Loss = -10981.5209201546
15
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[8.6233e-04, 9.9914e-01],
        [8.6544e-01, 1.3456e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3141, 0.6859], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1737, 0.1617],
         [0.6872, 0.1540]],

        [[0.5646, 0.1606],
         [0.6635, 0.6317]],

        [[0.6703, 0.1633],
         [0.7261, 0.5357]],

        [[0.6155, 0.1673],
         [0.5417, 0.5204]],

        [[0.5787, 0.1631],
         [0.6128, 0.5832]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.011530202595462608
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.005760373721807317
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.015770452305333478
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0020595690747782004
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.012139736576356674
Global Adjusted Rand Index: 0.0038892050839122562
Average Adjusted Rand Index: 0.0014681951055707787
10929.033726915199
[-0.0005985299320895325, 0.0038892050839122562] [-0.0013297662255183063, 0.0014681951055707787] [10979.437200945375, 10981.5209201546]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11034.772418641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22370.929201385217
Iteration 100: Loss = -11178.67272657797
Iteration 200: Loss = -11177.458565250638
Iteration 300: Loss = -11177.17454286095
Iteration 400: Loss = -11177.016487530109
Iteration 500: Loss = -11176.86759287031
Iteration 600: Loss = -11176.690772019458
Iteration 700: Loss = -11176.494097905963
Iteration 800: Loss = -11176.33210026936
Iteration 900: Loss = -11176.212263968966
Iteration 1000: Loss = -11176.112478907342
Iteration 1100: Loss = -11176.015614572798
Iteration 1200: Loss = -11175.90809080907
Iteration 1300: Loss = -11175.772381440267
Iteration 1400: Loss = -11175.583940753868
Iteration 1500: Loss = -11175.341076446966
Iteration 1600: Loss = -11175.095665862096
Iteration 1700: Loss = -11174.879629822623
Iteration 1800: Loss = -11174.722398420197
Iteration 1900: Loss = -11174.606618460963
Iteration 2000: Loss = -11174.483695898107
Iteration 2100: Loss = -11174.382251125853
Iteration 2200: Loss = -11174.285474948354
Iteration 2300: Loss = -11174.171238934716
Iteration 2400: Loss = -11174.012840989748
Iteration 2500: Loss = -11172.242886555081
Iteration 2600: Loss = -11033.01307738838
Iteration 2700: Loss = -10997.655092594568
Iteration 2800: Loss = -10985.689070236216
Iteration 2900: Loss = -10985.485803562146
Iteration 3000: Loss = -10985.393288804606
Iteration 3100: Loss = -10985.349935906137
Iteration 3200: Loss = -10985.32312876886
Iteration 3300: Loss = -10985.301753045256
Iteration 3400: Loss = -10983.73010159969
Iteration 3500: Loss = -10983.628446365625
Iteration 3600: Loss = -10983.614304786179
Iteration 3700: Loss = -10983.608106050644
Iteration 3800: Loss = -10983.603554119667
Iteration 3900: Loss = -10983.61861025972
1
Iteration 4000: Loss = -10983.596078926099
Iteration 4100: Loss = -10983.593618649491
Iteration 4200: Loss = -10983.591548194043
Iteration 4300: Loss = -10983.58916567123
Iteration 4400: Loss = -10983.587208157562
Iteration 4500: Loss = -10983.58609745917
Iteration 4600: Loss = -10983.595956466737
1
Iteration 4700: Loss = -10983.584543491957
Iteration 4800: Loss = -10983.59285425778
1
Iteration 4900: Loss = -10983.583035343088
Iteration 5000: Loss = -10983.58102679571
Iteration 5100: Loss = -10983.579044523898
Iteration 5200: Loss = -10983.578414197214
Iteration 5300: Loss = -10983.577962155256
Iteration 5400: Loss = -10983.58502748388
1
Iteration 5500: Loss = -10983.57723745353
Iteration 5600: Loss = -10983.577748947662
1
Iteration 5700: Loss = -10983.576514886567
Iteration 5800: Loss = -10983.57626963476
Iteration 5900: Loss = -10983.575755505854
Iteration 6000: Loss = -10983.576276712722
1
Iteration 6100: Loss = -10983.574337822522
Iteration 6200: Loss = -10983.57354191742
Iteration 6300: Loss = -10983.572628061955
Iteration 6400: Loss = -10983.572074907808
Iteration 6500: Loss = -10983.591784220715
1
Iteration 6600: Loss = -10983.570964484868
Iteration 6700: Loss = -10983.570124016756
Iteration 6800: Loss = -10983.57009753937
Iteration 6900: Loss = -10983.568492743172
Iteration 7000: Loss = -10983.563775618963
Iteration 7100: Loss = -10983.559821057686
Iteration 7200: Loss = -10983.559832409886
Iteration 7300: Loss = -10983.557879879949
Iteration 7400: Loss = -10983.559186028866
1
Iteration 7500: Loss = -10983.555836576505
Iteration 7600: Loss = -10983.555760328107
Iteration 7700: Loss = -10983.558051170372
1
Iteration 7800: Loss = -10983.555588500356
Iteration 7900: Loss = -10983.555544722101
Iteration 8000: Loss = -10983.555559299255
Iteration 8100: Loss = -10983.557168988013
1
Iteration 8200: Loss = -10983.562380218578
2
Iteration 8300: Loss = -10983.555708844206
3
Iteration 8400: Loss = -10983.567602921426
4
Iteration 8500: Loss = -10983.555281591101
Iteration 8600: Loss = -10983.570845594757
1
Iteration 8700: Loss = -10983.555203292017
Iteration 8800: Loss = -10983.555131953563
Iteration 8900: Loss = -10983.555337990929
1
Iteration 9000: Loss = -10983.555150950633
Iteration 9100: Loss = -10983.554767854479
Iteration 9200: Loss = -10983.484197543747
Iteration 9300: Loss = -10983.48253522485
Iteration 9400: Loss = -10983.48035051139
Iteration 9500: Loss = -10983.479030190048
Iteration 9600: Loss = -10983.479210451493
1
Iteration 9700: Loss = -10983.48248036167
2
Iteration 9800: Loss = -10983.47885653336
Iteration 9900: Loss = -10983.478758775416
Iteration 10000: Loss = -10983.482407680049
1
Iteration 10100: Loss = -10983.5143761258
2
Iteration 10200: Loss = -10983.479349479145
3
Iteration 10300: Loss = -10983.48256231763
4
Iteration 10400: Loss = -10983.479408339379
5
Iteration 10500: Loss = -10983.480589222348
6
Iteration 10600: Loss = -10983.518065757553
7
Iteration 10700: Loss = -10983.485281947515
8
Iteration 10800: Loss = -10983.492655334434
9
Iteration 10900: Loss = -10983.478700446718
Iteration 11000: Loss = -10983.47894744267
1
Iteration 11100: Loss = -10983.510932658934
2
Iteration 11200: Loss = -10983.478589962788
Iteration 11300: Loss = -10983.482386045795
1
Iteration 11400: Loss = -10983.507415599777
2
Iteration 11500: Loss = -10983.480573355067
3
Iteration 11600: Loss = -10983.483818468041
4
Iteration 11700: Loss = -10983.73272385563
5
Iteration 11800: Loss = -10983.478426336296
Iteration 11900: Loss = -10983.4785389506
1
Iteration 12000: Loss = -10983.478672944815
2
Iteration 12100: Loss = -10983.4978477207
3
Iteration 12200: Loss = -10983.480657642005
4
Iteration 12300: Loss = -10983.50295463659
5
Iteration 12400: Loss = -10983.478544883823
6
Iteration 12500: Loss = -10983.484865638336
7
Iteration 12600: Loss = -10983.67641080995
8
Iteration 12700: Loss = -10983.47857858977
9
Iteration 12800: Loss = -10983.478801857198
10
Iteration 12900: Loss = -10983.483722250368
11
Iteration 13000: Loss = -10983.481418776102
12
Iteration 13100: Loss = -10983.578903178139
13
Iteration 13200: Loss = -10983.478777815804
14
Iteration 13300: Loss = -10983.480302565717
15
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[0.8208, 0.1792],
        [0.2279, 0.7721]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5492, 0.4508], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2495, 0.0999],
         [0.5489, 0.2123]],

        [[0.6101, 0.0963],
         [0.5392, 0.5682]],

        [[0.6343, 0.0942],
         [0.6754, 0.6741]],

        [[0.7233, 0.1005],
         [0.5971, 0.6349]],

        [[0.5268, 0.1061],
         [0.5423, 0.6319]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208065164923572
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.8460828250156939
Average Adjusted Rand Index: 0.8456561796075771
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23915.47010457145
Iteration 100: Loss = -11178.460565875477
Iteration 200: Loss = -11177.283749090824
Iteration 300: Loss = -11177.023162603766
Iteration 400: Loss = -11176.897717977274
Iteration 500: Loss = -11176.820745725841
Iteration 600: Loss = -11176.73101326968
Iteration 700: Loss = -11176.56695588429
Iteration 800: Loss = -11176.33077316498
Iteration 900: Loss = -11176.235458884266
Iteration 1000: Loss = -11176.183656953408
Iteration 1100: Loss = -11176.143551840427
Iteration 1200: Loss = -11176.11233241837
Iteration 1300: Loss = -11176.088844267542
Iteration 1400: Loss = -11176.071920252427
Iteration 1500: Loss = -11176.0598089998
Iteration 1600: Loss = -11176.051041539733
Iteration 1700: Loss = -11176.044791639632
Iteration 1800: Loss = -11176.040301933435
Iteration 1900: Loss = -11176.036942467199
Iteration 2000: Loss = -11176.034365796975
Iteration 2100: Loss = -11176.032323524267
Iteration 2200: Loss = -11176.030668204612
Iteration 2300: Loss = -11176.029313301797
Iteration 2400: Loss = -11176.028004938618
Iteration 2500: Loss = -11176.026336107569
Iteration 2600: Loss = -11176.021429385306
Iteration 2700: Loss = -11175.984550356672
Iteration 2800: Loss = -11175.875054135679
Iteration 2900: Loss = -11175.48546463073
Iteration 3000: Loss = -11174.491505317252
Iteration 3100: Loss = -11174.111558574774
Iteration 3200: Loss = -11042.321389912726
Iteration 3300: Loss = -11041.729078112063
Iteration 3400: Loss = -11041.675910499873
Iteration 3500: Loss = -11041.617079166677
Iteration 3600: Loss = -11041.48892138029
Iteration 3700: Loss = -11041.009848743151
Iteration 3800: Loss = -10995.08421372836
Iteration 3900: Loss = -10994.428331347859
Iteration 4000: Loss = -10992.439678468556
Iteration 4100: Loss = -10992.42738500436
Iteration 4200: Loss = -10987.364553735277
Iteration 4300: Loss = -10987.261788450804
Iteration 4400: Loss = -10987.257011471258
Iteration 4500: Loss = -10987.240052036226
Iteration 4600: Loss = -10987.239202403925
Iteration 4700: Loss = -10987.238609027767
Iteration 4800: Loss = -10987.24054001354
1
Iteration 4900: Loss = -10987.237490907553
Iteration 5000: Loss = -10987.236838372777
Iteration 5100: Loss = -10987.235780055647
Iteration 5200: Loss = -10987.230816045341
Iteration 5300: Loss = -10987.188726350185
Iteration 5400: Loss = -10986.543427934117
Iteration 5500: Loss = -10986.542747184909
Iteration 5600: Loss = -10986.542851235085
1
Iteration 5700: Loss = -10986.54090953387
Iteration 5800: Loss = -10986.546375626369
1
Iteration 5900: Loss = -10986.536824936215
Iteration 6000: Loss = -10986.534594139242
Iteration 6100: Loss = -10983.541551512242
Iteration 6200: Loss = -10983.53865013941
Iteration 6300: Loss = -10983.536386300935
Iteration 6400: Loss = -10983.53438899342
Iteration 6500: Loss = -10983.533984674163
Iteration 6600: Loss = -10983.533251456736
Iteration 6700: Loss = -10983.531822186278
Iteration 6800: Loss = -10983.531640378558
Iteration 6900: Loss = -10983.533792590053
1
Iteration 7000: Loss = -10983.530327709146
Iteration 7100: Loss = -10983.530810741924
1
Iteration 7200: Loss = -10983.53030939574
Iteration 7300: Loss = -10983.530139169334
Iteration 7400: Loss = -10983.53009318685
Iteration 7500: Loss = -10983.532837974735
1
Iteration 7600: Loss = -10983.530231488186
2
Iteration 7700: Loss = -10983.53062950304
3
Iteration 7800: Loss = -10983.531927480244
4
Iteration 7900: Loss = -10983.52954956589
Iteration 8000: Loss = -10983.528101746668
Iteration 8100: Loss = -10983.531077018195
1
Iteration 8200: Loss = -10983.52775268639
Iteration 8300: Loss = -10983.52753820385
Iteration 8400: Loss = -10983.562109049759
1
Iteration 8500: Loss = -10983.526943038383
Iteration 8600: Loss = -10983.526966252177
Iteration 8700: Loss = -10983.526835327777
Iteration 8800: Loss = -10983.508660922476
Iteration 8900: Loss = -10983.508496233362
Iteration 9000: Loss = -10983.508615466559
1
Iteration 9100: Loss = -10983.510259148698
2
Iteration 9200: Loss = -10983.521033558138
3
Iteration 9300: Loss = -10983.507293717636
Iteration 9400: Loss = -10983.647225052267
1
Iteration 9500: Loss = -10983.502592532259
Iteration 9600: Loss = -10983.572580541884
1
Iteration 9700: Loss = -10983.505394740074
2
Iteration 9800: Loss = -10983.516182010178
3
Iteration 9900: Loss = -10983.492935235814
Iteration 10000: Loss = -10983.485546886253
Iteration 10100: Loss = -10983.479162284571
Iteration 10200: Loss = -10983.479472526398
1
Iteration 10300: Loss = -10983.480183150252
2
Iteration 10400: Loss = -10983.481914991095
3
Iteration 10500: Loss = -10983.613911660843
4
Iteration 10600: Loss = -10983.478601386025
Iteration 10700: Loss = -10983.478930044286
1
Iteration 10800: Loss = -10983.499875236268
2
Iteration 10900: Loss = -10983.478597033387
Iteration 11000: Loss = -10983.481366121745
1
Iteration 11100: Loss = -10983.478783632852
2
Iteration 11200: Loss = -10983.479866831703
3
Iteration 11300: Loss = -10983.48203163618
4
Iteration 11400: Loss = -10983.47923117973
5
Iteration 11500: Loss = -10983.480717349148
6
Iteration 11600: Loss = -10983.519044299384
7
Iteration 11700: Loss = -10983.479104525293
8
Iteration 11800: Loss = -10983.490769005872
9
Iteration 11900: Loss = -10983.669031879952
10
Iteration 12000: Loss = -10983.478480836155
Iteration 12100: Loss = -10983.500501650635
1
Iteration 12200: Loss = -10983.505871266087
2
Iteration 12300: Loss = -10983.488936945409
3
Iteration 12400: Loss = -10983.493468109884
4
Iteration 12500: Loss = -10983.500614908839
5
Iteration 12600: Loss = -10983.47812350303
Iteration 12700: Loss = -10983.479619217722
1
Iteration 12800: Loss = -10983.511971215812
2
Iteration 12900: Loss = -10983.540510174207
3
Iteration 13000: Loss = -10983.628245801025
4
Iteration 13100: Loss = -10983.477686370325
Iteration 13200: Loss = -10983.478087941287
1
Iteration 13300: Loss = -10983.488496688606
2
Iteration 13400: Loss = -10983.478951892226
3
Iteration 13500: Loss = -10983.479484283313
4
Iteration 13600: Loss = -10983.485942817395
5
Iteration 13700: Loss = -10983.47906478246
6
Iteration 13800: Loss = -10983.482199460763
7
Iteration 13900: Loss = -10983.478673774247
8
Iteration 14000: Loss = -10983.491627928148
9
Iteration 14100: Loss = -10983.481297217771
10
Iteration 14200: Loss = -10983.47897318632
11
Iteration 14300: Loss = -10983.571116388926
12
Iteration 14400: Loss = -10983.477388406493
Iteration 14500: Loss = -10983.487461842238
1
Iteration 14600: Loss = -10983.479175881821
2
Iteration 14700: Loss = -10983.486288150378
3
Iteration 14800: Loss = -10983.480175283365
4
Iteration 14900: Loss = -10983.486574838826
5
Iteration 15000: Loss = -10983.49069880506
6
Iteration 15100: Loss = -10983.479338000712
7
Iteration 15200: Loss = -10983.478114825994
8
Iteration 15300: Loss = -10983.478455130711
9
Iteration 15400: Loss = -10983.478465043543
10
Iteration 15500: Loss = -10983.480255133603
11
Iteration 15600: Loss = -10983.491128614456
12
Iteration 15700: Loss = -10983.477893136527
13
Iteration 15800: Loss = -10983.482933070643
14
Iteration 15900: Loss = -10983.48357988845
15
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[0.7712, 0.2288],
        [0.1799, 0.8201]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4508, 0.5492], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2123, 0.0998],
         [0.5269, 0.2497]],

        [[0.7169, 0.0964],
         [0.5892, 0.7197]],

        [[0.6408, 0.0939],
         [0.5348, 0.5163]],

        [[0.6398, 0.1005],
         [0.5432, 0.6722]],

        [[0.6508, 0.1060],
         [0.5786, 0.5928]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208065164923572
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824283882000855
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.8460828250156939
Average Adjusted Rand Index: 0.8456561796075771
11034.772418641836
[0.8460828250156939, 0.8460828250156939] [0.8456561796075771, 0.8456561796075771] [10983.480302565717, 10983.48357988845]
-------------------------------------
This iteration is 44
True Objective function: Loss = -10906.52526623401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22533.10237194678
Iteration 100: Loss = -11050.253727926967
Iteration 200: Loss = -11049.514656715068
Iteration 300: Loss = -11049.326533511037
Iteration 400: Loss = -11049.180983889608
Iteration 500: Loss = -11048.989460803232
Iteration 600: Loss = -11048.726338570987
Iteration 700: Loss = -11048.470684196043
Iteration 800: Loss = -11048.260539021581
Iteration 900: Loss = -11048.064067319767
Iteration 1000: Loss = -11047.867400035264
Iteration 1100: Loss = -11047.666386628778
Iteration 1200: Loss = -11047.451453951982
Iteration 1300: Loss = -11047.175210195634
Iteration 1400: Loss = -11046.460049634397
Iteration 1500: Loss = -11035.624909764298
Iteration 1600: Loss = -10954.239721123122
Iteration 1700: Loss = -10949.272917919128
Iteration 1800: Loss = -10948.849427702606
Iteration 1900: Loss = -10948.669658153614
Iteration 2000: Loss = -10948.58056191389
Iteration 2100: Loss = -10948.474008732155
Iteration 2200: Loss = -10948.037673818626
Iteration 2300: Loss = -10948.010734411919
Iteration 2400: Loss = -10947.990416802879
Iteration 2500: Loss = -10947.974395401656
Iteration 2600: Loss = -10947.961484037636
Iteration 2700: Loss = -10947.950084193813
Iteration 2800: Loss = -10947.939523602148
Iteration 2900: Loss = -10947.940014854872
1
Iteration 3000: Loss = -10947.916165618697
Iteration 3100: Loss = -10947.910776102151
Iteration 3200: Loss = -10947.905957335252
Iteration 3300: Loss = -10947.90150415642
Iteration 3400: Loss = -10947.897435536437
Iteration 3500: Loss = -10947.89407210633
Iteration 3600: Loss = -10947.900239601373
1
Iteration 3700: Loss = -10947.888487337426
Iteration 3800: Loss = -10947.89603223992
1
Iteration 3900: Loss = -10947.88481249756
Iteration 4000: Loss = -10947.881276566413
Iteration 4100: Loss = -10947.880009568806
Iteration 4200: Loss = -10947.876877229864
Iteration 4300: Loss = -10947.874695047363
Iteration 4400: Loss = -10947.880603428172
1
Iteration 4500: Loss = -10947.866954006682
Iteration 4600: Loss = -10947.85607134512
Iteration 4700: Loss = -10947.849958623074
Iteration 4800: Loss = -10947.839394392806
Iteration 4900: Loss = -10947.832654143514
Iteration 5000: Loss = -10947.820434792822
Iteration 5100: Loss = -10947.798301004714
Iteration 5200: Loss = -10947.70824193419
Iteration 5300: Loss = -10945.456311181106
Iteration 5400: Loss = -10945.283529132475
Iteration 5500: Loss = -10944.221336302518
Iteration 5600: Loss = -10944.179547182315
Iteration 5700: Loss = -10944.155084618214
Iteration 5800: Loss = -10942.772933537017
Iteration 5900: Loss = -10942.5797757641
Iteration 6000: Loss = -10942.461595639488
Iteration 6100: Loss = -10942.450706826097
Iteration 6200: Loss = -10942.431634827755
Iteration 6300: Loss = -10942.428099792061
Iteration 6400: Loss = -10942.42296231115
Iteration 6500: Loss = -10942.276097048336
Iteration 6600: Loss = -10942.274157050957
Iteration 6700: Loss = -10942.271963445679
Iteration 6800: Loss = -10942.270489749639
Iteration 6900: Loss = -10942.266784212097
Iteration 7000: Loss = -10942.269943594894
1
Iteration 7100: Loss = -10942.266545294417
Iteration 7200: Loss = -10942.266393482267
Iteration 7300: Loss = -10942.268238350287
1
Iteration 7400: Loss = -10942.266161578369
Iteration 7500: Loss = -10942.266026169627
Iteration 7600: Loss = -10942.269679668198
1
Iteration 7700: Loss = -10942.265634077416
Iteration 7800: Loss = -10942.265459294676
Iteration 7900: Loss = -10942.265361073461
Iteration 8000: Loss = -10942.265272823142
Iteration 8100: Loss = -10942.27541850026
1
Iteration 8200: Loss = -10942.265149513349
Iteration 8300: Loss = -10942.265141100224
Iteration 8400: Loss = -10942.265098994829
Iteration 8500: Loss = -10942.265030002844
Iteration 8600: Loss = -10942.264967032635
Iteration 8700: Loss = -10942.265014816401
Iteration 8800: Loss = -10942.264927735558
Iteration 8900: Loss = -10942.264905600752
Iteration 9000: Loss = -10942.264849553492
Iteration 9100: Loss = -10942.265827515826
1
Iteration 9200: Loss = -10942.264774206267
Iteration 9300: Loss = -10942.275647161561
1
Iteration 9400: Loss = -10942.26473902724
Iteration 9500: Loss = -10942.264681306799
Iteration 9600: Loss = -10942.264710960882
Iteration 9700: Loss = -10942.284601796577
1
Iteration 9800: Loss = -10942.26416568901
Iteration 9900: Loss = -10942.264348037368
1
Iteration 10000: Loss = -10942.264116347531
Iteration 10100: Loss = -10942.264095843855
Iteration 10200: Loss = -10942.289042584382
1
Iteration 10300: Loss = -10942.26405803711
Iteration 10400: Loss = -10942.331577570401
1
Iteration 10500: Loss = -10942.263962576513
Iteration 10600: Loss = -10942.277437393244
1
Iteration 10700: Loss = -10942.263025689517
Iteration 10800: Loss = -10942.416778776273
1
Iteration 10900: Loss = -10942.263011406543
Iteration 11000: Loss = -10942.263119886129
1
Iteration 11100: Loss = -10942.263049895415
Iteration 11200: Loss = -10942.262985894859
Iteration 11300: Loss = -10942.263021607525
Iteration 11400: Loss = -10942.470377839221
1
Iteration 11500: Loss = -10942.262927454438
Iteration 11600: Loss = -10942.267613161337
1
Iteration 11700: Loss = -10942.262950008668
Iteration 11800: Loss = -10942.298087463738
1
Iteration 11900: Loss = -10942.281313040905
2
Iteration 12000: Loss = -10942.359882169967
3
Iteration 12100: Loss = -10942.262951252103
Iteration 12200: Loss = -10942.263055976844
1
Iteration 12300: Loss = -10942.447280297934
2
Iteration 12400: Loss = -10942.262896286882
Iteration 12500: Loss = -10942.384268709679
1
Iteration 12600: Loss = -10942.262866903538
Iteration 12700: Loss = -10942.311204015208
1
Iteration 12800: Loss = -10942.262878100177
Iteration 12900: Loss = -10942.270093141842
1
Iteration 13000: Loss = -10942.26302348316
2
Iteration 13100: Loss = -10942.263094424503
3
Iteration 13200: Loss = -10942.263761302602
4
Iteration 13300: Loss = -10942.275919877071
5
Iteration 13400: Loss = -10942.262880628077
Iteration 13500: Loss = -10942.368486801664
1
Iteration 13600: Loss = -10942.262831676931
Iteration 13700: Loss = -10942.267711180002
1
Iteration 13800: Loss = -10942.26290688958
Iteration 13900: Loss = -10942.307453052237
1
Iteration 14000: Loss = -10942.262835238329
Iteration 14100: Loss = -10942.263009278311
1
Iteration 14200: Loss = -10942.689849777164
2
Iteration 14300: Loss = -10942.262894647602
Iteration 14400: Loss = -10942.262851581956
Iteration 14500: Loss = -10942.2634133443
1
Iteration 14600: Loss = -10942.263236235656
2
Iteration 14700: Loss = -10942.263231172134
3
Iteration 14800: Loss = -10942.272114498694
4
Iteration 14900: Loss = -10942.263218788656
5
Iteration 15000: Loss = -10942.262966842378
6
Iteration 15100: Loss = -10942.263867920854
7
Iteration 15200: Loss = -10942.269241222037
8
Iteration 15300: Loss = -10942.266321973899
9
Iteration 15400: Loss = -10942.2760029668
10
Iteration 15500: Loss = -10942.264542685056
11
Iteration 15600: Loss = -10942.28020873369
12
Iteration 15700: Loss = -10942.294033520111
13
Iteration 15800: Loss = -10942.263399728901
14
Iteration 15900: Loss = -10942.26283315555
Iteration 16000: Loss = -10942.26929991676
1
Iteration 16100: Loss = -10942.273706823926
2
Iteration 16200: Loss = -10942.271458742876
3
Iteration 16300: Loss = -10942.275008706401
4
Iteration 16400: Loss = -10942.26283600367
Iteration 16500: Loss = -10942.322257584952
1
Iteration 16600: Loss = -10942.262828355779
Iteration 16700: Loss = -10942.263127097698
1
Iteration 16800: Loss = -10942.265749789622
2
Iteration 16900: Loss = -10942.262817465908
Iteration 17000: Loss = -10942.263274454042
1
Iteration 17100: Loss = -10942.262795090983
Iteration 17200: Loss = -10942.264075800278
1
Iteration 17300: Loss = -10942.263608229605
2
Iteration 17400: Loss = -10942.264091481706
3
Iteration 17500: Loss = -10942.268377823026
4
Iteration 17600: Loss = -10942.271862534011
5
Iteration 17700: Loss = -10942.262773778175
Iteration 17800: Loss = -10942.265197184153
1
Iteration 17900: Loss = -10942.292417403523
2
Iteration 18000: Loss = -10942.264042606213
3
Iteration 18100: Loss = -10942.262923742559
4
Iteration 18200: Loss = -10942.262873826034
5
Iteration 18300: Loss = -10942.263366140322
6
Iteration 18400: Loss = -10942.263311813176
7
Iteration 18500: Loss = -10942.263625750315
8
Iteration 18600: Loss = -10942.317529239674
9
Iteration 18700: Loss = -10942.262820913304
Iteration 18800: Loss = -10942.265966713658
1
Iteration 18900: Loss = -10942.397822225172
2
Iteration 19000: Loss = -10942.271671083803
3
Iteration 19100: Loss = -10942.262816953931
Iteration 19200: Loss = -10942.26305681982
1
Iteration 19300: Loss = -10942.269575798877
2
Iteration 19400: Loss = -10942.262860119195
Iteration 19500: Loss = -10942.26280889852
Iteration 19600: Loss = -10942.266151842474
1
Iteration 19700: Loss = -10942.263522355015
2
Iteration 19800: Loss = -10942.263028417561
3
Iteration 19900: Loss = -10942.264707011997
4
pi: tensor([[0.6494, 0.3506],
        [0.2777, 0.7223]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9070, 0.0930], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1843, 0.0977],
         [0.5773, 0.2613]],

        [[0.6242, 0.0984],
         [0.7274, 0.5238]],

        [[0.7175, 0.0986],
         [0.7288, 0.6885]],

        [[0.6717, 0.1099],
         [0.6274, 0.5997]],

        [[0.5127, 0.1003],
         [0.5792, 0.6486]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.000972254293110363
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.5407782142033362
Average Adjusted Rand Index: 0.6997471036423624
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20614.342225424047
Iteration 100: Loss = -11049.633210914975
Iteration 200: Loss = -11049.328554885677
Iteration 300: Loss = -11049.27957047327
Iteration 400: Loss = -11049.250694985807
Iteration 500: Loss = -11049.229326863848
Iteration 600: Loss = -11049.211297798807
Iteration 700: Loss = -11049.195140604506
Iteration 800: Loss = -11049.181031351323
Iteration 900: Loss = -11049.168804002107
Iteration 1000: Loss = -11049.15747089328
Iteration 1100: Loss = -11049.145806883193
Iteration 1200: Loss = -11049.130110913267
Iteration 1300: Loss = -11049.09466064161
Iteration 1400: Loss = -11048.808888110032
Iteration 1500: Loss = -11046.98607930972
Iteration 1600: Loss = -10951.820219712221
Iteration 1700: Loss = -10948.26937754994
Iteration 1800: Loss = -10948.118395351987
Iteration 1900: Loss = -10948.032985782545
Iteration 2000: Loss = -10947.755343881832
Iteration 2100: Loss = -10945.352394357358
Iteration 2200: Loss = -10943.903353811853
Iteration 2300: Loss = -10942.34211669332
Iteration 2400: Loss = -10942.333597879575
Iteration 2500: Loss = -10942.331423827754
Iteration 2600: Loss = -10942.329363389585
Iteration 2700: Loss = -10942.32651784307
Iteration 2800: Loss = -10942.326638536717
1
Iteration 2900: Loss = -10942.32420000879
Iteration 3000: Loss = -10942.333011423643
1
Iteration 3100: Loss = -10942.321748393324
Iteration 3200: Loss = -10942.316435727653
Iteration 3300: Loss = -10942.314051094183
Iteration 3400: Loss = -10942.313039750432
Iteration 3500: Loss = -10942.3167666369
1
Iteration 3600: Loss = -10942.312565317161
Iteration 3700: Loss = -10942.312387752103
Iteration 3800: Loss = -10942.312309587121
Iteration 3900: Loss = -10942.31204653831
Iteration 4000: Loss = -10942.311666998054
Iteration 4100: Loss = -10942.310737207174
Iteration 4200: Loss = -10942.308114409396
Iteration 4300: Loss = -10942.30559071541
Iteration 4400: Loss = -10942.303865847565
Iteration 4500: Loss = -10942.303823074573
Iteration 4600: Loss = -10942.303930241418
1
Iteration 4700: Loss = -10942.303671626378
Iteration 4800: Loss = -10942.303584512052
Iteration 4900: Loss = -10942.30384636473
1
Iteration 5000: Loss = -10942.303467476882
Iteration 5100: Loss = -10942.303438618132
Iteration 5200: Loss = -10942.303473381451
Iteration 5300: Loss = -10942.303343291334
Iteration 5400: Loss = -10942.303312832995
Iteration 5500: Loss = -10942.303547452868
1
Iteration 5600: Loss = -10942.303227018412
Iteration 5700: Loss = -10942.303219437
Iteration 5800: Loss = -10942.303255916891
Iteration 5900: Loss = -10942.303144888765
Iteration 6000: Loss = -10942.303144168949
Iteration 6100: Loss = -10942.3031484158
Iteration 6200: Loss = -10942.30313083361
Iteration 6300: Loss = -10942.365879978302
1
Iteration 6400: Loss = -10942.303038576809
Iteration 6500: Loss = -10942.302696924156
Iteration 6600: Loss = -10942.303864627467
1
Iteration 6700: Loss = -10942.301953316153
Iteration 6800: Loss = -10942.340270809671
1
Iteration 6900: Loss = -10942.301012500946
Iteration 7000: Loss = -10942.30107869441
Iteration 7100: Loss = -10942.300835718068
Iteration 7200: Loss = -10942.302106436679
1
Iteration 7300: Loss = -10942.303921038701
2
Iteration 7400: Loss = -10942.300666771685
Iteration 7500: Loss = -10942.302121809083
1
Iteration 7600: Loss = -10942.307552079057
2
Iteration 7700: Loss = -10942.30065867655
Iteration 7800: Loss = -10942.300860141951
1
Iteration 7900: Loss = -10942.300749246582
Iteration 8000: Loss = -10942.301065268945
1
Iteration 8100: Loss = -10942.30064455242
Iteration 8200: Loss = -10942.300668264963
Iteration 8300: Loss = -10942.300679949793
Iteration 8400: Loss = -10942.301985391272
1
Iteration 8500: Loss = -10942.301158987766
2
Iteration 8600: Loss = -10942.300656309544
Iteration 8700: Loss = -10942.300838318706
1
Iteration 8800: Loss = -10942.304215693757
2
Iteration 8900: Loss = -10942.30072126839
Iteration 9000: Loss = -10942.300687991183
Iteration 9100: Loss = -10942.319729435667
1
Iteration 9200: Loss = -10942.330527860184
2
Iteration 9300: Loss = -10942.301120476777
3
Iteration 9400: Loss = -10942.300675120186
Iteration 9500: Loss = -10942.301274811798
1
Iteration 9600: Loss = -10942.373316078885
2
Iteration 9700: Loss = -10942.300657155884
Iteration 9800: Loss = -10942.300881602368
1
Iteration 9900: Loss = -10942.325127970089
2
Iteration 10000: Loss = -10942.300589755872
Iteration 10100: Loss = -10942.304797015504
1
Iteration 10200: Loss = -10942.300587919157
Iteration 10300: Loss = -10942.30208594139
1
Iteration 10400: Loss = -10942.300558477036
Iteration 10500: Loss = -10942.300730546662
1
Iteration 10600: Loss = -10942.300585592695
Iteration 10700: Loss = -10942.300699166704
1
Iteration 10800: Loss = -10942.300625008826
Iteration 10900: Loss = -10942.30063535152
Iteration 11000: Loss = -10942.300592108706
Iteration 11100: Loss = -10942.301561648486
1
Iteration 11200: Loss = -10942.300571002259
Iteration 11300: Loss = -10942.306354380375
1
Iteration 11400: Loss = -10942.302578921393
2
Iteration 11500: Loss = -10942.302534901259
3
Iteration 11600: Loss = -10942.30058589407
Iteration 11700: Loss = -10942.310431956215
1
Iteration 11800: Loss = -10942.303883254262
2
Iteration 11900: Loss = -10942.300696460039
3
Iteration 12000: Loss = -10942.300604918344
Iteration 12100: Loss = -10942.305537809772
1
Iteration 12200: Loss = -10942.300525525381
Iteration 12300: Loss = -10942.300773084604
1
Iteration 12400: Loss = -10942.385298727446
2
Iteration 12500: Loss = -10942.340723114366
3
Iteration 12600: Loss = -10942.306892118511
4
Iteration 12700: Loss = -10942.300532093634
Iteration 12800: Loss = -10942.309017827423
1
Iteration 12900: Loss = -10942.300553299685
Iteration 13000: Loss = -10942.302218504019
1
Iteration 13100: Loss = -10942.300530337237
Iteration 13200: Loss = -10942.300745743218
1
Iteration 13300: Loss = -10942.300553494986
Iteration 13400: Loss = -10942.300679247122
1
Iteration 13500: Loss = -10942.300540733646
Iteration 13600: Loss = -10942.301518319875
1
Iteration 13700: Loss = -10942.300578795915
Iteration 13800: Loss = -10942.318447601545
1
Iteration 13900: Loss = -10942.300547118315
Iteration 14000: Loss = -10942.302972135103
1
Iteration 14100: Loss = -10942.301574333116
2
Iteration 14200: Loss = -10942.305026929153
3
Iteration 14300: Loss = -10942.302248046699
4
Iteration 14400: Loss = -10942.300647797467
5
Iteration 14500: Loss = -10942.300669269674
6
Iteration 14600: Loss = -10942.354018335698
7
Iteration 14700: Loss = -10942.300653494154
8
Iteration 14800: Loss = -10942.300644465171
Iteration 14900: Loss = -10942.302671144558
1
Iteration 15000: Loss = -10942.301442434791
2
Iteration 15100: Loss = -10942.300871582076
3
Iteration 15200: Loss = -10942.32644582302
4
Iteration 15300: Loss = -10942.300605434277
Iteration 15400: Loss = -10942.30055137111
Iteration 15500: Loss = -10942.330219896436
1
Iteration 15600: Loss = -10942.300515864614
Iteration 15700: Loss = -10942.300518940014
Iteration 15800: Loss = -10942.30082209004
1
Iteration 15900: Loss = -10942.303541317267
2
Iteration 16000: Loss = -10942.32005176171
3
Iteration 16100: Loss = -10942.300509558876
Iteration 16200: Loss = -10942.526109533783
1
Iteration 16300: Loss = -10942.300502590986
Iteration 16400: Loss = -10942.300753477639
1
Iteration 16500: Loss = -10942.30136711965
2
Iteration 16600: Loss = -10942.315097795694
3
Iteration 16700: Loss = -10942.300526558856
Iteration 16800: Loss = -10942.300822900503
1
Iteration 16900: Loss = -10942.303151364138
2
Iteration 17000: Loss = -10942.300567696308
Iteration 17100: Loss = -10942.30954870886
1
Iteration 17200: Loss = -10942.300510959896
Iteration 17300: Loss = -10942.302392095797
1
Iteration 17400: Loss = -10942.300504608329
Iteration 17500: Loss = -10942.300666015331
1
Iteration 17600: Loss = -10942.300598964608
Iteration 17700: Loss = -10942.30303442441
1
Iteration 17800: Loss = -10942.302337625668
2
Iteration 17900: Loss = -10942.300513313601
Iteration 18000: Loss = -10942.30056510578
Iteration 18100: Loss = -10942.30081950206
1
Iteration 18200: Loss = -10942.300566595532
Iteration 18300: Loss = -10942.301873646178
1
Iteration 18400: Loss = -10942.302196361248
2
Iteration 18500: Loss = -10942.300658392562
Iteration 18600: Loss = -10942.300581831341
Iteration 18700: Loss = -10942.321478389065
1
Iteration 18800: Loss = -10942.300518329897
Iteration 18900: Loss = -10942.302943449375
1
Iteration 19000: Loss = -10942.300501557991
Iteration 19100: Loss = -10942.300756608054
1
Iteration 19200: Loss = -10942.300773826304
2
Iteration 19300: Loss = -10942.300792304233
3
Iteration 19400: Loss = -10942.309483452922
4
Iteration 19500: Loss = -10942.303460539466
5
Iteration 19600: Loss = -10942.311135685226
6
Iteration 19700: Loss = -10942.300663253172
7
Iteration 19800: Loss = -10942.300543588637
Iteration 19900: Loss = -10942.30499188223
1
pi: tensor([[0.7172, 0.2828],
        [0.3579, 0.6421]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1117, 0.8883], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2591, 0.0976],
         [0.5894, 0.1871]],

        [[0.5764, 0.0973],
         [0.6630, 0.6540]],

        [[0.5360, 0.0977],
         [0.5179, 0.6048]],

        [[0.6772, 0.1090],
         [0.6452, 0.7184]],

        [[0.6143, 0.0999],
         [0.6342, 0.7057]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.002773722627737226
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
Global Adjusted Rand Index: 0.5290426167830615
Average Adjusted Rand Index: 0.6921073973092879
10906.52526623401
[0.5407782142033362, 0.5290426167830615] [0.6997471036423624, 0.6921073973092879] [10942.40198201063, 10942.300995332569]
-------------------------------------
This iteration is 45
True Objective function: Loss = -10965.021113698027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22375.03283801975
Iteration 100: Loss = -11081.089098257236
Iteration 200: Loss = -11079.162476837344
Iteration 300: Loss = -11077.086869052226
Iteration 400: Loss = -11073.41525827829
Iteration 500: Loss = -11067.599770230276
Iteration 600: Loss = -11044.220639241297
Iteration 700: Loss = -10928.476325350917
Iteration 800: Loss = -10919.32005215626
Iteration 900: Loss = -10918.556989794606
Iteration 1000: Loss = -10918.465979790863
Iteration 1100: Loss = -10918.405974478814
Iteration 1200: Loss = -10918.332813690138
Iteration 1300: Loss = -10918.304932981135
Iteration 1400: Loss = -10918.266828477339
Iteration 1500: Loss = -10918.184993441879
Iteration 1600: Loss = -10918.173719562446
Iteration 1700: Loss = -10918.16300532074
Iteration 1800: Loss = -10918.152804734144
Iteration 1900: Loss = -10918.145243055558
Iteration 2000: Loss = -10918.138933469128
Iteration 2100: Loss = -10918.140633412528
1
Iteration 2200: Loss = -10918.124801576714
Iteration 2300: Loss = -10918.118768953138
Iteration 2400: Loss = -10918.11512130208
Iteration 2500: Loss = -10918.082069596283
Iteration 2600: Loss = -10918.051436585378
Iteration 2700: Loss = -10918.044735177213
Iteration 2800: Loss = -10918.040114919331
Iteration 2900: Loss = -10918.030749163974
Iteration 3000: Loss = -10918.020474897114
Iteration 3100: Loss = -10917.370707091062
Iteration 3200: Loss = -10917.36880819447
Iteration 3300: Loss = -10917.368484914468
Iteration 3400: Loss = -10917.366516561728
Iteration 3500: Loss = -10917.362159309781
Iteration 3600: Loss = -10917.305575265516
Iteration 3700: Loss = -10917.303951859705
Iteration 3800: Loss = -10917.30100467365
Iteration 3900: Loss = -10917.266134514302
Iteration 4000: Loss = -10917.264632767212
Iteration 4100: Loss = -10917.260998023608
Iteration 4200: Loss = -10917.26385117488
1
Iteration 4300: Loss = -10917.25810686563
Iteration 4400: Loss = -10917.246655526305
Iteration 4500: Loss = -10917.245952189933
Iteration 4600: Loss = -10917.245585158504
Iteration 4700: Loss = -10917.245745967111
1
Iteration 4800: Loss = -10917.24509289706
Iteration 4900: Loss = -10917.246226409234
1
Iteration 5000: Loss = -10917.247511614152
2
Iteration 5100: Loss = -10917.244299496313
Iteration 5200: Loss = -10917.244153988053
Iteration 5300: Loss = -10917.243435701972
Iteration 5400: Loss = -10917.243734914944
1
Iteration 5500: Loss = -10917.241033796274
Iteration 5600: Loss = -10917.24072099839
Iteration 5700: Loss = -10917.240446039108
Iteration 5800: Loss = -10917.240276862633
Iteration 5900: Loss = -10917.242343158128
1
Iteration 6000: Loss = -10917.240021491385
Iteration 6100: Loss = -10917.239816356469
Iteration 6200: Loss = -10917.239707701949
Iteration 6300: Loss = -10917.239605943276
Iteration 6400: Loss = -10917.268658476378
1
Iteration 6500: Loss = -10917.239503831774
Iteration 6600: Loss = -10917.239433840326
Iteration 6700: Loss = -10917.239713881363
1
Iteration 6800: Loss = -10917.239367496913
Iteration 6900: Loss = -10917.23980355807
1
Iteration 7000: Loss = -10917.239247170166
Iteration 7100: Loss = -10917.239249610393
Iteration 7200: Loss = -10917.25879412079
1
Iteration 7300: Loss = -10917.24290372584
2
Iteration 7400: Loss = -10917.242993861419
3
Iteration 7500: Loss = -10917.298207358637
4
Iteration 7600: Loss = -10917.239209245761
Iteration 7700: Loss = -10917.239085894764
Iteration 7800: Loss = -10917.238619121872
Iteration 7900: Loss = -10917.238211669053
Iteration 8000: Loss = -10917.236557425127
Iteration 8100: Loss = -10917.37043905988
1
Iteration 8200: Loss = -10917.238800569636
2
Iteration 8300: Loss = -10917.236326603957
Iteration 8400: Loss = -10917.23650560379
1
Iteration 8500: Loss = -10917.237218187009
2
Iteration 8600: Loss = -10917.399607130274
3
Iteration 8700: Loss = -10917.237812166677
4
Iteration 8800: Loss = -10917.236068240078
Iteration 8900: Loss = -10917.236891609302
1
Iteration 9000: Loss = -10917.236042232988
Iteration 9100: Loss = -10917.296287255338
1
Iteration 9200: Loss = -10917.240721367994
2
Iteration 9300: Loss = -10917.241557218735
3
Iteration 9400: Loss = -10917.236068330629
Iteration 9500: Loss = -10917.237454772734
1
Iteration 9600: Loss = -10917.236024362117
Iteration 9700: Loss = -10917.23569246178
Iteration 9800: Loss = -10917.237819968426
1
Iteration 9900: Loss = -10917.235977151751
2
Iteration 10000: Loss = -10917.23568483298
Iteration 10100: Loss = -10917.378855847664
1
Iteration 10200: Loss = -10917.234126828349
Iteration 10300: Loss = -10917.235357648744
1
Iteration 10400: Loss = -10917.255166094626
2
Iteration 10500: Loss = -10917.23957518655
3
Iteration 10600: Loss = -10917.236481987078
4
Iteration 10700: Loss = -10917.233965461504
Iteration 10800: Loss = -10917.236892205394
1
Iteration 10900: Loss = -10917.233642660516
Iteration 11000: Loss = -10917.237893372996
1
Iteration 11100: Loss = -10917.234139503928
2
Iteration 11200: Loss = -10917.23755271058
3
Iteration 11300: Loss = -10917.233203539017
Iteration 11400: Loss = -10917.23323435497
Iteration 11500: Loss = -10917.23328018777
Iteration 11600: Loss = -10917.236740485441
1
Iteration 11700: Loss = -10917.238506157017
2
Iteration 11800: Loss = -10917.23652529951
3
Iteration 11900: Loss = -10917.234660554735
4
Iteration 12000: Loss = -10917.486110308228
5
Iteration 12100: Loss = -10917.232873629386
Iteration 12200: Loss = -10917.348727646036
1
Iteration 12300: Loss = -10917.232874991054
Iteration 12400: Loss = -10917.239177342892
1
Iteration 12500: Loss = -10917.23284073118
Iteration 12600: Loss = -10917.236852058571
1
Iteration 12700: Loss = -10917.23284728109
Iteration 12800: Loss = -10917.232885791169
Iteration 12900: Loss = -10917.233038801194
1
Iteration 13000: Loss = -10917.232823556247
Iteration 13100: Loss = -10917.232757255546
Iteration 13200: Loss = -10917.234836027521
1
Iteration 13300: Loss = -10917.232503764812
Iteration 13400: Loss = -10917.640280171787
1
Iteration 13500: Loss = -10917.232526848167
Iteration 13600: Loss = -10917.232504119045
Iteration 13700: Loss = -10917.234570764193
1
Iteration 13800: Loss = -10917.232480861232
Iteration 13900: Loss = -10917.326556445767
1
Iteration 14000: Loss = -10917.232450714055
Iteration 14100: Loss = -10917.238198745488
1
Iteration 14200: Loss = -10917.232150032713
Iteration 14300: Loss = -10917.231876594924
Iteration 14400: Loss = -10917.257310944442
1
Iteration 14500: Loss = -10917.269546652493
2
Iteration 14600: Loss = -10917.22510980159
Iteration 14700: Loss = -10917.225988623826
1
Iteration 14800: Loss = -10917.226590777214
2
Iteration 14900: Loss = -10917.234395121246
3
Iteration 15000: Loss = -10917.224592562081
Iteration 15100: Loss = -10917.225661609851
1
Iteration 15200: Loss = -10917.242569672993
2
Iteration 15300: Loss = -10917.224589941647
Iteration 15400: Loss = -10917.225287178811
1
Iteration 15500: Loss = -10917.227660162542
2
Iteration 15600: Loss = -10917.224541648226
Iteration 15700: Loss = -10917.22468847429
1
Iteration 15800: Loss = -10917.22468135612
2
Iteration 15900: Loss = -10917.224601558342
Iteration 16000: Loss = -10917.224931176213
1
Iteration 16100: Loss = -10917.225225542943
2
Iteration 16200: Loss = -10917.224269282298
Iteration 16300: Loss = -10917.22796530771
1
Iteration 16400: Loss = -10917.22482104638
2
Iteration 16500: Loss = -10917.239084712122
3
Iteration 16600: Loss = -10917.224206984652
Iteration 16700: Loss = -10917.22422561336
Iteration 16800: Loss = -10917.229113896481
1
Iteration 16900: Loss = -10917.225648454869
2
Iteration 17000: Loss = -10917.226759315161
3
Iteration 17100: Loss = -10917.406985062731
4
Iteration 17200: Loss = -10917.223778778422
Iteration 17300: Loss = -10917.224284209256
1
Iteration 17400: Loss = -10917.241329058457
2
Iteration 17500: Loss = -10917.223788183912
Iteration 17600: Loss = -10917.232305665038
1
Iteration 17700: Loss = -10917.224993216025
2
Iteration 17800: Loss = -10917.223597406883
Iteration 17900: Loss = -10917.259939632984
1
Iteration 18000: Loss = -10917.22367957147
Iteration 18100: Loss = -10917.223860852893
1
Iteration 18200: Loss = -10917.223605890467
Iteration 18300: Loss = -10917.224809956675
1
Iteration 18400: Loss = -10917.2235559278
Iteration 18500: Loss = -10917.230183488975
1
Iteration 18600: Loss = -10917.223762204425
2
Iteration 18700: Loss = -10917.229355470974
3
Iteration 18800: Loss = -10917.36824316181
4
Iteration 18900: Loss = -10917.223460631549
Iteration 19000: Loss = -10917.224103869086
1
Iteration 19100: Loss = -10917.225061761179
2
Iteration 19200: Loss = -10917.22572931475
3
Iteration 19300: Loss = -10917.227467043458
4
Iteration 19400: Loss = -10917.318629875354
5
Iteration 19500: Loss = -10917.237175010894
6
Iteration 19600: Loss = -10917.22348594522
Iteration 19700: Loss = -10917.223600898644
1
Iteration 19800: Loss = -10917.33719295385
2
Iteration 19900: Loss = -10917.223279589005
pi: tensor([[0.7795, 0.2205],
        [0.2777, 0.7223]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6080, 0.3920], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2450, 0.1004],
         [0.5035, 0.1978]],

        [[0.5503, 0.1044],
         [0.6560, 0.6484]],

        [[0.6241, 0.0995],
         [0.6375, 0.5937]],

        [[0.6168, 0.0911],
         [0.5754, 0.6260]],

        [[0.5862, 0.0958],
         [0.6856, 0.5508]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7717887095983128
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8822165801964896
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844845420066659
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844846433231073
Global Adjusted Rand Index: 0.8533964840997509
Average Adjusted Rand Index: 0.852897568978167
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21430.834821687717
Iteration 100: Loss = -11082.203513447983
Iteration 200: Loss = -11080.432768375804
Iteration 300: Loss = -11078.876758158902
Iteration 400: Loss = -11077.58604537354
Iteration 500: Loss = -11076.689831669259
Iteration 600: Loss = -11075.965601158734
Iteration 700: Loss = -11075.315613167002
Iteration 800: Loss = -11071.627546115096
Iteration 900: Loss = -11001.104650328967
Iteration 1000: Loss = -10970.465504921292
Iteration 1100: Loss = -10961.710789876426
Iteration 1200: Loss = -10960.46284621281
Iteration 1300: Loss = -10960.229508319968
Iteration 1400: Loss = -10959.869550065705
Iteration 1500: Loss = -10959.773925786492
Iteration 1600: Loss = -10959.670638969912
Iteration 1700: Loss = -10957.12399025387
Iteration 1800: Loss = -10956.522370923274
Iteration 1900: Loss = -10956.369282962849
Iteration 2000: Loss = -10956.340071533126
Iteration 2100: Loss = -10956.198408900536
Iteration 2200: Loss = -10956.182516185512
Iteration 2300: Loss = -10956.17072822592
Iteration 2400: Loss = -10956.153372445071
Iteration 2500: Loss = -10956.146993749388
Iteration 2600: Loss = -10956.145371511375
Iteration 2700: Loss = -10956.140754175765
Iteration 2800: Loss = -10956.13822348009
Iteration 2900: Loss = -10956.13540181859
Iteration 3000: Loss = -10956.131882292519
Iteration 3100: Loss = -10956.126671300392
Iteration 3200: Loss = -10956.124501454098
Iteration 3300: Loss = -10956.123523700588
Iteration 3400: Loss = -10956.12128050791
Iteration 3500: Loss = -10956.114723705852
Iteration 3600: Loss = -10955.952536591445
Iteration 3700: Loss = -10955.951129771789
Iteration 3800: Loss = -10955.949496167817
Iteration 3900: Loss = -10955.945607507865
Iteration 4000: Loss = -10955.929607289712
Iteration 4100: Loss = -10955.918751951356
Iteration 4200: Loss = -10955.918245029505
Iteration 4300: Loss = -10955.916543910245
Iteration 4400: Loss = -10955.91620327067
Iteration 4500: Loss = -10955.915834597457
Iteration 4600: Loss = -10955.915636055763
Iteration 4700: Loss = -10955.915008743264
Iteration 4800: Loss = -10955.919196182349
1
Iteration 4900: Loss = -10955.91174239046
Iteration 5000: Loss = -10955.889859281207
Iteration 5100: Loss = -10955.888510353983
Iteration 5200: Loss = -10955.661427788282
Iteration 5300: Loss = -10955.58280270019
Iteration 5400: Loss = -10955.580232369352
Iteration 5500: Loss = -10955.578097022677
Iteration 5600: Loss = -10955.576148265087
Iteration 5700: Loss = -10955.579961189145
1
Iteration 5800: Loss = -10955.503925669424
Iteration 5900: Loss = -10955.502648637468
Iteration 6000: Loss = -10955.502041477288
Iteration 6100: Loss = -10955.498822929332
Iteration 6200: Loss = -10955.49757900311
Iteration 6300: Loss = -10955.497387963665
Iteration 6400: Loss = -10955.497169440821
Iteration 6500: Loss = -10955.496832954077
Iteration 6600: Loss = -10955.495940482198
Iteration 6700: Loss = -10955.495107349228
Iteration 6800: Loss = -10955.494624786606
Iteration 6900: Loss = -10955.493898338244
Iteration 7000: Loss = -10955.492607578937
Iteration 7100: Loss = -10955.425329684937
Iteration 7200: Loss = -10955.424664575376
Iteration 7300: Loss = -10955.425559178619
1
Iteration 7400: Loss = -10955.422639958644
Iteration 7500: Loss = -10955.423198480303
1
Iteration 7600: Loss = -10955.419164080195
Iteration 7700: Loss = -10955.4398847299
1
Iteration 7800: Loss = -10955.419101699932
Iteration 7900: Loss = -10955.435382822505
1
Iteration 8000: Loss = -10955.418950738054
Iteration 8100: Loss = -10955.481026001373
1
Iteration 8200: Loss = -10955.417886876989
Iteration 8300: Loss = -10955.41768985971
Iteration 8400: Loss = -10955.417953535058
1
Iteration 8500: Loss = -10955.417574478335
Iteration 8600: Loss = -10955.441704971108
1
Iteration 8700: Loss = -10955.41747174887
Iteration 8800: Loss = -10955.417425732036
Iteration 8900: Loss = -10955.42797592886
1
Iteration 9000: Loss = -10955.417314237571
Iteration 9100: Loss = -10955.417292844215
Iteration 9200: Loss = -10955.417881433474
1
Iteration 9300: Loss = -10955.417197178087
Iteration 9400: Loss = -10955.41715417195
Iteration 9500: Loss = -10955.417400819584
1
Iteration 9600: Loss = -10955.416888437707
Iteration 9700: Loss = -10955.912462169981
1
Iteration 9800: Loss = -10955.415067843789
Iteration 9900: Loss = -10955.41495974472
Iteration 10000: Loss = -10955.462429633122
1
Iteration 10100: Loss = -10955.414808676573
Iteration 10200: Loss = -10955.414365288005
Iteration 10300: Loss = -10955.415566260353
1
Iteration 10400: Loss = -10955.41374494415
Iteration 10500: Loss = -10955.414122408796
1
Iteration 10600: Loss = -10955.413688727545
Iteration 10700: Loss = -10955.413563594624
Iteration 10800: Loss = -10955.562224491096
1
Iteration 10900: Loss = -10955.411725063619
Iteration 11000: Loss = -10955.406551858061
Iteration 11100: Loss = -10955.466981374948
1
Iteration 11200: Loss = -10955.406530432587
Iteration 11300: Loss = -10955.40652316416
Iteration 11400: Loss = -10955.490330960074
1
Iteration 11500: Loss = -10955.402875394831
Iteration 11600: Loss = -10955.401037904216
Iteration 11700: Loss = -10955.436363529587
1
Iteration 11800: Loss = -10955.400987545661
Iteration 11900: Loss = -10955.40075149032
Iteration 12000: Loss = -10955.400641960076
Iteration 12100: Loss = -10955.401377315437
1
Iteration 12200: Loss = -10955.40052948151
Iteration 12300: Loss = -10955.39970066443
Iteration 12400: Loss = -10955.400141657507
1
Iteration 12500: Loss = -10955.39965051598
Iteration 12600: Loss = -10955.399623282312
Iteration 12700: Loss = -10955.40296648932
1
Iteration 12800: Loss = -10955.39893130109
Iteration 12900: Loss = -10955.398929660714
Iteration 13000: Loss = -10955.399019756387
Iteration 13100: Loss = -10955.398894369051
Iteration 13200: Loss = -10955.401127400788
1
Iteration 13300: Loss = -10955.398853291314
Iteration 13400: Loss = -10955.398785824085
Iteration 13500: Loss = -10955.5344987731
1
Iteration 13600: Loss = -10955.398727303455
Iteration 13700: Loss = -10955.398680783026
Iteration 13800: Loss = -10955.399897839336
1
Iteration 13900: Loss = -10955.398605931634
Iteration 14000: Loss = -10955.619318723138
1
Iteration 14100: Loss = -10955.398597733905
Iteration 14200: Loss = -10955.398527311594
Iteration 14300: Loss = -10955.459158153486
1
Iteration 14400: Loss = -10955.398445892872
Iteration 14500: Loss = -10955.398400516047
Iteration 14600: Loss = -10955.399469058488
1
Iteration 14700: Loss = -10955.398373327305
Iteration 14800: Loss = -10955.398364808178
Iteration 14900: Loss = -10955.39837007101
Iteration 15000: Loss = -10955.39618166362
Iteration 15100: Loss = -10955.396044161127
Iteration 15200: Loss = -10955.396135361603
Iteration 15300: Loss = -10955.396037403641
Iteration 15400: Loss = -10955.723735355456
1
Iteration 15500: Loss = -10955.396042371338
Iteration 15600: Loss = -10955.396000057746
Iteration 15700: Loss = -10955.398015957468
1
Iteration 15800: Loss = -10955.39602189541
Iteration 15900: Loss = -10955.402693151249
1
Iteration 16000: Loss = -10955.396058553435
Iteration 16100: Loss = -10955.39599140917
Iteration 16200: Loss = -10955.452171908853
1
Iteration 16300: Loss = -10955.39601358635
Iteration 16400: Loss = -10955.396042145052
Iteration 16500: Loss = -10955.398582192223
1
Iteration 16600: Loss = -10955.396015646076
Iteration 16700: Loss = -10955.396740334969
1
Iteration 16800: Loss = -10955.396038362238
Iteration 16900: Loss = -10955.395999818382
Iteration 17000: Loss = -10955.498042196688
1
Iteration 17100: Loss = -10955.395996065772
Iteration 17200: Loss = -10955.395998920925
Iteration 17300: Loss = -10955.397614000009
1
Iteration 17400: Loss = -10955.395978487762
Iteration 17500: Loss = -10955.406733867063
1
Iteration 17600: Loss = -10955.396046196296
Iteration 17700: Loss = -10955.396009019343
Iteration 17800: Loss = -10955.402313273862
1
Iteration 17900: Loss = -10955.395993038099
Iteration 18000: Loss = -10955.395975757916
Iteration 18100: Loss = -10955.396219606773
1
Iteration 18200: Loss = -10955.395991458745
Iteration 18300: Loss = -10955.396081807276
Iteration 18400: Loss = -10955.39602006948
Iteration 18500: Loss = -10955.395984103994
Iteration 18600: Loss = -10955.408945784515
1
Iteration 18700: Loss = -10955.395994259183
Iteration 18800: Loss = -10955.396028493244
Iteration 18900: Loss = -10955.396027768296
Iteration 19000: Loss = -10955.395570182036
Iteration 19100: Loss = -10955.500647373763
1
Iteration 19200: Loss = -10955.395500780967
Iteration 19300: Loss = -10955.39546536762
Iteration 19400: Loss = -10955.39692794931
1
Iteration 19500: Loss = -10955.395464766352
Iteration 19600: Loss = -10955.396142357313
1
Iteration 19700: Loss = -10955.395497002695
Iteration 19800: Loss = -10955.395477248398
Iteration 19900: Loss = -10955.395896861175
1
pi: tensor([[0.7334, 0.2666],
        [0.4207, 0.5793]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0733, 0.9267], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2533, 0.0855],
         [0.7135, 0.1844]],

        [[0.6910, 0.1065],
         [0.6642, 0.5882]],

        [[0.6451, 0.0999],
         [0.6255, 0.5977]],

        [[0.5261, 0.0917],
         [0.6790, 0.7094]],

        [[0.6572, 0.0963],
         [0.6843, 0.6204]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.08691034712869111
time is 1
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844845420066659
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080740404436667
Global Adjusted Rand Index: 0.4504155138992109
Average Adjusted Rand Index: 0.7085928162799456
10965.021113698027
[0.8533964840997509, 0.4504155138992109] [0.852897568978167, 0.7085928162799456] [10917.226462836421, 10955.39549493433]
-------------------------------------
This iteration is 46
True Objective function: Loss = -10977.411861841018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22420.455032935104
Iteration 100: Loss = -11015.1531507018
Iteration 200: Loss = -11013.87262781927
Iteration 300: Loss = -11013.19061796064
Iteration 400: Loss = -11012.043614782027
Iteration 500: Loss = -11011.821499671827
Iteration 600: Loss = -11011.684313350546
Iteration 700: Loss = -11011.581365577626
Iteration 800: Loss = -11011.507494517216
Iteration 900: Loss = -11011.449060925448
Iteration 1000: Loss = -11011.401176775022
Iteration 1100: Loss = -11011.35839024082
Iteration 1200: Loss = -11011.319463951988
Iteration 1300: Loss = -11011.28320367457
Iteration 1400: Loss = -11011.246846187165
Iteration 1500: Loss = -11011.20700892326
Iteration 1600: Loss = -11011.159733958713
Iteration 1700: Loss = -11011.101054391656
Iteration 1800: Loss = -11011.028843022588
Iteration 1900: Loss = -11010.943701872737
Iteration 2000: Loss = -11010.850763798124
Iteration 2100: Loss = -11010.75270565846
Iteration 2200: Loss = -11010.667960728677
Iteration 2300: Loss = -11010.611799161941
Iteration 2400: Loss = -11010.571718143883
Iteration 2500: Loss = -11010.545722507892
Iteration 2600: Loss = -11010.525786914946
Iteration 2700: Loss = -11010.507711698632
Iteration 2800: Loss = -11010.489421965174
Iteration 2900: Loss = -11010.469742732757
Iteration 3000: Loss = -11010.448185565148
Iteration 3100: Loss = -11010.424737102143
Iteration 3200: Loss = -11010.399943018878
Iteration 3300: Loss = -11010.374466361185
Iteration 3400: Loss = -11010.349094228604
Iteration 3500: Loss = -11010.32434660705
Iteration 3600: Loss = -11010.300412703733
Iteration 3700: Loss = -11010.277326099238
Iteration 3800: Loss = -11010.254955955028
Iteration 3900: Loss = -11010.233321810165
Iteration 4000: Loss = -11010.21226399984
Iteration 4100: Loss = -11010.191701300102
Iteration 4200: Loss = -11010.17156295328
Iteration 4300: Loss = -11010.151794282103
Iteration 4400: Loss = -11010.13230788959
Iteration 4500: Loss = -11010.112975630862
Iteration 4600: Loss = -11010.093592033762
Iteration 4700: Loss = -11010.073733801582
Iteration 4800: Loss = -11010.051586882842
Iteration 4900: Loss = -11009.999989208067
Iteration 5000: Loss = -11009.831108371249
Iteration 5100: Loss = -11009.683520574976
Iteration 5200: Loss = -11009.561032632568
Iteration 5300: Loss = -11009.466116886875
Iteration 5400: Loss = -11009.394833436007
Iteration 5500: Loss = -11009.341386678585
Iteration 5600: Loss = -11009.300826710274
Iteration 5700: Loss = -11009.269474202878
Iteration 5800: Loss = -11009.244737272347
Iteration 5900: Loss = -11009.224833571217
Iteration 6000: Loss = -11009.208521281598
Iteration 6100: Loss = -11009.194933916371
Iteration 6200: Loss = -11009.183534500682
Iteration 6300: Loss = -11009.173751251285
Iteration 6400: Loss = -11009.165375442008
Iteration 6500: Loss = -11009.158101145767
Iteration 6600: Loss = -11009.151702228035
Iteration 6700: Loss = -11009.146025081183
Iteration 6800: Loss = -11009.14105419684
Iteration 6900: Loss = -11009.136588188028
Iteration 7000: Loss = -11009.132585466412
Iteration 7100: Loss = -11009.128995360554
Iteration 7200: Loss = -11009.126245500582
Iteration 7300: Loss = -11009.1227742622
Iteration 7400: Loss = -11009.12012153396
Iteration 7500: Loss = -11009.117690155812
Iteration 7600: Loss = -11009.115464744713
Iteration 7700: Loss = -11009.113641864576
Iteration 7800: Loss = -11009.112075661777
Iteration 7900: Loss = -11009.10980932727
Iteration 8000: Loss = -11009.108457362538
Iteration 8100: Loss = -11009.10676201562
Iteration 8200: Loss = -11009.105390743664
Iteration 8300: Loss = -11009.10616745035
1
Iteration 8400: Loss = -11009.103097065281
Iteration 8500: Loss = -11009.101938774795
Iteration 8600: Loss = -11009.101011306351
Iteration 8700: Loss = -11009.100002654037
Iteration 8800: Loss = -11009.10789637115
1
Iteration 8900: Loss = -11009.098348539626
Iteration 9000: Loss = -11009.097552807087
Iteration 9100: Loss = -11009.096933625817
Iteration 9200: Loss = -11009.096536568633
Iteration 9300: Loss = -11009.095591074702
Iteration 9400: Loss = -11009.23115915959
1
Iteration 9500: Loss = -11009.094474274314
Iteration 9600: Loss = -11009.162771297475
1
Iteration 9700: Loss = -11009.093477113403
Iteration 9800: Loss = -11009.102042583534
1
Iteration 9900: Loss = -11009.092594680518
Iteration 10000: Loss = -11009.101616041853
1
Iteration 10100: Loss = -11009.091869173702
Iteration 10200: Loss = -11009.092244793248
1
Iteration 10300: Loss = -11009.091187019148
Iteration 10400: Loss = -11009.092631291232
1
Iteration 10500: Loss = -11009.090573183514
Iteration 10600: Loss = -11009.09029120735
Iteration 10700: Loss = -11009.090190800556
Iteration 10800: Loss = -11009.089785053644
Iteration 10900: Loss = -11009.090056185943
1
Iteration 11000: Loss = -11009.08932938665
Iteration 11100: Loss = -11009.096505590029
1
Iteration 11200: Loss = -11009.08894059314
Iteration 11300: Loss = -11009.089004493862
Iteration 11400: Loss = -11009.088639442725
Iteration 11500: Loss = -11009.088429727784
Iteration 11600: Loss = -11009.088536478592
1
Iteration 11700: Loss = -11009.088088110353
Iteration 11800: Loss = -11009.090809073277
1
Iteration 11900: Loss = -11009.087890021117
Iteration 12000: Loss = -11009.383818102366
1
Iteration 12100: Loss = -11009.087614420358
Iteration 12200: Loss = -11009.087476317602
Iteration 12300: Loss = -11009.088210691118
1
Iteration 12400: Loss = -11009.087276864397
Iteration 12500: Loss = -11009.096900206021
1
Iteration 12600: Loss = -11009.08713449908
Iteration 12700: Loss = -11009.089329178927
1
Iteration 12800: Loss = -11009.08695289298
Iteration 12900: Loss = -11009.087534254702
1
Iteration 13000: Loss = -11009.086786332846
Iteration 13100: Loss = -11009.09218634316
1
Iteration 13200: Loss = -11009.086651808848
Iteration 13300: Loss = -11009.086563176013
Iteration 13400: Loss = -11009.086699534182
1
Iteration 13500: Loss = -11009.086528843713
Iteration 13600: Loss = -11009.086443293158
Iteration 13700: Loss = -11009.093161382423
1
Iteration 13800: Loss = -11009.086341680804
Iteration 13900: Loss = -11009.086284487046
Iteration 14000: Loss = -11009.136289083102
1
Iteration 14100: Loss = -11009.086191030407
Iteration 14200: Loss = -11009.086149497156
Iteration 14300: Loss = -11009.08614278762
Iteration 14400: Loss = -11009.086322389527
1
Iteration 14500: Loss = -11009.086101035644
Iteration 14600: Loss = -11009.086048851383
Iteration 14700: Loss = -11009.089535494162
1
Iteration 14800: Loss = -11009.08600930952
Iteration 14900: Loss = -11009.085961438372
Iteration 15000: Loss = -11009.187294086209
1
Iteration 15100: Loss = -11009.085963182231
Iteration 15200: Loss = -11009.085973419165
Iteration 15300: Loss = -11009.086187000792
1
Iteration 15400: Loss = -11009.085968287734
Iteration 15500: Loss = -11009.085904184885
Iteration 15600: Loss = -11009.087020078456
1
Iteration 15700: Loss = -11009.08589501421
Iteration 15800: Loss = -11009.086184404268
1
Iteration 15900: Loss = -11009.167095539751
2
Iteration 16000: Loss = -11009.08603138195
3
Iteration 16100: Loss = -11009.085807389501
Iteration 16200: Loss = -11009.125556784069
1
Iteration 16300: Loss = -11009.087044238235
2
Iteration 16400: Loss = -11009.094009349095
3
Iteration 16500: Loss = -11009.08579663939
Iteration 16600: Loss = -11009.08581775117
Iteration 16700: Loss = -11009.085892282055
Iteration 16800: Loss = -11009.085780278005
Iteration 16900: Loss = -11009.098633550731
1
Iteration 17000: Loss = -11009.08579684681
Iteration 17100: Loss = -11009.085759364512
Iteration 17200: Loss = -11009.129141546167
1
Iteration 17300: Loss = -11009.085759870568
Iteration 17400: Loss = -11009.085739427064
Iteration 17500: Loss = -11009.090095610665
1
Iteration 17600: Loss = -11009.085707394093
Iteration 17700: Loss = -11009.085741339919
Iteration 17800: Loss = -11009.086522487476
1
Iteration 17900: Loss = -11009.08616597462
2
Iteration 18000: Loss = -11009.087102392496
3
Iteration 18100: Loss = -11009.085739132715
Iteration 18200: Loss = -11009.097186732768
1
Iteration 18300: Loss = -11009.085972832176
2
Iteration 18400: Loss = -11009.090794684655
3
Iteration 18500: Loss = -11009.085974723861
4
Iteration 18600: Loss = -11009.086115959057
5
Iteration 18700: Loss = -11009.09553879896
6
Iteration 18800: Loss = -11009.086455817807
7
Iteration 18900: Loss = -11009.087013121241
8
Iteration 19000: Loss = -11009.085653317352
Iteration 19100: Loss = -11009.087774391515
1
Iteration 19200: Loss = -11009.315970013447
2
Iteration 19300: Loss = -11009.085611531262
Iteration 19400: Loss = -11009.09970371072
1
Iteration 19500: Loss = -11009.085595478437
Iteration 19600: Loss = -11009.126540322513
1
Iteration 19700: Loss = -11009.085686485421
Iteration 19800: Loss = -11009.08680963655
1
Iteration 19900: Loss = -11009.085800249019
2
pi: tensor([[2.8337e-01, 7.1663e-01],
        [1.6274e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0471, 0.9529], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.0000, 0.2213],
         [0.5719, 0.1615]],

        [[0.5088, 0.2626],
         [0.5351, 0.6081]],

        [[0.7074, 0.1212],
         [0.6836, 0.5909]],

        [[0.7257, 0.2020],
         [0.5795, 0.5018]],

        [[0.7103, 0.1111],
         [0.6500, 0.5898]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.016449682236070833
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008264788662843189
Average Adjusted Rand Index: -0.0035977323400627217
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22815.194210444955
Iteration 100: Loss = -11015.517404224402
Iteration 200: Loss = -11014.20757930596
Iteration 300: Loss = -11013.536338717362
Iteration 400: Loss = -11012.363986517765
Iteration 500: Loss = -11012.136982190714
Iteration 600: Loss = -11012.037984099241
Iteration 700: Loss = -11011.883560662896
Iteration 800: Loss = -11011.62256397842
Iteration 900: Loss = -11011.486606661401
Iteration 1000: Loss = -11011.414225224968
Iteration 1100: Loss = -11011.36375180512
Iteration 1200: Loss = -11011.3216146419
Iteration 1300: Loss = -11011.281896854012
Iteration 1400: Loss = -11011.241213382382
Iteration 1500: Loss = -11011.196087830229
Iteration 1600: Loss = -11011.14270432955
Iteration 1700: Loss = -11011.075455764018
Iteration 1800: Loss = -11010.987336521132
Iteration 1900: Loss = -11010.876802347568
Iteration 2000: Loss = -11010.760934813392
Iteration 2100: Loss = -11010.672164173828
Iteration 2200: Loss = -11010.61924579773
Iteration 2300: Loss = -11010.59052176123
Iteration 2400: Loss = -11010.572434816411
Iteration 2500: Loss = -11010.559264094205
Iteration 2600: Loss = -11010.547622684795
Iteration 2700: Loss = -11010.534651759626
Iteration 2800: Loss = -11010.525619177793
Iteration 2900: Loss = -11010.518868917075
Iteration 3000: Loss = -11010.512367223439
Iteration 3100: Loss = -11010.506257506846
Iteration 3200: Loss = -11010.501092314436
Iteration 3300: Loss = -11010.496473189673
Iteration 3400: Loss = -11010.491771125091
Iteration 3500: Loss = -11010.486522748548
Iteration 3600: Loss = -11010.480290301168
Iteration 3700: Loss = -11010.47275767153
Iteration 3800: Loss = -11010.46334288697
Iteration 3900: Loss = -11010.451274715708
Iteration 4000: Loss = -11010.435502957444
Iteration 4100: Loss = -11010.414215102972
Iteration 4200: Loss = -11010.383616821355
Iteration 4300: Loss = -11010.33571357517
Iteration 4400: Loss = -11010.263024023228
Iteration 4500: Loss = -11010.178151310954
Iteration 4600: Loss = -11010.092274926625
Iteration 4700: Loss = -11010.003241230736
Iteration 4800: Loss = -11009.907520652543
Iteration 4900: Loss = -11009.806199612973
Iteration 5000: Loss = -11009.705460747406
Iteration 5100: Loss = -11009.612606274324
Iteration 5200: Loss = -11009.532250106857
Iteration 5300: Loss = -11009.465522197956
Iteration 5400: Loss = -11009.411364784904
Iteration 5500: Loss = -11009.367773048185
Iteration 5600: Loss = -11009.332719592681
Iteration 5700: Loss = -11009.304277031393
Iteration 5800: Loss = -11009.28096409473
Iteration 5900: Loss = -11009.261715737419
Iteration 6000: Loss = -11009.245419682336
Iteration 6100: Loss = -11009.231457986914
Iteration 6200: Loss = -11009.219089326587
Iteration 6300: Loss = -11009.207543286959
Iteration 6400: Loss = -11009.196097821905
Iteration 6500: Loss = -11009.184726302929
Iteration 6600: Loss = -11009.17448343042
Iteration 6700: Loss = -11009.165652874251
Iteration 6800: Loss = -11009.160677260921
Iteration 6900: Loss = -11009.151345830325
Iteration 7000: Loss = -11009.145538995994
Iteration 7100: Loss = -11009.140475261493
Iteration 7200: Loss = -11009.135868116346
Iteration 7300: Loss = -11009.13185666576
Iteration 7400: Loss = -11009.128310416645
Iteration 7500: Loss = -11009.124949139285
Iteration 7600: Loss = -11009.12194931207
Iteration 7700: Loss = -11009.121420357085
Iteration 7800: Loss = -11009.119030917998
Iteration 7900: Loss = -11009.114644386354
Iteration 8000: Loss = -11009.437967153413
1
Iteration 8100: Loss = -11009.110779672721
Iteration 8200: Loss = -11009.1090854866
Iteration 8300: Loss = -11009.11350227065
1
Iteration 8400: Loss = -11009.106029854018
Iteration 8500: Loss = -11009.10473153167
Iteration 8600: Loss = -11009.11322080441
1
Iteration 8700: Loss = -11009.10238134495
Iteration 8800: Loss = -11009.101324493842
Iteration 8900: Loss = -11009.132424814346
1
Iteration 9000: Loss = -11009.099422302166
Iteration 9100: Loss = -11009.110242525965
1
Iteration 9200: Loss = -11009.097811845975
Iteration 9300: Loss = -11009.097080242482
Iteration 9400: Loss = -11009.106948477825
1
Iteration 9500: Loss = -11009.095735940664
Iteration 9600: Loss = -11009.095140986527
Iteration 9700: Loss = -11009.104655703308
1
Iteration 9800: Loss = -11009.094101418521
Iteration 9900: Loss = -11009.09359051148
Iteration 10000: Loss = -11009.11944322016
1
Iteration 10100: Loss = -11009.092699870367
Iteration 10200: Loss = -11009.098337526872
1
Iteration 10300: Loss = -11009.09194245265
Iteration 10400: Loss = -11009.216782872229
1
Iteration 10500: Loss = -11009.091251486409
Iteration 10600: Loss = -11009.138563703395
1
Iteration 10700: Loss = -11009.090578922442
Iteration 10800: Loss = -11009.09048314012
Iteration 10900: Loss = -11009.090089641852
Iteration 11000: Loss = -11009.1767471272
1
Iteration 11100: Loss = -11009.089583881523
Iteration 11200: Loss = -11009.089403605309
Iteration 11300: Loss = -11009.089165448377
Iteration 11400: Loss = -11009.08898307018
Iteration 11500: Loss = -11009.08896906433
Iteration 11600: Loss = -11009.088612823965
Iteration 11700: Loss = -11009.088614409204
Iteration 11800: Loss = -11009.088277639805
Iteration 11900: Loss = -11009.088317565473
Iteration 12000: Loss = -11009.090596243677
1
Iteration 12100: Loss = -11009.088271339811
Iteration 12200: Loss = -11009.114084456072
1
Iteration 12300: Loss = -11009.087652869857
Iteration 12400: Loss = -11009.113811095967
1
Iteration 12500: Loss = -11009.087393663165
Iteration 12600: Loss = -11009.087266507988
Iteration 12700: Loss = -11009.087195327693
Iteration 12800: Loss = -11009.087103385778
Iteration 12900: Loss = -11009.377366707084
1
Iteration 13000: Loss = -11009.086934288336
Iteration 13100: Loss = -11009.086902370283
Iteration 13200: Loss = -11009.086774074234
Iteration 13300: Loss = -11009.087328618112
1
Iteration 13400: Loss = -11009.086639052332
Iteration 13500: Loss = -11009.091932078334
1
Iteration 13600: Loss = -11009.086557327693
Iteration 13700: Loss = -11009.086521965226
Iteration 13800: Loss = -11009.086536500781
Iteration 13900: Loss = -11009.086366264391
Iteration 14000: Loss = -11009.086362128548
Iteration 14100: Loss = -11009.087117906709
1
Iteration 14200: Loss = -11009.086266455437
Iteration 14300: Loss = -11009.107366329803
1
Iteration 14400: Loss = -11009.086204385469
Iteration 14500: Loss = -11009.086179175705
Iteration 14600: Loss = -11009.08886330337
1
Iteration 14700: Loss = -11009.086098230571
Iteration 14800: Loss = -11009.086091221354
Iteration 14900: Loss = -11009.086539251406
1
Iteration 15000: Loss = -11009.086069500187
Iteration 15100: Loss = -11009.086548368046
1
Iteration 15200: Loss = -11009.086181812512
2
Iteration 15300: Loss = -11009.09563341806
3
Iteration 15400: Loss = -11009.085968549614
Iteration 15500: Loss = -11009.10072622268
1
Iteration 15600: Loss = -11009.085886792087
Iteration 15700: Loss = -11009.085917514762
Iteration 15800: Loss = -11009.085916443075
Iteration 15900: Loss = -11009.085828427962
Iteration 16000: Loss = -11009.085845907812
Iteration 16100: Loss = -11009.08587801662
Iteration 16200: Loss = -11009.085785066894
Iteration 16300: Loss = -11009.08659027357
1
Iteration 16400: Loss = -11009.087222927434
2
Iteration 16500: Loss = -11009.085791214544
Iteration 16600: Loss = -11009.086254079179
1
Iteration 16700: Loss = -11009.106203376568
2
Iteration 16800: Loss = -11009.085754771633
Iteration 16900: Loss = -11009.08655676258
1
Iteration 17000: Loss = -11009.12393626647
2
Iteration 17100: Loss = -11009.08569364739
Iteration 17200: Loss = -11009.087366314961
1
Iteration 17300: Loss = -11009.085717785683
Iteration 17400: Loss = -11009.08578317509
Iteration 17500: Loss = -11009.086059174837
1
Iteration 17600: Loss = -11009.087112545436
2
Iteration 17700: Loss = -11009.08569630611
Iteration 17800: Loss = -11009.129972294832
1
Iteration 17900: Loss = -11009.085709149324
Iteration 18000: Loss = -11009.08569576627
Iteration 18100: Loss = -11009.08607947324
1
Iteration 18200: Loss = -11009.086244290675
2
Iteration 18300: Loss = -11009.252402203983
3
Iteration 18400: Loss = -11009.08568732729
Iteration 18500: Loss = -11009.085853668175
1
Iteration 18600: Loss = -11009.088153262594
2
Iteration 18700: Loss = -11009.085674691803
Iteration 18800: Loss = -11009.090088023082
1
Iteration 18900: Loss = -11009.085669279237
Iteration 19000: Loss = -11009.085679400476
Iteration 19100: Loss = -11009.095940108024
1
Iteration 19200: Loss = -11009.09211064006
2
Iteration 19300: Loss = -11009.085680741671
Iteration 19400: Loss = -11009.096723542652
1
Iteration 19500: Loss = -11009.085666119745
Iteration 19600: Loss = -11009.085707944037
Iteration 19700: Loss = -11009.085742872468
Iteration 19800: Loss = -11009.08578806193
Iteration 19900: Loss = -11009.085664664293
pi: tensor([[2.8275e-01, 7.1725e-01],
        [1.9971e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0473, 0.9527], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.0000, 0.2210],
         [0.6688, 0.1615]],

        [[0.6496, 0.2626],
         [0.6303, 0.6052]],

        [[0.5238, 0.1212],
         [0.5775, 0.5502]],

        [[0.5903, 0.1993],
         [0.6569, 0.5101]],

        [[0.6320, 0.1111],
         [0.6794, 0.5631]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.016449682236070833
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008264788662843189
Average Adjusted Rand Index: -0.0035977323400627217
10977.411861841018
[-0.0008264788662843189, -0.0008264788662843189] [-0.0035977323400627217, -0.0035977323400627217] [11009.085579395967, 11009.092894159576]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11145.975379451182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22247.69355619406
Iteration 100: Loss = -11279.979001858033
Iteration 200: Loss = -11279.300131280977
Iteration 300: Loss = -11279.104866862579
Iteration 400: Loss = -11278.95577049811
Iteration 500: Loss = -11278.77823929236
Iteration 600: Loss = -11278.617859668584
Iteration 700: Loss = -11278.53239032459
Iteration 800: Loss = -11278.481980776842
Iteration 900: Loss = -11278.441309149206
Iteration 1000: Loss = -11278.402637565847
Iteration 1100: Loss = -11278.363969079332
Iteration 1200: Loss = -11278.324860283468
Iteration 1300: Loss = -11278.285985545426
Iteration 1400: Loss = -11278.24791360154
Iteration 1500: Loss = -11278.211299282086
Iteration 1600: Loss = -11278.176442824968
Iteration 1700: Loss = -11278.143438351079
Iteration 1800: Loss = -11278.112221396477
Iteration 1900: Loss = -11278.082960954604
Iteration 2000: Loss = -11278.055671321072
Iteration 2100: Loss = -11278.03050663093
Iteration 2200: Loss = -11278.007492785435
Iteration 2300: Loss = -11277.986769674048
Iteration 2400: Loss = -11277.96823854801
Iteration 2500: Loss = -11277.951891926301
Iteration 2600: Loss = -11277.937498871048
Iteration 2700: Loss = -11277.924786616546
Iteration 2800: Loss = -11277.91330296668
Iteration 2900: Loss = -11277.902791733037
Iteration 3000: Loss = -11277.892546695286
Iteration 3100: Loss = -11277.881989001298
Iteration 3200: Loss = -11277.869944660848
Iteration 3300: Loss = -11277.854543356363
Iteration 3400: Loss = -11277.832280030554
Iteration 3500: Loss = -11277.795796645913
Iteration 3600: Loss = -11277.72740158288
Iteration 3700: Loss = -11277.595547680436
Iteration 3800: Loss = -11277.411470276038
Iteration 3900: Loss = -11277.228833699755
Iteration 4000: Loss = -11277.075583343221
Iteration 4100: Loss = -11276.971441165757
Iteration 4200: Loss = -11276.901896933901
Iteration 4300: Loss = -11276.848963228585
Iteration 4400: Loss = -11276.798933347078
Iteration 4500: Loss = -11276.73882877666
Iteration 4600: Loss = -11276.662294451115
Iteration 4700: Loss = -11276.58699405855
Iteration 4800: Loss = -11276.531485678326
Iteration 4900: Loss = -11276.49441380688
Iteration 5000: Loss = -11276.469362198777
Iteration 5100: Loss = -11276.45159405762
Iteration 5200: Loss = -11276.43847899093
Iteration 5300: Loss = -11276.428438704115
Iteration 5400: Loss = -11276.420517484974
Iteration 5500: Loss = -11276.414154467639
Iteration 5600: Loss = -11276.408886680068
Iteration 5700: Loss = -11276.404540183712
Iteration 5800: Loss = -11276.400839141013
Iteration 5900: Loss = -11276.397704471417
Iteration 6000: Loss = -11276.394960776146
Iteration 6100: Loss = -11276.392557952002
Iteration 6200: Loss = -11276.390466771627
Iteration 6300: Loss = -11276.388569833787
Iteration 6400: Loss = -11276.386995522635
Iteration 6500: Loss = -11276.385495855673
Iteration 6600: Loss = -11276.384212497616
Iteration 6700: Loss = -11276.382998711533
Iteration 6800: Loss = -11276.3819264426
Iteration 6900: Loss = -11276.380908782558
Iteration 7000: Loss = -11276.380054866431
Iteration 7100: Loss = -11276.379213093858
Iteration 7200: Loss = -11276.378497346224
Iteration 7300: Loss = -11276.404613037941
1
Iteration 7400: Loss = -11276.377166138773
Iteration 7500: Loss = -11276.376580694154
Iteration 7600: Loss = -11276.524493664781
1
Iteration 7700: Loss = -11276.375536297546
Iteration 7800: Loss = -11276.375053905866
Iteration 7900: Loss = -11276.379939336775
1
Iteration 8000: Loss = -11276.374219978921
Iteration 8100: Loss = -11276.373861748123
Iteration 8200: Loss = -11276.777602366625
1
Iteration 8300: Loss = -11276.37318181158
Iteration 8400: Loss = -11276.372871650983
Iteration 8500: Loss = -11276.554655899508
1
Iteration 8600: Loss = -11276.372359426272
Iteration 8700: Loss = -11276.372113985362
Iteration 8800: Loss = -11276.476099599186
1
Iteration 8900: Loss = -11276.371629036048
Iteration 9000: Loss = -11276.37143663976
Iteration 9100: Loss = -11276.866123345722
1
Iteration 9200: Loss = -11276.371071575093
Iteration 9300: Loss = -11276.370872710597
Iteration 9400: Loss = -11276.370733339654
Iteration 9500: Loss = -11276.37430847767
1
Iteration 9600: Loss = -11276.370435399713
Iteration 9700: Loss = -11276.370286063046
Iteration 9800: Loss = -11276.461934094714
1
Iteration 9900: Loss = -11276.370076399246
Iteration 10000: Loss = -11276.369968389366
Iteration 10100: Loss = -11276.411730078069
1
Iteration 10200: Loss = -11276.369758093813
Iteration 10300: Loss = -11276.369651461335
Iteration 10400: Loss = -11276.369994357814
1
Iteration 10500: Loss = -11276.369480684521
Iteration 10600: Loss = -11276.369416776379
Iteration 10700: Loss = -11276.49109491754
1
Iteration 10800: Loss = -11276.369273360413
Iteration 10900: Loss = -11276.36916029197
Iteration 11000: Loss = -11276.373296093147
1
Iteration 11100: Loss = -11276.369086577166
Iteration 11200: Loss = -11276.36898515244
Iteration 11300: Loss = -11276.803716211945
1
Iteration 11400: Loss = -11276.368895486463
Iteration 11500: Loss = -11276.368842307662
Iteration 11600: Loss = -11276.403825540201
1
Iteration 11700: Loss = -11276.36876012267
Iteration 11800: Loss = -11276.427050201928
1
Iteration 11900: Loss = -11276.368703311577
Iteration 12000: Loss = -11276.368676687045
Iteration 12100: Loss = -11276.532523641597
1
Iteration 12200: Loss = -11276.368548904795
Iteration 12300: Loss = -11276.369167040148
1
Iteration 12400: Loss = -11276.368479319284
Iteration 12500: Loss = -11276.3686135407
1
Iteration 12600: Loss = -11276.375046206289
2
Iteration 12700: Loss = -11276.36843432164
Iteration 12800: Loss = -11276.402668396282
1
Iteration 12900: Loss = -11276.368344354176
Iteration 13000: Loss = -11276.372326047864
1
Iteration 13100: Loss = -11276.368312290995
Iteration 13200: Loss = -11276.368653816553
1
Iteration 13300: Loss = -11276.36888584069
2
Iteration 13400: Loss = -11276.371515696334
3
Iteration 13500: Loss = -11276.381888071883
4
Iteration 13600: Loss = -11276.368267901718
Iteration 13700: Loss = -11276.422490387002
1
Iteration 13800: Loss = -11276.368192165504
Iteration 13900: Loss = -11276.368613862605
1
Iteration 14000: Loss = -11276.401288741532
2
Iteration 14100: Loss = -11276.36816039866
Iteration 14200: Loss = -11276.37230824754
1
Iteration 14300: Loss = -11276.371160215702
2
Iteration 14400: Loss = -11276.385905011593
3
Iteration 14500: Loss = -11276.394275794717
4
Iteration 14600: Loss = -11276.368142528001
Iteration 14700: Loss = -11276.368362720776
1
Iteration 14800: Loss = -11276.368442453675
2
Iteration 14900: Loss = -11276.36812770674
Iteration 15000: Loss = -11276.368087472392
Iteration 15100: Loss = -11276.368275452993
1
Iteration 15200: Loss = -11276.368060223533
Iteration 15300: Loss = -11276.381076422174
1
Iteration 15400: Loss = -11276.368050788793
Iteration 15500: Loss = -11276.376442521232
1
Iteration 15600: Loss = -11276.368017702649
Iteration 15700: Loss = -11276.371962730098
1
Iteration 15800: Loss = -11276.449208282262
2
Iteration 15900: Loss = -11276.368000293554
Iteration 16000: Loss = -11276.371576773043
1
Iteration 16100: Loss = -11276.367989481429
Iteration 16200: Loss = -11276.36817122874
1
Iteration 16300: Loss = -11276.368005755096
Iteration 16400: Loss = -11276.368178176675
1
Iteration 16500: Loss = -11276.419463971424
2
Iteration 16600: Loss = -11276.367969737023
Iteration 16700: Loss = -11276.368634037099
1
Iteration 16800: Loss = -11276.367960557276
Iteration 16900: Loss = -11276.43802655053
1
Iteration 17000: Loss = -11276.367956673732
Iteration 17100: Loss = -11276.376809989
1
Iteration 17200: Loss = -11276.368171818795
2
Iteration 17300: Loss = -11276.367991163626
Iteration 17400: Loss = -11276.42550347019
1
Iteration 17500: Loss = -11276.367974261
Iteration 17600: Loss = -11276.369202780224
1
Iteration 17700: Loss = -11276.367991857363
Iteration 17800: Loss = -11276.368503684425
1
Iteration 17900: Loss = -11276.368117630866
2
Iteration 18000: Loss = -11276.368681706213
3
Iteration 18100: Loss = -11276.367986930934
Iteration 18200: Loss = -11276.372790375155
1
Iteration 18300: Loss = -11276.369936636405
2
Iteration 18400: Loss = -11276.371108114277
3
Iteration 18500: Loss = -11276.375706194844
4
Iteration 18600: Loss = -11276.368023940091
Iteration 18700: Loss = -11276.367968026141
Iteration 18800: Loss = -11276.367943971542
Iteration 18900: Loss = -11276.367933488174
Iteration 19000: Loss = -11276.368908225742
1
Iteration 19100: Loss = -11276.367912507887
Iteration 19200: Loss = -11276.36919872526
1
Iteration 19300: Loss = -11276.36961585976
2
Iteration 19400: Loss = -11276.36793496826
Iteration 19500: Loss = -11276.36915110355
1
Iteration 19600: Loss = -11276.367934741465
Iteration 19700: Loss = -11276.40245621432
1
Iteration 19800: Loss = -11276.367973462297
Iteration 19900: Loss = -11276.368013394043
pi: tensor([[1.0000e+00, 3.2724e-06],
        [5.8636e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0574, 0.9426], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1859, 0.1887],
         [0.5983, 0.1674]],

        [[0.5873, 0.1933],
         [0.7260, 0.7239]],

        [[0.6259, 0.2261],
         [0.6514, 0.5044]],

        [[0.6782, 0.1169],
         [0.6824, 0.6482]],

        [[0.5951, 0.2017],
         [0.5891, 0.7009]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.012897625100762696
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.001684040076915292
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002821183194503557
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.002821183194503557
Global Adjusted Rand Index: -0.0022351446611192324
Average Adjusted Rand Index: -0.003675612911859437
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25396.420942095956
Iteration 100: Loss = -11279.355943736775
Iteration 200: Loss = -11278.777312260612
Iteration 300: Loss = -11278.702353707604
Iteration 400: Loss = -11278.66268354394
Iteration 500: Loss = -11278.622094631233
Iteration 600: Loss = -11278.57646149637
Iteration 700: Loss = -11278.526348757448
Iteration 800: Loss = -11278.474115835577
Iteration 900: Loss = -11278.425053624589
Iteration 1000: Loss = -11278.385875074515
Iteration 1100: Loss = -11278.358691528696
Iteration 1200: Loss = -11278.341021776567
Iteration 1300: Loss = -11278.329758717255
Iteration 1400: Loss = -11278.322375809305
Iteration 1500: Loss = -11278.317273110339
Iteration 1600: Loss = -11278.313449158693
Iteration 1700: Loss = -11278.310439932027
Iteration 1800: Loss = -11278.307990934658
Iteration 1900: Loss = -11278.305910647685
Iteration 2000: Loss = -11278.304162070794
Iteration 2100: Loss = -11278.302626294686
Iteration 2200: Loss = -11278.301295046012
Iteration 2300: Loss = -11278.300075807978
Iteration 2400: Loss = -11278.299065219157
Iteration 2500: Loss = -11278.298105708205
Iteration 2600: Loss = -11278.297277232186
Iteration 2700: Loss = -11278.296576005921
Iteration 2800: Loss = -11278.295907680887
Iteration 2900: Loss = -11278.29527125594
Iteration 3000: Loss = -11278.294726399623
Iteration 3100: Loss = -11278.2942280578
Iteration 3200: Loss = -11278.293747755037
Iteration 3300: Loss = -11278.293323924667
Iteration 3400: Loss = -11278.292918458961
Iteration 3500: Loss = -11278.292595824541
Iteration 3600: Loss = -11278.292267654417
Iteration 3700: Loss = -11278.291984136762
Iteration 3800: Loss = -11278.291666949232
Iteration 3900: Loss = -11278.291425026167
Iteration 4000: Loss = -11278.291222759384
Iteration 4100: Loss = -11278.290957817433
Iteration 4200: Loss = -11278.290771107542
Iteration 4300: Loss = -11278.290598028536
Iteration 4400: Loss = -11278.29042119249
Iteration 4500: Loss = -11278.290253636196
Iteration 4600: Loss = -11278.290083306687
Iteration 4700: Loss = -11278.289906081747
Iteration 4800: Loss = -11278.289817748808
Iteration 4900: Loss = -11278.289637024585
Iteration 5000: Loss = -11278.289574378849
Iteration 5100: Loss = -11278.289448169524
Iteration 5200: Loss = -11278.289334518127
Iteration 5300: Loss = -11278.289213357999
Iteration 5400: Loss = -11278.289674733285
1
Iteration 5500: Loss = -11278.28902324487
Iteration 5600: Loss = -11278.289026355298
Iteration 5700: Loss = -11278.288904299623
Iteration 5800: Loss = -11278.288840981953
Iteration 5900: Loss = -11278.289027032448
1
Iteration 6000: Loss = -11278.288727457266
Iteration 6100: Loss = -11278.288710539822
Iteration 6200: Loss = -11278.288663300966
Iteration 6300: Loss = -11278.288637926375
Iteration 6400: Loss = -11278.288526909733
Iteration 6500: Loss = -11278.288507134868
Iteration 6600: Loss = -11278.288621017757
1
Iteration 6700: Loss = -11278.288709560236
2
Iteration 6800: Loss = -11278.290408873565
3
Iteration 6900: Loss = -11278.288274788914
Iteration 7000: Loss = -11278.288239136942
Iteration 7100: Loss = -11278.288208317374
Iteration 7200: Loss = -11278.288178407935
Iteration 7300: Loss = -11278.288141439141
Iteration 7400: Loss = -11278.28820193522
Iteration 7500: Loss = -11278.310971548692
1
Iteration 7600: Loss = -11278.287999399283
Iteration 7700: Loss = -11278.28820149306
1
Iteration 7800: Loss = -11278.291348588546
2
Iteration 7900: Loss = -11278.287964564672
Iteration 8000: Loss = -11278.306716808525
1
Iteration 8100: Loss = -11278.287939638509
Iteration 8200: Loss = -11278.49564686727
1
Iteration 8300: Loss = -11278.288357043954
2
Iteration 8400: Loss = -11278.287925480747
Iteration 8500: Loss = -11278.28937702421
1
Iteration 8600: Loss = -11278.288101384926
2
Iteration 8700: Loss = -11278.582473563372
3
Iteration 8800: Loss = -11278.287808245725
Iteration 8900: Loss = -11278.28987791391
1
Iteration 9000: Loss = -11278.288237229739
2
Iteration 9100: Loss = -11278.307967858935
3
Iteration 9200: Loss = -11278.287758528413
Iteration 9300: Loss = -11278.288883473166
1
Iteration 9400: Loss = -11278.32335986093
2
Iteration 9500: Loss = -11278.28953959867
3
Iteration 9600: Loss = -11278.339066365723
4
Iteration 9700: Loss = -11278.287680949506
Iteration 9800: Loss = -11278.302962427886
1
Iteration 9900: Loss = -11278.287688526058
Iteration 10000: Loss = -11278.392875949172
1
Iteration 10100: Loss = -11278.287663516445
Iteration 10200: Loss = -11278.315875550126
1
Iteration 10300: Loss = -11278.287854992726
2
Iteration 10400: Loss = -11278.287637334392
Iteration 10500: Loss = -11278.287679933706
Iteration 10600: Loss = -11278.323638001073
1
Iteration 10700: Loss = -11278.288152279123
2
Iteration 10800: Loss = -11278.287948093119
3
Iteration 10900: Loss = -11278.287680352145
Iteration 11000: Loss = -11278.315568978165
1
Iteration 11100: Loss = -11278.312242577806
2
Iteration 11200: Loss = -11278.326468754183
3
Iteration 11300: Loss = -11278.28759655612
Iteration 11400: Loss = -11278.289277948656
1
Iteration 11500: Loss = -11278.28803692678
2
Iteration 11600: Loss = -11278.405065392486
3
Iteration 11700: Loss = -11278.287558214948
Iteration 11800: Loss = -11278.31309605699
1
Iteration 11900: Loss = -11278.288015408156
2
Iteration 12000: Loss = -11278.32091667335
3
Iteration 12100: Loss = -11278.287623588445
Iteration 12200: Loss = -11278.292124748681
1
Iteration 12300: Loss = -11278.293819263214
2
Iteration 12400: Loss = -11278.287614990515
Iteration 12500: Loss = -11278.304412333517
1
Iteration 12600: Loss = -11278.287555569947
Iteration 12700: Loss = -11278.291361221856
1
Iteration 12800: Loss = -11278.299748062309
2
Iteration 12900: Loss = -11278.288175786374
3
Iteration 13000: Loss = -11278.287831245343
4
Iteration 13100: Loss = -11278.287615117022
Iteration 13200: Loss = -11278.28910517356
1
Iteration 13300: Loss = -11278.297400166675
2
Iteration 13400: Loss = -11278.292894353852
3
Iteration 13500: Loss = -11278.287650948763
Iteration 13600: Loss = -11278.288056436868
1
Iteration 13700: Loss = -11278.318970792463
2
Iteration 13800: Loss = -11278.497658455182
3
Iteration 13900: Loss = -11278.288813767214
4
Iteration 14000: Loss = -11278.28762893998
Iteration 14100: Loss = -11278.301929707739
1
Iteration 14200: Loss = -11278.2916755974
2
Iteration 14300: Loss = -11278.298695479029
3
Iteration 14400: Loss = -11278.28823227977
4
Iteration 14500: Loss = -11278.311178048632
5
Iteration 14600: Loss = -11278.291731728614
6
Iteration 14700: Loss = -11278.287641325709
Iteration 14800: Loss = -11278.303947383287
1
Iteration 14900: Loss = -11278.456102552715
2
Iteration 15000: Loss = -11278.287917642525
3
Iteration 15100: Loss = -11278.288275458033
4
Iteration 15200: Loss = -11278.394465549134
5
Iteration 15300: Loss = -11278.37150301546
6
Iteration 15400: Loss = -11278.287573020682
Iteration 15500: Loss = -11278.287613380879
Iteration 15600: Loss = -11278.301151288817
1
Iteration 15700: Loss = -11278.292126045788
2
Iteration 15800: Loss = -11278.287556282725
Iteration 15900: Loss = -11278.287603276303
Iteration 16000: Loss = -11278.321871063425
1
Iteration 16100: Loss = -11278.446054366776
2
Iteration 16200: Loss = -11278.287925757699
3
Iteration 16300: Loss = -11278.28755668586
Iteration 16400: Loss = -11278.287565333965
Iteration 16500: Loss = -11278.287621944943
Iteration 16600: Loss = -11278.303714079111
1
Iteration 16700: Loss = -11278.320195980945
2
Iteration 16800: Loss = -11278.308761273856
3
Iteration 16900: Loss = -11278.304801679227
4
Iteration 17000: Loss = -11278.28913600851
5
Iteration 17100: Loss = -11278.292494832749
6
Iteration 17200: Loss = -11278.287571203917
Iteration 17300: Loss = -11278.290401820792
1
Iteration 17400: Loss = -11278.306612089931
2
Iteration 17500: Loss = -11278.298749790978
3
Iteration 17600: Loss = -11278.287623360118
Iteration 17700: Loss = -11278.289730683111
1
Iteration 17800: Loss = -11278.289399244932
2
Iteration 17900: Loss = -11278.287692149806
Iteration 18000: Loss = -11278.511278498523
1
Iteration 18100: Loss = -11278.287692703097
Iteration 18200: Loss = -11278.287626136844
Iteration 18300: Loss = -11278.290330502838
1
Iteration 18400: Loss = -11278.28897640083
2
Iteration 18500: Loss = -11278.287595571785
Iteration 18600: Loss = -11278.460469730648
1
Iteration 18700: Loss = -11278.287604939236
Iteration 18800: Loss = -11278.289898953102
1
Iteration 18900: Loss = -11278.28760151784
Iteration 19000: Loss = -11278.28772561169
1
Iteration 19100: Loss = -11278.304672822034
2
Iteration 19200: Loss = -11278.28765836592
Iteration 19300: Loss = -11278.289156533845
1
Iteration 19400: Loss = -11278.299495963927
2
Iteration 19500: Loss = -11278.289258635454
3
Iteration 19600: Loss = -11278.287757101654
Iteration 19700: Loss = -11278.28784210658
Iteration 19800: Loss = -11278.290216032026
1
Iteration 19900: Loss = -11278.290384772716
2
pi: tensor([[1.8432e-06, 1.0000e+00],
        [2.3003e-02, 9.7700e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2004, 0.7996], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1983, 0.1826],
         [0.5523, 0.1697]],

        [[0.6108, 0.1792],
         [0.6495, 0.5083]],

        [[0.6608, 0.1089],
         [0.5143, 0.6588]],

        [[0.6875, 0.1297],
         [0.5166, 0.6347]],

        [[0.6766, 0.1753],
         [0.5794, 0.7017]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
11145.975379451182
[-0.0022351446611192324, 0.0] [-0.003675612911859437, 0.0] [11276.439718369307, 11278.289531847075]
-------------------------------------
This iteration is 48
True Objective function: Loss = -10962.698076748846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21246.02915163577
Iteration 100: Loss = -11081.675318214384
Iteration 200: Loss = -11080.511482491076
Iteration 300: Loss = -11080.228536487723
Iteration 400: Loss = -11080.087040960263
Iteration 500: Loss = -11079.973282797368
Iteration 600: Loss = -11079.833459441716
Iteration 700: Loss = -11079.595222352282
Iteration 800: Loss = -11078.923784364995
Iteration 900: Loss = -11076.725230778704
Iteration 1000: Loss = -11074.686961249461
Iteration 1100: Loss = -11073.799626507176
Iteration 1200: Loss = -11065.865856956929
Iteration 1300: Loss = -11055.57150714502
Iteration 1400: Loss = -11054.825732210566
Iteration 1500: Loss = -11054.612391667819
Iteration 1600: Loss = -11054.558856539523
Iteration 1700: Loss = -11054.525107844302
Iteration 1800: Loss = -11054.506054025269
Iteration 1900: Loss = -11054.492259461831
Iteration 2000: Loss = -11054.481506183003
Iteration 2100: Loss = -11054.472716890814
Iteration 2200: Loss = -11054.465556100391
Iteration 2300: Loss = -11054.494205550223
1
Iteration 2400: Loss = -11054.45476548366
Iteration 2500: Loss = -11054.450607911689
Iteration 2600: Loss = -11054.447378376612
Iteration 2700: Loss = -11054.443821447787
Iteration 2800: Loss = -11054.440950050122
Iteration 2900: Loss = -11054.438474084827
Iteration 3000: Loss = -11054.436054678981
Iteration 3100: Loss = -11054.433883567988
Iteration 3200: Loss = -11054.431622523203
Iteration 3300: Loss = -11054.428798292085
Iteration 3400: Loss = -11054.460578346634
1
Iteration 3500: Loss = -11054.422520332279
Iteration 3600: Loss = -11054.421321265127
Iteration 3700: Loss = -11054.420245921952
Iteration 3800: Loss = -11054.41912029554
Iteration 3900: Loss = -11054.418205524757
Iteration 4000: Loss = -11054.453241134068
1
Iteration 4100: Loss = -11054.416396974082
Iteration 4200: Loss = -11054.415580062083
Iteration 4300: Loss = -11054.416108118387
1
Iteration 4400: Loss = -11054.414049893821
Iteration 4500: Loss = -11054.413395795507
Iteration 4600: Loss = -11054.413062189951
Iteration 4700: Loss = -11054.41218443794
Iteration 4800: Loss = -11054.43757128645
1
Iteration 4900: Loss = -11054.41093520153
Iteration 5000: Loss = -11054.410010149482
Iteration 5100: Loss = -11054.409018753606
Iteration 5200: Loss = -11054.408608664735
Iteration 5300: Loss = -11054.413334029996
1
Iteration 5400: Loss = -11054.407902148128
Iteration 5500: Loss = -11054.407583903841
Iteration 5600: Loss = -11054.407366520532
Iteration 5700: Loss = -11054.407064270663
Iteration 5800: Loss = -11054.406758034662
Iteration 5900: Loss = -11054.406533676834
Iteration 6000: Loss = -11054.406300030676
Iteration 6100: Loss = -11054.40624414657
Iteration 6200: Loss = -11054.405879654414
Iteration 6300: Loss = -11054.406749042593
1
Iteration 6400: Loss = -11054.405450643702
Iteration 6500: Loss = -11054.40530229184
Iteration 6600: Loss = -11054.405070627025
Iteration 6700: Loss = -11054.404899961322
Iteration 6800: Loss = -11054.408611778581
1
Iteration 6900: Loss = -11054.40449504482
Iteration 7000: Loss = -11054.404958612136
1
Iteration 7100: Loss = -11054.4039883481
Iteration 7200: Loss = -11054.40380507611
Iteration 7300: Loss = -11054.403872019722
Iteration 7400: Loss = -11054.4035676913
Iteration 7500: Loss = -11054.40346755081
Iteration 7600: Loss = -11054.403797546327
1
Iteration 7700: Loss = -11054.40327952669
Iteration 7800: Loss = -11054.403608331517
1
Iteration 7900: Loss = -11054.403060298062
Iteration 8000: Loss = -11054.402963340013
Iteration 8100: Loss = -11054.402960476353
Iteration 8200: Loss = -11054.592237313387
1
Iteration 8300: Loss = -11054.402689766564
Iteration 8400: Loss = -11054.423526220824
1
Iteration 8500: Loss = -11054.402527590084
Iteration 8600: Loss = -11054.402479005787
Iteration 8700: Loss = -11054.402525056263
Iteration 8800: Loss = -11054.405687744003
1
Iteration 8900: Loss = -11054.410151666794
2
Iteration 9000: Loss = -11054.524094582679
3
Iteration 9100: Loss = -11054.402183351247
Iteration 9200: Loss = -11054.425360342391
1
Iteration 9300: Loss = -11054.40210941841
Iteration 9400: Loss = -11054.449370259914
1
Iteration 9500: Loss = -11054.40205535199
Iteration 9600: Loss = -11054.402694723534
1
Iteration 9700: Loss = -11054.403108490094
2
Iteration 9800: Loss = -11054.402531472997
3
Iteration 9900: Loss = -11054.401909321741
Iteration 10000: Loss = -11054.401931102766
Iteration 10100: Loss = -11054.401833727807
Iteration 10200: Loss = -11054.401804717676
Iteration 10300: Loss = -11054.402443220588
1
Iteration 10400: Loss = -11054.40177960209
Iteration 10500: Loss = -11054.40181251934
Iteration 10600: Loss = -11054.401688407908
Iteration 10700: Loss = -11054.40191088959
1
Iteration 10800: Loss = -11054.440116239433
2
Iteration 10900: Loss = -11054.401604007882
Iteration 11000: Loss = -11054.488379370103
1
Iteration 11100: Loss = -11054.401614743672
Iteration 11200: Loss = -11054.513951329085
1
Iteration 11300: Loss = -11054.401564694619
Iteration 11400: Loss = -11054.406201792493
1
Iteration 11500: Loss = -11054.402233569075
2
Iteration 11600: Loss = -11054.4019014648
3
Iteration 11700: Loss = -11054.401478707618
Iteration 11800: Loss = -11054.409377124335
1
Iteration 11900: Loss = -11054.405166872944
2
Iteration 12000: Loss = -11054.662919491417
3
Iteration 12100: Loss = -11054.401485714325
Iteration 12200: Loss = -11054.42425015995
1
Iteration 12300: Loss = -11054.40164051161
2
Iteration 12400: Loss = -11054.401488702302
Iteration 12500: Loss = -11054.490935774513
1
Iteration 12600: Loss = -11054.401394291554
Iteration 12700: Loss = -11054.575593900823
1
Iteration 12800: Loss = -11054.401388438204
Iteration 12900: Loss = -11054.461925630172
1
Iteration 13000: Loss = -11054.40142823232
Iteration 13100: Loss = -11054.432960149936
1
Iteration 13200: Loss = -11054.401349384656
Iteration 13300: Loss = -11054.451583401218
1
Iteration 13400: Loss = -11054.40130767311
Iteration 13500: Loss = -11054.423104011548
1
Iteration 13600: Loss = -11054.401568736592
2
Iteration 13700: Loss = -11054.47735519753
3
Iteration 13800: Loss = -11054.401367845181
Iteration 13900: Loss = -11054.409810303034
1
Iteration 14000: Loss = -11054.408706490463
2
Iteration 14100: Loss = -11054.401979950739
3
Iteration 14200: Loss = -11054.40203761709
4
Iteration 14300: Loss = -11054.401331150704
Iteration 14400: Loss = -11054.402698486989
1
Iteration 14500: Loss = -11054.62153216457
2
Iteration 14600: Loss = -11054.401312424812
Iteration 14700: Loss = -11054.521093575917
1
Iteration 14800: Loss = -11054.401305161737
Iteration 14900: Loss = -11054.780781502097
1
Iteration 15000: Loss = -11054.40130291495
Iteration 15100: Loss = -11054.529796349836
1
Iteration 15200: Loss = -11054.406479083327
2
Iteration 15300: Loss = -11054.401366055847
Iteration 15400: Loss = -11054.40314859951
1
Iteration 15500: Loss = -11054.409368244545
2
Iteration 15600: Loss = -11054.447706083965
3
Iteration 15700: Loss = -11054.401299542476
Iteration 15800: Loss = -11054.402912042275
1
Iteration 15900: Loss = -11054.401273520029
Iteration 16000: Loss = -11054.40210759579
1
Iteration 16100: Loss = -11054.413941736193
2
Iteration 16200: Loss = -11054.40163601028
3
Iteration 16300: Loss = -11054.401781795064
4
Iteration 16400: Loss = -11054.402684711724
5
Iteration 16500: Loss = -11054.407070649988
6
Iteration 16600: Loss = -11054.447276940078
7
Iteration 16700: Loss = -11054.401297092309
Iteration 16800: Loss = -11054.4041399511
1
Iteration 16900: Loss = -11054.40131011681
Iteration 17000: Loss = -11054.402357762265
1
Iteration 17100: Loss = -11054.402033640097
2
Iteration 17200: Loss = -11054.401326442485
Iteration 17300: Loss = -11054.401755030676
1
Iteration 17400: Loss = -11054.401291505608
Iteration 17500: Loss = -11054.44447001462
1
Iteration 17600: Loss = -11054.401250001809
Iteration 17700: Loss = -11054.429642887031
1
Iteration 17800: Loss = -11054.404632678055
2
Iteration 17900: Loss = -11054.401288349478
Iteration 18000: Loss = -11054.402262678143
1
Iteration 18100: Loss = -11054.401751720035
2
Iteration 18200: Loss = -11054.448262113623
3
Iteration 18300: Loss = -11054.40123836041
Iteration 18400: Loss = -11054.40397743521
1
Iteration 18500: Loss = -11054.410643802928
2
Iteration 18600: Loss = -11054.476834043764
3
Iteration 18700: Loss = -11054.401294480613
Iteration 18800: Loss = -11054.40981959431
1
Iteration 18900: Loss = -11054.418328401938
2
Iteration 19000: Loss = -11054.402320534788
3
Iteration 19100: Loss = -11054.401308505405
Iteration 19200: Loss = -11054.489061361055
1
Iteration 19300: Loss = -11054.401299924919
Iteration 19400: Loss = -11054.401262446701
Iteration 19500: Loss = -11054.401387335693
1
Iteration 19600: Loss = -11054.402323841636
2
Iteration 19700: Loss = -11054.401289473335
Iteration 19800: Loss = -11054.40169907854
1
Iteration 19900: Loss = -11054.401327235153
pi: tensor([[1.5010e-07, 1.0000e+00],
        [2.4270e-01, 7.5730e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.2489e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2228, 0.1534],
         [0.6453, 0.1744]],

        [[0.6982, 0.0818],
         [0.6005, 0.6419]],

        [[0.5243, 0.1878],
         [0.6827, 0.5924]],

        [[0.5689, 0.0905],
         [0.5394, 0.6161]],

        [[0.6504, 0.1775],
         [0.6383, 0.5767]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 79
Adjusted Rand Index: 0.3304996323966418
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.0731295336585442
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.03447634940525084
Average Adjusted Rand Index: 0.0779056455064477
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22293.280163104875
Iteration 100: Loss = -11081.333611397331
Iteration 200: Loss = -11080.4987176449
Iteration 300: Loss = -11080.148992413307
Iteration 400: Loss = -11079.762433349166
Iteration 500: Loss = -11079.14874092194
Iteration 600: Loss = -11078.644448366162
Iteration 700: Loss = -11078.052301759275
Iteration 800: Loss = -11077.495103627356
Iteration 900: Loss = -11077.21511334356
Iteration 1000: Loss = -11077.056130170076
Iteration 1100: Loss = -11076.929795078586
Iteration 1200: Loss = -11076.799640828944
Iteration 1300: Loss = -11076.642712075865
Iteration 1400: Loss = -11076.440173974837
Iteration 1500: Loss = -11076.172814006259
Iteration 1600: Loss = -11075.830478135895
Iteration 1700: Loss = -11075.427938344366
Iteration 1800: Loss = -11074.754057636424
Iteration 1900: Loss = -11073.326352577284
Iteration 2000: Loss = -11072.718062894188
Iteration 2100: Loss = -11054.8793049291
Iteration 2200: Loss = -11054.57276325733
Iteration 2300: Loss = -11054.512759949783
Iteration 2400: Loss = -11054.48484640342
Iteration 2500: Loss = -11054.46797530453
Iteration 2600: Loss = -11054.45649600798
Iteration 2700: Loss = -11054.448182258999
Iteration 2800: Loss = -11054.441924562536
Iteration 2900: Loss = -11054.437089298825
Iteration 3000: Loss = -11054.433162303156
Iteration 3100: Loss = -11054.42990193114
Iteration 3200: Loss = -11054.427134852303
Iteration 3300: Loss = -11054.424806684918
Iteration 3400: Loss = -11054.4227398724
Iteration 3500: Loss = -11054.428616540828
1
Iteration 3600: Loss = -11054.419407913047
Iteration 3700: Loss = -11054.417961725623
Iteration 3800: Loss = -11054.421538459857
1
Iteration 3900: Loss = -11054.41553828327
Iteration 4000: Loss = -11054.414493793884
Iteration 4100: Loss = -11054.413624986335
Iteration 4200: Loss = -11054.412726561492
Iteration 4300: Loss = -11054.412003021682
Iteration 4400: Loss = -11054.4113481058
Iteration 4500: Loss = -11054.410724023308
Iteration 4600: Loss = -11054.410172203758
Iteration 4700: Loss = -11054.409662357528
Iteration 4800: Loss = -11054.409100591156
Iteration 4900: Loss = -11054.409208930414
1
Iteration 5000: Loss = -11054.408254650345
Iteration 5100: Loss = -11054.459957598428
1
Iteration 5200: Loss = -11054.407512245381
Iteration 5300: Loss = -11054.407210766232
Iteration 5400: Loss = -11054.406918871064
Iteration 5500: Loss = -11054.406553316547
Iteration 5600: Loss = -11054.424248097746
1
Iteration 5700: Loss = -11054.406006387731
Iteration 5800: Loss = -11054.405768042743
Iteration 5900: Loss = -11054.405643316353
Iteration 6000: Loss = -11054.40530359879
Iteration 6100: Loss = -11054.449762828568
1
Iteration 6200: Loss = -11054.404933801005
Iteration 6300: Loss = -11054.404715723742
Iteration 6400: Loss = -11054.405070801467
1
Iteration 6500: Loss = -11054.404367286583
Iteration 6600: Loss = -11054.404225958906
Iteration 6700: Loss = -11054.404167451781
Iteration 6800: Loss = -11054.403951346934
Iteration 6900: Loss = -11054.443706959231
1
Iteration 7000: Loss = -11054.40367224768
Iteration 7100: Loss = -11054.403570847851
Iteration 7200: Loss = -11054.410156697164
1
Iteration 7300: Loss = -11054.403355222916
Iteration 7400: Loss = -11054.403252015101
Iteration 7500: Loss = -11054.403122279491
Iteration 7600: Loss = -11054.40302162621
Iteration 7700: Loss = -11054.407558953792
1
Iteration 7800: Loss = -11054.402868798217
Iteration 7900: Loss = -11054.407499486995
1
Iteration 8000: Loss = -11054.402711989973
Iteration 8100: Loss = -11054.40265153414
Iteration 8200: Loss = -11054.402657813042
Iteration 8300: Loss = -11054.40250238081
Iteration 8400: Loss = -11054.402492569667
Iteration 8500: Loss = -11054.446215304439
1
Iteration 8600: Loss = -11054.402359169795
Iteration 8700: Loss = -11054.403489975019
1
Iteration 8800: Loss = -11054.402221883378
Iteration 8900: Loss = -11054.40457523697
1
Iteration 9000: Loss = -11054.402105590585
Iteration 9100: Loss = -11054.418893024987
1
Iteration 9200: Loss = -11054.402051015808
Iteration 9300: Loss = -11054.424842726316
1
Iteration 9400: Loss = -11054.40204289949
Iteration 9500: Loss = -11054.402013304607
Iteration 9600: Loss = -11054.401911400686
Iteration 9700: Loss = -11054.401889840781
Iteration 9800: Loss = -11054.401826700647
Iteration 9900: Loss = -11054.621466346185
1
Iteration 10000: Loss = -11054.401780527502
Iteration 10100: Loss = -11054.41170193173
1
Iteration 10200: Loss = -11054.401699778644
Iteration 10300: Loss = -11054.63259541528
1
Iteration 10400: Loss = -11054.401711446111
Iteration 10500: Loss = -11054.407913509744
1
Iteration 10600: Loss = -11054.40163025272
Iteration 10700: Loss = -11054.420168091672
1
Iteration 10800: Loss = -11054.478701778597
2
Iteration 10900: Loss = -11054.410006148937
3
Iteration 11000: Loss = -11054.401824453216
4
Iteration 11100: Loss = -11054.401641818155
Iteration 11200: Loss = -11054.406655612025
1
Iteration 11300: Loss = -11054.401551468882
Iteration 11400: Loss = -11054.401773097034
1
Iteration 11500: Loss = -11054.401496342813
Iteration 11600: Loss = -11054.401866424794
1
Iteration 11700: Loss = -11054.401502216388
Iteration 11800: Loss = -11054.403302577262
1
Iteration 11900: Loss = -11054.401442418186
Iteration 12000: Loss = -11054.401783834486
1
Iteration 12100: Loss = -11054.401556038663
2
Iteration 12200: Loss = -11054.40187636309
3
Iteration 12300: Loss = -11054.40152690271
Iteration 12400: Loss = -11054.40222011892
1
Iteration 12500: Loss = -11054.410466515905
2
Iteration 12600: Loss = -11054.402737914192
3
Iteration 12700: Loss = -11054.403354881126
4
Iteration 12800: Loss = -11054.401382501726
Iteration 12900: Loss = -11054.401684522416
1
Iteration 13000: Loss = -11054.661647494668
2
Iteration 13100: Loss = -11054.40139350958
Iteration 13200: Loss = -11054.421286061954
1
Iteration 13300: Loss = -11054.401346131666
Iteration 13400: Loss = -11054.401501656857
1
Iteration 13500: Loss = -11054.433925076391
2
Iteration 13600: Loss = -11054.401322851361
Iteration 13700: Loss = -11054.405034090369
1
Iteration 13800: Loss = -11054.401332621115
Iteration 13900: Loss = -11054.402024912348
1
Iteration 14000: Loss = -11054.401280663078
Iteration 14100: Loss = -11054.409603969969
1
Iteration 14200: Loss = -11054.40133273805
Iteration 14300: Loss = -11054.473941774457
1
Iteration 14400: Loss = -11054.401319119972
Iteration 14500: Loss = -11054.40225958729
1
Iteration 14600: Loss = -11054.401319914692
Iteration 14700: Loss = -11054.401441272586
1
Iteration 14800: Loss = -11054.40245525799
2
Iteration 14900: Loss = -11054.40131609
Iteration 15000: Loss = -11054.40143124732
1
Iteration 15100: Loss = -11054.401462975868
2
Iteration 15200: Loss = -11054.423267509204
3
Iteration 15300: Loss = -11054.404341350093
4
Iteration 15400: Loss = -11054.40193900188
5
Iteration 15500: Loss = -11054.401933797095
6
Iteration 15600: Loss = -11054.410403737025
7
Iteration 15700: Loss = -11054.401415217511
Iteration 15800: Loss = -11054.401361879138
Iteration 15900: Loss = -11054.433984880323
1
Iteration 16000: Loss = -11054.40152675361
2
Iteration 16100: Loss = -11054.401313705894
Iteration 16200: Loss = -11054.401760136197
1
Iteration 16300: Loss = -11054.40229866596
2
Iteration 16400: Loss = -11054.402936145514
3
Iteration 16500: Loss = -11054.414033260668
4
Iteration 16600: Loss = -11054.421402839605
5
Iteration 16700: Loss = -11054.489697909485
6
Iteration 16800: Loss = -11054.40126563258
Iteration 16900: Loss = -11054.405261709548
1
Iteration 17000: Loss = -11054.401289194306
Iteration 17100: Loss = -11054.401507177074
1
Iteration 17200: Loss = -11054.401240848743
Iteration 17300: Loss = -11054.401710471302
1
Iteration 17400: Loss = -11054.401271577744
Iteration 17500: Loss = -11054.42841629435
1
Iteration 17600: Loss = -11054.401237230639
Iteration 17700: Loss = -11054.494609331863
1
Iteration 17800: Loss = -11054.401301226711
Iteration 17900: Loss = -11054.409382346734
1
Iteration 18000: Loss = -11054.401290106467
Iteration 18100: Loss = -11054.49511731101
1
Iteration 18200: Loss = -11054.467244116022
2
Iteration 18300: Loss = -11054.401285751966
Iteration 18400: Loss = -11054.401581775208
1
Iteration 18500: Loss = -11054.401425226879
2
Iteration 18600: Loss = -11054.401489866797
3
Iteration 18700: Loss = -11054.423071711662
4
Iteration 18800: Loss = -11054.40128986918
Iteration 18900: Loss = -11054.4038102569
1
Iteration 19000: Loss = -11054.401271464838
Iteration 19100: Loss = -11054.401436714337
1
Iteration 19200: Loss = -11054.401859243737
2
Iteration 19300: Loss = -11054.40126850419
Iteration 19400: Loss = -11054.418308408303
1
Iteration 19500: Loss = -11054.401305050113
Iteration 19600: Loss = -11054.4023387226
1
Iteration 19700: Loss = -11054.401766913923
2
Iteration 19800: Loss = -11054.408408565692
3
Iteration 19900: Loss = -11054.439250266365
4
pi: tensor([[7.5432e-01, 2.4568e-01],
        [1.0000e+00, 1.5460e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.0346e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1751, 0.1533],
         [0.6375, 0.2216]],

        [[0.6814, 0.0819],
         [0.7106, 0.7045]],

        [[0.6223, 0.1862],
         [0.6314, 0.5543]],

        [[0.7126, 0.0913],
         [0.7007, 0.5327]],

        [[0.5332, 0.1770],
         [0.6434, 0.6803]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 21
Adjusted Rand Index: 0.3304996323966418
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.0731295336585442
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.03447634940525084
Average Adjusted Rand Index: 0.0779056455064477
10962.698076748846
[0.03447634940525084, 0.03447634940525084] [0.0779056455064477, 0.0779056455064477] [11054.411323540786, 11054.430572474335]
-------------------------------------
This iteration is 49
True Objective function: Loss = -10833.547747695342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22892.66229488442
Iteration 100: Loss = -10919.518628141726
Iteration 200: Loss = -10918.434748063984
Iteration 300: Loss = -10918.110810607272
Iteration 400: Loss = -10917.900868855388
Iteration 500: Loss = -10917.638662673355
Iteration 600: Loss = -10917.31645162912
Iteration 700: Loss = -10917.061753665912
Iteration 800: Loss = -10916.863688314304
Iteration 900: Loss = -10916.687780936914
Iteration 1000: Loss = -10916.541108653593
Iteration 1100: Loss = -10916.394836921907
Iteration 1200: Loss = -10916.228892541603
Iteration 1300: Loss = -10915.993624613066
Iteration 1400: Loss = -10915.747490299404
Iteration 1500: Loss = -10915.536212534093
Iteration 1600: Loss = -10915.31330936601
Iteration 1700: Loss = -10915.080211682884
Iteration 1800: Loss = -10914.81110373098
Iteration 1900: Loss = -10914.48464043498
Iteration 2000: Loss = -10913.995852655937
Iteration 2100: Loss = -10913.188176966987
Iteration 2200: Loss = -10792.03962110536
Iteration 2300: Loss = -10790.26002621958
Iteration 2400: Loss = -10790.152339548933
Iteration 2500: Loss = -10790.111806029512
Iteration 2600: Loss = -10790.082966331547
Iteration 2700: Loss = -10790.069717092358
Iteration 2800: Loss = -10790.050816831208
Iteration 2900: Loss = -10790.041247894907
Iteration 3000: Loss = -10790.035604639514
Iteration 3100: Loss = -10790.030501663534
Iteration 3200: Loss = -10790.027154599164
Iteration 3300: Loss = -10790.023823836682
Iteration 3400: Loss = -10790.022085348015
Iteration 3500: Loss = -10790.020789837
Iteration 3600: Loss = -10790.020650561853
Iteration 3700: Loss = -10790.01944042904
Iteration 3800: Loss = -10790.01950628687
Iteration 3900: Loss = -10790.017143211338
Iteration 4000: Loss = -10790.016257578975
Iteration 4100: Loss = -10790.017046091103
1
Iteration 4200: Loss = -10790.014317212908
Iteration 4300: Loss = -10790.012639878718
Iteration 4400: Loss = -10790.010829335533
Iteration 4500: Loss = -10790.009826319832
Iteration 4600: Loss = -10790.009240275265
Iteration 4700: Loss = -10790.008951440952
Iteration 4800: Loss = -10790.008586572872
Iteration 4900: Loss = -10790.00824242593
Iteration 5000: Loss = -10790.008109717268
Iteration 5100: Loss = -10790.00785759348
Iteration 5200: Loss = -10790.00934386712
1
Iteration 5300: Loss = -10790.007467910049
Iteration 5400: Loss = -10790.010185637846
1
Iteration 5500: Loss = -10790.008272651743
2
Iteration 5600: Loss = -10790.009896850775
3
Iteration 5700: Loss = -10790.007635310372
4
Iteration 5800: Loss = -10790.00776528471
5
Iteration 5900: Loss = -10790.006767062994
Iteration 6000: Loss = -10790.00877775145
1
Iteration 6100: Loss = -10790.006359704195
Iteration 6200: Loss = -10790.006272686958
Iteration 6300: Loss = -10790.011450976966
1
Iteration 6400: Loss = -10790.005948152091
Iteration 6500: Loss = -10790.008930140617
1
Iteration 6600: Loss = -10790.00588041171
Iteration 6700: Loss = -10790.006819127866
1
Iteration 6800: Loss = -10790.005881515952
Iteration 6900: Loss = -10790.008298290448
1
Iteration 7000: Loss = -10790.005256815712
Iteration 7100: Loss = -10790.005195905022
Iteration 7200: Loss = -10790.006721130612
1
Iteration 7300: Loss = -10790.018821079842
2
Iteration 7400: Loss = -10790.006339776004
3
Iteration 7500: Loss = -10790.005080230712
Iteration 7600: Loss = -10790.005004220748
Iteration 7700: Loss = -10790.034487476103
1
Iteration 7800: Loss = -10790.004958352309
Iteration 7900: Loss = -10790.010419852391
1
Iteration 8000: Loss = -10790.004888549378
Iteration 8100: Loss = -10790.00484381834
Iteration 8200: Loss = -10790.009764591174
1
Iteration 8300: Loss = -10790.004828502775
Iteration 8400: Loss = -10790.004799436467
Iteration 8500: Loss = -10790.005251664656
1
Iteration 8600: Loss = -10790.00476054139
Iteration 8700: Loss = -10790.004770544185
Iteration 8800: Loss = -10790.004782633208
Iteration 8900: Loss = -10790.005401163426
1
Iteration 9000: Loss = -10790.074565617235
2
Iteration 9100: Loss = -10790.00470222411
Iteration 9200: Loss = -10790.01038749414
1
Iteration 9300: Loss = -10790.004679174015
Iteration 9400: Loss = -10790.004848574652
1
Iteration 9500: Loss = -10790.004710398583
Iteration 9600: Loss = -10790.017210483342
1
Iteration 9700: Loss = -10790.00505181136
2
Iteration 9800: Loss = -10790.004911069484
3
Iteration 9900: Loss = -10790.010446773149
4
Iteration 10000: Loss = -10790.004644838784
Iteration 10100: Loss = -10790.004611962191
Iteration 10200: Loss = -10790.009926614557
1
Iteration 10300: Loss = -10790.00564834522
2
Iteration 10400: Loss = -10790.018286774788
3
Iteration 10500: Loss = -10790.00468036328
Iteration 10600: Loss = -10790.012522422203
1
Iteration 10700: Loss = -10790.011262525892
2
Iteration 10800: Loss = -10790.071620129718
3
Iteration 10900: Loss = -10790.004560107463
Iteration 11000: Loss = -10790.013460139864
1
Iteration 11100: Loss = -10790.004562318798
Iteration 11200: Loss = -10790.004737084488
1
Iteration 11300: Loss = -10790.004597617402
Iteration 11400: Loss = -10790.004535666929
Iteration 11500: Loss = -10790.031275247975
1
Iteration 11600: Loss = -10790.004527572402
Iteration 11700: Loss = -10790.004527713252
Iteration 11800: Loss = -10790.014307330752
1
Iteration 11900: Loss = -10790.00448985826
Iteration 12000: Loss = -10790.052857623234
1
Iteration 12100: Loss = -10790.009127805286
2
Iteration 12200: Loss = -10790.00596909296
3
Iteration 12300: Loss = -10790.006940298323
4
Iteration 12400: Loss = -10790.112014382692
5
Iteration 12500: Loss = -10790.009693641357
6
Iteration 12600: Loss = -10790.011070112208
7
Iteration 12700: Loss = -10790.065007365376
8
Iteration 12800: Loss = -10790.0044976554
Iteration 12900: Loss = -10790.005779212126
1
Iteration 13000: Loss = -10790.004510004408
Iteration 13100: Loss = -10790.013692636088
1
Iteration 13200: Loss = -10790.004485298248
Iteration 13300: Loss = -10790.02007953169
1
Iteration 13400: Loss = -10790.062618762175
2
Iteration 13500: Loss = -10790.006168221309
3
Iteration 13600: Loss = -10790.004500442124
Iteration 13700: Loss = -10790.005086056262
1
Iteration 13800: Loss = -10790.009012520524
2
Iteration 13900: Loss = -10790.004513167954
Iteration 14000: Loss = -10790.005019328806
1
Iteration 14100: Loss = -10790.004485983845
Iteration 14200: Loss = -10790.00448086952
Iteration 14300: Loss = -10790.008108659948
1
Iteration 14400: Loss = -10790.004474900112
Iteration 14500: Loss = -10790.007512157676
1
Iteration 14600: Loss = -10790.006178908108
2
Iteration 14700: Loss = -10790.008810880854
3
Iteration 14800: Loss = -10790.005155806133
4
Iteration 14900: Loss = -10790.010913562379
5
Iteration 15000: Loss = -10790.028291701617
6
Iteration 15100: Loss = -10790.05082578871
7
Iteration 15200: Loss = -10790.012631994015
8
Iteration 15300: Loss = -10790.004493794539
Iteration 15400: Loss = -10790.00573159566
1
Iteration 15500: Loss = -10790.00448336295
Iteration 15600: Loss = -10790.004623067778
1
Iteration 15700: Loss = -10790.004502741127
Iteration 15800: Loss = -10790.00455984479
Iteration 15900: Loss = -10790.010624918536
1
Iteration 16000: Loss = -10790.004644408242
Iteration 16100: Loss = -10790.004747232222
1
Iteration 16200: Loss = -10790.004490444464
Iteration 16300: Loss = -10790.004913730027
1
Iteration 16400: Loss = -10790.016668947059
2
Iteration 16500: Loss = -10790.004482936522
Iteration 16600: Loss = -10790.004505991159
Iteration 16700: Loss = -10790.004532377185
Iteration 16800: Loss = -10790.004640730825
1
Iteration 16900: Loss = -10790.005065517093
2
Iteration 17000: Loss = -10790.006782257522
3
Iteration 17100: Loss = -10790.011261828637
4
Iteration 17200: Loss = -10790.005172342284
5
Iteration 17300: Loss = -10790.00515955305
6
Iteration 17400: Loss = -10790.007837607176
7
Iteration 17500: Loss = -10790.094201415808
8
Iteration 17600: Loss = -10790.004781915455
9
Iteration 17700: Loss = -10790.004515313454
Iteration 17800: Loss = -10790.008026865533
1
Iteration 17900: Loss = -10790.029082442552
2
Iteration 18000: Loss = -10790.005934270985
3
Iteration 18100: Loss = -10790.004477065464
Iteration 18200: Loss = -10790.004609486976
1
Iteration 18300: Loss = -10790.015609390779
2
Iteration 18400: Loss = -10790.004462764538
Iteration 18500: Loss = -10790.005024211818
1
Iteration 18600: Loss = -10790.016165970428
2
Iteration 18700: Loss = -10790.004716593603
3
Iteration 18800: Loss = -10790.052889726958
4
Iteration 18900: Loss = -10790.004483772493
Iteration 19000: Loss = -10790.005041170794
1
Iteration 19100: Loss = -10790.004468738663
Iteration 19200: Loss = -10790.004600546534
1
Iteration 19300: Loss = -10790.004482899876
Iteration 19400: Loss = -10790.00505975451
1
Iteration 19500: Loss = -10790.014994052764
2
Iteration 19600: Loss = -10790.01251137924
3
Iteration 19700: Loss = -10790.005956483768
4
Iteration 19800: Loss = -10790.004710503525
5
Iteration 19900: Loss = -10790.005972876721
6
pi: tensor([[0.7699, 0.2301],
        [0.2044, 0.7956]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4830, 0.5170], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2519, 0.0956],
         [0.5548, 0.1950]],

        [[0.6846, 0.1042],
         [0.6347, 0.5344]],

        [[0.7112, 0.1053],
         [0.7276, 0.5328]],

        [[0.5405, 0.0946],
         [0.6522, 0.6427]],

        [[0.5513, 0.1056],
         [0.6024, 0.6445]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721128416103438
Global Adjusted Rand Index: 0.8460920103112441
Average Adjusted Rand Index: 0.8459754087704374
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24947.468730260032
Iteration 100: Loss = -10919.488060748827
Iteration 200: Loss = -10918.288406323236
Iteration 300: Loss = -10917.77848447306
Iteration 400: Loss = -10916.899912683404
Iteration 500: Loss = -10915.994255477688
Iteration 600: Loss = -10914.25563122607
Iteration 700: Loss = -10913.020834306742
Iteration 800: Loss = -10912.040278659922
Iteration 900: Loss = -10911.17993321127
Iteration 1000: Loss = -10910.572191470406
Iteration 1100: Loss = -10910.223608679964
Iteration 1200: Loss = -10910.0535051431
Iteration 1300: Loss = -10909.9918610526
Iteration 1400: Loss = -10909.965739610476
Iteration 1500: Loss = -10909.949821642413
Iteration 1600: Loss = -10909.938553454613
Iteration 1700: Loss = -10909.929956759217
Iteration 1800: Loss = -10909.923246725486
Iteration 1900: Loss = -10909.917690452308
Iteration 2000: Loss = -10909.913119271732
Iteration 2100: Loss = -10909.90927344968
Iteration 2200: Loss = -10909.906057489348
Iteration 2300: Loss = -10909.903227328763
Iteration 2400: Loss = -10909.900769487449
Iteration 2500: Loss = -10909.898638356452
Iteration 2600: Loss = -10909.896753750438
Iteration 2700: Loss = -10909.895036126372
Iteration 2800: Loss = -10909.89346323994
Iteration 2900: Loss = -10909.892115450653
Iteration 3000: Loss = -10909.890820187968
Iteration 3100: Loss = -10909.88962219976
Iteration 3200: Loss = -10909.88842992659
Iteration 3300: Loss = -10909.887428995624
Iteration 3400: Loss = -10909.886543576953
Iteration 3500: Loss = -10909.88577676898
Iteration 3600: Loss = -10909.885092838831
Iteration 3700: Loss = -10909.884362086501
Iteration 3800: Loss = -10909.883744350223
Iteration 3900: Loss = -10909.8831923263
Iteration 4000: Loss = -10909.882620763074
Iteration 4100: Loss = -10909.88210241273
Iteration 4200: Loss = -10909.881692566698
Iteration 4300: Loss = -10909.881270693253
Iteration 4400: Loss = -10909.8809547238
Iteration 4500: Loss = -10909.880538288598
Iteration 4600: Loss = -10909.880238745034
Iteration 4700: Loss = -10909.879950774126
Iteration 4800: Loss = -10909.879680533246
Iteration 4900: Loss = -10909.879419744382
Iteration 5000: Loss = -10909.879153228989
Iteration 5100: Loss = -10909.878930455396
Iteration 5200: Loss = -10909.878744983846
Iteration 5300: Loss = -10909.878530228867
Iteration 5400: Loss = -10909.878334495847
Iteration 5500: Loss = -10909.878149949585
Iteration 5600: Loss = -10909.877969836174
Iteration 5700: Loss = -10909.877812288578
Iteration 5800: Loss = -10909.877652336161
Iteration 5900: Loss = -10909.877503296952
Iteration 6000: Loss = -10909.877401525206
Iteration 6100: Loss = -10909.877244102569
Iteration 6200: Loss = -10909.877143502175
Iteration 6300: Loss = -10909.876984414435
Iteration 6400: Loss = -10909.876897208576
Iteration 6500: Loss = -10909.876791622662
Iteration 6600: Loss = -10909.876739577563
Iteration 6700: Loss = -10909.87657339994
Iteration 6800: Loss = -10909.876530590625
Iteration 6900: Loss = -10909.876411690015
Iteration 7000: Loss = -10909.877352164525
1
Iteration 7100: Loss = -10909.876276927203
Iteration 7200: Loss = -10909.876201394298
Iteration 7300: Loss = -10909.87608348525
Iteration 7400: Loss = -10909.879249851745
1
Iteration 7500: Loss = -10909.87599299394
Iteration 7600: Loss = -10909.875986279723
Iteration 7700: Loss = -10909.875808726469
Iteration 7800: Loss = -10909.875865056249
Iteration 7900: Loss = -10909.8757283842
Iteration 8000: Loss = -10909.875819823063
Iteration 8100: Loss = -10909.87563179459
Iteration 8200: Loss = -10909.875591378632
Iteration 8300: Loss = -10909.875579800215
Iteration 8400: Loss = -10909.875508710005
Iteration 8500: Loss = -10909.875728457735
1
Iteration 8600: Loss = -10909.875974475963
2
Iteration 8700: Loss = -10909.875408134263
Iteration 8800: Loss = -10909.877466537178
1
Iteration 8900: Loss = -10909.87541092237
Iteration 9000: Loss = -10909.884818257324
1
Iteration 9100: Loss = -10909.8753228537
Iteration 9200: Loss = -10909.875330949519
Iteration 9300: Loss = -10909.875388656093
Iteration 9400: Loss = -10909.87526713246
Iteration 9500: Loss = -10909.875242675358
Iteration 9600: Loss = -10909.875654783176
1
Iteration 9700: Loss = -10909.875216968076
Iteration 9800: Loss = -10909.875215522954
Iteration 9900: Loss = -10909.907364314815
1
Iteration 10000: Loss = -10909.875175401407
Iteration 10100: Loss = -10909.875171672236
Iteration 10200: Loss = -10909.875126114515
Iteration 10300: Loss = -10909.875250993762
1
Iteration 10400: Loss = -10909.875107235122
Iteration 10500: Loss = -10909.875119641416
Iteration 10600: Loss = -10909.875611890904
1
Iteration 10700: Loss = -10909.875085395395
Iteration 10800: Loss = -10909.87509805623
Iteration 10900: Loss = -10909.875077353952
Iteration 11000: Loss = -10909.875081425553
Iteration 11100: Loss = -10909.875056172505
Iteration 11200: Loss = -10909.87503055479
Iteration 11300: Loss = -10909.875111907826
Iteration 11400: Loss = -10909.874991310016
Iteration 11500: Loss = -10909.87500725646
Iteration 11600: Loss = -10909.875804142652
1
Iteration 11700: Loss = -10909.874987738476
Iteration 11800: Loss = -10909.8750200307
Iteration 11900: Loss = -10909.875330642923
1
Iteration 12000: Loss = -10909.875751277019
2
Iteration 12100: Loss = -10909.875188793749
3
Iteration 12200: Loss = -10909.874963497537
Iteration 12300: Loss = -10909.877768495599
1
Iteration 12400: Loss = -10909.874990565231
Iteration 12500: Loss = -10909.916773101357
1
Iteration 12600: Loss = -10909.875068481957
Iteration 12700: Loss = -10909.875054079055
Iteration 12800: Loss = -10909.875001850325
Iteration 12900: Loss = -10909.876791134951
1
Iteration 13000: Loss = -10909.875169501267
2
Iteration 13100: Loss = -10909.87532062189
3
Iteration 13200: Loss = -10909.87524037712
4
Iteration 13300: Loss = -10909.906906400249
5
Iteration 13400: Loss = -10909.881591052965
6
Iteration 13500: Loss = -10909.884999614196
7
Iteration 13600: Loss = -10909.875744205472
8
Iteration 13700: Loss = -10909.87517622239
9
Iteration 13800: Loss = -10909.87738626383
10
Iteration 13900: Loss = -10909.879095508797
11
Iteration 14000: Loss = -10909.878717555337
12
Iteration 14100: Loss = -10909.882780081849
13
Iteration 14200: Loss = -10909.87742641714
14
Iteration 14300: Loss = -10909.9179809256
15
Stopping early at iteration 14300 due to no improvement.
pi: tensor([[1.0000e+00, 8.4733e-08],
        [1.4599e-01, 8.5401e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9288, 0.0712], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1584, 0.1593],
         [0.6581, 0.5475]],

        [[0.6012, 0.1869],
         [0.5542, 0.6483]],

        [[0.6440, 0.2042],
         [0.5413, 0.6892]],

        [[0.6599, 0.1914],
         [0.5883, 0.6777]],

        [[0.6311, 0.1047],
         [0.6005, 0.5997]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.002821183194503557
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 4.85601903559462e-05
Global Adjusted Rand Index: 0.004459652025068455
Average Adjusted Rand Index: 0.0067012623336440685
10833.547747695342
[0.8460920103112441, 0.004459652025068455] [0.8459754087704374, 0.0067012623336440685] [10790.004482962382, 10909.9179809256]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11018.505550269521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19751.755292837013
Iteration 100: Loss = -11012.16332349611
Iteration 200: Loss = -11011.811873184295
Iteration 300: Loss = -11011.610334028152
Iteration 400: Loss = -11011.171219808963
Iteration 500: Loss = -11010.009694730936
Iteration 600: Loss = -11009.256670667955
Iteration 700: Loss = -11008.76846938691
Iteration 800: Loss = -11008.49875660764
Iteration 900: Loss = -11008.38647008214
Iteration 1000: Loss = -11008.335659232847
Iteration 1100: Loss = -11008.297131241688
Iteration 1200: Loss = -11008.258219341556
Iteration 1300: Loss = -11008.212712697195
Iteration 1400: Loss = -11008.15458063268
Iteration 1500: Loss = -11008.077197644221
Iteration 1600: Loss = -11007.97951381151
Iteration 1700: Loss = -11007.867731397992
Iteration 1800: Loss = -11007.749027102305
Iteration 1900: Loss = -11007.631984048443
Iteration 2000: Loss = -11007.518281013723
Iteration 2100: Loss = -11007.407590403584
Iteration 2200: Loss = -11007.298713789995
Iteration 2300: Loss = -11007.186171039102
Iteration 2400: Loss = -11007.072178747692
Iteration 2500: Loss = -11006.962883433442
Iteration 2600: Loss = -11006.870952288291
Iteration 2700: Loss = -11006.803922291414
Iteration 2800: Loss = -11006.75559626245
Iteration 2900: Loss = -11006.755978287405
1
Iteration 3000: Loss = -11006.683637077873
Iteration 3100: Loss = -11006.64453632837
Iteration 3200: Loss = -11006.59433936001
Iteration 3300: Loss = -11006.534745060679
Iteration 3400: Loss = -11006.488965852535
Iteration 3500: Loss = -11006.467252479793
Iteration 3600: Loss = -11006.463981706278
Iteration 3700: Loss = -11006.456434197518
Iteration 3800: Loss = -11006.457054300365
1
Iteration 3900: Loss = -11006.455184867711
Iteration 4000: Loss = -11006.459524606296
1
Iteration 4100: Loss = -11006.455512771812
2
Iteration 4200: Loss = -11006.455276137049
Iteration 4300: Loss = -11006.475892136647
1
Iteration 4400: Loss = -11006.454961586182
Iteration 4500: Loss = -11006.454973398657
Iteration 4600: Loss = -11006.468827224886
1
Iteration 4700: Loss = -11006.454927873421
Iteration 4800: Loss = -11006.456111583093
1
Iteration 4900: Loss = -11006.455006952005
Iteration 5000: Loss = -11006.455005972079
Iteration 5100: Loss = -11006.467997311713
1
Iteration 5200: Loss = -11006.454967191214
Iteration 5300: Loss = -11006.455176559642
1
Iteration 5400: Loss = -11006.45496606123
Iteration 5500: Loss = -11006.454958692415
Iteration 5600: Loss = -11006.454939387688
Iteration 5700: Loss = -11006.455725185855
1
Iteration 5800: Loss = -11006.45492322937
Iteration 5900: Loss = -11006.454974220824
Iteration 6000: Loss = -11006.455374991554
1
Iteration 6100: Loss = -11006.454929334992
Iteration 6200: Loss = -11006.455725640284
1
Iteration 6300: Loss = -11006.454949596264
Iteration 6400: Loss = -11006.454954479472
Iteration 6500: Loss = -11006.469483849865
1
Iteration 6600: Loss = -11006.454953608329
Iteration 6700: Loss = -11006.454925717082
Iteration 6800: Loss = -11006.455169520807
1
Iteration 6900: Loss = -11006.454938886585
Iteration 7000: Loss = -11006.659024096512
1
Iteration 7100: Loss = -11006.45494334252
Iteration 7200: Loss = -11006.454979831153
Iteration 7300: Loss = -11006.457075960829
1
Iteration 7400: Loss = -11006.454935869262
Iteration 7500: Loss = -11006.45498304433
Iteration 7600: Loss = -11006.455100259825
1
Iteration 7700: Loss = -11006.45494020902
Iteration 7800: Loss = -11006.45712117345
1
Iteration 7900: Loss = -11006.454981194873
Iteration 8000: Loss = -11006.45496024861
Iteration 8100: Loss = -11006.456541792451
1
Iteration 8200: Loss = -11006.455028077551
Iteration 8300: Loss = -11006.454925307165
Iteration 8400: Loss = -11006.486865027971
1
Iteration 8500: Loss = -11006.454921767143
Iteration 8600: Loss = -11006.454957864926
Iteration 8700: Loss = -11006.485449926276
1
Iteration 8800: Loss = -11006.454976232932
Iteration 8900: Loss = -11006.454911651315
Iteration 9000: Loss = -11006.461977423454
1
Iteration 9100: Loss = -11006.454922908306
Iteration 9200: Loss = -11006.454948433322
Iteration 9300: Loss = -11006.455713917056
1
Iteration 9400: Loss = -11006.454920033628
Iteration 9500: Loss = -11006.454920423877
Iteration 9600: Loss = -11006.45826161396
1
Iteration 9700: Loss = -11006.454929896336
Iteration 9800: Loss = -11006.466841954625
1
Iteration 9900: Loss = -11006.455001613469
Iteration 10000: Loss = -11006.455681815249
1
Iteration 10100: Loss = -11006.475293419757
2
Iteration 10200: Loss = -11006.492774750279
3
Iteration 10300: Loss = -11006.458676883227
4
Iteration 10400: Loss = -11006.45501311218
Iteration 10500: Loss = -11006.454978439504
Iteration 10600: Loss = -11006.455023075836
Iteration 10700: Loss = -11006.486035331054
1
Iteration 10800: Loss = -11006.45497937543
Iteration 10900: Loss = -11006.455359244092
1
Iteration 11000: Loss = -11006.460408802774
2
Iteration 11100: Loss = -11006.503521798346
3
Iteration 11200: Loss = -11006.457591978648
4
Iteration 11300: Loss = -11006.45580039406
5
Iteration 11400: Loss = -11006.456932263345
6
Iteration 11500: Loss = -11006.456236355423
7
Iteration 11600: Loss = -11006.458501129951
8
Iteration 11700: Loss = -11006.457447720137
9
Iteration 11800: Loss = -11006.455974834937
10
Iteration 11900: Loss = -11006.455085460628
11
Iteration 12000: Loss = -11006.56352233904
12
Iteration 12100: Loss = -11006.552603526163
13
Iteration 12200: Loss = -11006.642925741957
14
Iteration 12300: Loss = -11006.480123015783
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[0.8461, 0.1539],
        [0.8176, 0.1824]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7808, 0.2192], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1782, 0.1358],
         [0.6561, 0.1034]],

        [[0.5620, 0.1448],
         [0.5774, 0.5448]],

        [[0.7154, 0.1178],
         [0.6793, 0.5305]],

        [[0.6742, 0.1305],
         [0.6623, 0.6643]],

        [[0.7142, 0.1317],
         [0.6236, 0.5802]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.01150857271232653
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0028334070515487393
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0011225368650757482
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: 0.004062736860938532
Average Adjusted Rand Index: 0.004317808605445441
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22126.171052919355
Iteration 100: Loss = -11013.11720334392
Iteration 200: Loss = -11011.933537724104
Iteration 300: Loss = -11011.51505125103
Iteration 400: Loss = -11011.070647684932
Iteration 500: Loss = -11010.358256602876
Iteration 600: Loss = -11009.504388193463
Iteration 700: Loss = -11009.061115803506
Iteration 800: Loss = -11008.73934676928
Iteration 900: Loss = -11008.576022607373
Iteration 1000: Loss = -11008.473374484627
Iteration 1100: Loss = -11008.417247018033
Iteration 1200: Loss = -11008.375810615393
Iteration 1300: Loss = -11008.34486297624
Iteration 1400: Loss = -11008.322117429923
Iteration 1500: Loss = -11008.304860482142
Iteration 1600: Loss = -11008.290923686574
Iteration 1700: Loss = -11008.27946211083
Iteration 1800: Loss = -11008.270225399101
Iteration 1900: Loss = -11008.26289068928
Iteration 2000: Loss = -11008.256984768139
Iteration 2100: Loss = -11008.251965788284
Iteration 2200: Loss = -11008.247587701966
Iteration 2300: Loss = -11008.243560703198
Iteration 2400: Loss = -11008.239758770373
Iteration 2500: Loss = -11008.236069209346
Iteration 2600: Loss = -11008.232581216038
Iteration 2700: Loss = -11008.229269417394
Iteration 2800: Loss = -11008.226180708914
Iteration 2900: Loss = -11008.223264758693
Iteration 3000: Loss = -11008.220607561007
Iteration 3100: Loss = -11008.218150460698
Iteration 3200: Loss = -11008.215840122748
Iteration 3300: Loss = -11008.213642894809
Iteration 3400: Loss = -11008.211554643709
Iteration 3500: Loss = -11008.20956938165
Iteration 3600: Loss = -11008.207539830939
Iteration 3700: Loss = -11008.205608765615
Iteration 3800: Loss = -11008.203703041381
Iteration 3900: Loss = -11008.201790624164
Iteration 4000: Loss = -11008.199911986845
Iteration 4100: Loss = -11008.198082862404
Iteration 4200: Loss = -11008.1963257441
Iteration 4300: Loss = -11008.194662934178
Iteration 4400: Loss = -11008.193037259338
Iteration 4500: Loss = -11008.191450471115
Iteration 4600: Loss = -11008.19003974272
Iteration 4700: Loss = -11008.188652205607
Iteration 4800: Loss = -11008.187443377092
Iteration 4900: Loss = -11008.186237714364
Iteration 5000: Loss = -11008.185174984228
Iteration 5100: Loss = -11008.184135818052
Iteration 5200: Loss = -11008.183222763219
Iteration 5300: Loss = -11008.182367840154
Iteration 5400: Loss = -11008.181593633253
Iteration 5500: Loss = -11008.180831731655
Iteration 5600: Loss = -11008.180148324163
Iteration 5700: Loss = -11008.17951254915
Iteration 5800: Loss = -11008.17898313928
Iteration 5900: Loss = -11008.17845043084
Iteration 6000: Loss = -11008.177980318424
Iteration 6100: Loss = -11008.177520355699
Iteration 6200: Loss = -11008.177119106067
Iteration 6300: Loss = -11008.176719876907
Iteration 6400: Loss = -11008.176330196526
Iteration 6500: Loss = -11008.176013230268
Iteration 6600: Loss = -11008.175714839428
Iteration 6700: Loss = -11008.175440063502
Iteration 6800: Loss = -11008.175135572385
Iteration 6900: Loss = -11008.176125632262
1
Iteration 7000: Loss = -11008.174641891199
Iteration 7100: Loss = -11008.174438451193
Iteration 7200: Loss = -11008.174228525333
Iteration 7300: Loss = -11008.174067079215
Iteration 7400: Loss = -11008.173867259324
Iteration 7500: Loss = -11008.173706019614
Iteration 7600: Loss = -11008.173984951198
1
Iteration 7700: Loss = -11008.173436642071
Iteration 7800: Loss = -11008.173379257227
Iteration 7900: Loss = -11008.173699618392
1
Iteration 8000: Loss = -11008.173075249986
Iteration 8100: Loss = -11008.173445173661
1
Iteration 8200: Loss = -11008.172939207478
Iteration 8300: Loss = -11008.172910469955
Iteration 8400: Loss = -11008.172694641931
Iteration 8500: Loss = -11008.172533760831
Iteration 8600: Loss = -11008.172450816712
Iteration 8700: Loss = -11008.172384617965
Iteration 8800: Loss = -11008.179244251618
1
Iteration 8900: Loss = -11008.172230503975
Iteration 9000: Loss = -11008.172178823164
Iteration 9100: Loss = -11008.179531765487
1
Iteration 9200: Loss = -11008.172037212666
Iteration 9300: Loss = -11008.17220845043
1
Iteration 9400: Loss = -11008.1720879767
Iteration 9500: Loss = -11008.171926183519
Iteration 9600: Loss = -11008.17297595809
1
Iteration 9700: Loss = -11008.171833588473
Iteration 9800: Loss = -11008.178850362263
1
Iteration 9900: Loss = -11008.171725420241
Iteration 10000: Loss = -11008.173389657153
1
Iteration 10100: Loss = -11008.171670992622
Iteration 10200: Loss = -11008.17685081059
1
Iteration 10300: Loss = -11008.171566757377
Iteration 10400: Loss = -11008.172189335168
1
Iteration 10500: Loss = -11008.17154048627
Iteration 10600: Loss = -11008.425198273928
1
Iteration 10700: Loss = -11008.171513527457
Iteration 10800: Loss = -11008.171475302952
Iteration 10900: Loss = -11008.17462472072
1
Iteration 11000: Loss = -11008.1715454491
Iteration 11100: Loss = -11008.1714002511
Iteration 11200: Loss = -11008.171636379064
1
Iteration 11300: Loss = -11008.17135092359
Iteration 11400: Loss = -11008.171349918315
Iteration 11500: Loss = -11008.17274289727
1
Iteration 11600: Loss = -11008.171295945904
Iteration 11700: Loss = -11008.29375019746
1
Iteration 11800: Loss = -11008.171297194318
Iteration 11900: Loss = -11008.171322077767
Iteration 12000: Loss = -11008.171275785078
Iteration 12100: Loss = -11008.172831527572
1
Iteration 12200: Loss = -11008.203239082592
2
Iteration 12300: Loss = -11008.171263829914
Iteration 12400: Loss = -11008.17220434465
1
Iteration 12500: Loss = -11008.171212770601
Iteration 12600: Loss = -11008.172916893656
1
Iteration 12700: Loss = -11008.171193777163
Iteration 12800: Loss = -11008.171230217475
Iteration 12900: Loss = -11008.171165260293
Iteration 13000: Loss = -11008.192450261075
1
Iteration 13100: Loss = -11008.171163068662
Iteration 13200: Loss = -11008.173396215296
1
Iteration 13300: Loss = -11008.171433297397
2
Iteration 13400: Loss = -11008.173460333439
3
Iteration 13500: Loss = -11008.172993585013
4
Iteration 13600: Loss = -11008.171131579747
Iteration 13700: Loss = -11008.175867445867
1
Iteration 13800: Loss = -11008.171118264418
Iteration 13900: Loss = -11008.171130010822
Iteration 14000: Loss = -11008.171460739395
1
Iteration 14100: Loss = -11008.1710844892
Iteration 14200: Loss = -11008.17167968595
1
Iteration 14300: Loss = -11008.171079684409
Iteration 14400: Loss = -11008.171094535155
Iteration 14500: Loss = -11008.171747788549
1
Iteration 14600: Loss = -11008.171082403029
Iteration 14700: Loss = -11008.17503114477
1
Iteration 14800: Loss = -11008.171074209082
Iteration 14900: Loss = -11008.172035605972
1
Iteration 15000: Loss = -11008.171054523325
Iteration 15100: Loss = -11008.17218326185
1
Iteration 15200: Loss = -11008.171073757383
Iteration 15300: Loss = -11008.17275102286
1
Iteration 15400: Loss = -11008.171083611718
Iteration 15500: Loss = -11008.176724054807
1
Iteration 15600: Loss = -11008.171085360913
Iteration 15700: Loss = -11008.172796022192
1
Iteration 15800: Loss = -11008.171081654224
Iteration 15900: Loss = -11008.17113118204
Iteration 16000: Loss = -11008.171009571313
Iteration 16100: Loss = -11008.171185190817
1
Iteration 16200: Loss = -11008.171053253654
Iteration 16300: Loss = -11008.217449986107
1
Iteration 16400: Loss = -11008.171051603844
Iteration 16500: Loss = -11008.170994739003
Iteration 16600: Loss = -11008.171335116655
1
Iteration 16700: Loss = -11008.17102379147
Iteration 16800: Loss = -11008.235053735702
1
Iteration 16900: Loss = -11008.171050647825
Iteration 17000: Loss = -11008.171046713891
Iteration 17100: Loss = -11008.171964294406
1
Iteration 17200: Loss = -11008.17103749607
Iteration 17300: Loss = -11008.171036158798
Iteration 17400: Loss = -11008.171781320463
1
Iteration 17500: Loss = -11008.171042464559
Iteration 17600: Loss = -11008.191484439913
1
Iteration 17700: Loss = -11008.171044069293
Iteration 17800: Loss = -11008.17103807969
Iteration 17900: Loss = -11008.173020784181
1
Iteration 18000: Loss = -11008.171021940858
Iteration 18100: Loss = -11008.171042234602
Iteration 18200: Loss = -11008.171034685423
Iteration 18300: Loss = -11008.171130336812
Iteration 18400: Loss = -11008.17103188073
Iteration 18500: Loss = -11008.171058979246
Iteration 18600: Loss = -11008.171102494138
Iteration 18700: Loss = -11008.17103389266
Iteration 18800: Loss = -11008.513177073539
1
Iteration 18900: Loss = -11008.171024198307
Iteration 19000: Loss = -11008.171018813477
Iteration 19100: Loss = -11008.171703841468
1
Iteration 19200: Loss = -11008.171089105743
Iteration 19300: Loss = -11008.171004067872
Iteration 19400: Loss = -11008.25852876629
1
Iteration 19500: Loss = -11008.171040624486
Iteration 19600: Loss = -11008.171032994398
Iteration 19700: Loss = -11008.17121082235
1
Iteration 19800: Loss = -11008.292955356208
2
Iteration 19900: Loss = -11008.171029483216
pi: tensor([[1.0000e+00, 1.7233e-07],
        [4.3172e-01, 5.6828e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9186, 0.0814], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1592, 0.2062],
         [0.6671, 0.3101]],

        [[0.5701, 0.2154],
         [0.7012, 0.7243]],

        [[0.5265, 0.2026],
         [0.6382, 0.6047]],

        [[0.7310, 0.2714],
         [0.5595, 0.5856]],

        [[0.6944, 0.1465],
         [0.5458, 0.5562]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.00014584426996972635
Average Adjusted Rand Index: -0.0006966101412660377
11018.505550269521
[0.004062736860938532, -0.00014584426996972635] [0.004317808605445441, -0.0006966101412660377] [11006.480123015783, 11008.171711787767]
-------------------------------------
This iteration is 51
True Objective function: Loss = -10677.856156730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23162.02968459702
Iteration 100: Loss = -10791.866517906667
Iteration 200: Loss = -10791.12111307624
Iteration 300: Loss = -10790.242027357359
Iteration 400: Loss = -10788.934990947282
Iteration 500: Loss = -10788.378998833878
Iteration 600: Loss = -10788.13411627499
Iteration 700: Loss = -10788.024922502822
Iteration 800: Loss = -10787.963225934132
Iteration 900: Loss = -10787.918668122436
Iteration 1000: Loss = -10787.876986203279
Iteration 1100: Loss = -10787.83269934829
Iteration 1200: Loss = -10787.788655467944
Iteration 1300: Loss = -10787.750877883633
Iteration 1400: Loss = -10787.72193652838
Iteration 1500: Loss = -10787.70075550465
Iteration 1600: Loss = -10787.685337729496
Iteration 1700: Loss = -10787.67388958885
Iteration 1800: Loss = -10787.66522800821
Iteration 1900: Loss = -10787.658521011172
Iteration 2000: Loss = -10787.653232912546
Iteration 2100: Loss = -10787.648835979779
Iteration 2200: Loss = -10787.645333664914
Iteration 2300: Loss = -10787.642359973332
Iteration 2400: Loss = -10787.639861829011
Iteration 2500: Loss = -10787.637686001424
Iteration 2600: Loss = -10787.635818154975
Iteration 2700: Loss = -10787.634123831118
Iteration 2800: Loss = -10787.632640596103
Iteration 2900: Loss = -10787.63136394391
Iteration 3000: Loss = -10787.630286893207
Iteration 3100: Loss = -10787.629247629444
Iteration 3200: Loss = -10787.628395753594
Iteration 3300: Loss = -10787.627641100671
Iteration 3400: Loss = -10787.626982219645
Iteration 3500: Loss = -10787.62632979679
Iteration 3600: Loss = -10787.625762517659
Iteration 3700: Loss = -10787.625233991252
Iteration 3800: Loss = -10787.624759449423
Iteration 3900: Loss = -10787.62436118432
Iteration 4000: Loss = -10787.623962869358
Iteration 4100: Loss = -10787.623599620818
Iteration 4200: Loss = -10787.623291295451
Iteration 4300: Loss = -10787.622956407908
Iteration 4400: Loss = -10787.622678300913
Iteration 4500: Loss = -10787.622396859273
Iteration 4600: Loss = -10787.622160894547
Iteration 4700: Loss = -10787.621934119055
Iteration 4800: Loss = -10787.62171940798
Iteration 4900: Loss = -10787.621519325603
Iteration 5000: Loss = -10787.621369012772
Iteration 5100: Loss = -10787.6211537384
Iteration 5200: Loss = -10787.62102802088
Iteration 5300: Loss = -10787.620892585814
Iteration 5400: Loss = -10787.62071823877
Iteration 5500: Loss = -10787.620569357183
Iteration 5600: Loss = -10787.620495802948
Iteration 5700: Loss = -10787.620348297172
Iteration 5800: Loss = -10787.620232405126
Iteration 5900: Loss = -10787.620129846362
Iteration 6000: Loss = -10787.6200199013
Iteration 6100: Loss = -10787.619957521221
Iteration 6200: Loss = -10787.619845579455
Iteration 6300: Loss = -10787.619781744383
Iteration 6400: Loss = -10787.619710065184
Iteration 6500: Loss = -10787.619608130219
Iteration 6600: Loss = -10787.619567900087
Iteration 6700: Loss = -10787.619531070775
Iteration 6800: Loss = -10787.619488958697
Iteration 6900: Loss = -10787.619943436588
1
Iteration 7000: Loss = -10787.619385181872
Iteration 7100: Loss = -10787.619292982494
Iteration 7200: Loss = -10787.619835961983
1
Iteration 7300: Loss = -10787.61922155497
Iteration 7400: Loss = -10787.619214852351
Iteration 7500: Loss = -10787.619115175998
Iteration 7600: Loss = -10787.61909207521
Iteration 7700: Loss = -10787.626055153376
1
Iteration 7800: Loss = -10787.676511177136
2
Iteration 7900: Loss = -10787.619141655832
Iteration 8000: Loss = -10787.61892555281
Iteration 8100: Loss = -10787.618999025472
Iteration 8200: Loss = -10787.61890750049
Iteration 8300: Loss = -10787.619145546214
1
Iteration 8400: Loss = -10787.618838475764
Iteration 8500: Loss = -10787.619593096604
1
Iteration 8600: Loss = -10787.618773711016
Iteration 8700: Loss = -10787.63414308938
1
Iteration 8800: Loss = -10787.618759372257
Iteration 8900: Loss = -10787.618718426133
Iteration 9000: Loss = -10787.974720740907
1
Iteration 9100: Loss = -10787.618696474967
Iteration 9200: Loss = -10787.618660642174
Iteration 9300: Loss = -10787.618672081522
Iteration 9400: Loss = -10787.618956195543
1
Iteration 9500: Loss = -10787.618626650079
Iteration 9600: Loss = -10787.618644771876
Iteration 9700: Loss = -10787.618601283479
Iteration 9800: Loss = -10787.618572473135
Iteration 9900: Loss = -10787.62723333185
1
Iteration 10000: Loss = -10787.618615161638
Iteration 10100: Loss = -10787.618547523025
Iteration 10200: Loss = -10787.661143376165
1
Iteration 10300: Loss = -10787.61853225796
Iteration 10400: Loss = -10787.618532151428
Iteration 10500: Loss = -10787.690694340436
1
Iteration 10600: Loss = -10787.618496097472
Iteration 10700: Loss = -10787.61852710126
Iteration 10800: Loss = -10787.621531756953
1
Iteration 10900: Loss = -10787.618485884192
Iteration 11000: Loss = -10787.618475421963
Iteration 11100: Loss = -10787.618593210562
1
Iteration 11200: Loss = -10787.61848135892
Iteration 11300: Loss = -10787.618936705188
1
Iteration 11400: Loss = -10787.61834633229
Iteration 11500: Loss = -10787.618334053508
Iteration 11600: Loss = -10787.618337486425
Iteration 11700: Loss = -10787.618310357195
Iteration 11800: Loss = -10787.747153184284
1
Iteration 11900: Loss = -10787.618380668397
Iteration 12000: Loss = -10787.618309391493
Iteration 12100: Loss = -10787.691227171872
1
Iteration 12200: Loss = -10787.61828219848
Iteration 12300: Loss = -10787.91534590986
1
Iteration 12400: Loss = -10787.618273672004
Iteration 12500: Loss = -10787.618722922118
1
Iteration 12600: Loss = -10787.618318197969
Iteration 12700: Loss = -10787.61828914987
Iteration 12800: Loss = -10787.618507275996
1
Iteration 12900: Loss = -10787.626931467938
2
Iteration 13000: Loss = -10787.618255344118
Iteration 13100: Loss = -10787.618612004511
1
Iteration 13200: Loss = -10787.699445355976
2
Iteration 13300: Loss = -10787.618258045823
Iteration 13400: Loss = -10787.618372579422
1
Iteration 13500: Loss = -10787.645972219967
2
Iteration 13600: Loss = -10787.621958004856
3
Iteration 13700: Loss = -10787.618276222927
Iteration 13800: Loss = -10787.619354568445
1
Iteration 13900: Loss = -10787.653120734458
2
Iteration 14000: Loss = -10787.618273725695
Iteration 14100: Loss = -10787.619985614232
1
Iteration 14200: Loss = -10787.618281386343
Iteration 14300: Loss = -10787.618508395472
1
Iteration 14400: Loss = -10787.6220535248
2
Iteration 14500: Loss = -10787.618253032033
Iteration 14600: Loss = -10787.618346007117
Iteration 14700: Loss = -10787.826106091867
1
Iteration 14800: Loss = -10787.618187981963
Iteration 14900: Loss = -10787.61851040113
1
Iteration 15000: Loss = -10787.619143652304
2
Iteration 15100: Loss = -10787.618250587722
Iteration 15200: Loss = -10787.639776476202
1
Iteration 15300: Loss = -10787.61824075238
Iteration 15400: Loss = -10787.618391514046
1
Iteration 15500: Loss = -10787.671228274685
2
Iteration 15600: Loss = -10787.61823242756
Iteration 15700: Loss = -10787.620536898032
1
Iteration 15800: Loss = -10787.618242352151
Iteration 15900: Loss = -10787.618551854885
1
Iteration 16000: Loss = -10787.736428291082
2
Iteration 16100: Loss = -10787.618196241365
Iteration 16200: Loss = -10787.64556197117
1
Iteration 16300: Loss = -10787.618174072913
Iteration 16400: Loss = -10787.619009047903
1
Iteration 16500: Loss = -10787.790136978769
2
Iteration 16600: Loss = -10787.618191390662
Iteration 16700: Loss = -10787.618466121563
1
Iteration 16800: Loss = -10787.640694347647
2
Iteration 16900: Loss = -10787.618260988973
Iteration 17000: Loss = -10787.618301533492
Iteration 17100: Loss = -10787.618586987088
1
Iteration 17200: Loss = -10787.658815831963
2
Iteration 17300: Loss = -10787.61815905734
Iteration 17400: Loss = -10787.633547916534
1
Iteration 17500: Loss = -10787.625523170213
2
Iteration 17600: Loss = -10787.618260739615
3
Iteration 17700: Loss = -10787.61826213176
4
Iteration 17800: Loss = -10787.619029858639
5
Iteration 17900: Loss = -10787.62973973426
6
Iteration 18000: Loss = -10787.654765085179
7
Iteration 18100: Loss = -10787.61823241367
Iteration 18200: Loss = -10787.618306887858
Iteration 18300: Loss = -10787.61905759208
1
Iteration 18400: Loss = -10787.618561807492
2
Iteration 18500: Loss = -10787.618221211167
Iteration 18600: Loss = -10787.618269825616
Iteration 18700: Loss = -10787.64469122525
1
Iteration 18800: Loss = -10787.618170797732
Iteration 18900: Loss = -10787.618700967285
1
Iteration 19000: Loss = -10787.636489959075
2
Iteration 19100: Loss = -10787.618173890784
Iteration 19200: Loss = -10787.61859127557
1
Iteration 19300: Loss = -10787.61970169067
2
Iteration 19400: Loss = -10787.64344211725
3
Iteration 19500: Loss = -10787.618170194575
Iteration 19600: Loss = -10787.62112922011
1
Iteration 19700: Loss = -10787.642827138548
2
Iteration 19800: Loss = -10787.618192075452
Iteration 19900: Loss = -10787.622756334224
1
pi: tensor([[9.8745e-01, 1.2555e-02],
        [3.3113e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9633, 0.0367], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1612, 0.1230],
         [0.6273, 0.1238]],

        [[0.6767, 0.1075],
         [0.5052, 0.5665]],

        [[0.6956, 0.0883],
         [0.6478, 0.6926]],

        [[0.6615, 0.1620],
         [0.5958, 0.7215]],

        [[0.6641, 0.1593],
         [0.6788, 0.5802]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.01717781179455718
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.0006803382252891437
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0019209323236274677
Global Adjusted Rand Index: -0.0014320759649205448
Average Adjusted Rand Index: -0.0029081600603304237
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22652.76273999939
Iteration 100: Loss = -10793.934704764404
Iteration 200: Loss = -10792.337816353382
Iteration 300: Loss = -10792.008234281124
Iteration 400: Loss = -10791.815375731467
Iteration 500: Loss = -10791.625007968316
Iteration 600: Loss = -10791.477855577717
Iteration 700: Loss = -10791.369543135941
Iteration 800: Loss = -10791.269235688602
Iteration 900: Loss = -10791.17121243577
Iteration 1000: Loss = -10791.076859280214
Iteration 1100: Loss = -10790.990860212894
Iteration 1200: Loss = -10790.914756693046
Iteration 1300: Loss = -10790.848071557133
Iteration 1400: Loss = -10790.789916680853
Iteration 1500: Loss = -10790.739030835755
Iteration 1600: Loss = -10790.694214890631
Iteration 1700: Loss = -10790.654510071792
Iteration 1800: Loss = -10790.619038841001
Iteration 1900: Loss = -10790.58713768923
Iteration 2000: Loss = -10790.55826328274
Iteration 2100: Loss = -10790.532204289773
Iteration 2200: Loss = -10790.50884941049
Iteration 2300: Loss = -10790.488309519422
Iteration 2400: Loss = -10790.470083607892
Iteration 2500: Loss = -10790.453890014214
Iteration 2600: Loss = -10790.43941170884
Iteration 2700: Loss = -10790.426436338354
Iteration 2800: Loss = -10790.41481799486
Iteration 2900: Loss = -10790.404486519545
Iteration 3000: Loss = -10790.395257535629
Iteration 3100: Loss = -10790.387096790457
Iteration 3200: Loss = -10790.379687803723
Iteration 3300: Loss = -10790.373025923369
Iteration 3400: Loss = -10790.366918124377
Iteration 3500: Loss = -10790.361241062557
Iteration 3600: Loss = -10790.355841943294
Iteration 3700: Loss = -10790.350629985847
Iteration 3800: Loss = -10790.345532590265
Iteration 3900: Loss = -10790.340451699027
Iteration 4000: Loss = -10790.335063083277
Iteration 4100: Loss = -10790.32926134131
Iteration 4200: Loss = -10790.322817347223
Iteration 4300: Loss = -10790.31552263835
Iteration 4400: Loss = -10790.307039740104
Iteration 4500: Loss = -10790.298545980684
Iteration 4600: Loss = -10790.284817776455
Iteration 4700: Loss = -10790.269629404711
Iteration 4800: Loss = -10790.25043919061
Iteration 4900: Loss = -10790.225420600773
Iteration 5000: Loss = -10790.195446242162
Iteration 5100: Loss = -10790.164789391514
Iteration 5200: Loss = -10790.138936296618
Iteration 5300: Loss = -10790.122637295666
Iteration 5400: Loss = -10790.113811569672
Iteration 5500: Loss = -10790.1092786263
Iteration 5600: Loss = -10790.107183570148
Iteration 5700: Loss = -10790.105466251827
Iteration 5800: Loss = -10790.10459619922
Iteration 5900: Loss = -10790.116860167873
1
Iteration 6000: Loss = -10790.103483350005
Iteration 6100: Loss = -10790.104078927847
1
Iteration 6200: Loss = -10790.10339801242
Iteration 6300: Loss = -10790.103808410937
1
Iteration 6400: Loss = -10790.103137414273
Iteration 6500: Loss = -10790.103073909138
Iteration 6600: Loss = -10790.10254368012
Iteration 6700: Loss = -10790.10270368001
1
Iteration 6800: Loss = -10790.103835219248
2
Iteration 6900: Loss = -10790.101622688926
Iteration 7000: Loss = -10790.10155351411
Iteration 7100: Loss = -10790.107077972147
1
Iteration 7200: Loss = -10790.101343721088
Iteration 7300: Loss = -10790.101299062295
Iteration 7400: Loss = -10790.10126494404
Iteration 7500: Loss = -10790.101471657228
1
Iteration 7600: Loss = -10790.101161326469
Iteration 7700: Loss = -10790.101930049905
1
Iteration 7800: Loss = -10790.101479627228
2
Iteration 7900: Loss = -10790.101034252379
Iteration 8000: Loss = -10790.100979765608
Iteration 8100: Loss = -10790.106811431018
1
Iteration 8200: Loss = -10790.100923024309
Iteration 8300: Loss = -10790.103900991982
1
Iteration 8400: Loss = -10790.13338537888
2
Iteration 8500: Loss = -10790.11244447775
3
Iteration 8600: Loss = -10790.101887402583
4
Iteration 8700: Loss = -10790.101792219844
5
Iteration 8800: Loss = -10790.105203243178
6
Iteration 8900: Loss = -10790.103929302133
7
Iteration 9000: Loss = -10790.101279769322
8
Iteration 9100: Loss = -10790.106015321313
9
Iteration 9200: Loss = -10790.156283271053
10
Iteration 9300: Loss = -10790.118931556714
11
Iteration 9400: Loss = -10790.101039145537
12
Iteration 9500: Loss = -10790.10542725714
13
Iteration 9600: Loss = -10790.101333283006
14
Iteration 9700: Loss = -10790.100925850562
Iteration 9800: Loss = -10790.234729337863
1
Iteration 9900: Loss = -10790.101364982087
2
Iteration 10000: Loss = -10790.111825668808
3
Iteration 10100: Loss = -10790.123179440923
4
Iteration 10200: Loss = -10790.106317537593
5
Iteration 10300: Loss = -10790.106515203563
6
Iteration 10400: Loss = -10790.114179027567
7
Iteration 10500: Loss = -10790.101640656354
8
Iteration 10600: Loss = -10790.101113707242
9
Iteration 10700: Loss = -10790.10099111451
Iteration 10800: Loss = -10790.100998494074
Iteration 10900: Loss = -10790.126501661873
1
Iteration 11000: Loss = -10790.141518217886
2
Iteration 11100: Loss = -10790.168608035347
3
Iteration 11200: Loss = -10790.101427722357
4
Iteration 11300: Loss = -10790.10313455997
5
Iteration 11400: Loss = -10790.166445835857
6
Iteration 11500: Loss = -10790.128993168986
7
Iteration 11600: Loss = -10790.101315279062
8
Iteration 11700: Loss = -10790.114399724587
9
Iteration 11800: Loss = -10790.117005111408
10
Iteration 11900: Loss = -10790.101346094569
11
Iteration 12000: Loss = -10790.103029729336
12
Iteration 12100: Loss = -10790.1160894622
13
Iteration 12200: Loss = -10790.101326373278
14
Iteration 12300: Loss = -10790.107817320457
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[0.7447, 0.2553],
        [0.1427, 0.8573]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0220, 0.9780], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1831, 0.2632],
         [0.5253, 0.1509]],

        [[0.7304, 0.1657],
         [0.7108, 0.5076]],

        [[0.5170, 0.1671],
         [0.7192, 0.6875]],

        [[0.5260, 0.1629],
         [0.5605, 0.5185]],

        [[0.7104, 0.1648],
         [0.6711, 0.6644]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.016025857647765086
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: 0.007848706559942445
Average Adjusted Rand Index: 0.005719543094795164
10677.856156730853
[-0.0014320759649205448, 0.007848706559942445] [-0.0029081600603304237, 0.005719543094795164] [10787.618186937987, 10790.107817320457]
-------------------------------------
This iteration is 52
True Objective function: Loss = -10934.406228228168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20716.540429399534
Iteration 100: Loss = -11053.42934096387
Iteration 200: Loss = -11052.827999513887
Iteration 300: Loss = -11052.701631891001
Iteration 400: Loss = -11052.635460503177
Iteration 500: Loss = -11052.583909596868
Iteration 600: Loss = -11052.535721171836
Iteration 700: Loss = -11052.486728283722
Iteration 800: Loss = -11052.436436773916
Iteration 900: Loss = -11052.38483116144
Iteration 1000: Loss = -11052.330208806814
Iteration 1100: Loss = -11052.26931358391
Iteration 1200: Loss = -11052.197648918365
Iteration 1300: Loss = -11052.106763196995
Iteration 1400: Loss = -11051.967124561512
Iteration 1500: Loss = -11051.610579212655
Iteration 1600: Loss = -11051.022988755998
Iteration 1700: Loss = -11050.790702714969
Iteration 1800: Loss = -11050.689138935995
Iteration 1900: Loss = -11050.633806150776
Iteration 2000: Loss = -11050.601744090527
Iteration 2100: Loss = -11050.582682323464
Iteration 2200: Loss = -11050.570929029866
Iteration 2300: Loss = -11050.56248569441
Iteration 2400: Loss = -11050.555542100396
Iteration 2500: Loss = -11050.549359728448
Iteration 2600: Loss = -11050.543579880958
Iteration 2700: Loss = -11050.538253649305
Iteration 2800: Loss = -11050.533508398694
Iteration 2900: Loss = -11050.529368665048
Iteration 3000: Loss = -11050.525893573207
Iteration 3100: Loss = -11050.523031395802
Iteration 3200: Loss = -11050.520681816326
Iteration 3300: Loss = -11050.518769719312
Iteration 3400: Loss = -11050.517317761694
Iteration 3500: Loss = -11050.51611885461
Iteration 3600: Loss = -11050.515152946202
Iteration 3700: Loss = -11050.514375328856
Iteration 3800: Loss = -11050.51383368079
Iteration 3900: Loss = -11050.513324975449
Iteration 4000: Loss = -11050.512948676294
Iteration 4100: Loss = -11050.512671803264
Iteration 4200: Loss = -11050.512409187244
Iteration 4300: Loss = -11050.512184160456
Iteration 4400: Loss = -11050.512038302028
Iteration 4500: Loss = -11050.511880408567
Iteration 4600: Loss = -11050.511766518885
Iteration 4700: Loss = -11050.511651984474
Iteration 4800: Loss = -11050.51163785322
Iteration 4900: Loss = -11050.51251077615
1
Iteration 5000: Loss = -11050.51145034695
Iteration 5100: Loss = -11050.51142547861
Iteration 5200: Loss = -11050.521970423342
1
Iteration 5300: Loss = -11050.511326083517
Iteration 5400: Loss = -11050.511298413223
Iteration 5500: Loss = -11050.511290571496
Iteration 5600: Loss = -11050.511243210405
Iteration 5700: Loss = -11050.511231692946
Iteration 5800: Loss = -11050.511209647486
Iteration 5900: Loss = -11050.51115126472
Iteration 6000: Loss = -11050.511284395616
1
Iteration 6100: Loss = -11050.511151137032
Iteration 6200: Loss = -11050.5110761571
Iteration 6300: Loss = -11050.51160704367
1
Iteration 6400: Loss = -11050.511076911409
Iteration 6500: Loss = -11050.511065126428
Iteration 6600: Loss = -11050.511048190257
Iteration 6700: Loss = -11050.511051648142
Iteration 6800: Loss = -11050.51102287561
Iteration 6900: Loss = -11050.51103255312
Iteration 7000: Loss = -11050.511072121186
Iteration 7100: Loss = -11050.51101714287
Iteration 7200: Loss = -11050.511290157405
1
Iteration 7300: Loss = -11050.51124864305
2
Iteration 7400: Loss = -11050.513253531779
3
Iteration 7500: Loss = -11050.51106563355
Iteration 7600: Loss = -11050.51178036656
1
Iteration 7700: Loss = -11050.511036842114
Iteration 7800: Loss = -11050.511565135199
1
Iteration 7900: Loss = -11050.51126605162
2
Iteration 8000: Loss = -11050.511376353594
3
Iteration 8100: Loss = -11050.514288206341
4
Iteration 8200: Loss = -11050.51331809728
5
Iteration 8300: Loss = -11050.51094868662
Iteration 8400: Loss = -11050.540558280112
1
Iteration 8500: Loss = -11050.510932722944
Iteration 8600: Loss = -11050.510929554748
Iteration 8700: Loss = -11050.51096159537
Iteration 8800: Loss = -11050.510941555951
Iteration 8900: Loss = -11050.828305803852
1
Iteration 9000: Loss = -11050.510921032836
Iteration 9100: Loss = -11050.511317654158
1
Iteration 9200: Loss = -11050.512998445925
2
Iteration 9300: Loss = -11050.510909604918
Iteration 9400: Loss = -11050.511581389468
1
Iteration 9500: Loss = -11050.729467241588
2
Iteration 9600: Loss = -11050.510945104661
Iteration 9700: Loss = -11050.511031475588
Iteration 9800: Loss = -11050.512012203428
1
Iteration 9900: Loss = -11050.514060393656
2
Iteration 10000: Loss = -11050.513082482586
3
Iteration 10100: Loss = -11050.520409021232
4
Iteration 10200: Loss = -11050.513139407358
5
Iteration 10300: Loss = -11050.511523009984
6
Iteration 10400: Loss = -11050.615180108787
7
Iteration 10500: Loss = -11050.510954529547
Iteration 10600: Loss = -11050.511607042887
1
Iteration 10700: Loss = -11050.680096083388
2
Iteration 10800: Loss = -11050.510939670796
Iteration 10900: Loss = -11050.514452906567
1
Iteration 11000: Loss = -11050.51091683668
Iteration 11100: Loss = -11050.59740374414
1
Iteration 11200: Loss = -11050.510881876458
Iteration 11300: Loss = -11050.512352103824
1
Iteration 11400: Loss = -11050.51116826306
2
Iteration 11500: Loss = -11050.511214280903
3
Iteration 11600: Loss = -11050.51273361185
4
Iteration 11700: Loss = -11050.6582828272
5
Iteration 11800: Loss = -11050.51088243879
Iteration 11900: Loss = -11050.511211013323
1
Iteration 12000: Loss = -11050.511409381235
2
Iteration 12100: Loss = -11050.510971083635
Iteration 12200: Loss = -11050.608724902806
1
Iteration 12300: Loss = -11050.510909771276
Iteration 12400: Loss = -11050.515553827592
1
Iteration 12500: Loss = -11050.510940933522
Iteration 12600: Loss = -11050.51124403227
1
Iteration 12700: Loss = -11050.521430898618
2
Iteration 12800: Loss = -11050.519276509938
3
Iteration 12900: Loss = -11050.522423183194
4
Iteration 13000: Loss = -11050.510930089044
Iteration 13100: Loss = -11050.515313466747
1
Iteration 13200: Loss = -11050.513398942816
2
Iteration 13300: Loss = -11050.563782297293
3
Iteration 13400: Loss = -11050.510881166578
Iteration 13500: Loss = -11050.517844676448
1
Iteration 13600: Loss = -11050.54809293758
2
Iteration 13700: Loss = -11050.510895570578
Iteration 13800: Loss = -11050.511318616469
1
Iteration 13900: Loss = -11050.511720130226
2
Iteration 14000: Loss = -11050.51098544953
Iteration 14100: Loss = -11050.551567613933
1
Iteration 14200: Loss = -11050.51381090375
2
Iteration 14300: Loss = -11050.513780499457
3
Iteration 14400: Loss = -11050.513126789141
4
Iteration 14500: Loss = -11050.511598482617
5
Iteration 14600: Loss = -11050.518413881975
6
Iteration 14700: Loss = -11050.511396240436
7
Iteration 14800: Loss = -11050.511314565385
8
Iteration 14900: Loss = -11050.511857395712
9
Iteration 15000: Loss = -11050.516167172416
10
Iteration 15100: Loss = -11050.517590390651
11
Iteration 15200: Loss = -11050.516937433173
12
Iteration 15300: Loss = -11050.510933013275
Iteration 15400: Loss = -11050.512763224431
1
Iteration 15500: Loss = -11050.510903444709
Iteration 15600: Loss = -11050.511635252755
1
Iteration 15700: Loss = -11050.51217037529
2
Iteration 15800: Loss = -11050.511213687692
3
Iteration 15900: Loss = -11050.518160452471
4
Iteration 16000: Loss = -11050.511021040624
5
Iteration 16100: Loss = -11050.518728256977
6
Iteration 16200: Loss = -11050.512257470687
7
Iteration 16300: Loss = -11050.510962692095
Iteration 16400: Loss = -11050.527408941804
1
Iteration 16500: Loss = -11050.513786049463
2
Iteration 16600: Loss = -11050.511525670034
3
Iteration 16700: Loss = -11050.5109628165
Iteration 16800: Loss = -11050.871139392559
1
Iteration 16900: Loss = -11050.515540629092
2
Iteration 17000: Loss = -11050.510934555197
Iteration 17100: Loss = -11050.511998679025
1
Iteration 17200: Loss = -11050.510912615486
Iteration 17300: Loss = -11050.58405410614
1
Iteration 17400: Loss = -11050.510893293214
Iteration 17500: Loss = -11050.511593178335
1
Iteration 17600: Loss = -11050.51129072028
2
Iteration 17700: Loss = -11050.51246007213
3
Iteration 17800: Loss = -11050.514139873332
4
Iteration 17900: Loss = -11050.512131962934
5
Iteration 18000: Loss = -11050.5118140312
6
Iteration 18100: Loss = -11050.522399079975
7
Iteration 18200: Loss = -11050.511986029343
8
Iteration 18300: Loss = -11050.514386076555
9
Iteration 18400: Loss = -11050.510957829527
Iteration 18500: Loss = -11050.512564534649
1
Iteration 18600: Loss = -11050.511034043053
Iteration 18700: Loss = -11050.51480561973
1
Iteration 18800: Loss = -11050.531502655245
2
Iteration 18900: Loss = -11050.511197693295
3
Iteration 19000: Loss = -11050.510999117445
Iteration 19100: Loss = -11050.511121332991
1
Iteration 19200: Loss = -11050.511026905204
Iteration 19300: Loss = -11050.51680529023
1
Iteration 19400: Loss = -11050.532050160513
2
Iteration 19500: Loss = -11050.511684011477
3
Iteration 19600: Loss = -11050.511475140227
4
Iteration 19700: Loss = -11050.511148736468
5
Iteration 19800: Loss = -11050.511146681609
6
Iteration 19900: Loss = -11050.513294702747
7
pi: tensor([[0.0461, 0.9539],
        [0.0147, 0.9853]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2752, 0.7248], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1881, 0.1756],
         [0.6513, 0.1636]],

        [[0.6802, 0.1096],
         [0.6397, 0.5837]],

        [[0.7067, 0.1677],
         [0.7075, 0.5602]],

        [[0.6935, 0.0671],
         [0.7089, 0.5495]],

        [[0.7295, 0.1947],
         [0.6625, 0.7060]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005093525538803316
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24056.47558995558
Iteration 100: Loss = -11053.963316470084
Iteration 200: Loss = -11053.21759668249
Iteration 300: Loss = -11052.988761177645
Iteration 400: Loss = -11052.867602416574
Iteration 500: Loss = -11052.793344698412
Iteration 600: Loss = -11052.739733744756
Iteration 700: Loss = -11052.689118800827
Iteration 800: Loss = -11052.612317260273
Iteration 900: Loss = -11052.424673591471
Iteration 1000: Loss = -11052.22701310784
Iteration 1100: Loss = -11052.117117819344
Iteration 1200: Loss = -11052.02448756679
Iteration 1300: Loss = -11051.946932485878
Iteration 1400: Loss = -11051.894453183835
Iteration 1500: Loss = -11051.864701108641
Iteration 1600: Loss = -11051.849161143637
Iteration 1700: Loss = -11051.840688170563
Iteration 1800: Loss = -11051.835384920258
Iteration 1900: Loss = -11051.831428404563
Iteration 2000: Loss = -11051.827901427785
Iteration 2100: Loss = -11051.824215272416
Iteration 2200: Loss = -11051.819827915357
Iteration 2300: Loss = -11051.813782504301
Iteration 2400: Loss = -11051.803712232411
Iteration 2500: Loss = -11051.781335106512
Iteration 2600: Loss = -11051.704815691579
Iteration 2700: Loss = -11051.600082314439
Iteration 2800: Loss = -11051.577271946544
Iteration 2900: Loss = -11051.568879253187
Iteration 3000: Loss = -11051.564817212082
Iteration 3100: Loss = -11051.562311209618
Iteration 3200: Loss = -11051.560480755514
Iteration 3300: Loss = -11051.55901729171
Iteration 3400: Loss = -11051.55775289594
Iteration 3500: Loss = -11051.55668265123
Iteration 3600: Loss = -11051.55574860458
Iteration 3700: Loss = -11051.554922125155
Iteration 3800: Loss = -11051.554125870258
Iteration 3900: Loss = -11051.553378514427
Iteration 4000: Loss = -11051.552564511223
Iteration 4100: Loss = -11051.551660503063
Iteration 4200: Loss = -11051.550589108536
Iteration 4300: Loss = -11051.54919547838
Iteration 4400: Loss = -11051.54703954982
Iteration 4500: Loss = -11051.54317677707
Iteration 4600: Loss = -11051.535129833424
Iteration 4700: Loss = -11051.49680697434
Iteration 4800: Loss = -11050.730682430863
Iteration 4900: Loss = -11050.543213894314
Iteration 5000: Loss = -11050.51922551404
Iteration 5100: Loss = -11050.513992066135
Iteration 5200: Loss = -11050.513415658392
Iteration 5300: Loss = -11050.512032218272
Iteration 5400: Loss = -11050.511664490043
Iteration 5500: Loss = -11050.511494815977
Iteration 5600: Loss = -11050.51131083306
Iteration 5700: Loss = -11050.523266347382
1
Iteration 5800: Loss = -11050.511159569081
Iteration 5900: Loss = -11050.511124801074
Iteration 6000: Loss = -11050.511145082199
Iteration 6100: Loss = -11050.511085440587
Iteration 6200: Loss = -11050.541584824774
1
Iteration 6300: Loss = -11050.511011226245
Iteration 6400: Loss = -11050.511034650928
Iteration 6500: Loss = -11050.511016024753
Iteration 6600: Loss = -11050.510996310722
Iteration 6700: Loss = -11050.51156885132
1
Iteration 6800: Loss = -11050.510969789964
Iteration 6900: Loss = -11050.511665349477
1
Iteration 7000: Loss = -11050.52489644745
2
Iteration 7100: Loss = -11050.511335379497
3
Iteration 7200: Loss = -11050.511016628388
Iteration 7300: Loss = -11050.510941547751
Iteration 7400: Loss = -11050.510964694133
Iteration 7500: Loss = -11050.510977126669
Iteration 7600: Loss = -11050.51252382595
1
Iteration 7700: Loss = -11050.51099020477
Iteration 7800: Loss = -11050.510956385595
Iteration 7900: Loss = -11050.511264256242
1
Iteration 8000: Loss = -11050.510992755728
Iteration 8100: Loss = -11050.510953710645
Iteration 8200: Loss = -11050.511635033066
1
Iteration 8300: Loss = -11050.511471276477
2
Iteration 8400: Loss = -11050.520953736315
3
Iteration 8500: Loss = -11050.512457139446
4
Iteration 8600: Loss = -11050.512876150013
5
Iteration 8700: Loss = -11050.516721264983
6
Iteration 8800: Loss = -11050.510902492135
Iteration 8900: Loss = -11050.51089929454
Iteration 9000: Loss = -11050.511070188128
1
Iteration 9100: Loss = -11050.510996214798
Iteration 9200: Loss = -11050.598325517303
1
Iteration 9300: Loss = -11050.549205142872
2
Iteration 9400: Loss = -11050.537315418673
3
Iteration 9500: Loss = -11050.512375364182
4
Iteration 9600: Loss = -11050.511004613785
Iteration 9700: Loss = -11050.512098568972
1
Iteration 9800: Loss = -11050.693567172542
2
Iteration 9900: Loss = -11050.510899236042
Iteration 10000: Loss = -11050.512628328954
1
Iteration 10100: Loss = -11050.51089843052
Iteration 10200: Loss = -11050.511133027821
1
Iteration 10300: Loss = -11050.510913852544
Iteration 10400: Loss = -11050.511312635583
1
Iteration 10500: Loss = -11050.513543609013
2
Iteration 10600: Loss = -11050.510934954398
Iteration 10700: Loss = -11050.564800199
1
Iteration 10800: Loss = -11050.510946303431
Iteration 10900: Loss = -11050.513363274105
1
Iteration 11000: Loss = -11050.511401796228
2
Iteration 11100: Loss = -11050.511468702018
3
Iteration 11200: Loss = -11050.78468851954
4
Iteration 11300: Loss = -11050.510935820032
Iteration 11400: Loss = -11050.512664248563
1
Iteration 11500: Loss = -11050.511884686877
2
Iteration 11600: Loss = -11050.510883479423
Iteration 11700: Loss = -11050.523962703295
1
Iteration 11800: Loss = -11050.511953469995
2
Iteration 11900: Loss = -11050.512736507822
3
Iteration 12000: Loss = -11050.515504619829
4
Iteration 12100: Loss = -11050.510899488581
Iteration 12200: Loss = -11050.511690130908
1
Iteration 12300: Loss = -11050.51089940295
Iteration 12400: Loss = -11050.680122703578
1
Iteration 12500: Loss = -11050.510891712584
Iteration 12600: Loss = -11050.51090033994
Iteration 12700: Loss = -11050.511083471638
1
Iteration 12800: Loss = -11050.510930147644
Iteration 12900: Loss = -11050.544000248097
1
Iteration 13000: Loss = -11050.510924709559
Iteration 13100: Loss = -11050.510912027532
Iteration 13200: Loss = -11050.524272867
1
Iteration 13300: Loss = -11050.510972400183
Iteration 13400: Loss = -11050.510981331567
Iteration 13500: Loss = -11050.569665570394
1
Iteration 13600: Loss = -11050.510898670043
Iteration 13700: Loss = -11050.535152756134
1
Iteration 13800: Loss = -11050.51138339215
2
Iteration 13900: Loss = -11050.530937028567
3
Iteration 14000: Loss = -11050.511459542695
4
Iteration 14100: Loss = -11050.549494932411
5
Iteration 14200: Loss = -11050.51106652722
6
Iteration 14300: Loss = -11050.51115149949
7
Iteration 14400: Loss = -11050.511025509586
8
Iteration 14500: Loss = -11050.511716642824
9
Iteration 14600: Loss = -11050.511347091857
10
Iteration 14700: Loss = -11050.510912293239
Iteration 14800: Loss = -11050.512659477507
1
Iteration 14900: Loss = -11050.519215581406
2
Iteration 15000: Loss = -11050.548331042113
3
Iteration 15100: Loss = -11050.519964940944
4
Iteration 15200: Loss = -11050.512582519483
5
Iteration 15300: Loss = -11050.587204551397
6
Iteration 15400: Loss = -11050.58109362296
7
Iteration 15500: Loss = -11050.519527703349
8
Iteration 15600: Loss = -11050.58368064531
9
Iteration 15700: Loss = -11050.512692365739
10
Iteration 15800: Loss = -11050.513685302252
11
Iteration 15900: Loss = -11050.511123615372
12
Iteration 16000: Loss = -11050.511517281999
13
Iteration 16100: Loss = -11050.51090994912
Iteration 16200: Loss = -11050.513995496694
1
Iteration 16300: Loss = -11050.512326667049
2
Iteration 16400: Loss = -11050.574198028407
3
Iteration 16500: Loss = -11050.515157573867
4
Iteration 16600: Loss = -11050.51456528896
5
Iteration 16700: Loss = -11050.51087661445
Iteration 16800: Loss = -11050.51224731727
1
Iteration 16900: Loss = -11050.535789972768
2
Iteration 17000: Loss = -11050.513331025279
3
Iteration 17100: Loss = -11050.56315612213
4
Iteration 17200: Loss = -11050.512532122273
5
Iteration 17300: Loss = -11050.510935647997
Iteration 17400: Loss = -11050.514720024554
1
Iteration 17500: Loss = -11050.51198126599
2
Iteration 17600: Loss = -11050.511403069379
3
Iteration 17700: Loss = -11050.510945661994
Iteration 17800: Loss = -11050.511439314216
1
Iteration 17900: Loss = -11050.511179933304
2
Iteration 18000: Loss = -11050.511191261763
3
Iteration 18100: Loss = -11050.513378269243
4
Iteration 18200: Loss = -11050.511754428166
5
Iteration 18300: Loss = -11050.589486772313
6
Iteration 18400: Loss = -11050.513098154455
7
Iteration 18500: Loss = -11050.51096659716
Iteration 18600: Loss = -11050.511527217752
1
Iteration 18700: Loss = -11050.510979723194
Iteration 18800: Loss = -11050.512022684317
1
Iteration 18900: Loss = -11050.541328069137
2
Iteration 19000: Loss = -11050.558796230924
3
Iteration 19100: Loss = -11050.512404808767
4
Iteration 19200: Loss = -11050.511041874539
Iteration 19300: Loss = -11050.51360694599
1
Iteration 19400: Loss = -11050.510920222552
Iteration 19500: Loss = -11050.512394170735
1
Iteration 19600: Loss = -11050.510907607499
Iteration 19700: Loss = -11050.511168176487
1
Iteration 19800: Loss = -11050.511037080618
2
Iteration 19900: Loss = -11050.531689217596
3
pi: tensor([[0.0463, 0.9537],
        [0.0145, 0.9855]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2746, 0.7254], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1880, 0.1755],
         [0.5001, 0.1637]],

        [[0.5148, 0.1093],
         [0.6081, 0.6464]],

        [[0.6269, 0.1674],
         [0.7196, 0.5200]],

        [[0.6907, 0.0669],
         [0.5670, 0.5985]],

        [[0.6324, 0.1948],
         [0.6129, 0.6582]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0005093525538803316
Average Adjusted Rand Index: -0.0003077958928485548
10934.406228228168
[-0.0005093525538803316, -0.0005093525538803316] [-0.0003077958928485548, -0.0003077958928485548] [11050.512579036338, 11050.510908820224]
-------------------------------------
This iteration is 53
True Objective function: Loss = -10872.846071305974
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22037.752159633503
Iteration 100: Loss = -10998.364272170384
Iteration 200: Loss = -10997.586080501038
Iteration 300: Loss = -10997.411016347134
Iteration 400: Loss = -10997.330195798735
Iteration 500: Loss = -10997.28393132829
Iteration 600: Loss = -10997.25324585143
Iteration 700: Loss = -10997.230621610137
Iteration 800: Loss = -10997.212547288964
Iteration 900: Loss = -10997.19704306402
Iteration 1000: Loss = -10997.182461179804
Iteration 1100: Loss = -10997.16696472206
Iteration 1200: Loss = -10997.147059731202
Iteration 1300: Loss = -10997.114030808043
Iteration 1400: Loss = -10997.054774461507
Iteration 1500: Loss = -10997.018398652865
Iteration 1600: Loss = -10997.00547380619
Iteration 1700: Loss = -10996.994846002319
Iteration 1800: Loss = -10996.984927340784
Iteration 1900: Loss = -10996.975185827336
Iteration 2000: Loss = -10996.964843802996
Iteration 2100: Loss = -10996.95291037539
Iteration 2200: Loss = -10996.937717818224
Iteration 2300: Loss = -10996.915791101703
Iteration 2400: Loss = -10996.879323888936
Iteration 2500: Loss = -10996.81287114458
Iteration 2600: Loss = -10996.711491364824
Iteration 2700: Loss = -10996.621497213999
Iteration 2800: Loss = -10996.546428248239
Iteration 2900: Loss = -10996.472949556732
Iteration 3000: Loss = -10996.4086194622
Iteration 3100: Loss = -10996.354680801032
Iteration 3200: Loss = -10996.314080076412
Iteration 3300: Loss = -10996.289714701283
Iteration 3400: Loss = -10996.277342390777
Iteration 3500: Loss = -10996.271280200834
Iteration 3600: Loss = -10996.268026992242
Iteration 3700: Loss = -10996.266119276237
Iteration 3800: Loss = -10996.264821609528
Iteration 3900: Loss = -10996.263789628985
Iteration 4000: Loss = -10996.262896463775
Iteration 4100: Loss = -10996.262169557165
Iteration 4200: Loss = -10996.261501502122
Iteration 4300: Loss = -10996.260934043956
Iteration 4400: Loss = -10996.26034140861
Iteration 4500: Loss = -10996.27368459596
1
Iteration 4600: Loss = -10996.259278446467
Iteration 4700: Loss = -10996.258829863174
Iteration 4800: Loss = -10996.272314134967
1
Iteration 4900: Loss = -10996.257990449709
Iteration 5000: Loss = -10996.257583725237
Iteration 5100: Loss = -10996.260184226112
1
Iteration 5200: Loss = -10996.256844109723
Iteration 5300: Loss = -10996.256505732843
Iteration 5400: Loss = -10996.257354694164
1
Iteration 5500: Loss = -10996.255883698037
Iteration 5600: Loss = -10996.255616833623
Iteration 5700: Loss = -10996.255358082724
Iteration 5800: Loss = -10996.25504728531
Iteration 5900: Loss = -10996.254831359454
Iteration 6000: Loss = -10996.25457252031
Iteration 6100: Loss = -10996.254361186067
Iteration 6200: Loss = -10996.25413691975
Iteration 6300: Loss = -10996.25393962289
Iteration 6400: Loss = -10996.253733880354
Iteration 6500: Loss = -10996.253556700474
Iteration 6600: Loss = -10996.253377081724
Iteration 6700: Loss = -10996.254405794578
1
Iteration 6800: Loss = -10996.25303851677
Iteration 6900: Loss = -10996.252960928192
Iteration 7000: Loss = -10996.252805925165
Iteration 7100: Loss = -10996.252638789936
Iteration 7200: Loss = -10996.252525705617
Iteration 7300: Loss = -10996.252384246907
Iteration 7400: Loss = -10996.252270787038
Iteration 7500: Loss = -10996.252755050804
1
Iteration 7600: Loss = -10996.252087126813
Iteration 7700: Loss = -10996.256219109646
1
Iteration 7800: Loss = -10996.25187796122
Iteration 7900: Loss = -10996.251799290565
Iteration 8000: Loss = -10996.274834122185
1
Iteration 8100: Loss = -10996.251669177273
Iteration 8200: Loss = -10996.26165680051
1
Iteration 8300: Loss = -10996.25151171318
Iteration 8400: Loss = -10996.251421347544
Iteration 8500: Loss = -10996.26231484835
1
Iteration 8600: Loss = -10996.25130753453
Iteration 8700: Loss = -10996.251408898182
1
Iteration 8800: Loss = -10996.25117876883
Iteration 8900: Loss = -10996.251127829435
Iteration 9000: Loss = -10996.25222780292
1
Iteration 9100: Loss = -10996.251059214868
Iteration 9200: Loss = -10996.251004653132
Iteration 9300: Loss = -10996.251543601731
1
Iteration 9400: Loss = -10996.25096392226
Iteration 9500: Loss = -10996.250867940096
Iteration 9600: Loss = -10996.3980444567
1
Iteration 9700: Loss = -10996.250833382335
Iteration 9800: Loss = -10996.293624647047
1
Iteration 9900: Loss = -10996.250734463936
Iteration 10000: Loss = -10996.59319162022
1
Iteration 10100: Loss = -10996.250707158459
Iteration 10200: Loss = -10996.25067408749
Iteration 10300: Loss = -10996.25120542609
1
Iteration 10400: Loss = -10996.25055861304
Iteration 10500: Loss = -10996.316280689844
1
Iteration 10600: Loss = -10996.250574017975
Iteration 10700: Loss = -10996.39443933878
1
Iteration 10800: Loss = -10996.250519875508
Iteration 10900: Loss = -10996.251820825533
1
Iteration 11000: Loss = -10996.250516542805
Iteration 11100: Loss = -10996.276634821204
1
Iteration 11200: Loss = -10996.2504519489
Iteration 11300: Loss = -10996.36153892509
1
Iteration 11400: Loss = -10996.250386207872
Iteration 11500: Loss = -10996.250395998903
Iteration 11600: Loss = -10996.251177896183
1
Iteration 11700: Loss = -10996.250658189025
2
Iteration 11800: Loss = -10996.251173346252
3
Iteration 11900: Loss = -10996.250458998265
Iteration 12000: Loss = -10996.251971105248
1
Iteration 12100: Loss = -10996.250357757739
Iteration 12200: Loss = -10996.320756960213
1
Iteration 12300: Loss = -10996.250594339608
2
Iteration 12400: Loss = -10996.51762031011
3
Iteration 12500: Loss = -10996.250298149067
Iteration 12600: Loss = -10996.250380057854
Iteration 12700: Loss = -10996.250305074982
Iteration 12800: Loss = -10996.250286550257
Iteration 12900: Loss = -10996.250312294711
Iteration 13000: Loss = -10996.257644798985
1
Iteration 13100: Loss = -10996.252257561922
2
Iteration 13200: Loss = -10996.266227484888
3
Iteration 13300: Loss = -10996.250261336192
Iteration 13400: Loss = -10996.314123303788
1
Iteration 13500: Loss = -10996.250255100513
Iteration 13600: Loss = -10996.25536295349
1
Iteration 13700: Loss = -10996.36255697082
2
Iteration 13800: Loss = -10996.250361176546
3
Iteration 13900: Loss = -10996.385112970856
4
Iteration 14000: Loss = -10996.251174402929
5
Iteration 14100: Loss = -10996.250560750952
6
Iteration 14200: Loss = -10996.250263283437
Iteration 14300: Loss = -10996.402110979529
1
Iteration 14400: Loss = -10996.251745711354
2
Iteration 14500: Loss = -10996.26003036481
3
Iteration 14600: Loss = -10996.250229681837
Iteration 14700: Loss = -10996.556140884368
1
Iteration 14800: Loss = -10996.253422859661
2
Iteration 14900: Loss = -10996.322730933398
3
Iteration 15000: Loss = -10996.253379831182
4
Iteration 15100: Loss = -10996.282218507906
5
Iteration 15200: Loss = -10996.250499884014
6
Iteration 15300: Loss = -10996.25050038982
7
Iteration 15400: Loss = -10996.25060772426
8
Iteration 15500: Loss = -10996.367388911745
9
Iteration 15600: Loss = -10996.250203762265
Iteration 15700: Loss = -10996.32900581999
1
Iteration 15800: Loss = -10996.250139111153
Iteration 15900: Loss = -10996.250365535927
1
Iteration 16000: Loss = -10996.250233715442
Iteration 16100: Loss = -10996.251068271296
1
Iteration 16200: Loss = -10996.25075090452
2
Iteration 16300: Loss = -10996.250248363333
Iteration 16400: Loss = -10996.254248309457
1
Iteration 16500: Loss = -10996.250288455529
Iteration 16600: Loss = -10996.250630224542
1
Iteration 16700: Loss = -10996.250722266199
2
Iteration 16800: Loss = -10996.25083110438
3
Iteration 16900: Loss = -10996.25016783106
Iteration 17000: Loss = -10996.25188181509
1
Iteration 17100: Loss = -10996.251512236558
2
Iteration 17200: Loss = -10996.250162757786
Iteration 17300: Loss = -10996.256054532232
1
Iteration 17400: Loss = -10996.251315920292
2
Iteration 17500: Loss = -10996.250351555318
3
Iteration 17600: Loss = -10996.250423430538
4
Iteration 17700: Loss = -10996.250233805591
Iteration 17800: Loss = -10996.32825036117
1
Iteration 17900: Loss = -10996.250116814677
Iteration 18000: Loss = -10996.250615209765
1
Iteration 18100: Loss = -10996.252805431015
2
Iteration 18200: Loss = -10996.250802144627
3
Iteration 18300: Loss = -10996.279481941781
4
Iteration 18400: Loss = -10996.250174623443
Iteration 18500: Loss = -10996.251325745434
1
Iteration 18600: Loss = -10996.250279566795
2
Iteration 18700: Loss = -10996.250391940388
3
Iteration 18800: Loss = -10996.250107364503
Iteration 18900: Loss = -10996.25821967127
1
Iteration 19000: Loss = -10996.250148932824
Iteration 19100: Loss = -10996.250825423856
1
Iteration 19200: Loss = -10996.250339874834
2
Iteration 19300: Loss = -10996.25111769974
3
Iteration 19400: Loss = -10996.250438654452
4
Iteration 19500: Loss = -10996.253739732292
5
Iteration 19600: Loss = -10996.251355552855
6
Iteration 19700: Loss = -10996.250782130795
7
Iteration 19800: Loss = -10996.252378374416
8
Iteration 19900: Loss = -10996.250567880672
9
pi: tensor([[1.6340e-05, 9.9998e-01],
        [5.9301e-02, 9.4070e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0119, 0.9881], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1620, 0.2465],
         [0.5232, 0.1626]],

        [[0.6677, 0.1760],
         [0.6717, 0.5476]],

        [[0.5348, 0.1602],
         [0.6522, 0.6300]],

        [[0.6940, 0.1802],
         [0.7021, 0.6739]],

        [[0.5933, 0.1243],
         [0.6233, 0.6573]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21893.926371683363
Iteration 100: Loss = -10996.864105242465
Iteration 200: Loss = -10985.747723268203
Iteration 300: Loss = -10985.502357893987
Iteration 400: Loss = -10985.461280151972
Iteration 500: Loss = -10985.445657717888
Iteration 600: Loss = -10985.43644446883
Iteration 700: Loss = -10985.430204283162
Iteration 800: Loss = -10985.425339847772
Iteration 900: Loss = -10985.420862607387
Iteration 1000: Loss = -10985.415649525426
Iteration 1100: Loss = -10985.406370479877
Iteration 1200: Loss = -10985.303430650036
Iteration 1300: Loss = -10850.575423931237
Iteration 1400: Loss = -10833.38887088992
Iteration 1500: Loss = -10833.28517656001
Iteration 1600: Loss = -10833.247632400555
Iteration 1700: Loss = -10833.21242230075
Iteration 1800: Loss = -10832.891638178244
Iteration 1900: Loss = -10832.857353563455
Iteration 2000: Loss = -10832.853663035203
Iteration 2100: Loss = -10832.850516998258
Iteration 2200: Loss = -10832.846847607312
Iteration 2300: Loss = -10832.80013836026
Iteration 2400: Loss = -10832.791328370196
Iteration 2500: Loss = -10832.769647769119
Iteration 2600: Loss = -10832.747653824128
Iteration 2700: Loss = -10832.675024841301
Iteration 2800: Loss = -10832.67128165752
Iteration 2900: Loss = -10832.623534381306
Iteration 3000: Loss = -10832.621975535068
Iteration 3100: Loss = -10832.617760472756
Iteration 3200: Loss = -10832.616020288177
Iteration 3300: Loss = -10832.615492690547
Iteration 3400: Loss = -10832.615679627294
1
Iteration 3500: Loss = -10832.614386168461
Iteration 3600: Loss = -10832.613182353562
Iteration 3700: Loss = -10832.627816951497
1
Iteration 3800: Loss = -10832.610303657888
Iteration 3900: Loss = -10832.592913701377
Iteration 4000: Loss = -10832.536766272182
Iteration 4100: Loss = -10832.53655926643
Iteration 4200: Loss = -10832.536399400722
Iteration 4300: Loss = -10832.537106108464
1
Iteration 4400: Loss = -10832.535925241895
Iteration 4500: Loss = -10832.537317031016
1
Iteration 4600: Loss = -10832.534597104172
Iteration 4700: Loss = -10832.534817735155
1
Iteration 4800: Loss = -10832.532669881432
Iteration 4900: Loss = -10832.536797552804
1
Iteration 5000: Loss = -10832.532171280385
Iteration 5100: Loss = -10832.531724703544
Iteration 5200: Loss = -10832.53410984483
1
Iteration 5300: Loss = -10832.53999953439
2
Iteration 5400: Loss = -10832.530914920057
Iteration 5500: Loss = -10832.530958320236
Iteration 5600: Loss = -10832.533951925565
1
Iteration 5700: Loss = -10832.531025661578
Iteration 5800: Loss = -10832.53129344388
1
Iteration 5900: Loss = -10832.52788654421
Iteration 6000: Loss = -10832.52973891075
1
Iteration 6100: Loss = -10832.529299717378
2
Iteration 6200: Loss = -10832.528088444613
3
Iteration 6300: Loss = -10832.528197047353
4
Iteration 6400: Loss = -10832.527140728786
Iteration 6500: Loss = -10832.532771418833
1
Iteration 6600: Loss = -10832.527704848446
2
Iteration 6700: Loss = -10832.526518179
Iteration 6800: Loss = -10832.527338824564
1
Iteration 6900: Loss = -10832.525026121228
Iteration 7000: Loss = -10832.524845008911
Iteration 7100: Loss = -10832.527851093779
1
Iteration 7200: Loss = -10832.540778967492
2
Iteration 7300: Loss = -10832.523937052092
Iteration 7400: Loss = -10832.522866111949
Iteration 7500: Loss = -10832.52161332331
Iteration 7600: Loss = -10832.52840074708
1
Iteration 7700: Loss = -10832.521686461714
Iteration 7800: Loss = -10832.521705588215
Iteration 7900: Loss = -10832.521364129721
Iteration 8000: Loss = -10832.521505741319
1
Iteration 8100: Loss = -10832.520517129407
Iteration 8200: Loss = -10832.5214354319
1
Iteration 8300: Loss = -10832.520374490081
Iteration 8400: Loss = -10832.524471361581
1
Iteration 8500: Loss = -10832.518963177365
Iteration 8600: Loss = -10832.519187690546
1
Iteration 8700: Loss = -10832.52394614743
2
Iteration 8800: Loss = -10832.518887297665
Iteration 8900: Loss = -10832.558002309881
1
Iteration 9000: Loss = -10832.518832210844
Iteration 9100: Loss = -10832.521575753528
1
Iteration 9200: Loss = -10832.51498568638
Iteration 9300: Loss = -10832.517587040673
1
Iteration 9400: Loss = -10832.514827898824
Iteration 9500: Loss = -10832.5149011594
Iteration 9600: Loss = -10832.514429322697
Iteration 9700: Loss = -10832.514776099824
1
Iteration 9800: Loss = -10832.51443189437
Iteration 9900: Loss = -10832.514500633806
Iteration 10000: Loss = -10832.514300903742
Iteration 10100: Loss = -10832.514322958676
Iteration 10200: Loss = -10832.514281306128
Iteration 10300: Loss = -10832.51578809245
1
Iteration 10400: Loss = -10832.514267976958
Iteration 10500: Loss = -10832.514692545801
1
Iteration 10600: Loss = -10832.514524215278
2
Iteration 10700: Loss = -10832.514220582596
Iteration 10800: Loss = -10832.51382080746
Iteration 10900: Loss = -10832.513156142268
Iteration 11000: Loss = -10832.497126790295
Iteration 11100: Loss = -10832.49468473277
Iteration 11200: Loss = -10832.507181859428
1
Iteration 11300: Loss = -10832.505231228488
2
Iteration 11400: Loss = -10832.493625164796
Iteration 11500: Loss = -10832.486257086088
Iteration 11600: Loss = -10832.48581954682
Iteration 11700: Loss = -10832.486042006183
1
Iteration 11800: Loss = -10832.4975042562
2
Iteration 11900: Loss = -10832.485876957311
Iteration 12000: Loss = -10832.485417509222
Iteration 12100: Loss = -10832.485402476594
Iteration 12200: Loss = -10832.485287103187
Iteration 12300: Loss = -10832.485256512606
Iteration 12400: Loss = -10832.485254041456
Iteration 12500: Loss = -10832.488949814868
1
Iteration 12600: Loss = -10832.544708320698
2
Iteration 12700: Loss = -10832.495969420737
3
Iteration 12800: Loss = -10832.484485879455
Iteration 12900: Loss = -10832.485222338579
1
Iteration 13000: Loss = -10832.49148320786
2
Iteration 13100: Loss = -10832.48683997983
3
Iteration 13200: Loss = -10832.506076347832
4
Iteration 13300: Loss = -10832.484432675043
Iteration 13400: Loss = -10832.484605323762
1
Iteration 13500: Loss = -10832.573733119138
2
Iteration 13600: Loss = -10832.484029932712
Iteration 13700: Loss = -10832.544616795129
1
Iteration 13800: Loss = -10832.483717688621
Iteration 13900: Loss = -10832.51390579118
1
Iteration 14000: Loss = -10832.520884417452
2
Iteration 14100: Loss = -10832.512304339853
3
Iteration 14200: Loss = -10832.483131766694
Iteration 14300: Loss = -10832.48425542246
1
Iteration 14400: Loss = -10832.482685041447
Iteration 14500: Loss = -10832.482903527038
1
Iteration 14600: Loss = -10832.495381657183
2
Iteration 14700: Loss = -10832.482641005914
Iteration 14800: Loss = -10832.483728654264
1
Iteration 14900: Loss = -10832.4826083215
Iteration 15000: Loss = -10832.483344603686
1
Iteration 15100: Loss = -10832.48167601061
Iteration 15200: Loss = -10832.485390186146
1
Iteration 15300: Loss = -10832.487919536354
2
Iteration 15400: Loss = -10832.481794678715
3
Iteration 15500: Loss = -10832.482477565201
4
Iteration 15600: Loss = -10832.48774144198
5
Iteration 15700: Loss = -10832.48187421312
6
Iteration 15800: Loss = -10832.481500484462
Iteration 15900: Loss = -10832.481598338549
Iteration 16000: Loss = -10832.482763976188
1
Iteration 16100: Loss = -10832.521093740339
2
Iteration 16200: Loss = -10832.48708147846
3
Iteration 16300: Loss = -10832.481260423268
Iteration 16400: Loss = -10832.485874100013
1
Iteration 16500: Loss = -10832.482307898164
2
Iteration 16600: Loss = -10832.480114536791
Iteration 16700: Loss = -10832.486698066024
1
Iteration 16800: Loss = -10832.479811995463
Iteration 16900: Loss = -10832.501903530923
1
Iteration 17000: Loss = -10832.50547326488
2
Iteration 17100: Loss = -10832.479822001074
Iteration 17200: Loss = -10832.47969028453
Iteration 17300: Loss = -10832.481481795769
1
Iteration 17400: Loss = -10832.480401295441
2
Iteration 17500: Loss = -10832.479681786794
Iteration 17600: Loss = -10832.498579312221
1
Iteration 17700: Loss = -10832.47965008561
Iteration 17800: Loss = -10832.480428214438
1
Iteration 17900: Loss = -10832.479644862315
Iteration 18000: Loss = -10832.479756844956
1
Iteration 18100: Loss = -10832.479539330849
Iteration 18200: Loss = -10832.480931710357
1
Iteration 18300: Loss = -10832.479564021722
Iteration 18400: Loss = -10832.479534115793
Iteration 18500: Loss = -10832.479630157975
Iteration 18600: Loss = -10832.602964149355
1
Iteration 18700: Loss = -10832.465493631975
Iteration 18800: Loss = -10832.465888251374
1
Iteration 18900: Loss = -10832.464534158713
Iteration 19000: Loss = -10832.464550634759
Iteration 19100: Loss = -10832.464454160581
Iteration 19200: Loss = -10832.46577866995
1
Iteration 19300: Loss = -10832.464316945554
Iteration 19400: Loss = -10832.464921595456
1
Iteration 19500: Loss = -10832.677279996547
2
Iteration 19600: Loss = -10832.464297520519
Iteration 19700: Loss = -10832.465236530017
1
Iteration 19800: Loss = -10832.464300012682
Iteration 19900: Loss = -10832.468085810066
1
pi: tensor([[0.7857, 0.2143],
        [0.1628, 0.8372]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4996, 0.5004], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2637, 0.1062],
         [0.6253, 0.1958]],

        [[0.6870, 0.0992],
         [0.5054, 0.7014]],

        [[0.6936, 0.0982],
         [0.7273, 0.6479]],

        [[0.6573, 0.0989],
         [0.6945, 0.5446]],

        [[0.5751, 0.1053],
         [0.5595, 0.7038]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7025982975809663
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208065164923572
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
Global Adjusted Rand Index: 0.8534821210131345
Average Adjusted Rand Index: 0.8546201686871949
10872.846071305974
[0.0, 0.8534821210131345] [0.0, 0.8546201686871949] [10996.250386287784, 10832.477082427275]
-------------------------------------
This iteration is 54
True Objective function: Loss = -10897.516518219172
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24105.268755395588
Iteration 100: Loss = -10987.69893698761
Iteration 200: Loss = -10941.685898884192
Iteration 300: Loss = -10861.895778183823
Iteration 400: Loss = -10861.66750508319
Iteration 500: Loss = -10861.597060856717
Iteration 600: Loss = -10861.564606308086
Iteration 700: Loss = -10861.574799750751
1
Iteration 800: Loss = -10861.535413610485
Iteration 900: Loss = -10861.527948667986
Iteration 1000: Loss = -10861.522762587812
Iteration 1100: Loss = -10861.518963625342
Iteration 1200: Loss = -10861.51615501352
Iteration 1300: Loss = -10861.51394748265
Iteration 1400: Loss = -10861.512226720539
Iteration 1500: Loss = -10861.510772881844
Iteration 1600: Loss = -10861.509700717274
Iteration 1700: Loss = -10861.508965809036
Iteration 1800: Loss = -10861.507965666278
Iteration 1900: Loss = -10861.507349892694
Iteration 2000: Loss = -10861.506829287426
Iteration 2100: Loss = -10861.50632326403
Iteration 2200: Loss = -10861.506035730463
Iteration 2300: Loss = -10861.505600079963
Iteration 2400: Loss = -10861.505425237674
Iteration 2500: Loss = -10861.505022826414
Iteration 2600: Loss = -10861.504779030049
Iteration 2700: Loss = -10861.5203469121
1
Iteration 2800: Loss = -10861.504403344497
Iteration 2900: Loss = -10861.504215015517
Iteration 3000: Loss = -10861.52564271746
1
Iteration 3100: Loss = -10861.50396400327
Iteration 3200: Loss = -10861.503861928577
Iteration 3300: Loss = -10861.547815726166
1
Iteration 3400: Loss = -10861.503650585944
Iteration 3500: Loss = -10861.503537802291
Iteration 3600: Loss = -10861.503687856846
1
Iteration 3700: Loss = -10861.503407005688
Iteration 3800: Loss = -10861.503353072061
Iteration 3900: Loss = -10861.503299345703
Iteration 4000: Loss = -10861.503214435837
Iteration 4100: Loss = -10861.504644410634
1
Iteration 4200: Loss = -10861.503143370213
Iteration 4300: Loss = -10861.503092586008
Iteration 4400: Loss = -10861.503065327657
Iteration 4500: Loss = -10861.503041369604
Iteration 4600: Loss = -10861.50303570374
Iteration 4700: Loss = -10861.502975750307
Iteration 4800: Loss = -10861.50716100616
1
Iteration 4900: Loss = -10861.50288828544
Iteration 5000: Loss = -10861.502877000012
Iteration 5100: Loss = -10861.502870365664
Iteration 5200: Loss = -10861.502828040499
Iteration 5300: Loss = -10861.50382564994
1
Iteration 5400: Loss = -10861.502811262952
Iteration 5500: Loss = -10861.50277327689
Iteration 5600: Loss = -10861.502807908262
Iteration 5700: Loss = -10861.502768653458
Iteration 5800: Loss = -10861.507588906123
1
Iteration 5900: Loss = -10861.502728057643
Iteration 6000: Loss = -10861.503834023843
1
Iteration 6100: Loss = -10861.502712795786
Iteration 6200: Loss = -10861.509372356237
1
Iteration 6300: Loss = -10861.502697796665
Iteration 6400: Loss = -10861.505616598235
1
Iteration 6500: Loss = -10861.503194930763
2
Iteration 6600: Loss = -10861.502684469708
Iteration 6700: Loss = -10861.504888925914
1
Iteration 6800: Loss = -10861.502643727426
Iteration 6900: Loss = -10861.502728635702
Iteration 7000: Loss = -10861.502679462337
Iteration 7100: Loss = -10861.502631807689
Iteration 7200: Loss = -10861.506219383256
1
Iteration 7300: Loss = -10861.502630476512
Iteration 7400: Loss = -10861.502785811148
1
Iteration 7500: Loss = -10861.505711409058
2
Iteration 7600: Loss = -10861.502593554136
Iteration 7700: Loss = -10861.50267824363
Iteration 7800: Loss = -10861.503480313411
1
Iteration 7900: Loss = -10861.515944309574
2
Iteration 8000: Loss = -10861.503708690689
3
Iteration 8100: Loss = -10861.505757946901
4
Iteration 8200: Loss = -10861.529328806957
5
Iteration 8300: Loss = -10861.532839283089
6
Iteration 8400: Loss = -10861.502577368576
Iteration 8500: Loss = -10861.50320446042
1
Iteration 8600: Loss = -10861.502583653095
Iteration 8700: Loss = -10861.502816064882
1
Iteration 8800: Loss = -10861.50253811916
Iteration 8900: Loss = -10861.503549981282
1
Iteration 9000: Loss = -10861.502560447805
Iteration 9100: Loss = -10861.511110303107
1
Iteration 9200: Loss = -10861.502541924554
Iteration 9300: Loss = -10861.502562014868
Iteration 9400: Loss = -10861.508941540818
1
Iteration 9500: Loss = -10861.502559771778
Iteration 9600: Loss = -10861.502603893818
Iteration 9700: Loss = -10861.502552607666
Iteration 9800: Loss = -10861.50586326044
1
Iteration 9900: Loss = -10861.502541707094
Iteration 10000: Loss = -10861.502788187352
1
Iteration 10100: Loss = -10861.549178285439
2
Iteration 10200: Loss = -10861.521781845708
3
Iteration 10300: Loss = -10861.502619597246
Iteration 10400: Loss = -10861.502596050277
Iteration 10500: Loss = -10861.507010920697
1
Iteration 10600: Loss = -10861.502522235085
Iteration 10700: Loss = -10861.502849810586
1
Iteration 10800: Loss = -10861.50250476642
Iteration 10900: Loss = -10861.502814941226
1
Iteration 11000: Loss = -10861.502533281975
Iteration 11100: Loss = -10861.542289351783
1
Iteration 11200: Loss = -10861.502560168223
Iteration 11300: Loss = -10861.504826981374
1
Iteration 11400: Loss = -10861.502834542314
2
Iteration 11500: Loss = -10861.502543939354
Iteration 11600: Loss = -10861.502569508722
Iteration 11700: Loss = -10861.537110345742
1
Iteration 11800: Loss = -10861.644125862838
2
Iteration 11900: Loss = -10861.50934962956
3
Iteration 12000: Loss = -10861.502633187774
Iteration 12100: Loss = -10861.503762000884
1
Iteration 12200: Loss = -10861.502532127668
Iteration 12300: Loss = -10861.50297902326
1
Iteration 12400: Loss = -10861.502550849877
Iteration 12500: Loss = -10861.504002538677
1
Iteration 12600: Loss = -10861.50255205947
Iteration 12700: Loss = -10861.50585949238
1
Iteration 12800: Loss = -10861.502982070844
2
Iteration 12900: Loss = -10861.514674145736
3
Iteration 13000: Loss = -10861.513001935002
4
Iteration 13100: Loss = -10861.502815827125
5
Iteration 13200: Loss = -10861.504169666414
6
Iteration 13300: Loss = -10861.502768182974
7
Iteration 13400: Loss = -10861.502581864237
Iteration 13500: Loss = -10861.503102745588
1
Iteration 13600: Loss = -10861.506000421145
2
Iteration 13700: Loss = -10861.508268580712
3
Iteration 13800: Loss = -10861.502590274567
Iteration 13900: Loss = -10861.503207348527
1
Iteration 14000: Loss = -10861.510718108217
2
Iteration 14100: Loss = -10861.731822572992
3
Iteration 14200: Loss = -10861.502578381269
Iteration 14300: Loss = -10861.503298449987
1
Iteration 14400: Loss = -10861.503889355441
2
Iteration 14500: Loss = -10861.502957195222
3
Iteration 14600: Loss = -10861.502604358742
Iteration 14700: Loss = -10861.525231745214
1
Iteration 14800: Loss = -10861.502536548818
Iteration 14900: Loss = -10861.511623459573
1
Iteration 15000: Loss = -10861.50255017283
Iteration 15100: Loss = -10861.522443169508
1
Iteration 15200: Loss = -10861.502558768485
Iteration 15300: Loss = -10861.504541821012
1
Iteration 15400: Loss = -10861.50253466958
Iteration 15500: Loss = -10861.504898052493
1
Iteration 15600: Loss = -10861.502533372503
Iteration 15700: Loss = -10861.513747675086
1
Iteration 15800: Loss = -10861.502565425537
Iteration 15900: Loss = -10861.502536352804
Iteration 16000: Loss = -10861.582769349756
1
Iteration 16100: Loss = -10861.502550056444
Iteration 16200: Loss = -10861.50250644283
Iteration 16300: Loss = -10861.849441380744
1
Iteration 16400: Loss = -10861.502551411124
Iteration 16500: Loss = -10861.502526755256
Iteration 16600: Loss = -10861.504147235775
1
Iteration 16700: Loss = -10861.502578408992
Iteration 16800: Loss = -10861.546178998287
1
Iteration 16900: Loss = -10861.502565859308
Iteration 17000: Loss = -10861.502732593985
1
Iteration 17100: Loss = -10861.521468828509
2
Iteration 17200: Loss = -10861.502552617625
Iteration 17300: Loss = -10861.502543829263
Iteration 17400: Loss = -10861.50367717037
1
Iteration 17500: Loss = -10861.50252046912
Iteration 17600: Loss = -10861.504315852819
1
Iteration 17700: Loss = -10861.502604314426
Iteration 17800: Loss = -10861.502786955642
1
Iteration 17900: Loss = -10861.532502346008
2
Iteration 18000: Loss = -10861.502562518875
Iteration 18100: Loss = -10861.502605500968
Iteration 18200: Loss = -10861.50302249031
1
Iteration 18300: Loss = -10861.52349877722
2
Iteration 18400: Loss = -10861.502616021582
Iteration 18500: Loss = -10861.502652631705
Iteration 18600: Loss = -10861.6999304909
1
Iteration 18700: Loss = -10861.523649709665
2
Iteration 18800: Loss = -10861.505584975113
3
Iteration 18900: Loss = -10861.502746984046
Iteration 19000: Loss = -10861.502687649843
Iteration 19100: Loss = -10861.507338570615
1
Iteration 19200: Loss = -10861.678207731558
2
Iteration 19300: Loss = -10861.502731201157
Iteration 19400: Loss = -10861.50362298585
1
Iteration 19500: Loss = -10861.503221982884
2
Iteration 19600: Loss = -10861.51691445631
3
Iteration 19700: Loss = -10861.502626244042
Iteration 19800: Loss = -10861.503339007071
1
Iteration 19900: Loss = -10861.508807050066
2
pi: tensor([[0.7535, 0.2465],
        [0.2154, 0.7846]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4479, 0.5521], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.0930],
         [0.6135, 0.2455]],

        [[0.6467, 0.1016],
         [0.6659, 0.5600]],

        [[0.6493, 0.1093],
         [0.5595, 0.5791]],

        [[0.7159, 0.1012],
         [0.5523, 0.6040]],

        [[0.6349, 0.1024],
         [0.7010, 0.5137]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369480537608971
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.868359320800143
Average Adjusted Rand Index: 0.8686812501319456
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20581.733153854668
Iteration 100: Loss = -10991.211159706065
Iteration 200: Loss = -10990.551577525348
Iteration 300: Loss = -10990.364287532266
Iteration 400: Loss = -10990.258854209836
Iteration 500: Loss = -10990.11635874378
Iteration 600: Loss = -10989.890738159294
Iteration 700: Loss = -10989.384055456358
Iteration 800: Loss = -10988.184752577894
Iteration 900: Loss = -10985.075485142175
Iteration 1000: Loss = -10983.977154645225
Iteration 1100: Loss = -10982.473264287279
Iteration 1200: Loss = -10972.241534544648
Iteration 1300: Loss = -10956.432906991751
Iteration 1400: Loss = -10956.22582472223
Iteration 1500: Loss = -10956.16951778942
Iteration 1600: Loss = -10956.128715984174
Iteration 1700: Loss = -10956.071968832579
Iteration 1800: Loss = -10955.723102146487
Iteration 1900: Loss = -10937.569857876539
Iteration 2000: Loss = -10897.025921498429
Iteration 2100: Loss = -10881.988241682868
Iteration 2200: Loss = -10874.513290297818
Iteration 2300: Loss = -10874.181911394962
Iteration 2400: Loss = -10873.274959454318
Iteration 2500: Loss = -10873.180730473947
Iteration 2600: Loss = -10871.02193355147
Iteration 2700: Loss = -10870.859683836676
Iteration 2800: Loss = -10870.844260988348
Iteration 2900: Loss = -10870.83526579416
Iteration 3000: Loss = -10870.830511026857
Iteration 3100: Loss = -10870.826268377234
Iteration 3200: Loss = -10870.820873246414
Iteration 3300: Loss = -10870.808978348638
Iteration 3400: Loss = -10870.803806663678
Iteration 3500: Loss = -10870.7937818008
Iteration 3600: Loss = -10870.770038603196
Iteration 3700: Loss = -10870.767517133658
Iteration 3800: Loss = -10870.765017152253
Iteration 3900: Loss = -10870.759814257272
Iteration 4000: Loss = -10870.7561912041
Iteration 4100: Loss = -10870.75510820933
Iteration 4200: Loss = -10870.754222034222
Iteration 4300: Loss = -10870.75350975751
Iteration 4400: Loss = -10870.752797794354
Iteration 4500: Loss = -10870.752100551466
Iteration 4600: Loss = -10870.7516372084
Iteration 4700: Loss = -10870.75106214766
Iteration 4800: Loss = -10870.75058103686
Iteration 4900: Loss = -10870.75066436172
Iteration 5000: Loss = -10870.749833661035
Iteration 5100: Loss = -10870.749155894873
Iteration 5200: Loss = -10870.748954290966
Iteration 5300: Loss = -10870.746899852847
Iteration 5400: Loss = -10870.74592159114
Iteration 5500: Loss = -10870.74506291874
Iteration 5600: Loss = -10870.743450844568
Iteration 5700: Loss = -10870.726028700543
Iteration 5800: Loss = -10870.723953497143
Iteration 5900: Loss = -10870.724507545532
1
Iteration 6000: Loss = -10870.723686067666
Iteration 6100: Loss = -10870.712812472719
Iteration 6200: Loss = -10870.712398350688
Iteration 6300: Loss = -10870.712352964194
Iteration 6400: Loss = -10870.72373315511
1
Iteration 6500: Loss = -10870.712049982327
Iteration 6600: Loss = -10870.71180998915
Iteration 6700: Loss = -10870.762360393275
1
Iteration 6800: Loss = -10870.710859355184
Iteration 6900: Loss = -10870.705604878227
Iteration 7000: Loss = -10870.698451587332
Iteration 7100: Loss = -10870.697950138056
Iteration 7200: Loss = -10870.69831793675
1
Iteration 7300: Loss = -10870.695749226796
Iteration 7400: Loss = -10870.69736046792
1
Iteration 7500: Loss = -10870.702326056604
2
Iteration 7600: Loss = -10870.738119954365
3
Iteration 7700: Loss = -10870.695029134267
Iteration 7800: Loss = -10870.694946956806
Iteration 7900: Loss = -10870.695211861837
1
Iteration 8000: Loss = -10870.69480387873
Iteration 8100: Loss = -10870.694704998397
Iteration 8200: Loss = -10870.694707752345
Iteration 8300: Loss = -10870.689085132695
Iteration 8400: Loss = -10869.867404915525
Iteration 8500: Loss = -10869.857466259919
Iteration 8600: Loss = -10869.857182572117
Iteration 8700: Loss = -10869.893434517364
1
Iteration 8800: Loss = -10869.834634220873
Iteration 8900: Loss = -10869.833716228737
Iteration 9000: Loss = -10869.85283514212
1
Iteration 9100: Loss = -10869.9053794489
2
Iteration 9200: Loss = -10869.833033475883
Iteration 9300: Loss = -10869.833600730955
1
Iteration 9400: Loss = -10869.879622753357
2
Iteration 9500: Loss = -10869.828547788973
Iteration 9600: Loss = -10869.782222611755
Iteration 9700: Loss = -10869.725733995176
Iteration 9800: Loss = -10869.713265598486
Iteration 9900: Loss = -10869.696985486493
Iteration 10000: Loss = -10869.70891302396
1
Iteration 10100: Loss = -10869.756689193699
2
Iteration 10200: Loss = -10869.696639093894
Iteration 10300: Loss = -10869.705524459498
1
Iteration 10400: Loss = -10869.739593507324
2
Iteration 10500: Loss = -10869.695006261167
Iteration 10600: Loss = -10869.705195643657
1
Iteration 10700: Loss = -10869.706757828571
2
Iteration 10800: Loss = -10869.693211568374
Iteration 10900: Loss = -10869.700431151025
1
Iteration 11000: Loss = -10869.689466458278
Iteration 11100: Loss = -10869.684592617783
Iteration 11200: Loss = -10869.684512086276
Iteration 11300: Loss = -10869.685432288536
1
Iteration 11400: Loss = -10869.725934243499
2
Iteration 11500: Loss = -10869.664732995514
Iteration 11600: Loss = -10869.665211757612
1
Iteration 11700: Loss = -10869.666195193717
2
Iteration 11800: Loss = -10869.66458627948
Iteration 11900: Loss = -10869.913935684828
1
Iteration 12000: Loss = -10869.663648091719
Iteration 12100: Loss = -10869.67720196835
1
Iteration 12200: Loss = -10869.661936035793
Iteration 12300: Loss = -10869.35216823486
Iteration 12400: Loss = -10869.563007200073
1
Iteration 12500: Loss = -10868.955193489206
Iteration 12600: Loss = -10868.952522599628
Iteration 12700: Loss = -10868.945573462053
Iteration 12800: Loss = -10868.947704683027
1
Iteration 12900: Loss = -10869.0218896603
2
Iteration 13000: Loss = -10868.943141569918
Iteration 13100: Loss = -10868.943228026761
Iteration 13200: Loss = -10868.955716414257
1
Iteration 13300: Loss = -10868.942551081258
Iteration 13400: Loss = -10869.03154542551
1
Iteration 13500: Loss = -10868.94219557064
Iteration 13600: Loss = -10868.96141754589
1
Iteration 13700: Loss = -10868.936486070716
Iteration 13800: Loss = -10868.937704421478
1
Iteration 13900: Loss = -10868.942757043082
2
Iteration 14000: Loss = -10868.943966294324
3
Iteration 14100: Loss = -10869.084105819638
4
Iteration 14200: Loss = -10868.934393432779
Iteration 14300: Loss = -10868.933985483558
Iteration 14400: Loss = -10868.943512097208
1
Iteration 14500: Loss = -10868.933961650713
Iteration 14600: Loss = -10868.9339706015
Iteration 14700: Loss = -10868.93898193207
1
Iteration 14800: Loss = -10868.929807736065
Iteration 14900: Loss = -10868.928717358282
Iteration 15000: Loss = -10868.925014674454
Iteration 15100: Loss = -10868.961017315234
1
Iteration 15200: Loss = -10868.91646874675
Iteration 15300: Loss = -10868.911213488338
Iteration 15400: Loss = -10868.91129775454
Iteration 15500: Loss = -10868.937680879677
1
Iteration 15600: Loss = -10868.914765167932
2
Iteration 15700: Loss = -10868.911677907663
3
Iteration 15800: Loss = -10868.90248470601
Iteration 15900: Loss = -10868.907266220634
1
Iteration 16000: Loss = -10868.886346986928
Iteration 16100: Loss = -10868.890705210506
1
Iteration 16200: Loss = -10868.88159166323
Iteration 16300: Loss = -10868.87534616944
Iteration 16400: Loss = -10868.879889147145
1
Iteration 16500: Loss = -10869.053583694797
2
Iteration 16600: Loss = -10868.873591996502
Iteration 16700: Loss = -10868.869216093864
Iteration 16800: Loss = -10868.868361942059
Iteration 16900: Loss = -10868.872244984352
1
Iteration 17000: Loss = -10868.868566924155
2
Iteration 17100: Loss = -10868.885043928854
3
Iteration 17200: Loss = -10868.867271762516
Iteration 17300: Loss = -10868.867514230205
1
Iteration 17400: Loss = -10868.868115348412
2
Iteration 17500: Loss = -10868.86846939647
3
Iteration 17600: Loss = -10868.866756960258
Iteration 17700: Loss = -10868.606153452703
Iteration 17800: Loss = -10868.605838180627
Iteration 17900: Loss = -10868.60595600572
1
Iteration 18000: Loss = -10868.605717482895
Iteration 18100: Loss = -10868.603686632416
Iteration 18200: Loss = -10868.639559623109
1
Iteration 18300: Loss = -10868.600671145601
Iteration 18400: Loss = -10868.605875203162
1
Iteration 18500: Loss = -10868.606908061947
2
Iteration 18600: Loss = -10868.59440856153
Iteration 18700: Loss = -10868.580740813932
Iteration 18800: Loss = -10867.33931434407
Iteration 18900: Loss = -10867.180014083755
Iteration 19000: Loss = -10867.182371595336
1
Iteration 19100: Loss = -10867.180106951662
Iteration 19200: Loss = -10867.179874029021
Iteration 19300: Loss = -10867.179914050173
Iteration 19400: Loss = -10867.179954061892
Iteration 19500: Loss = -10867.180317834625
1
Iteration 19600: Loss = -10867.143463093169
Iteration 19700: Loss = -10867.135393311883
Iteration 19800: Loss = -10867.135582430785
1
Iteration 19900: Loss = -10867.136030730311
2
pi: tensor([[0.7647, 0.2353],
        [0.2013, 0.7987]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4481, 0.5519], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.0937],
         [0.6381, 0.2427]],

        [[0.7099, 0.1053],
         [0.5456, 0.6375]],

        [[0.5584, 0.1089],
         [0.7206, 0.7052]],

        [[0.7274, 0.1009],
         [0.6702, 0.5385]],

        [[0.6626, 0.1025],
         [0.5534, 0.5842]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080725364594837
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026048523603536
time is 4
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
Global Adjusted Rand Index: 0.8534796933050963
Average Adjusted Rand Index: 0.8546196643088966
10897.516518219172
[0.868359320800143, 0.8534796933050963] [0.8686812501319456, 0.8546196643088966] [10861.535933509827, 10867.134643236774]
-------------------------------------
This iteration is 55
True Objective function: Loss = -10774.414750480853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22538.615867372097
Iteration 100: Loss = -10875.360185227963
Iteration 200: Loss = -10875.149132337996
Iteration 300: Loss = -10875.098273172556
Iteration 400: Loss = -10875.048776492005
Iteration 500: Loss = -10874.972191480038
Iteration 600: Loss = -10874.773222720025
Iteration 700: Loss = -10874.189054712599
Iteration 800: Loss = -10873.967008621617
Iteration 900: Loss = -10873.286329823814
Iteration 1000: Loss = -10869.182309661846
Iteration 1100: Loss = -10787.032688007363
Iteration 1200: Loss = -10729.168562833953
Iteration 1300: Loss = -10727.319151981012
Iteration 1400: Loss = -10727.162533689047
Iteration 1500: Loss = -10727.133645732425
Iteration 1600: Loss = -10727.117370669883
Iteration 1700: Loss = -10727.080882829257
Iteration 1800: Loss = -10727.07375341069
Iteration 1900: Loss = -10727.071069991065
Iteration 2000: Loss = -10727.069124437465
Iteration 2100: Loss = -10727.06699369439
Iteration 2200: Loss = -10727.064348659054
Iteration 2300: Loss = -10727.062968467515
Iteration 2400: Loss = -10727.060612103594
Iteration 2500: Loss = -10727.05668447725
Iteration 2600: Loss = -10727.054089284664
Iteration 2700: Loss = -10727.05301448696
Iteration 2800: Loss = -10727.05170542792
Iteration 2900: Loss = -10727.071623288844
1
Iteration 3000: Loss = -10727.04871115018
Iteration 3100: Loss = -10727.048281477972
Iteration 3200: Loss = -10727.052756038962
1
Iteration 3300: Loss = -10727.04706534572
Iteration 3400: Loss = -10727.045325235202
Iteration 3500: Loss = -10726.928486990015
Iteration 3600: Loss = -10726.926279990728
Iteration 3700: Loss = -10726.92341483773
Iteration 3800: Loss = -10726.921124299164
Iteration 3900: Loss = -10726.920344260992
Iteration 4000: Loss = -10726.920083379202
Iteration 4100: Loss = -10726.790090232995
Iteration 4200: Loss = -10726.789259489458
Iteration 4300: Loss = -10726.78925607153
Iteration 4400: Loss = -10726.7878670602
Iteration 4500: Loss = -10726.599559187745
Iteration 4600: Loss = -10726.599040984456
Iteration 4700: Loss = -10726.594086706045
Iteration 4800: Loss = -10726.593063970076
Iteration 4900: Loss = -10726.5909760661
Iteration 5000: Loss = -10726.589960340958
Iteration 5100: Loss = -10726.590346443674
1
Iteration 5200: Loss = -10726.589370126565
Iteration 5300: Loss = -10726.588793442967
Iteration 5400: Loss = -10726.58782074625
Iteration 5500: Loss = -10726.586339076503
Iteration 5600: Loss = -10726.57189107034
Iteration 5700: Loss = -10726.571796632763
Iteration 5800: Loss = -10726.571598412873
Iteration 5900: Loss = -10726.571498681213
Iteration 6000: Loss = -10726.571431742368
Iteration 6100: Loss = -10726.575362556732
1
Iteration 6200: Loss = -10726.56749764438
Iteration 6300: Loss = -10726.567541482913
Iteration 6400: Loss = -10726.567467266077
Iteration 6500: Loss = -10726.569363193252
1
Iteration 6600: Loss = -10726.567670987779
2
Iteration 6700: Loss = -10726.5675010065
Iteration 6800: Loss = -10726.57505294866
1
Iteration 6900: Loss = -10726.56700453493
Iteration 7000: Loss = -10726.566793666298
Iteration 7100: Loss = -10726.566762406097
Iteration 7200: Loss = -10726.567185674769
1
Iteration 7300: Loss = -10726.567880825609
2
Iteration 7400: Loss = -10726.568322860647
3
Iteration 7500: Loss = -10726.566795483213
Iteration 7600: Loss = -10726.56657445161
Iteration 7700: Loss = -10726.566778157667
1
Iteration 7800: Loss = -10726.571138938252
2
Iteration 7900: Loss = -10726.566647692543
Iteration 8000: Loss = -10726.566973087836
1
Iteration 8100: Loss = -10726.567838844254
2
Iteration 8200: Loss = -10726.607272928417
3
Iteration 8300: Loss = -10726.565526255981
Iteration 8400: Loss = -10726.566396307744
1
Iteration 8500: Loss = -10726.565133232381
Iteration 8600: Loss = -10726.564888146026
Iteration 8700: Loss = -10726.56494688179
Iteration 8800: Loss = -10726.564100628499
Iteration 8900: Loss = -10726.563277019588
Iteration 9000: Loss = -10726.563073660096
Iteration 9100: Loss = -10726.62065134531
1
Iteration 9200: Loss = -10726.57000384733
2
Iteration 9300: Loss = -10726.563019973997
Iteration 9400: Loss = -10726.56276784747
Iteration 9500: Loss = -10726.575934205073
1
Iteration 9600: Loss = -10726.562038302927
Iteration 9700: Loss = -10726.565031797556
1
Iteration 9800: Loss = -10726.593084306724
2
Iteration 9900: Loss = -10725.687227068976
Iteration 10000: Loss = -10725.729440742236
1
Iteration 10100: Loss = -10725.681728771337
Iteration 10200: Loss = -10725.681247803614
Iteration 10300: Loss = -10725.68104602322
Iteration 10400: Loss = -10725.693095883282
1
Iteration 10500: Loss = -10725.679497966943
Iteration 10600: Loss = -10725.683327648483
1
Iteration 10700: Loss = -10725.684096679284
2
Iteration 10800: Loss = -10725.753564101427
3
Iteration 10900: Loss = -10725.678337527757
Iteration 11000: Loss = -10725.678156217815
Iteration 11100: Loss = -10725.709984945302
1
Iteration 11200: Loss = -10725.782214136229
2
Iteration 11300: Loss = -10725.686697180327
3
Iteration 11400: Loss = -10725.685444621426
4
Iteration 11500: Loss = -10725.679336318864
5
Iteration 11600: Loss = -10725.678503470463
6
Iteration 11700: Loss = -10725.680042286476
7
Iteration 11800: Loss = -10725.679447220848
8
Iteration 11900: Loss = -10725.685654090996
9
Iteration 12000: Loss = -10725.681030698854
10
Iteration 12100: Loss = -10725.697083722163
11
Iteration 12200: Loss = -10725.678383285023
12
Iteration 12300: Loss = -10725.903202666514
13
Iteration 12400: Loss = -10725.670256739626
Iteration 12500: Loss = -10725.63762585132
Iteration 12600: Loss = -10725.629088650738
Iteration 12700: Loss = -10725.62961838614
1
Iteration 12800: Loss = -10725.645742174958
2
Iteration 12900: Loss = -10725.62897761342
Iteration 13000: Loss = -10725.629301456358
1
Iteration 13100: Loss = -10725.633075133092
2
Iteration 13200: Loss = -10725.634482053618
3
Iteration 13300: Loss = -10725.654055493365
4
Iteration 13400: Loss = -10725.636803829897
5
Iteration 13500: Loss = -10725.661734320722
6
Iteration 13600: Loss = -10725.629723145934
7
Iteration 13700: Loss = -10725.663181837359
8
Iteration 13800: Loss = -10725.626880326927
Iteration 13900: Loss = -10725.625742135122
Iteration 14000: Loss = -10725.62591842794
1
Iteration 14100: Loss = -10725.62719925292
2
Iteration 14200: Loss = -10725.638539372967
3
Iteration 14300: Loss = -10725.627062813497
4
Iteration 14400: Loss = -10725.643927131317
5
Iteration 14500: Loss = -10725.64154260497
6
Iteration 14600: Loss = -10725.625029042076
Iteration 14700: Loss = -10725.624619196591
Iteration 14800: Loss = -10725.62509303801
1
Iteration 14900: Loss = -10725.660002308632
2
Iteration 15000: Loss = -10725.624552978294
Iteration 15100: Loss = -10725.625410566183
1
Iteration 15200: Loss = -10725.670557810412
2
Iteration 15300: Loss = -10725.639795889865
3
Iteration 15400: Loss = -10725.770460189016
4
Iteration 15500: Loss = -10725.624552226425
Iteration 15600: Loss = -10725.624869277159
1
Iteration 15700: Loss = -10725.624713011539
2
Iteration 15800: Loss = -10725.62817845577
3
Iteration 15900: Loss = -10725.624954466412
4
Iteration 16000: Loss = -10725.624563989768
Iteration 16100: Loss = -10725.624640550928
Iteration 16200: Loss = -10725.639546671462
1
Iteration 16300: Loss = -10725.624441299833
Iteration 16400: Loss = -10725.624624649954
1
Iteration 16500: Loss = -10725.73738413353
2
Iteration 16600: Loss = -10725.624472854412
Iteration 16700: Loss = -10725.62447208081
Iteration 16800: Loss = -10725.652359060365
1
Iteration 16900: Loss = -10725.61792111582
Iteration 17000: Loss = -10725.618106403726
1
Iteration 17100: Loss = -10725.644628670878
2
Iteration 17200: Loss = -10725.620269587758
3
Iteration 17300: Loss = -10725.619435892519
4
Iteration 17400: Loss = -10725.644406405381
5
Iteration 17500: Loss = -10725.630769491872
6
Iteration 17600: Loss = -10725.74114980023
7
Iteration 17700: Loss = -10725.616757179963
Iteration 17800: Loss = -10725.617630638255
1
Iteration 17900: Loss = -10725.616976072111
2
Iteration 18000: Loss = -10725.613756671783
Iteration 18100: Loss = -10725.613297004638
Iteration 18200: Loss = -10725.627924018185
1
Iteration 18300: Loss = -10725.618606713606
2
Iteration 18400: Loss = -10725.612790235169
Iteration 18500: Loss = -10725.615141577211
1
Iteration 18600: Loss = -10725.625547417374
2
Iteration 18700: Loss = -10725.612676853327
Iteration 18800: Loss = -10725.655542237504
1
Iteration 18900: Loss = -10725.613417945005
2
Iteration 19000: Loss = -10725.612689015796
Iteration 19100: Loss = -10725.61280665607
1
Iteration 19200: Loss = -10725.613719879166
2
Iteration 19300: Loss = -10725.614327295296
3
Iteration 19400: Loss = -10725.611266648648
Iteration 19500: Loss = -10725.61171833632
1
Iteration 19600: Loss = -10725.640271017026
2
Iteration 19700: Loss = -10725.587841536453
Iteration 19800: Loss = -10725.580850659408
Iteration 19900: Loss = -10725.581435048001
1
pi: tensor([[0.7754, 0.2246],
        [0.2527, 0.7473]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4387, 0.5613], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.0952],
         [0.6422, 0.2439]],

        [[0.5303, 0.1023],
         [0.5077, 0.6512]],

        [[0.5535, 0.0943],
         [0.6432, 0.5717]],

        [[0.5428, 0.0906],
         [0.5073, 0.6508]],

        [[0.6911, 0.0979],
         [0.6659, 0.5456]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.8168490785299649
Average Adjusted Rand Index: 0.8162416358210033
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22902.520311579883
Iteration 100: Loss = -10876.449192262526
Iteration 200: Loss = -10875.600066299641
Iteration 300: Loss = -10875.393822293592
Iteration 400: Loss = -10875.286990179384
Iteration 500: Loss = -10875.213452392356
Iteration 600: Loss = -10875.150115190952
Iteration 700: Loss = -10875.085265274865
Iteration 800: Loss = -10875.009767038846
Iteration 900: Loss = -10874.916695247288
Iteration 1000: Loss = -10874.807899007026
Iteration 1100: Loss = -10874.697172674425
Iteration 1200: Loss = -10874.594854291656
Iteration 1300: Loss = -10874.507174853194
Iteration 1400: Loss = -10874.43511995869
Iteration 1500: Loss = -10874.370161300663
Iteration 1600: Loss = -10874.294427139
Iteration 1700: Loss = -10874.170262165382
Iteration 1800: Loss = -10873.994938410708
Iteration 1900: Loss = -10873.833942757974
Iteration 2000: Loss = -10873.690523327
Iteration 2100: Loss = -10873.566295998939
Iteration 2200: Loss = -10873.468321020502
Iteration 2300: Loss = -10873.394540249868
Iteration 2400: Loss = -10873.337808809893
Iteration 2500: Loss = -10873.290505849436
Iteration 2600: Loss = -10873.242663613004
Iteration 2700: Loss = -10873.131081179274
Iteration 2800: Loss = -10872.802273147696
Iteration 2900: Loss = -10872.474302245351
Iteration 3000: Loss = -10872.082292319255
Iteration 3100: Loss = -10871.764564532281
Iteration 3200: Loss = -10871.579648371706
Iteration 3300: Loss = -10871.47914162785
Iteration 3400: Loss = -10871.421448761472
Iteration 3500: Loss = -10871.389031892271
Iteration 3600: Loss = -10871.369444134236
Iteration 3700: Loss = -10871.356737633741
Iteration 3800: Loss = -10871.348139744694
Iteration 3900: Loss = -10871.342035191532
Iteration 4000: Loss = -10871.33755761037
Iteration 4100: Loss = -10871.334138418928
Iteration 4200: Loss = -10871.331438935144
Iteration 4300: Loss = -10871.329294662672
Iteration 4400: Loss = -10871.327485345777
Iteration 4500: Loss = -10871.325994194092
Iteration 4600: Loss = -10871.32471126258
Iteration 4700: Loss = -10871.323528888846
Iteration 4800: Loss = -10871.322508604146
Iteration 4900: Loss = -10871.321592972448
Iteration 5000: Loss = -10871.320781873716
Iteration 5100: Loss = -10871.320031291776
Iteration 5200: Loss = -10871.319390058006
Iteration 5300: Loss = -10871.318766266364
Iteration 5400: Loss = -10871.318171330417
Iteration 5500: Loss = -10871.317696965429
Iteration 5600: Loss = -10871.317211698864
Iteration 5700: Loss = -10871.316797500825
Iteration 5800: Loss = -10871.316353147013
Iteration 5900: Loss = -10871.315972884548
Iteration 6000: Loss = -10871.315635001723
Iteration 6100: Loss = -10871.31530472607
Iteration 6200: Loss = -10871.314968415561
Iteration 6300: Loss = -10871.314615483217
Iteration 6400: Loss = -10871.31429149078
Iteration 6500: Loss = -10871.313868069474
Iteration 6600: Loss = -10871.313345560642
Iteration 6700: Loss = -10871.312276290888
Iteration 6800: Loss = -10871.310041420787
Iteration 6900: Loss = -10871.309604369204
Iteration 7000: Loss = -10871.309356433505
Iteration 7100: Loss = -10871.309387581287
Iteration 7200: Loss = -10871.309011826781
Iteration 7300: Loss = -10871.308868568942
Iteration 7400: Loss = -10871.30945232382
1
Iteration 7500: Loss = -10871.308577107822
Iteration 7600: Loss = -10871.30848412859
Iteration 7700: Loss = -10871.308371081703
Iteration 7800: Loss = -10871.308214321503
Iteration 7900: Loss = -10871.308177861049
Iteration 8000: Loss = -10871.308052691553
Iteration 8100: Loss = -10871.307966732526
Iteration 8200: Loss = -10871.307870013528
Iteration 8300: Loss = -10871.308258265432
1
Iteration 8400: Loss = -10871.323508239337
2
Iteration 8500: Loss = -10871.307612227843
Iteration 8600: Loss = -10871.307567013966
Iteration 8700: Loss = -10871.307634865487
Iteration 8800: Loss = -10871.307439441538
Iteration 8900: Loss = -10871.797903962031
1
Iteration 9000: Loss = -10871.307303676558
Iteration 9100: Loss = -10871.30720214284
Iteration 9200: Loss = -10871.307202072478
Iteration 9300: Loss = -10871.307130739164
Iteration 9400: Loss = -10871.30711245575
Iteration 9500: Loss = -10871.307081450883
Iteration 9600: Loss = -10871.314803286832
1
Iteration 9700: Loss = -10871.306995995685
Iteration 9800: Loss = -10871.306922877106
Iteration 9900: Loss = -10871.400037598285
1
Iteration 10000: Loss = -10871.30688075143
Iteration 10100: Loss = -10871.306851309093
Iteration 10200: Loss = -10871.306790669287
Iteration 10300: Loss = -10871.30692141158
1
Iteration 10400: Loss = -10871.306738120946
Iteration 10500: Loss = -10871.306756440657
Iteration 10600: Loss = -10871.306949856731
1
Iteration 10700: Loss = -10871.306675196633
Iteration 10800: Loss = -10871.746390035767
1
Iteration 10900: Loss = -10871.306633689554
Iteration 11000: Loss = -10871.306639991913
Iteration 11100: Loss = -10871.31420054282
1
Iteration 11200: Loss = -10871.306559398925
Iteration 11300: Loss = -10871.30650427875
Iteration 11400: Loss = -10871.306522841442
Iteration 11500: Loss = -10871.306724148988
1
Iteration 11600: Loss = -10871.306484532439
Iteration 11700: Loss = -10871.307244146003
1
Iteration 11800: Loss = -10871.30641672966
Iteration 11900: Loss = -10871.318674940905
1
Iteration 12000: Loss = -10871.30635927752
Iteration 12100: Loss = -10871.306324753341
Iteration 12200: Loss = -10871.401939052503
1
Iteration 12300: Loss = -10871.30636212789
Iteration 12400: Loss = -10871.306315872109
Iteration 12500: Loss = -10871.306926443674
1
Iteration 12600: Loss = -10871.306318652956
Iteration 12700: Loss = -10871.306319609384
Iteration 12800: Loss = -10871.306327935828
Iteration 12900: Loss = -10871.3062909602
Iteration 13000: Loss = -10871.306319055506
Iteration 13100: Loss = -10871.30637095721
Iteration 13200: Loss = -10871.306279515651
Iteration 13300: Loss = -10871.306269234276
Iteration 13400: Loss = -10871.311338595957
1
Iteration 13500: Loss = -10871.306257850483
Iteration 13600: Loss = -10871.30625339058
Iteration 13700: Loss = -10871.419380725603
1
Iteration 13800: Loss = -10871.306249584035
Iteration 13900: Loss = -10871.306238224755
Iteration 14000: Loss = -10871.339525622954
1
Iteration 14100: Loss = -10871.352870041019
2
Iteration 14200: Loss = -10871.306246728174
Iteration 14300: Loss = -10871.306348377992
1
Iteration 14400: Loss = -10871.306486722138
2
Iteration 14500: Loss = -10871.306396469303
3
Iteration 14600: Loss = -10871.30619849813
Iteration 14700: Loss = -10871.309583248625
1
Iteration 14800: Loss = -10871.306237103745
Iteration 14900: Loss = -10871.306171181459
Iteration 15000: Loss = -10871.308625571217
1
Iteration 15100: Loss = -10871.306155530894
Iteration 15200: Loss = -10871.306177845197
Iteration 15300: Loss = -10871.307474769234
1
Iteration 15400: Loss = -10871.306146610314
Iteration 15500: Loss = -10871.310170936675
1
Iteration 15600: Loss = -10871.307737660532
2
Iteration 15700: Loss = -10871.3061534809
Iteration 15800: Loss = -10871.306362741765
1
Iteration 15900: Loss = -10871.306132464035
Iteration 16000: Loss = -10871.306200167432
Iteration 16100: Loss = -10871.306118290426
Iteration 16200: Loss = -10871.30662138427
1
Iteration 16300: Loss = -10871.306162649149
Iteration 16400: Loss = -10871.389693271682
1
Iteration 16500: Loss = -10871.30614218582
Iteration 16600: Loss = -10871.306274099821
1
Iteration 16700: Loss = -10871.306468110392
2
Iteration 16800: Loss = -10871.306336200749
3
Iteration 16900: Loss = -10871.307505640514
4
Iteration 17000: Loss = -10871.306135487828
Iteration 17100: Loss = -10871.306453213423
1
Iteration 17200: Loss = -10871.306148834752
Iteration 17300: Loss = -10871.315680578045
1
Iteration 17400: Loss = -10871.30616806432
Iteration 17500: Loss = -10871.568071701458
1
Iteration 17600: Loss = -10871.30617842763
Iteration 17700: Loss = -10871.30615248359
Iteration 17800: Loss = -10871.306125917085
Iteration 17900: Loss = -10871.306148328247
Iteration 18000: Loss = -10871.31857198603
1
Iteration 18100: Loss = -10871.306198946984
Iteration 18200: Loss = -10871.306141321762
Iteration 18300: Loss = -10871.306773372045
1
Iteration 18400: Loss = -10871.306183764998
Iteration 18500: Loss = -10871.338104748078
1
Iteration 18600: Loss = -10871.30612815866
Iteration 18700: Loss = -10871.309691841765
1
Iteration 18800: Loss = -10871.306101590948
Iteration 18900: Loss = -10871.339646070564
1
Iteration 19000: Loss = -10871.306089887737
Iteration 19100: Loss = -10871.306066852021
Iteration 19200: Loss = -10871.306586015766
1
Iteration 19300: Loss = -10871.306162087712
Iteration 19400: Loss = -10871.30658646089
1
Iteration 19500: Loss = -10871.306131212188
Iteration 19600: Loss = -10871.30802637615
1
Iteration 19700: Loss = -10871.316186940794
2
Iteration 19800: Loss = -10871.306114092255
Iteration 19900: Loss = -10871.30673431937
1
pi: tensor([[1.0000e+00, 1.2441e-08],
        [1.4297e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9500, 0.0500], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1611, 0.1982],
         [0.5547, 0.0239]],

        [[0.5150, 0.1548],
         [0.6979, 0.5847]],

        [[0.5825, 0.1698],
         [0.6414, 0.6608]],

        [[0.6094, 0.0842],
         [0.7215, 0.6697]],

        [[0.6308, 0.1460],
         [0.5689, 0.5340]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0035158395898187145
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.003243945514707375
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.010092363144852444
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.00038912871648324923
Global Adjusted Rand Index: -0.00018813467580984936
Average Adjusted Rand Index: -0.0024037150176901686
10774.414750480853
[0.8168490785299649, -0.00018813467580984936] [0.8162416358210033, -0.0024037150176901686] [10725.581034849667, 10871.306081978477]
-------------------------------------
This iteration is 56
True Objective function: Loss = -10948.866004451182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24034.108036032583
Iteration 100: Loss = -11086.960999723018
Iteration 200: Loss = -11085.551869549816
Iteration 300: Loss = -11085.200455735025
Iteration 400: Loss = -11085.046729526082
Iteration 500: Loss = -11084.913813069235
Iteration 600: Loss = -11084.630860450501
Iteration 700: Loss = -11082.892367751194
Iteration 800: Loss = -11082.193012621998
Iteration 900: Loss = -11081.8703585802
Iteration 1000: Loss = -11081.603059946603
Iteration 1100: Loss = -11081.450071833471
Iteration 1200: Loss = -11081.367705264562
Iteration 1300: Loss = -11081.310776078835
Iteration 1400: Loss = -11081.266479190992
Iteration 1500: Loss = -11081.230198998637
Iteration 1600: Loss = -11081.199834167879
Iteration 1700: Loss = -11081.174195474829
Iteration 1800: Loss = -11081.152571997542
Iteration 1900: Loss = -11081.134357772016
Iteration 2000: Loss = -11081.119170544232
Iteration 2100: Loss = -11081.106587367465
Iteration 2200: Loss = -11081.096267398485
Iteration 2300: Loss = -11081.08794845072
Iteration 2400: Loss = -11081.081415514529
Iteration 2500: Loss = -11081.076383070173
Iteration 2600: Loss = -11081.072607289037
Iteration 2700: Loss = -11081.069814104632
Iteration 2800: Loss = -11081.067767663793
Iteration 2900: Loss = -11081.06621777832
Iteration 3000: Loss = -11081.065055969873
Iteration 3100: Loss = -11081.064245437885
Iteration 3200: Loss = -11081.06359857825
Iteration 3300: Loss = -11081.063053827378
Iteration 3400: Loss = -11081.062639851569
Iteration 3500: Loss = -11081.062285798424
Iteration 3600: Loss = -11081.061943633666
Iteration 3700: Loss = -11081.061694819751
Iteration 3800: Loss = -11081.061386051268
Iteration 3900: Loss = -11081.06100294377
Iteration 4000: Loss = -11081.060628564988
Iteration 4100: Loss = -11081.06014194403
Iteration 4200: Loss = -11081.062139770882
1
Iteration 4300: Loss = -11081.058173130568
Iteration 4400: Loss = -11081.056026782897
Iteration 4500: Loss = -11081.051577631386
Iteration 4600: Loss = -11081.03976467336
Iteration 4700: Loss = -11080.993321155274
Iteration 4800: Loss = -11076.86370592606
Iteration 4900: Loss = -10912.958608799065
Iteration 5000: Loss = -10912.683003020848
Iteration 5100: Loss = -10912.594828717394
Iteration 5200: Loss = -10912.547149868169
Iteration 5300: Loss = -10912.466420084209
Iteration 5400: Loss = -10912.434769605703
Iteration 5500: Loss = -10912.405176006354
Iteration 5600: Loss = -10912.380592329786
Iteration 5700: Loss = -10912.346976412435
Iteration 5800: Loss = -10912.327748829459
Iteration 5900: Loss = -10912.334987261114
1
Iteration 6000: Loss = -10912.324483839659
Iteration 6100: Loss = -10912.323737987539
Iteration 6200: Loss = -10912.32170587601
Iteration 6300: Loss = -10912.31678097416
Iteration 6400: Loss = -10912.312661953856
Iteration 6500: Loss = -10912.260743236004
Iteration 6600: Loss = -10912.19830381298
Iteration 6700: Loss = -10912.172702227383
Iteration 6800: Loss = -10912.170902517759
Iteration 6900: Loss = -10912.170157914365
Iteration 7000: Loss = -10912.168760617707
Iteration 7100: Loss = -10912.166113840833
Iteration 7200: Loss = -10912.163255347985
Iteration 7300: Loss = -10912.161305983213
Iteration 7400: Loss = -10911.955016104337
Iteration 7500: Loss = -10911.948063584738
Iteration 7600: Loss = -10911.946368572366
Iteration 7700: Loss = -10911.94476827366
Iteration 7800: Loss = -10911.937361502625
Iteration 7900: Loss = -10911.935404434724
Iteration 8000: Loss = -10911.96624337679
1
Iteration 8100: Loss = -10911.957222595105
2
Iteration 8200: Loss = -10911.934007573676
Iteration 8300: Loss = -10911.933756852495
Iteration 8400: Loss = -10911.932878135207
Iteration 8500: Loss = -10911.930701309526
Iteration 8600: Loss = -10911.92750638676
Iteration 8700: Loss = -10911.915872015043
Iteration 8800: Loss = -10911.895109753486
Iteration 8900: Loss = -10911.909377495695
1
Iteration 9000: Loss = -10911.894704046601
Iteration 9100: Loss = -10911.895476755024
1
Iteration 9200: Loss = -10911.895715163664
2
Iteration 9300: Loss = -10911.891846486304
Iteration 9400: Loss = -10911.894039424678
1
Iteration 9500: Loss = -10911.735459975152
Iteration 9600: Loss = -10911.652074380623
Iteration 9700: Loss = -10911.650672281157
Iteration 9800: Loss = -10911.650567977082
Iteration 9900: Loss = -10911.650408536014
Iteration 10000: Loss = -10911.649932872542
Iteration 10100: Loss = -10911.658602457384
1
Iteration 10200: Loss = -10911.65686226051
2
Iteration 10300: Loss = -10911.648060631529
Iteration 10400: Loss = -10911.654839104023
1
Iteration 10500: Loss = -10911.830908634443
2
Iteration 10600: Loss = -10911.645313998539
Iteration 10700: Loss = -10911.645853428876
1
Iteration 10800: Loss = -10911.765479479198
2
Iteration 10900: Loss = -10911.649217901746
3
Iteration 11000: Loss = -10911.645259722807
Iteration 11100: Loss = -10911.645722279294
1
Iteration 11200: Loss = -10911.659574694242
2
Iteration 11300: Loss = -10911.656390576438
3
Iteration 11400: Loss = -10911.645261591053
Iteration 11500: Loss = -10911.644885375932
Iteration 11600: Loss = -10911.649119974158
1
Iteration 11700: Loss = -10911.644085279224
Iteration 11800: Loss = -10911.710613288484
1
Iteration 11900: Loss = -10911.646098704108
2
Iteration 12000: Loss = -10911.896014609747
3
Iteration 12100: Loss = -10911.653747277183
4
Iteration 12200: Loss = -10911.65549806648
5
Iteration 12300: Loss = -10911.642533760247
Iteration 12400: Loss = -10911.642800834634
1
Iteration 12500: Loss = -10911.65599964904
2
Iteration 12600: Loss = -10911.642476414741
Iteration 12700: Loss = -10911.64919236153
1
Iteration 12800: Loss = -10911.643471726513
2
Iteration 12900: Loss = -10911.644248199225
3
Iteration 13000: Loss = -10911.64545492705
4
Iteration 13100: Loss = -10911.642485237217
Iteration 13200: Loss = -10911.642891269625
1
Iteration 13300: Loss = -10911.66763109035
2
Iteration 13400: Loss = -10911.651931374432
3
Iteration 13500: Loss = -10911.641989933525
Iteration 13600: Loss = -10911.642482257144
1
Iteration 13700: Loss = -10911.658176284142
2
Iteration 13800: Loss = -10911.6444019507
3
Iteration 13900: Loss = -10911.64253201164
4
Iteration 14000: Loss = -10911.642648883304
5
Iteration 14100: Loss = -10911.664124081282
6
Iteration 14200: Loss = -10911.715406489971
7
Iteration 14300: Loss = -10911.641798436554
Iteration 14400: Loss = -10911.642628203492
1
Iteration 14500: Loss = -10911.642466711784
2
Iteration 14600: Loss = -10911.648532626992
3
Iteration 14700: Loss = -10911.673156142535
4
Iteration 14800: Loss = -10911.651147256114
5
Iteration 14900: Loss = -10911.643697386999
6
Iteration 15000: Loss = -10911.641969634547
7
Iteration 15100: Loss = -10911.64259489045
8
Iteration 15200: Loss = -10911.643444894156
9
Iteration 15300: Loss = -10911.698744107525
10
Iteration 15400: Loss = -10911.642227406222
11
Iteration 15500: Loss = -10911.641596397272
Iteration 15600: Loss = -10911.641872742774
1
Iteration 15700: Loss = -10911.642375944699
2
Iteration 15800: Loss = -10911.642957018084
3
Iteration 15900: Loss = -10911.642535920515
4
Iteration 16000: Loss = -10911.734018963118
5
Iteration 16100: Loss = -10911.646045600342
6
Iteration 16200: Loss = -10911.64188973424
7
Iteration 16300: Loss = -10911.64156362502
Iteration 16400: Loss = -10911.690458241863
1
Iteration 16500: Loss = -10911.644892604736
2
Iteration 16600: Loss = -10911.64164172993
Iteration 16700: Loss = -10911.641553779205
Iteration 16800: Loss = -10911.647811614024
1
Iteration 16900: Loss = -10911.658454188908
2
Iteration 17000: Loss = -10911.661214427886
3
Iteration 17100: Loss = -10911.646243587906
4
Iteration 17200: Loss = -10911.64101383124
Iteration 17300: Loss = -10911.643055127948
1
Iteration 17400: Loss = -10911.645119465737
2
Iteration 17500: Loss = -10911.64640323645
3
Iteration 17600: Loss = -10911.64271536344
4
Iteration 17700: Loss = -10911.696368084475
5
Iteration 17800: Loss = -10911.650482036193
6
Iteration 17900: Loss = -10911.645427785144
7
Iteration 18000: Loss = -10911.641509136347
8
Iteration 18100: Loss = -10911.640162132706
Iteration 18200: Loss = -10911.641565907386
1
Iteration 18300: Loss = -10911.639717026892
Iteration 18400: Loss = -10911.639283615064
Iteration 18500: Loss = -10911.64825825027
1
Iteration 18600: Loss = -10911.648729536702
2
Iteration 18700: Loss = -10911.69488610865
3
Iteration 18800: Loss = -10911.640204661455
4
Iteration 18900: Loss = -10911.639008180766
Iteration 19000: Loss = -10911.640319472152
1
Iteration 19100: Loss = -10911.665190656104
2
Iteration 19200: Loss = -10911.710347978473
3
Iteration 19300: Loss = -10911.819588843708
4
Iteration 19400: Loss = -10911.639525997112
5
Iteration 19500: Loss = -10911.643081772121
6
Iteration 19600: Loss = -10911.638967497393
Iteration 19700: Loss = -10911.639633694851
1
Iteration 19800: Loss = -10911.694128958963
2
Iteration 19900: Loss = -10911.654577066922
3
pi: tensor([[0.7636, 0.2364],
        [0.2556, 0.7444]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4306, 0.5694], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2083, 0.0986],
         [0.6994, 0.2524]],

        [[0.5417, 0.0894],
         [0.6263, 0.6779]],

        [[0.6984, 0.0943],
         [0.6462, 0.5103]],

        [[0.5673, 0.1067],
         [0.6807, 0.6769]],

        [[0.7005, 0.0986],
         [0.5934, 0.5923]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.6044273961975466
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448573745636614
Global Adjusted Rand Index: 0.8460923166521691
Average Adjusted Rand Index: 0.8503401412079631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22854.82693127467
Iteration 100: Loss = -11086.564446798116
Iteration 200: Loss = -11083.350386549093
Iteration 300: Loss = -11082.749445551335
Iteration 400: Loss = -11082.60729512725
Iteration 500: Loss = -11082.558430085064
Iteration 600: Loss = -11082.535297370607
Iteration 700: Loss = -11082.52193045439
Iteration 800: Loss = -11082.513594453661
Iteration 900: Loss = -11082.508037515296
Iteration 1000: Loss = -11082.504135316953
Iteration 1100: Loss = -11082.501177792532
Iteration 1200: Loss = -11082.498968610134
Iteration 1300: Loss = -11082.497229175571
Iteration 1400: Loss = -11082.495766761944
Iteration 1500: Loss = -11082.494511316692
Iteration 1600: Loss = -11082.493588479256
Iteration 1700: Loss = -11082.492983641867
Iteration 1800: Loss = -11082.492490683868
Iteration 1900: Loss = -11082.492019476984
Iteration 2000: Loss = -11082.491699125741
Iteration 2100: Loss = -11082.491316732156
Iteration 2200: Loss = -11082.490997912972
Iteration 2300: Loss = -11082.490730570962
Iteration 2400: Loss = -11082.490396985264
Iteration 2500: Loss = -11082.49014149921
Iteration 2600: Loss = -11082.489883635957
Iteration 2700: Loss = -11082.489550923061
Iteration 2800: Loss = -11082.489241433323
Iteration 2900: Loss = -11082.488933642244
Iteration 3000: Loss = -11082.48862923428
Iteration 3100: Loss = -11082.488175807683
Iteration 3200: Loss = -11082.4868522766
Iteration 3300: Loss = -11082.486325558237
Iteration 3400: Loss = -11082.486060689036
Iteration 3500: Loss = -11082.485832767396
Iteration 3600: Loss = -11082.485516705068
Iteration 3700: Loss = -11082.484986369995
Iteration 3800: Loss = -11082.48455497109
Iteration 3900: Loss = -11082.484373666211
Iteration 4000: Loss = -11082.484241319398
Iteration 4100: Loss = -11082.484080188782
Iteration 4200: Loss = -11082.484358844753
1
Iteration 4300: Loss = -11082.483874593814
Iteration 4400: Loss = -11082.483750364501
Iteration 4500: Loss = -11082.483660399683
Iteration 4600: Loss = -11082.483589106463
Iteration 4700: Loss = -11082.48358334664
Iteration 4800: Loss = -11082.483381867496
Iteration 4900: Loss = -11082.485833480661
1
Iteration 5000: Loss = -11082.483288503314
Iteration 5100: Loss = -11082.483391436366
1
Iteration 5200: Loss = -11082.483143297324
Iteration 5300: Loss = -11082.483114441435
Iteration 5400: Loss = -11082.483027360326
Iteration 5500: Loss = -11082.482969123048
Iteration 5600: Loss = -11082.482923722704
Iteration 5700: Loss = -11082.482911203857
Iteration 5800: Loss = -11082.482876526557
Iteration 5900: Loss = -11082.48282147198
Iteration 6000: Loss = -11082.488810365954
1
Iteration 6100: Loss = -11082.482708061958
Iteration 6200: Loss = -11082.482635195765
Iteration 6300: Loss = -11082.482619741952
Iteration 6400: Loss = -11082.48257477826
Iteration 6500: Loss = -11082.482542815806
Iteration 6600: Loss = -11082.482216771326
Iteration 6700: Loss = -11082.482252814973
Iteration 6800: Loss = -11082.482136393692
Iteration 6900: Loss = -11082.484483039594
1
Iteration 7000: Loss = -11082.482075284255
Iteration 7100: Loss = -11082.481951222575
Iteration 7200: Loss = -11082.481937837114
Iteration 7300: Loss = -11082.481833744125
Iteration 7400: Loss = -11082.481494313437
Iteration 7500: Loss = -11082.481463563709
Iteration 7600: Loss = -11082.4814635028
Iteration 7700: Loss = -11082.481393612721
Iteration 7800: Loss = -11082.481404221213
Iteration 7900: Loss = -11082.515159836421
1
Iteration 8000: Loss = -11082.48141740323
Iteration 8100: Loss = -11082.481438740395
Iteration 8200: Loss = -11082.49128115614
1
Iteration 8300: Loss = -11082.481264094926
Iteration 8400: Loss = -11082.481421441567
1
Iteration 8500: Loss = -11082.481149378098
Iteration 8600: Loss = -11082.482197185009
1
Iteration 8700: Loss = -11082.481086188636
Iteration 8800: Loss = -11082.490829330722
1
Iteration 8900: Loss = -11082.481080069745
Iteration 9000: Loss = -11082.481035256722
Iteration 9100: Loss = -11082.485151224735
1
Iteration 9200: Loss = -11082.480181634366
Iteration 9300: Loss = -11082.480192645817
Iteration 9400: Loss = -11082.480763407932
1
Iteration 9500: Loss = -11082.48019491332
Iteration 9600: Loss = -11082.480274900461
Iteration 9700: Loss = -11082.480215941512
Iteration 9800: Loss = -11082.485043809458
1
Iteration 9900: Loss = -11082.48820282462
2
Iteration 10000: Loss = -11082.483459214558
3
Iteration 10100: Loss = -11082.483383017274
4
Iteration 10200: Loss = -11082.480158364284
Iteration 10300: Loss = -11082.480219152492
Iteration 10400: Loss = -11082.479941645302
Iteration 10500: Loss = -11082.515854149788
1
Iteration 10600: Loss = -11082.479889335349
Iteration 10700: Loss = -11082.480167758513
1
Iteration 10800: Loss = -11082.479930111058
Iteration 10900: Loss = -11082.479883513324
Iteration 11000: Loss = -11082.480883853197
1
Iteration 11100: Loss = -11082.479940171426
Iteration 11200: Loss = -11082.497170381766
1
Iteration 11300: Loss = -11082.481581795106
2
Iteration 11400: Loss = -11082.482531139352
3
Iteration 11500: Loss = -11082.479960162398
Iteration 11600: Loss = -11082.56189652671
1
Iteration 11700: Loss = -11082.47981800116
Iteration 11800: Loss = -11082.480924811975
1
Iteration 11900: Loss = -11082.479864859435
Iteration 12000: Loss = -11082.479904065749
Iteration 12100: Loss = -11082.479830932392
Iteration 12200: Loss = -11082.481131127151
1
Iteration 12300: Loss = -11082.479835470942
Iteration 12400: Loss = -11082.549587019092
1
Iteration 12500: Loss = -11082.479846261958
Iteration 12600: Loss = -11082.47980064131
Iteration 12700: Loss = -11082.47995708287
1
Iteration 12800: Loss = -11082.481695469938
2
Iteration 12900: Loss = -11082.480815361921
3
Iteration 13000: Loss = -11082.479872553758
Iteration 13100: Loss = -11082.479843322253
Iteration 13200: Loss = -11082.482334823248
1
Iteration 13300: Loss = -11082.481521642248
2
Iteration 13400: Loss = -11082.480246100937
3
Iteration 13500: Loss = -11082.479863668535
Iteration 13600: Loss = -11082.479822592051
Iteration 13700: Loss = -11082.485729022304
1
Iteration 13800: Loss = -11082.481481459929
2
Iteration 13900: Loss = -11082.482192625932
3
Iteration 14000: Loss = -11082.479709196574
Iteration 14100: Loss = -11082.479869613844
1
Iteration 14200: Loss = -11082.490382162836
2
Iteration 14300: Loss = -11082.486763183595
3
Iteration 14400: Loss = -11082.490669190238
4
Iteration 14500: Loss = -11082.47999829556
5
Iteration 14600: Loss = -11082.479410009093
Iteration 14700: Loss = -11082.480473065161
1
Iteration 14800: Loss = -11082.540667974177
2
Iteration 14900: Loss = -11082.479378461003
Iteration 15000: Loss = -11082.581236074877
1
Iteration 15100: Loss = -11082.47938407246
Iteration 15200: Loss = -11082.47991868258
1
Iteration 15300: Loss = -11082.479504784125
2
Iteration 15400: Loss = -11082.479513414337
3
Iteration 15500: Loss = -11082.48495112428
4
Iteration 15600: Loss = -11082.479386305877
Iteration 15700: Loss = -11082.47963743039
1
Iteration 15800: Loss = -11082.480879572578
2
Iteration 15900: Loss = -11082.479380962386
Iteration 16000: Loss = -11082.56420835218
1
Iteration 16100: Loss = -11082.47927686848
Iteration 16200: Loss = -11082.48324653068
1
Iteration 16300: Loss = -11082.47957792234
2
Iteration 16400: Loss = -11082.479586677247
3
Iteration 16500: Loss = -11082.479360327712
Iteration 16600: Loss = -11082.491563052585
1
Iteration 16700: Loss = -11082.479265035818
Iteration 16800: Loss = -11082.48296514476
1
Iteration 16900: Loss = -11082.479270817534
Iteration 17000: Loss = -11082.480940489864
1
Iteration 17100: Loss = -11082.480425548467
2
Iteration 17200: Loss = -11082.48644830144
3
Iteration 17300: Loss = -11082.479313826147
Iteration 17400: Loss = -11082.479439806579
1
Iteration 17500: Loss = -11082.479289029436
Iteration 17600: Loss = -11082.479376302488
Iteration 17700: Loss = -11082.479545077125
1
Iteration 17800: Loss = -11082.484894420993
2
Iteration 17900: Loss = -11082.480318167503
3
Iteration 18000: Loss = -11082.47980144703
4
Iteration 18100: Loss = -11082.483314003986
5
Iteration 18200: Loss = -11082.47923606278
Iteration 18300: Loss = -11082.479519845689
1
Iteration 18400: Loss = -11082.479454902335
2
Iteration 18500: Loss = -11082.48269827012
3
Iteration 18600: Loss = -11082.4805658422
4
Iteration 18700: Loss = -11082.479611693707
5
Iteration 18800: Loss = -11082.480144030733
6
Iteration 18900: Loss = -11082.53179869056
7
Iteration 19000: Loss = -11082.540249083571
8
Iteration 19100: Loss = -11082.552550077075
9
Iteration 19200: Loss = -11082.479613608411
10
Iteration 19300: Loss = -11082.479232038817
Iteration 19400: Loss = -11082.48044655688
1
Iteration 19500: Loss = -11082.493927391433
2
Iteration 19600: Loss = -11082.479272193375
Iteration 19700: Loss = -11082.479622907722
1
Iteration 19800: Loss = -11082.48005154001
2
Iteration 19900: Loss = -11082.484179876243
3
pi: tensor([[3.7816e-07, 1.0000e+00],
        [9.5111e-01, 4.8887e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9886, 0.0114], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1627, 0.1647],
         [0.6487, 0.1630]],

        [[0.6527, 0.3136],
         [0.6546, 0.5623]],

        [[0.6551, 0.2084],
         [0.5514, 0.7178]],

        [[0.7259, 0.1696],
         [0.5818, 0.7204]],

        [[0.6768, 0.1701],
         [0.6266, 0.5627]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
Global Adjusted Rand Index: -0.0018953359775715454
Average Adjusted Rand Index: -0.0031443829566715183
10948.866004451182
[0.8460923166521691, -0.0018953359775715454] [0.8503401412079631, -0.0031443829566715183] [10911.64227294357, 11082.499046910729]
-------------------------------------
This iteration is 57
True Objective function: Loss = -10791.154254579336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20745.748727250008
Iteration 100: Loss = -10835.854095109473
Iteration 200: Loss = -10835.07955557537
Iteration 300: Loss = -10834.772009561062
Iteration 400: Loss = -10834.60987363648
Iteration 500: Loss = -10834.509897320808
Iteration 600: Loss = -10834.42560174979
Iteration 700: Loss = -10834.370435345185
Iteration 800: Loss = -10834.326644338178
Iteration 900: Loss = -10834.287907780406
Iteration 1000: Loss = -10834.252673402383
Iteration 1100: Loss = -10834.220607902656
Iteration 1200: Loss = -10834.19278395509
Iteration 1300: Loss = -10834.170434209738
Iteration 1400: Loss = -10834.1534889572
Iteration 1500: Loss = -10834.140494649582
Iteration 1600: Loss = -10834.129654592325
Iteration 1700: Loss = -10834.119561503428
Iteration 1800: Loss = -10834.108974348148
Iteration 1900: Loss = -10834.096483146974
Iteration 2000: Loss = -10834.079224514979
Iteration 2100: Loss = -10834.04936865325
Iteration 2200: Loss = -10833.971728347216
Iteration 2300: Loss = -10833.553173357128
Iteration 2400: Loss = -10833.291567998254
Iteration 2500: Loss = -10833.2135991371
Iteration 2600: Loss = -10833.164269721552
Iteration 2700: Loss = -10833.129049334957
Iteration 2800: Loss = -10833.101406418677
Iteration 2900: Loss = -10833.078035701332
Iteration 3000: Loss = -10833.057127918708
Iteration 3100: Loss = -10833.037507039115
Iteration 3200: Loss = -10833.018096962294
Iteration 3300: Loss = -10832.99687087044
Iteration 3400: Loss = -10832.96555190419
Iteration 3500: Loss = -10832.850002898229
Iteration 3600: Loss = -10832.656357608792
Iteration 3700: Loss = -10832.633216001099
Iteration 3800: Loss = -10832.623760231223
Iteration 3900: Loss = -10832.618274828617
Iteration 4000: Loss = -10832.614693498212
Iteration 4100: Loss = -10832.61209450491
Iteration 4200: Loss = -10832.610169135576
Iteration 4300: Loss = -10832.608665818852
Iteration 4400: Loss = -10832.607488182863
Iteration 4500: Loss = -10832.60657448221
Iteration 4600: Loss = -10832.605887833637
Iteration 4700: Loss = -10832.605316642375
Iteration 4800: Loss = -10832.604934169309
Iteration 4900: Loss = -10832.604529193197
Iteration 5000: Loss = -10832.604198378463
Iteration 5100: Loss = -10832.603904450008
Iteration 5200: Loss = -10832.603624751144
Iteration 5300: Loss = -10832.60341280283
Iteration 5400: Loss = -10832.603143851567
Iteration 5500: Loss = -10832.60294863857
Iteration 5600: Loss = -10832.60271987303
Iteration 5700: Loss = -10832.602529451964
Iteration 5800: Loss = -10832.602368364074
Iteration 5900: Loss = -10832.602200009122
Iteration 6000: Loss = -10832.602035047286
Iteration 6100: Loss = -10832.601870844434
Iteration 6200: Loss = -10832.601731982242
Iteration 6300: Loss = -10832.601599731322
Iteration 6400: Loss = -10832.602200455276
1
Iteration 6500: Loss = -10832.601341294785
Iteration 6600: Loss = -10832.601277868727
Iteration 6700: Loss = -10832.601122793276
Iteration 6800: Loss = -10832.602325563543
1
Iteration 6900: Loss = -10832.60147622628
2
Iteration 7000: Loss = -10832.600829336625
Iteration 7100: Loss = -10832.600725318574
Iteration 7200: Loss = -10832.600641793242
Iteration 7300: Loss = -10832.600548112776
Iteration 7400: Loss = -10832.600497536323
Iteration 7500: Loss = -10832.671912309916
1
Iteration 7600: Loss = -10832.605053034515
2
Iteration 7700: Loss = -10832.603082381767
3
Iteration 7800: Loss = -10832.600257509894
Iteration 7900: Loss = -10832.60019854808
Iteration 8000: Loss = -10832.600171434335
Iteration 8100: Loss = -10832.60008853104
Iteration 8200: Loss = -10832.659227403085
1
Iteration 8300: Loss = -10832.600009610602
Iteration 8400: Loss = -10832.599928322414
Iteration 8500: Loss = -10832.599889225314
Iteration 8600: Loss = -10832.600161259628
1
Iteration 8700: Loss = -10832.59981773638
Iteration 8800: Loss = -10832.599778581673
Iteration 8900: Loss = -10832.690899440933
1
Iteration 9000: Loss = -10832.599702365462
Iteration 9100: Loss = -10832.599670729223
Iteration 9200: Loss = -10832.599652388244
Iteration 9300: Loss = -10832.600304899814
1
Iteration 9400: Loss = -10832.599611590058
Iteration 9500: Loss = -10832.5995630956
Iteration 9600: Loss = -10832.599952886489
1
Iteration 9700: Loss = -10832.599507585395
Iteration 9800: Loss = -10832.599486641202
Iteration 9900: Loss = -10832.59945485099
Iteration 10000: Loss = -10832.59971084689
1
Iteration 10100: Loss = -10832.599421965984
Iteration 10200: Loss = -10832.59944987072
Iteration 10300: Loss = -10832.59937850987
Iteration 10400: Loss = -10832.599376599474
Iteration 10500: Loss = -10832.599365447373
Iteration 10600: Loss = -10832.615835822724
1
Iteration 10700: Loss = -10832.599341621217
Iteration 10800: Loss = -10832.59934605067
Iteration 10900: Loss = -10832.600088650846
1
Iteration 11000: Loss = -10832.599322459257
Iteration 11100: Loss = -10832.599324208852
Iteration 11200: Loss = -10832.599304336447
Iteration 11300: Loss = -10832.599952627123
1
Iteration 11400: Loss = -10832.599270854573
Iteration 11500: Loss = -10832.59926421995
Iteration 11600: Loss = -10832.603620531774
1
Iteration 11700: Loss = -10832.599238168288
Iteration 11800: Loss = -10832.599222984276
Iteration 11900: Loss = -10832.605957451684
1
Iteration 12000: Loss = -10832.599195091834
Iteration 12100: Loss = -10832.599202132034
Iteration 12200: Loss = -10832.622136517792
1
Iteration 12300: Loss = -10832.599156674049
Iteration 12400: Loss = -10832.5992016955
Iteration 12500: Loss = -10832.611621710937
1
Iteration 12600: Loss = -10832.599177838587
Iteration 12700: Loss = -10832.599152959756
Iteration 12800: Loss = -10832.696247433276
1
Iteration 12900: Loss = -10832.599150920576
Iteration 13000: Loss = -10832.59917496685
Iteration 13100: Loss = -10832.600108137718
1
Iteration 13200: Loss = -10832.599118859707
Iteration 13300: Loss = -10833.02020941182
1
Iteration 13400: Loss = -10832.59914691189
Iteration 13500: Loss = -10832.599139817956
Iteration 13600: Loss = -10832.599178447623
Iteration 13700: Loss = -10832.599149772896
Iteration 13800: Loss = -10832.59912561263
Iteration 13900: Loss = -10832.599161425933
Iteration 14000: Loss = -10832.599153295387
Iteration 14100: Loss = -10832.606951228294
1
Iteration 14200: Loss = -10832.599128679565
Iteration 14300: Loss = -10832.599372130548
1
Iteration 14400: Loss = -10832.599109866977
Iteration 14500: Loss = -10832.599122973757
Iteration 14600: Loss = -10832.599329356646
1
Iteration 14700: Loss = -10832.599072289562
Iteration 14800: Loss = -10832.599533623428
1
Iteration 14900: Loss = -10832.599101507672
Iteration 15000: Loss = -10832.60113838306
1
Iteration 15100: Loss = -10832.600100072885
2
Iteration 15200: Loss = -10832.599130917246
Iteration 15300: Loss = -10832.863704798223
1
Iteration 15400: Loss = -10832.599100681855
Iteration 15500: Loss = -10832.675733373486
1
Iteration 15600: Loss = -10832.599060244935
Iteration 15700: Loss = -10832.599933968024
1
Iteration 15800: Loss = -10832.599249450328
2
Iteration 15900: Loss = -10832.599096072328
Iteration 16000: Loss = -10832.599352645426
1
Iteration 16100: Loss = -10832.599090040527
Iteration 16200: Loss = -10832.604412242239
1
Iteration 16300: Loss = -10832.599101045605
Iteration 16400: Loss = -10832.599089894953
Iteration 16500: Loss = -10832.59920979048
1
Iteration 16600: Loss = -10832.600796685689
2
Iteration 16700: Loss = -10832.59935285663
3
Iteration 16800: Loss = -10832.600860873104
4
Iteration 16900: Loss = -10832.599191401285
5
Iteration 17000: Loss = -10832.59910989167
Iteration 17100: Loss = -10832.599304889234
1
Iteration 17200: Loss = -10832.599115148978
Iteration 17300: Loss = -10832.599990805527
1
Iteration 17400: Loss = -10832.599086275515
Iteration 17500: Loss = -10832.612883955746
1
Iteration 17600: Loss = -10832.599858110234
2
Iteration 17700: Loss = -10832.599229804677
3
Iteration 17800: Loss = -10832.617786926858
4
Iteration 17900: Loss = -10832.602521526042
5
Iteration 18000: Loss = -10832.599133931331
Iteration 18100: Loss = -10832.600210714374
1
Iteration 18200: Loss = -10832.600830940966
2
Iteration 18300: Loss = -10832.64544669636
3
Iteration 18400: Loss = -10832.600238977226
4
Iteration 18500: Loss = -10832.5992553642
5
Iteration 18600: Loss = -10832.739646538002
6
Iteration 18700: Loss = -10832.600436367919
7
Iteration 18800: Loss = -10832.599543746433
8
Iteration 18900: Loss = -10832.599176961168
Iteration 19000: Loss = -10832.599103973433
Iteration 19100: Loss = -10832.602642555163
1
Iteration 19200: Loss = -10832.599452428125
2
Iteration 19300: Loss = -10832.6072911456
3
Iteration 19400: Loss = -10832.600350549083
4
Iteration 19500: Loss = -10832.59912266793
Iteration 19600: Loss = -10832.626318851539
1
Iteration 19700: Loss = -10832.599091525872
Iteration 19800: Loss = -10832.599518715939
1
Iteration 19900: Loss = -10832.599832463442
2
pi: tensor([[9.8784e-01, 1.2162e-02],
        [1.0000e+00, 4.8197e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9335, 0.0665], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1603, 0.1064],
         [0.6829, 0.0753]],

        [[0.6384, 0.1358],
         [0.5441, 0.6295]],

        [[0.7249, 0.0867],
         [0.5459, 0.7107]],

        [[0.6564, 0.1653],
         [0.6802, 0.5420]],

        [[0.5324, 0.2876],
         [0.5724, 0.5189]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.018335442091456804
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: -0.006096049957872825
Global Adjusted Rand Index: 0.0017766166647092138
Average Adjusted Rand Index: 0.0024478784267167957
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21716.88761226323
Iteration 100: Loss = -10836.107060159811
Iteration 200: Loss = -10835.192862370848
Iteration 300: Loss = -10834.863587087548
Iteration 400: Loss = -10834.693938365936
Iteration 500: Loss = -10834.591160993114
Iteration 600: Loss = -10834.516996786026
Iteration 700: Loss = -10834.456547527072
Iteration 800: Loss = -10834.403110307554
Iteration 900: Loss = -10834.35165641794
Iteration 1000: Loss = -10834.296529496592
Iteration 1100: Loss = -10834.239754298427
Iteration 1200: Loss = -10834.169085351647
Iteration 1300: Loss = -10834.041648230523
Iteration 1400: Loss = -10833.776528009756
Iteration 1500: Loss = -10833.57105507773
Iteration 1600: Loss = -10833.439572716152
Iteration 1700: Loss = -10833.348469297734
Iteration 1800: Loss = -10833.28324249919
Iteration 1900: Loss = -10833.235782884405
Iteration 2000: Loss = -10833.200371194343
Iteration 2100: Loss = -10833.172759044533
Iteration 2200: Loss = -10833.149982108722
Iteration 2300: Loss = -10833.130132452427
Iteration 2400: Loss = -10833.112122589559
Iteration 2500: Loss = -10833.095265977126
Iteration 2600: Loss = -10833.079066621984
Iteration 2700: Loss = -10833.063228476141
Iteration 2800: Loss = -10833.047468502264
Iteration 2900: Loss = -10833.03157190171
Iteration 3000: Loss = -10833.01526668973
Iteration 3100: Loss = -10832.9979125359
Iteration 3200: Loss = -10832.977295575587
Iteration 3300: Loss = -10832.943865724139
Iteration 3400: Loss = -10832.839384620132
Iteration 3500: Loss = -10832.676699497864
Iteration 3600: Loss = -10832.638947555151
Iteration 3700: Loss = -10832.627324745552
Iteration 3800: Loss = -10832.620535388278
Iteration 3900: Loss = -10832.616634256015
Iteration 4000: Loss = -10832.61410271613
Iteration 4100: Loss = -10832.612210576515
Iteration 4200: Loss = -10832.610661061792
Iteration 4300: Loss = -10832.609443655041
Iteration 4400: Loss = -10832.608481644393
Iteration 4500: Loss = -10832.607706564377
Iteration 4600: Loss = -10832.607065477476
Iteration 4700: Loss = -10832.606574905652
Iteration 4800: Loss = -10832.60613263811
Iteration 4900: Loss = -10832.605769336029
Iteration 5000: Loss = -10832.605398121641
Iteration 5100: Loss = -10832.605096172685
Iteration 5200: Loss = -10832.604764351125
Iteration 5300: Loss = -10832.604483249801
Iteration 5400: Loss = -10832.604228724376
Iteration 5500: Loss = -10832.604002137636
Iteration 5600: Loss = -10832.603755888254
Iteration 5700: Loss = -10832.603552813518
Iteration 5800: Loss = -10832.603370974479
Iteration 5900: Loss = -10832.603153041797
Iteration 6000: Loss = -10832.602930277844
Iteration 6100: Loss = -10832.602786647933
Iteration 6200: Loss = -10832.604508226852
1
Iteration 6300: Loss = -10832.602426229603
Iteration 6400: Loss = -10832.602266067428
Iteration 6500: Loss = -10832.603215679257
1
Iteration 6600: Loss = -10832.602054912843
Iteration 6700: Loss = -10832.60185072177
Iteration 6800: Loss = -10832.601839463392
Iteration 6900: Loss = -10832.60158535273
Iteration 7000: Loss = -10832.60145792315
Iteration 7100: Loss = -10832.60137186237
Iteration 7200: Loss = -10832.601281765797
Iteration 7300: Loss = -10832.601167700339
Iteration 7400: Loss = -10832.60634018003
1
Iteration 7500: Loss = -10832.600980273859
Iteration 7600: Loss = -10832.60351745365
1
Iteration 7700: Loss = -10832.624696366056
2
Iteration 7800: Loss = -10832.60133564448
3
Iteration 7900: Loss = -10832.600661560145
Iteration 8000: Loss = -10832.602263003966
1
Iteration 8100: Loss = -10832.600472558732
Iteration 8200: Loss = -10832.60122345651
1
Iteration 8300: Loss = -10832.60032686283
Iteration 8400: Loss = -10832.658517903481
1
Iteration 8500: Loss = -10832.600220401917
Iteration 8600: Loss = -10832.600171997563
Iteration 8700: Loss = -10832.609683887995
1
Iteration 8800: Loss = -10832.60010710645
Iteration 8900: Loss = -10832.600033650458
Iteration 9000: Loss = -10833.003733407852
1
Iteration 9100: Loss = -10832.599940282922
Iteration 9200: Loss = -10832.599893540346
Iteration 9300: Loss = -10832.599848826872
Iteration 9400: Loss = -10832.624375165055
1
Iteration 9500: Loss = -10832.599820533123
Iteration 9600: Loss = -10832.599727551085
Iteration 9700: Loss = -10832.600160053555
1
Iteration 9800: Loss = -10832.59977032671
Iteration 9900: Loss = -10832.599664277093
Iteration 10000: Loss = -10832.599607079723
Iteration 10100: Loss = -10832.600441584611
1
Iteration 10200: Loss = -10832.5995785721
Iteration 10300: Loss = -10832.59955609076
Iteration 10400: Loss = -10832.602581143006
1
Iteration 10500: Loss = -10832.59949796843
Iteration 10600: Loss = -10832.599492927833
Iteration 10700: Loss = -10832.605896437171
1
Iteration 10800: Loss = -10832.599477625321
Iteration 10900: Loss = -10832.599424991637
Iteration 11000: Loss = -10832.599407167127
Iteration 11100: Loss = -10832.599470642386
Iteration 11200: Loss = -10832.599405451307
Iteration 11300: Loss = -10832.599385984024
Iteration 11400: Loss = -10832.616255469988
1
Iteration 11500: Loss = -10832.599320062722
Iteration 11600: Loss = -10832.599300438487
Iteration 11700: Loss = -10832.603867464106
1
Iteration 11800: Loss = -10832.599328440423
Iteration 11900: Loss = -10832.599263772878
Iteration 12000: Loss = -10832.81060292722
1
Iteration 12100: Loss = -10832.599274144697
Iteration 12200: Loss = -10832.599221852683
Iteration 12300: Loss = -10832.599253658922
Iteration 12400: Loss = -10832.599343407079
Iteration 12500: Loss = -10832.599196467852
Iteration 12600: Loss = -10832.599245856723
Iteration 12700: Loss = -10832.61047800525
1
Iteration 12800: Loss = -10832.599203950398
Iteration 12900: Loss = -10832.599208496591
Iteration 13000: Loss = -10832.863398612471
1
Iteration 13100: Loss = -10832.599213610527
Iteration 13200: Loss = -10832.599211301838
Iteration 13300: Loss = -10832.59970657948
1
Iteration 13400: Loss = -10832.625573870473
2
Iteration 13500: Loss = -10832.59918169984
Iteration 13600: Loss = -10832.60860448029
1
Iteration 13700: Loss = -10832.599147339824
Iteration 13800: Loss = -10832.599686631023
1
Iteration 13900: Loss = -10832.599122616752
Iteration 14000: Loss = -10832.602029749421
1
Iteration 14100: Loss = -10832.599149864676
Iteration 14200: Loss = -10832.59918249714
Iteration 14300: Loss = -10832.600540357733
1
Iteration 14400: Loss = -10832.699027781395
2
Iteration 14500: Loss = -10832.599114436194
Iteration 14600: Loss = -10832.608798152374
1
Iteration 14700: Loss = -10832.599151548817
Iteration 14800: Loss = -10832.599201555327
Iteration 14900: Loss = -10832.606163788334
1
Iteration 15000: Loss = -10832.599113181723
Iteration 15100: Loss = -10832.616924563621
1
Iteration 15200: Loss = -10832.599094857642
Iteration 15300: Loss = -10832.615705775434
1
Iteration 15400: Loss = -10832.599115081017
Iteration 15500: Loss = -10832.659253705247
1
Iteration 15600: Loss = -10832.5991196593
Iteration 15700: Loss = -10832.599109023931
Iteration 15800: Loss = -10832.59931986591
1
Iteration 15900: Loss = -10832.59926195677
2
Iteration 16000: Loss = -10832.599457516082
3
Iteration 16100: Loss = -10832.604492560147
4
Iteration 16200: Loss = -10832.599125714658
Iteration 16300: Loss = -10832.5991119122
Iteration 16400: Loss = -10832.599255651268
1
Iteration 16500: Loss = -10832.603921608357
2
Iteration 16600: Loss = -10832.617620668127
3
Iteration 16700: Loss = -10832.599108531756
Iteration 16800: Loss = -10832.599817414355
1
Iteration 16900: Loss = -10832.5996825573
2
Iteration 17000: Loss = -10832.60824137085
3
Iteration 17100: Loss = -10832.599219591477
4
Iteration 17200: Loss = -10832.63306315189
5
Iteration 17300: Loss = -10832.599141956336
Iteration 17400: Loss = -10832.599547258054
1
Iteration 17500: Loss = -10832.59944839869
2
Iteration 17600: Loss = -10832.599169491566
Iteration 17700: Loss = -10832.599086197835
Iteration 17800: Loss = -10832.61554910144
1
Iteration 17900: Loss = -10832.599077195153
Iteration 18000: Loss = -10832.983478831982
1
Iteration 18100: Loss = -10832.599101944514
Iteration 18200: Loss = -10832.599289853331
1
Iteration 18300: Loss = -10832.599104473671
Iteration 18400: Loss = -10832.681359635306
1
Iteration 18500: Loss = -10832.599113125863
Iteration 18600: Loss = -10832.877538085988
1
Iteration 18700: Loss = -10832.599068596695
Iteration 18800: Loss = -10832.601498800286
1
Iteration 18900: Loss = -10832.63900158445
2
Iteration 19000: Loss = -10832.634910973724
3
Iteration 19100: Loss = -10832.59945782828
4
Iteration 19200: Loss = -10832.600657062565
5
Iteration 19300: Loss = -10832.599163072406
Iteration 19400: Loss = -10832.599123708349
Iteration 19500: Loss = -10832.599098104314
Iteration 19600: Loss = -10832.599176327367
Iteration 19700: Loss = -10832.599873104185
1
Iteration 19800: Loss = -10832.600055544135
2
Iteration 19900: Loss = -10832.641033021924
3
pi: tensor([[7.5748e-06, 9.9999e-01],
        [1.2160e-02, 9.8784e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0665, 0.9335], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0752, 0.1064],
         [0.7212, 0.1602]],

        [[0.6081, 0.1358],
         [0.6617, 0.5275]],

        [[0.6821, 0.0867],
         [0.6438, 0.5173]],

        [[0.6586, 0.1653],
         [0.6774, 0.5387]],

        [[0.6488, 0.2877],
         [0.5963, 0.5118]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.018335442091456804
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
Global Adjusted Rand Index: 0.0017766166647092138
Average Adjusted Rand Index: 0.0024478784267167957
10791.154254579336
[0.0017766166647092138, 0.0017766166647092138] [0.0024478784267167957, 0.0024478784267167957] [10832.599162349334, 10832.599077605948]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11042.045445665199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23520.3260970324
Iteration 100: Loss = -11120.999959872084
Iteration 200: Loss = -11117.689003855328
Iteration 300: Loss = -11116.311315931498
Iteration 400: Loss = -11114.570850570753
Iteration 500: Loss = -11113.805509201617
Iteration 600: Loss = -11113.092249130355
Iteration 700: Loss = -11112.539117893955
Iteration 800: Loss = -11112.172060560675
Iteration 900: Loss = -11111.930971899483
Iteration 1000: Loss = -11111.761579637696
Iteration 1100: Loss = -11111.640389903901
Iteration 1200: Loss = -11111.55236236157
Iteration 1300: Loss = -11111.497302391686
Iteration 1400: Loss = -11111.463115805487
Iteration 1500: Loss = -11111.438538680879
Iteration 1600: Loss = -11111.419145220209
Iteration 1700: Loss = -11111.4028416253
Iteration 1800: Loss = -11111.388661242941
Iteration 1900: Loss = -11111.376243227493
Iteration 2000: Loss = -11111.365528944523
Iteration 2100: Loss = -11111.356513014773
Iteration 2200: Loss = -11111.349070725502
Iteration 2300: Loss = -11111.34296041873
Iteration 2400: Loss = -11111.33796536142
Iteration 2500: Loss = -11111.33375593209
Iteration 2600: Loss = -11111.330214332651
Iteration 2700: Loss = -11111.327327252004
Iteration 2800: Loss = -11111.324900262229
Iteration 2900: Loss = -11111.322858790681
Iteration 3000: Loss = -11111.321198555474
Iteration 3100: Loss = -11111.319791222382
Iteration 3200: Loss = -11111.318650696132
Iteration 3300: Loss = -11111.31763649653
Iteration 3400: Loss = -11111.316813751637
Iteration 3500: Loss = -11111.316101035718
Iteration 3600: Loss = -11111.315486605454
Iteration 3700: Loss = -11111.314921922023
Iteration 3800: Loss = -11111.314486776555
Iteration 3900: Loss = -11111.314015492622
Iteration 4000: Loss = -11111.313659543206
Iteration 4100: Loss = -11111.313341021798
Iteration 4200: Loss = -11111.313074844562
Iteration 4300: Loss = -11111.312810155841
Iteration 4400: Loss = -11111.312566367016
Iteration 4500: Loss = -11111.31242709443
Iteration 4600: Loss = -11111.31218793654
Iteration 4700: Loss = -11111.312062033156
Iteration 4800: Loss = -11111.311869660947
Iteration 4900: Loss = -11111.311795250242
Iteration 5000: Loss = -11111.31164619742
Iteration 5100: Loss = -11111.311492212704
Iteration 5200: Loss = -11111.311347464223
Iteration 5300: Loss = -11111.311205548998
Iteration 5400: Loss = -11111.311132980656
Iteration 5500: Loss = -11111.31092751552
Iteration 5600: Loss = -11111.310738813158
Iteration 5700: Loss = -11111.310601280495
Iteration 5800: Loss = -11111.31028001479
Iteration 5900: Loss = -11111.315765302876
1
Iteration 6000: Loss = -11111.309441320558
Iteration 6100: Loss = -11111.308609552596
Iteration 6200: Loss = -11111.306866968525
Iteration 6300: Loss = -11111.300169456164
Iteration 6400: Loss = -11111.23495081193
Iteration 6500: Loss = -10987.23284277732
Iteration 6600: Loss = -10980.058803939157
Iteration 6700: Loss = -10979.75508774311
Iteration 6800: Loss = -10979.59132987824
Iteration 6900: Loss = -10979.565853428241
Iteration 7000: Loss = -10979.519468062357
Iteration 7100: Loss = -10979.507114303771
Iteration 7200: Loss = -10979.487137802185
Iteration 7300: Loss = -10979.455548179434
Iteration 7400: Loss = -10979.445478965727
Iteration 7500: Loss = -10979.412988336653
Iteration 7600: Loss = -10979.382901432587
Iteration 7700: Loss = -10979.375720127726
Iteration 7800: Loss = -10979.35499417273
Iteration 7900: Loss = -10979.349882533488
Iteration 8000: Loss = -10979.378620836469
1
Iteration 8100: Loss = -10979.346208286179
Iteration 8200: Loss = -10979.336736386314
Iteration 8300: Loss = -10979.335739731592
Iteration 8400: Loss = -10979.33244571076
Iteration 8500: Loss = -10979.324399421073
Iteration 8600: Loss = -10979.303751189767
Iteration 8700: Loss = -10979.317406400314
1
Iteration 8800: Loss = -10979.273831860864
Iteration 8900: Loss = -10979.272800561583
Iteration 9000: Loss = -10979.272490508918
Iteration 9100: Loss = -10979.271502342193
Iteration 9200: Loss = -10979.287110289479
1
Iteration 9300: Loss = -10979.27032241501
Iteration 9400: Loss = -10979.269423324968
Iteration 9500: Loss = -10979.242768884953
Iteration 9600: Loss = -10979.241618933702
Iteration 9700: Loss = -10979.241317377513
Iteration 9800: Loss = -10979.241229749408
Iteration 9900: Loss = -10979.24104739358
Iteration 10000: Loss = -10979.24621660344
1
Iteration 10100: Loss = -10979.393142885257
2
Iteration 10200: Loss = -10979.240615392906
Iteration 10300: Loss = -10979.24058546601
Iteration 10400: Loss = -10979.240797914574
1
Iteration 10500: Loss = -10979.320720429954
2
Iteration 10600: Loss = -10979.238702360948
Iteration 10700: Loss = -10979.246206730362
1
Iteration 10800: Loss = -10979.238103978212
Iteration 10900: Loss = -10979.248090342167
1
Iteration 11000: Loss = -10979.231510791795
Iteration 11100: Loss = -10979.261375907818
1
Iteration 11200: Loss = -10979.230726034968
Iteration 11300: Loss = -10979.230752665037
Iteration 11400: Loss = -10979.253627904582
1
Iteration 11500: Loss = -10979.23020360667
Iteration 11600: Loss = -10979.244043221403
1
Iteration 11700: Loss = -10979.227028197314
Iteration 11800: Loss = -10979.4094050784
1
Iteration 11900: Loss = -10979.22688966469
Iteration 12000: Loss = -10979.226612419297
Iteration 12100: Loss = -10979.228334883657
1
Iteration 12200: Loss = -10979.224646183202
Iteration 12300: Loss = -10979.224640575481
Iteration 12400: Loss = -10979.227152961652
1
Iteration 12500: Loss = -10979.224587196954
Iteration 12600: Loss = -10979.224571585757
Iteration 12700: Loss = -10979.224752171507
1
Iteration 12800: Loss = -10979.24380327636
2
Iteration 12900: Loss = -10979.224808786583
3
Iteration 13000: Loss = -10979.22427531315
Iteration 13100: Loss = -10979.245471986886
1
Iteration 13200: Loss = -10979.23974969594
2
Iteration 13300: Loss = -10979.38497671467
3
Iteration 13400: Loss = -10979.224100774123
Iteration 13500: Loss = -10979.224070805001
Iteration 13600: Loss = -10979.223366125469
Iteration 13700: Loss = -10979.224653337928
1
Iteration 13800: Loss = -10979.223052883586
Iteration 13900: Loss = -10979.293255794953
1
Iteration 14000: Loss = -10979.223052835116
Iteration 14100: Loss = -10979.223036943322
Iteration 14200: Loss = -10979.223079871947
Iteration 14300: Loss = -10979.227722614138
1
Iteration 14400: Loss = -10979.225747557146
2
Iteration 14500: Loss = -10979.226306705823
3
Iteration 14600: Loss = -10979.22807423161
4
Iteration 14700: Loss = -10979.23494809481
5
Iteration 14800: Loss = -10979.223055494842
Iteration 14900: Loss = -10979.224060464292
1
Iteration 15000: Loss = -10979.223816248214
2
Iteration 15100: Loss = -10979.22687072973
3
Iteration 15200: Loss = -10979.224695843137
4
Iteration 15300: Loss = -10979.224292469544
5
Iteration 15400: Loss = -10979.223147698856
Iteration 15500: Loss = -10979.22307709908
Iteration 15600: Loss = -10979.229323597892
1
Iteration 15700: Loss = -10979.2230303152
Iteration 15800: Loss = -10979.22325679027
1
Iteration 15900: Loss = -10979.223023804028
Iteration 16000: Loss = -10979.223327200274
1
Iteration 16100: Loss = -10979.229582852957
2
Iteration 16200: Loss = -10979.223009683668
Iteration 16300: Loss = -10979.224355886538
1
Iteration 16400: Loss = -10979.233550487563
2
Iteration 16500: Loss = -10979.301203950135
3
Iteration 16600: Loss = -10979.223971945887
4
Iteration 16700: Loss = -10979.223391376154
5
Iteration 16800: Loss = -10979.242340982917
6
Iteration 16900: Loss = -10979.22329072922
7
Iteration 17000: Loss = -10979.223025949685
Iteration 17100: Loss = -10979.233924315164
1
Iteration 17200: Loss = -10979.222953667273
Iteration 17300: Loss = -10979.22932726055
1
Iteration 17400: Loss = -10979.222938628394
Iteration 17500: Loss = -10979.22329515845
1
Iteration 17600: Loss = -10979.222952437281
Iteration 17700: Loss = -10979.2229227799
Iteration 17800: Loss = -10979.229256913794
1
Iteration 17900: Loss = -10979.222927196435
Iteration 18000: Loss = -10979.222974777414
Iteration 18100: Loss = -10979.227843407922
1
Iteration 18200: Loss = -10979.222927173663
Iteration 18300: Loss = -10979.224117825952
1
Iteration 18400: Loss = -10979.222921396493
Iteration 18500: Loss = -10979.225603169174
1
Iteration 18600: Loss = -10979.222931705463
Iteration 18700: Loss = -10979.307618416524
1
Iteration 18800: Loss = -10979.222941765043
Iteration 18900: Loss = -10979.22293323173
Iteration 19000: Loss = -10979.223077803337
1
Iteration 19100: Loss = -10979.22293837746
Iteration 19200: Loss = -10979.717300042374
1
Iteration 19300: Loss = -10979.222930494372
Iteration 19400: Loss = -10979.222841234458
Iteration 19500: Loss = -10979.24401799196
1
Iteration 19600: Loss = -10979.254286237725
2
Iteration 19700: Loss = -10979.26856948883
3
Iteration 19800: Loss = -10979.227805931112
4
Iteration 19900: Loss = -10979.222911437566
pi: tensor([[0.6736, 0.3264],
        [0.2431, 0.7569]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4017, 0.5983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1986, 0.1004],
         [0.5460, 0.2447]],

        [[0.5169, 0.0907],
         [0.6680, 0.5048]],

        [[0.7283, 0.1070],
         [0.6888, 0.5137]],

        [[0.6779, 0.1087],
         [0.5016, 0.5217]],

        [[0.6594, 0.0938],
         [0.5609, 0.6548]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7720123474937528
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448275862068966
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9206289602688308
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 89
Adjusted Rand Index: 0.6044769870174135
Global Adjusted Rand Index: 0.8167231545733723
Average Adjusted Rand Index: 0.8203888314013741
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26694.174299455157
Iteration 100: Loss = -11121.464295868042
Iteration 200: Loss = -11120.589936629463
Iteration 300: Loss = -11115.048513869344
Iteration 400: Loss = -11112.60879899908
Iteration 500: Loss = -11112.008779821843
Iteration 600: Loss = -11111.75992882706
Iteration 700: Loss = -11111.615901844993
Iteration 800: Loss = -11111.528033959603
Iteration 900: Loss = -11111.458702622409
Iteration 1000: Loss = -11111.404899039231
Iteration 1100: Loss = -11111.370721807403
Iteration 1200: Loss = -11111.35061713559
Iteration 1300: Loss = -11111.337170396902
Iteration 1400: Loss = -11111.328168484873
Iteration 1500: Loss = -11111.322204941494
Iteration 1600: Loss = -11111.318563855697
Iteration 1700: Loss = -11111.316403829882
Iteration 1800: Loss = -11111.315117057015
Iteration 1900: Loss = -11111.314210577639
Iteration 2000: Loss = -11111.31354659965
Iteration 2100: Loss = -11111.313000119031
Iteration 2200: Loss = -11111.31257253505
Iteration 2300: Loss = -11111.312299338426
Iteration 2400: Loss = -11111.311998271945
Iteration 2500: Loss = -11111.31176982142
Iteration 2600: Loss = -11111.31154982723
Iteration 2700: Loss = -11111.311377568107
Iteration 2800: Loss = -11111.311162442938
Iteration 2900: Loss = -11111.31094153487
Iteration 3000: Loss = -11111.310721306265
Iteration 3100: Loss = -11111.310517281097
Iteration 3200: Loss = -11111.31020019452
Iteration 3300: Loss = -11111.309735755662
Iteration 3400: Loss = -11111.309000200594
Iteration 3500: Loss = -11111.307418961514
Iteration 3600: Loss = -11111.302577515997
Iteration 3700: Loss = -11111.27132865307
Iteration 3800: Loss = -11085.743662929692
Iteration 3900: Loss = -10980.011813267016
Iteration 4000: Loss = -10979.565685296842
Iteration 4100: Loss = -10979.50463699084
Iteration 4200: Loss = -10979.475585985569
Iteration 4300: Loss = -10979.464264956781
Iteration 4400: Loss = -10979.442081899186
Iteration 4500: Loss = -10979.423913368453
Iteration 4600: Loss = -10979.388780287703
Iteration 4700: Loss = -10979.379167772935
Iteration 4800: Loss = -10979.348324134802
Iteration 4900: Loss = -10979.320397417157
Iteration 5000: Loss = -10979.315413830513
Iteration 5100: Loss = -10979.29125038257
Iteration 5200: Loss = -10979.27754935454
Iteration 5300: Loss = -10979.27803796878
1
Iteration 5400: Loss = -10979.275127751942
Iteration 5500: Loss = -10979.2742555289
Iteration 5600: Loss = -10979.272784348004
Iteration 5700: Loss = -10979.271596832594
Iteration 5800: Loss = -10979.271345036996
Iteration 5900: Loss = -10979.275430075624
1
Iteration 6000: Loss = -10979.268945917722
Iteration 6100: Loss = -10979.269037415523
Iteration 6200: Loss = -10979.267840909453
Iteration 6300: Loss = -10979.266666655258
Iteration 6400: Loss = -10979.266013026981
Iteration 6500: Loss = -10979.26605312192
Iteration 6600: Loss = -10979.264960168526
Iteration 6700: Loss = -10979.264830146236
Iteration 6800: Loss = -10979.265387491207
1
Iteration 6900: Loss = -10979.26452318586
Iteration 7000: Loss = -10979.263419598445
Iteration 7100: Loss = -10979.26184731613
Iteration 7200: Loss = -10979.266317938704
1
Iteration 7300: Loss = -10979.261736263168
Iteration 7400: Loss = -10979.259467351885
Iteration 7500: Loss = -10979.234278347021
Iteration 7600: Loss = -10979.233907706192
Iteration 7700: Loss = -10979.23343907435
Iteration 7800: Loss = -10979.234105149812
1
Iteration 7900: Loss = -10979.238144062901
2
Iteration 8000: Loss = -10979.22969716607
Iteration 8100: Loss = -10979.235926741418
1
Iteration 8200: Loss = -10979.229673874197
Iteration 8300: Loss = -10979.259976828584
1
Iteration 8400: Loss = -10979.229593543598
Iteration 8500: Loss = -10979.229448290633
Iteration 8600: Loss = -10979.22946953863
Iteration 8700: Loss = -10979.228628986219
Iteration 8800: Loss = -10979.230556826724
1
Iteration 8900: Loss = -10979.224660529066
Iteration 9000: Loss = -10979.22461457008
Iteration 9100: Loss = -10979.224743981516
1
Iteration 9200: Loss = -10979.224604096265
Iteration 9300: Loss = -10979.224545037177
Iteration 9400: Loss = -10979.225962459994
1
Iteration 9500: Loss = -10979.224531317494
Iteration 9600: Loss = -10979.224358417125
Iteration 9700: Loss = -10979.224391888978
Iteration 9800: Loss = -10979.224269295477
Iteration 9900: Loss = -10979.32157160775
1
Iteration 10000: Loss = -10979.224197843405
Iteration 10100: Loss = -10979.224132709925
Iteration 10200: Loss = -10979.224214243693
Iteration 10300: Loss = -10979.329558806325
1
Iteration 10400: Loss = -10979.223572194724
Iteration 10500: Loss = -10979.228493958426
1
Iteration 10600: Loss = -10979.223313239925
Iteration 10700: Loss = -10979.224889453593
1
Iteration 10800: Loss = -10979.22299761382
Iteration 10900: Loss = -10979.224942356681
1
Iteration 11000: Loss = -10979.231652800074
2
Iteration 11100: Loss = -10979.306307541421
3
Iteration 11200: Loss = -10979.22296113094
Iteration 11300: Loss = -10979.228135545733
1
Iteration 11400: Loss = -10979.222966282536
Iteration 11500: Loss = -10979.229345681004
1
Iteration 11600: Loss = -10979.222947244352
Iteration 11700: Loss = -10979.222995090537
Iteration 11800: Loss = -10979.222938897767
Iteration 11900: Loss = -10979.222953090908
Iteration 12000: Loss = -10979.513144356146
1
Iteration 12100: Loss = -10979.222915463508
Iteration 12200: Loss = -10979.222912205967
Iteration 12300: Loss = -10979.283322557396
1
Iteration 12400: Loss = -10979.222879651208
Iteration 12500: Loss = -10979.222897143893
Iteration 12600: Loss = -10979.226818870116
1
Iteration 12700: Loss = -10979.222892927079
Iteration 12800: Loss = -10979.222910225979
Iteration 12900: Loss = -10979.222969071505
Iteration 13000: Loss = -10979.222889128278
Iteration 13100: Loss = -10979.235739559139
1
Iteration 13200: Loss = -10979.222899349568
Iteration 13300: Loss = -10979.223287585031
1
Iteration 13400: Loss = -10979.222986042809
Iteration 13500: Loss = -10979.32466772581
1
Iteration 13600: Loss = -10979.222910338602
Iteration 13700: Loss = -10979.259908591148
1
Iteration 13800: Loss = -10979.222879068986
Iteration 13900: Loss = -10979.22956445691
1
Iteration 14000: Loss = -10979.222902035193
Iteration 14100: Loss = -10979.230038635113
1
Iteration 14200: Loss = -10979.222919204907
Iteration 14300: Loss = -10979.222927880232
Iteration 14400: Loss = -10979.22301987618
Iteration 14500: Loss = -10979.222871843758
Iteration 14600: Loss = -10979.304791554463
1
Iteration 14700: Loss = -10979.222879900602
Iteration 14800: Loss = -10979.222900326075
Iteration 14900: Loss = -10979.22292962229
Iteration 15000: Loss = -10979.222896791058
Iteration 15100: Loss = -10979.222860067457
Iteration 15200: Loss = -10979.22407510563
1
Iteration 15300: Loss = -10979.222879381407
Iteration 15400: Loss = -10979.238746813991
1
Iteration 15500: Loss = -10979.461164870907
2
Iteration 15600: Loss = -10979.222888362365
Iteration 15700: Loss = -10979.233967851638
1
Iteration 15800: Loss = -10979.222894099721
Iteration 15900: Loss = -10979.227012787478
1
Iteration 16000: Loss = -10979.222887674785
Iteration 16100: Loss = -10979.268174719235
1
Iteration 16200: Loss = -10979.223672590584
2
Iteration 16300: Loss = -10979.222906829533
Iteration 16400: Loss = -10979.241174008286
1
Iteration 16500: Loss = -10979.22287113421
Iteration 16600: Loss = -10979.263210726365
1
Iteration 16700: Loss = -10979.222901696601
Iteration 16800: Loss = -10979.222891188349
Iteration 16900: Loss = -10979.22291558768
Iteration 17000: Loss = -10979.22330424065
1
Iteration 17100: Loss = -10979.231492187166
2
Iteration 17200: Loss = -10979.227453515849
3
Iteration 17300: Loss = -10979.228016682142
4
Iteration 17400: Loss = -10979.23510302282
5
Iteration 17500: Loss = -10979.232777488407
6
Iteration 17600: Loss = -10979.262307984525
7
Iteration 17700: Loss = -10979.227680585205
8
Iteration 17800: Loss = -10979.222938466843
Iteration 17900: Loss = -10979.22329856231
1
Iteration 18000: Loss = -10979.223488606105
2
Iteration 18100: Loss = -10979.223453861876
3
Iteration 18200: Loss = -10979.380946822297
4
Iteration 18300: Loss = -10979.222895317513
Iteration 18400: Loss = -10979.23276215734
1
Iteration 18500: Loss = -10979.222854182966
Iteration 18600: Loss = -10979.714418573452
1
Iteration 18700: Loss = -10979.222877898399
Iteration 18800: Loss = -10979.222851177577
Iteration 18900: Loss = -10979.240092114298
1
Iteration 19000: Loss = -10979.222872674485
Iteration 19100: Loss = -10979.222875171328
Iteration 19200: Loss = -10979.224064173308
1
Iteration 19300: Loss = -10979.222869284553
Iteration 19400: Loss = -10979.222877758035
Iteration 19500: Loss = -10979.222928498411
Iteration 19600: Loss = -10979.222871056918
Iteration 19700: Loss = -10979.259270517716
1
Iteration 19800: Loss = -10979.222863923009
Iteration 19900: Loss = -10979.22287178003
pi: tensor([[0.7541, 0.2459],
        [0.3257, 0.6743]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5972, 0.4028], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2455, 0.1007],
         [0.6283, 0.1981]],

        [[0.6828, 0.0909],
         [0.6228, 0.6738]],

        [[0.6550, 0.1063],
         [0.6501, 0.5333]],

        [[0.6808, 0.1090],
         [0.5528, 0.6851]],

        [[0.6673, 0.0942],
         [0.6992, 0.7307]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7720123474937528
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448275862068966
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9206289602688308
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 11
Adjusted Rand Index: 0.6044769870174135
Global Adjusted Rand Index: 0.8167231545733723
Average Adjusted Rand Index: 0.8203888314013741
11042.045445665199
[0.8167231545733723, 0.8167231545733723] [0.8203888314013741, 0.8203888314013741] [10979.230835837265, 10979.224339596194]
-------------------------------------
This iteration is 59
True Objective function: Loss = -10789.347162430035
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22318.981549235632
Iteration 100: Loss = -10843.50823770753
Iteration 200: Loss = -10842.527591771843
Iteration 300: Loss = -10842.193085337836
Iteration 400: Loss = -10842.03649746712
Iteration 500: Loss = -10841.938526236621
Iteration 600: Loss = -10841.85675658832
Iteration 700: Loss = -10841.772124257684
Iteration 800: Loss = -10841.687986873361
Iteration 900: Loss = -10841.629995231819
Iteration 1000: Loss = -10841.593898096535
Iteration 1100: Loss = -10841.566038607556
Iteration 1200: Loss = -10841.541788115039
Iteration 1300: Loss = -10841.519001081668
Iteration 1400: Loss = -10841.496069920002
Iteration 1500: Loss = -10841.47157911037
Iteration 1600: Loss = -10841.443535465349
Iteration 1700: Loss = -10841.40936996033
Iteration 1800: Loss = -10841.364984198957
Iteration 1900: Loss = -10841.304707107365
Iteration 2000: Loss = -10841.22126313683
Iteration 2100: Loss = -10841.109700193836
Iteration 2200: Loss = -10840.973346851031
Iteration 2300: Loss = -10840.823913500531
Iteration 2400: Loss = -10840.675306854117
Iteration 2500: Loss = -10840.537909303854
Iteration 2600: Loss = -10840.41716732338
Iteration 2700: Loss = -10840.31744335905
Iteration 2800: Loss = -10840.240132580535
Iteration 2900: Loss = -10840.182011087027
Iteration 3000: Loss = -10840.138036491106
Iteration 3100: Loss = -10840.104524852162
Iteration 3200: Loss = -10840.078834950968
Iteration 3300: Loss = -10840.05917943205
Iteration 3400: Loss = -10840.04376172334
Iteration 3500: Loss = -10840.031463421254
Iteration 3600: Loss = -10840.021387933728
Iteration 3700: Loss = -10840.013032246887
Iteration 3800: Loss = -10840.006099224584
Iteration 3900: Loss = -10840.00025117331
Iteration 4000: Loss = -10839.99524506951
Iteration 4100: Loss = -10839.990935809421
Iteration 4200: Loss = -10839.987201792424
Iteration 4300: Loss = -10839.983892681028
Iteration 4400: Loss = -10839.980906603725
Iteration 4500: Loss = -10839.97829598099
Iteration 4600: Loss = -10839.975951856826
Iteration 4700: Loss = -10839.973847006666
Iteration 4800: Loss = -10839.971972943536
Iteration 4900: Loss = -10839.970298773123
Iteration 5000: Loss = -10839.968775677557
Iteration 5100: Loss = -10839.967413637269
Iteration 5200: Loss = -10839.96616476085
Iteration 5300: Loss = -10839.964948073059
Iteration 5400: Loss = -10839.963942060176
Iteration 5500: Loss = -10839.962982459076
Iteration 5600: Loss = -10839.962111643114
Iteration 5700: Loss = -10839.961255613318
Iteration 5800: Loss = -10839.960526647888
Iteration 5900: Loss = -10839.959794796152
Iteration 6000: Loss = -10839.959207679136
Iteration 6100: Loss = -10839.958600477
Iteration 6200: Loss = -10839.958037484676
Iteration 6300: Loss = -10839.957541692336
Iteration 6400: Loss = -10839.960181574008
1
Iteration 6500: Loss = -10839.956624657252
Iteration 6600: Loss = -10839.956198553486
Iteration 6700: Loss = -10839.955800827807
Iteration 6800: Loss = -10839.95544565216
Iteration 6900: Loss = -10839.955099400417
Iteration 7000: Loss = -10839.954807806385
Iteration 7100: Loss = -10839.95451300559
Iteration 7200: Loss = -10839.954454830036
Iteration 7300: Loss = -10839.953974861332
Iteration 7400: Loss = -10839.953757539952
Iteration 7500: Loss = -10839.953539263326
Iteration 7600: Loss = -10839.95330609038
Iteration 7700: Loss = -10839.953215749452
Iteration 7800: Loss = -10839.9536452002
1
Iteration 7900: Loss = -10839.953518981409
2
Iteration 8000: Loss = -10839.95264203408
Iteration 8100: Loss = -10839.95247865591
Iteration 8200: Loss = -10839.955560130971
1
Iteration 8300: Loss = -10839.952120913376
Iteration 8400: Loss = -10839.951991558613
Iteration 8500: Loss = -10839.951967853147
Iteration 8600: Loss = -10839.951733037746
Iteration 8700: Loss = -10839.970699142737
1
Iteration 8800: Loss = -10839.951526104029
Iteration 8900: Loss = -10839.951437620513
Iteration 9000: Loss = -10839.956829988454
1
Iteration 9100: Loss = -10839.951226274055
Iteration 9200: Loss = -10839.95112599278
Iteration 9300: Loss = -10839.95255462365
1
Iteration 9400: Loss = -10839.95094503096
Iteration 9500: Loss = -10839.950897482866
Iteration 9600: Loss = -10839.97315917104
1
Iteration 9700: Loss = -10839.950734983435
Iteration 9800: Loss = -10839.950664185042
Iteration 9900: Loss = -10839.957846930994
1
Iteration 10000: Loss = -10839.950572277849
Iteration 10100: Loss = -10839.950479326848
Iteration 10200: Loss = -10839.967986268226
1
Iteration 10300: Loss = -10839.95039652712
Iteration 10400: Loss = -10839.950340627272
Iteration 10500: Loss = -10839.950246973925
Iteration 10600: Loss = -10839.950897766377
1
Iteration 10700: Loss = -10839.950186996737
Iteration 10800: Loss = -10839.950131381005
Iteration 10900: Loss = -10839.953324015862
1
Iteration 11000: Loss = -10839.950068127408
Iteration 11100: Loss = -10839.950026522927
Iteration 11200: Loss = -10839.95255279648
1
Iteration 11300: Loss = -10839.949983127857
Iteration 11400: Loss = -10839.949935258848
Iteration 11500: Loss = -10839.952567047862
1
Iteration 11600: Loss = -10839.94992447694
Iteration 11700: Loss = -10839.949886797269
Iteration 11800: Loss = -10840.107638139529
1
Iteration 11900: Loss = -10839.949803059591
Iteration 12000: Loss = -10839.956611240443
1
Iteration 12100: Loss = -10839.966232081917
2
Iteration 12200: Loss = -10839.949783487726
Iteration 12300: Loss = -10839.95167265955
1
Iteration 12400: Loss = -10839.9497214356
Iteration 12500: Loss = -10839.949896033706
1
Iteration 12600: Loss = -10839.949715140794
Iteration 12700: Loss = -10839.951870523002
1
Iteration 12800: Loss = -10839.949717235522
Iteration 12900: Loss = -10839.962650264786
1
Iteration 13000: Loss = -10839.94966302027
Iteration 13100: Loss = -10839.966477820504
1
Iteration 13200: Loss = -10839.94962336735
Iteration 13300: Loss = -10840.033884083661
1
Iteration 13400: Loss = -10839.949597579405
Iteration 13500: Loss = -10839.995253564226
1
Iteration 13600: Loss = -10839.954838059317
2
Iteration 13700: Loss = -10839.94960276708
Iteration 13800: Loss = -10839.94987867468
1
Iteration 13900: Loss = -10840.078424983041
2
Iteration 14000: Loss = -10839.949597646046
Iteration 14100: Loss = -10839.999980474247
1
Iteration 14200: Loss = -10839.961099136874
2
Iteration 14300: Loss = -10839.949720671848
3
Iteration 14400: Loss = -10839.96277505561
4
Iteration 14500: Loss = -10839.949519843767
Iteration 14600: Loss = -10839.949914804656
1
Iteration 14700: Loss = -10839.949681402191
2
Iteration 14800: Loss = -10839.94965249432
3
Iteration 14900: Loss = -10840.032963589578
4
Iteration 15000: Loss = -10839.949485242609
Iteration 15100: Loss = -10839.95210161996
1
Iteration 15200: Loss = -10839.949570389936
Iteration 15300: Loss = -10839.949730686802
1
Iteration 15400: Loss = -10839.955492061816
2
Iteration 15500: Loss = -10839.94941597553
Iteration 15600: Loss = -10839.95251656695
1
Iteration 15700: Loss = -10839.949487628171
Iteration 15800: Loss = -10839.951868281132
1
Iteration 15900: Loss = -10839.94947398587
Iteration 16000: Loss = -10840.013533184161
1
Iteration 16100: Loss = -10839.950100415715
2
Iteration 16200: Loss = -10839.949823201085
3
Iteration 16300: Loss = -10839.952506178892
4
Iteration 16400: Loss = -10839.949429419077
Iteration 16500: Loss = -10839.949471287977
Iteration 16600: Loss = -10839.952146714255
1
Iteration 16700: Loss = -10839.94943247061
Iteration 16800: Loss = -10839.949477008346
Iteration 16900: Loss = -10839.949688557257
1
Iteration 17000: Loss = -10839.94993573132
2
Iteration 17100: Loss = -10839.949557211541
Iteration 17200: Loss = -10839.951916111604
1
Iteration 17300: Loss = -10839.951375532242
2
Iteration 17400: Loss = -10839.950139856792
3
Iteration 17500: Loss = -10839.956842458605
4
Iteration 17600: Loss = -10839.949396601292
Iteration 17700: Loss = -10839.949570467908
1
Iteration 17800: Loss = -10839.952430484389
2
Iteration 17900: Loss = -10839.975196395919
3
Iteration 18000: Loss = -10839.96720533165
4
Iteration 18100: Loss = -10839.94950883102
5
Iteration 18200: Loss = -10839.949874954356
6
Iteration 18300: Loss = -10839.950210727875
7
Iteration 18400: Loss = -10839.949573740038
8
Iteration 18500: Loss = -10839.951959722253
9
Iteration 18600: Loss = -10839.949398381917
Iteration 18700: Loss = -10839.949764293639
1
Iteration 18800: Loss = -10839.949425272192
Iteration 18900: Loss = -10839.949424813007
Iteration 19000: Loss = -10839.953245351182
1
Iteration 19100: Loss = -10839.949438822929
Iteration 19200: Loss = -10839.94968964248
1
Iteration 19300: Loss = -10839.950432831869
2
Iteration 19400: Loss = -10839.952303863414
3
Iteration 19500: Loss = -10839.949387803626
Iteration 19600: Loss = -10839.986171656468
1
Iteration 19700: Loss = -10839.94939732938
Iteration 19800: Loss = -10839.950553738903
1
Iteration 19900: Loss = -10839.949371143482
pi: tensor([[9.8264e-01, 1.7359e-02],
        [1.8320e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9891, 0.0109], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1566, 0.2721],
         [0.5106, 0.1973]],

        [[0.6494, 0.2035],
         [0.5721, 0.6363]],

        [[0.6172, 0.1389],
         [0.5720, 0.5094]],

        [[0.5094, 0.1738],
         [0.5322, 0.5768]],

        [[0.7220, 0.1956],
         [0.5154, 0.6986]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: 4.7527069434922995e-05
Average Adjusted Rand Index: -0.000274284788402875
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21642.357950723475
Iteration 100: Loss = -10843.404010040933
Iteration 200: Loss = -10842.438051825979
Iteration 300: Loss = -10842.147549522859
Iteration 400: Loss = -10842.01249762626
Iteration 500: Loss = -10841.913974971925
Iteration 600: Loss = -10841.811024080296
Iteration 700: Loss = -10841.712749975119
Iteration 800: Loss = -10841.675634845646
Iteration 900: Loss = -10841.657445540428
Iteration 1000: Loss = -10841.643193885206
Iteration 1100: Loss = -10841.6307476037
Iteration 1200: Loss = -10841.619217599746
Iteration 1300: Loss = -10841.60797877048
Iteration 1400: Loss = -10841.596522097007
Iteration 1500: Loss = -10841.584284495133
Iteration 1600: Loss = -10841.57094541211
Iteration 1700: Loss = -10841.555678311468
Iteration 1800: Loss = -10841.537901242877
Iteration 1900: Loss = -10841.516458142994
Iteration 2000: Loss = -10841.490108042577
Iteration 2100: Loss = -10841.456883141842
Iteration 2200: Loss = -10841.414316823755
Iteration 2300: Loss = -10841.360290687675
Iteration 2400: Loss = -10841.297816779232
Iteration 2500: Loss = -10841.2359777051
Iteration 2600: Loss = -10841.180678889536
Iteration 2700: Loss = -10841.132726801827
Iteration 2800: Loss = -10841.092571041389
Iteration 2900: Loss = -10841.060667068336
Iteration 3000: Loss = -10841.03593125337
Iteration 3100: Loss = -10841.018047854917
Iteration 3200: Loss = -10841.005438764014
Iteration 3300: Loss = -10840.9966903206
Iteration 3400: Loss = -10840.990596941627
Iteration 3500: Loss = -10840.98645436297
Iteration 3600: Loss = -10840.983637056683
Iteration 3700: Loss = -10840.981627600358
Iteration 3800: Loss = -10840.980228535149
Iteration 3900: Loss = -10840.994410939722
1
Iteration 4000: Loss = -10840.978589597715
Iteration 4100: Loss = -10840.978217899861
Iteration 4200: Loss = -10840.977798235583
Iteration 4300: Loss = -10840.97743930619
Iteration 4400: Loss = -10840.977907858432
1
Iteration 4500: Loss = -10840.977125201644
Iteration 4600: Loss = -10840.979465001383
1
Iteration 4700: Loss = -10840.97685725687
Iteration 4800: Loss = -10840.976738584806
Iteration 4900: Loss = -10840.976666267597
Iteration 5000: Loss = -10840.97656989841
Iteration 5100: Loss = -10840.979330074966
1
Iteration 5200: Loss = -10840.976433240909
Iteration 5300: Loss = -10840.976341475056
Iteration 5400: Loss = -10840.976930076356
1
Iteration 5500: Loss = -10840.976161479079
Iteration 5600: Loss = -10840.976114007919
Iteration 5700: Loss = -10840.97636340995
1
Iteration 5800: Loss = -10840.97594667711
Iteration 5900: Loss = -10840.976583017618
1
Iteration 6000: Loss = -10840.975799853833
Iteration 6100: Loss = -10840.975718789636
Iteration 6200: Loss = -10840.975775900464
Iteration 6300: Loss = -10840.975582236657
Iteration 6400: Loss = -10840.981801340187
1
Iteration 6500: Loss = -10840.975448148887
Iteration 6600: Loss = -10840.975333157357
Iteration 6700: Loss = -10840.975865744664
1
Iteration 6800: Loss = -10840.97519173654
Iteration 6900: Loss = -10841.001361253468
1
Iteration 7000: Loss = -10840.975055358473
Iteration 7100: Loss = -10840.975004215714
Iteration 7200: Loss = -10840.975100992178
Iteration 7300: Loss = -10840.974831070742
Iteration 7400: Loss = -10840.975446765384
1
Iteration 7500: Loss = -10840.974739228932
Iteration 7600: Loss = -10840.974642857233
Iteration 7700: Loss = -10840.974536980175
Iteration 7800: Loss = -10840.97450341925
Iteration 7900: Loss = -10840.974473655815
Iteration 8000: Loss = -10840.974371409193
Iteration 8100: Loss = -10840.97439566466
Iteration 8200: Loss = -10840.990761159166
1
Iteration 8300: Loss = -10840.976787987145
2
Iteration 8400: Loss = -10840.97420185494
Iteration 8500: Loss = -10840.976712504013
1
Iteration 8600: Loss = -10840.974314273752
2
Iteration 8700: Loss = -10840.974240548707
Iteration 8800: Loss = -10840.988951971683
1
Iteration 8900: Loss = -10840.973975283894
Iteration 9000: Loss = -10840.973917588804
Iteration 9100: Loss = -10840.977513858155
1
Iteration 9200: Loss = -10840.976792651389
2
Iteration 9300: Loss = -10840.998310606843
3
Iteration 9400: Loss = -10840.978430443274
4
Iteration 9500: Loss = -10840.977166675597
5
Iteration 9600: Loss = -10841.15476038734
6
Iteration 9700: Loss = -10840.97731149558
7
Iteration 9800: Loss = -10840.974081191445
8
Iteration 9900: Loss = -10840.977975387164
9
Iteration 10000: Loss = -10840.977483159211
10
Iteration 10100: Loss = -10840.979062517436
11
Iteration 10200: Loss = -10840.974601552758
12
Iteration 10300: Loss = -10840.991978907963
13
Iteration 10400: Loss = -10840.973538671922
Iteration 10500: Loss = -10840.973599192746
Iteration 10600: Loss = -10840.977225139963
1
Iteration 10700: Loss = -10840.97418661087
2
Iteration 10800: Loss = -10840.9803782869
3
Iteration 10900: Loss = -10840.973808101644
4
Iteration 11000: Loss = -10840.987506533567
5
Iteration 11100: Loss = -10840.988033725058
6
Iteration 11200: Loss = -10840.975103649122
7
Iteration 11300: Loss = -10840.974171011672
8
Iteration 11400: Loss = -10840.97976132289
9
Iteration 11500: Loss = -10840.985707925389
10
Iteration 11600: Loss = -10841.02787334589
11
Iteration 11700: Loss = -10840.980304490815
12
Iteration 11800: Loss = -10840.973208053301
Iteration 11900: Loss = -10841.053754857854
1
Iteration 12000: Loss = -10840.974112180013
2
Iteration 12100: Loss = -10840.98957965107
3
Iteration 12200: Loss = -10840.973507508435
4
Iteration 12300: Loss = -10840.973263533007
Iteration 12400: Loss = -10841.002367231062
1
Iteration 12500: Loss = -10840.979308908098
2
Iteration 12600: Loss = -10840.994012602516
3
Iteration 12700: Loss = -10840.99322075935
4
Iteration 12800: Loss = -10840.992109394414
5
Iteration 12900: Loss = -10840.976147374293
6
Iteration 13000: Loss = -10840.982650407006
7
Iteration 13100: Loss = -10840.974160264192
8
Iteration 13200: Loss = -10840.973701301693
9
Iteration 13300: Loss = -10840.979868821256
10
Iteration 13400: Loss = -10840.97908448875
11
Iteration 13500: Loss = -10840.975628572858
12
Iteration 13600: Loss = -10840.983377616158
13
Iteration 13700: Loss = -10841.022191462851
14
Iteration 13800: Loss = -10840.976709964993
15
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[9.0657e-01, 9.3429e-02],
        [9.9921e-01, 7.9139e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5767, 0.4233], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1571, 0.1641],
         [0.6258, 0.1722]],

        [[0.5572, 0.1544],
         [0.6549, 0.6412]],

        [[0.6985, 0.1536],
         [0.6698, 0.5720]],

        [[0.6378, 0.1585],
         [0.6460, 0.6400]],

        [[0.6567, 0.1832],
         [0.5647, 0.6512]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.006738692547152536
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0009711150057751206
Average Adjusted Rand Index: -0.0013477385094305071
10789.347162430035
[4.7527069434922995e-05, 0.0009711150057751206] [-0.000274284788402875, -0.0013477385094305071] [10839.95048255245, 10840.976709964993]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11062.587622567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19360.062494769405
Iteration 100: Loss = -11166.287724235333
Iteration 200: Loss = -11165.883594884499
Iteration 300: Loss = -11165.672326461987
Iteration 400: Loss = -11164.685080665076
Iteration 500: Loss = -11162.490016631504
Iteration 600: Loss = -11161.314050976313
Iteration 700: Loss = -11160.219737692092
Iteration 800: Loss = -11159.917722534527
Iteration 900: Loss = -11159.797429531849
Iteration 1000: Loss = -11159.74007936196
Iteration 1100: Loss = -11159.722478907597
Iteration 1200: Loss = -11159.714723708094
Iteration 1300: Loss = -11159.71009987839
Iteration 1400: Loss = -11159.707090954069
Iteration 1500: Loss = -11159.70496848451
Iteration 1600: Loss = -11159.703395255361
Iteration 1700: Loss = -11159.702151014171
Iteration 1800: Loss = -11159.701132071255
Iteration 1900: Loss = -11159.700273871933
Iteration 2000: Loss = -11159.699494784547
Iteration 2100: Loss = -11159.698789432834
Iteration 2200: Loss = -11159.698166687254
Iteration 2300: Loss = -11159.69755700321
Iteration 2400: Loss = -11159.696991481722
Iteration 2500: Loss = -11159.696420615024
Iteration 2600: Loss = -11159.69586455604
Iteration 2700: Loss = -11159.69537543961
Iteration 2800: Loss = -11159.694926772472
Iteration 2900: Loss = -11159.69441157075
Iteration 3000: Loss = -11159.694007694221
Iteration 3100: Loss = -11159.69354524624
Iteration 3200: Loss = -11159.693134975996
Iteration 3300: Loss = -11159.692746297462
Iteration 3400: Loss = -11159.69238756716
Iteration 3500: Loss = -11159.692038421768
Iteration 3600: Loss = -11159.691720447077
Iteration 3700: Loss = -11159.697951914339
1
Iteration 3800: Loss = -11159.69108494231
Iteration 3900: Loss = -11159.690810332557
Iteration 4000: Loss = -11159.697797337583
1
Iteration 4100: Loss = -11159.690288696627
Iteration 4200: Loss = -11159.690052155192
Iteration 4300: Loss = -11159.689808651892
Iteration 4400: Loss = -11159.689586918597
Iteration 4500: Loss = -11159.689378778272
Iteration 4600: Loss = -11159.68916376179
Iteration 4700: Loss = -11159.689053305612
Iteration 4800: Loss = -11159.688829984658
Iteration 4900: Loss = -11159.688662546096
Iteration 5000: Loss = -11159.688509076217
Iteration 5100: Loss = -11159.688307760645
Iteration 5200: Loss = -11159.694045360287
1
Iteration 5300: Loss = -11159.688040573428
Iteration 5400: Loss = -11159.68794485505
Iteration 5500: Loss = -11159.687853886802
Iteration 5600: Loss = -11159.687704280339
Iteration 5700: Loss = -11159.707842088437
1
Iteration 5800: Loss = -11159.687501268752
Iteration 5900: Loss = -11159.687391861553
Iteration 6000: Loss = -11159.687312296383
Iteration 6100: Loss = -11159.687245076859
Iteration 6200: Loss = -11159.69237331845
1
Iteration 6300: Loss = -11159.687044782822
Iteration 6400: Loss = -11159.687174562214
1
Iteration 6500: Loss = -11159.686926104594
Iteration 6600: Loss = -11159.686859441092
Iteration 6700: Loss = -11159.686923569658
Iteration 6800: Loss = -11159.686721598986
Iteration 6900: Loss = -11159.688405163122
1
Iteration 7000: Loss = -11159.68660716956
Iteration 7100: Loss = -11159.700260382731
1
Iteration 7200: Loss = -11159.686503914561
Iteration 7300: Loss = -11159.687090496529
1
Iteration 7400: Loss = -11159.68644431877
Iteration 7500: Loss = -11159.686412506779
Iteration 7600: Loss = -11159.686387174977
Iteration 7700: Loss = -11159.69954637941
1
Iteration 7800: Loss = -11159.68633655315
Iteration 7900: Loss = -11159.686258359725
Iteration 8000: Loss = -11159.740537564528
1
Iteration 8100: Loss = -11159.686201813343
Iteration 8200: Loss = -11159.751096527662
1
Iteration 8300: Loss = -11159.686170350615
Iteration 8400: Loss = -11159.689538451046
1
Iteration 8500: Loss = -11159.686127127447
Iteration 8600: Loss = -11159.686043542213
Iteration 8700: Loss = -11159.701917750885
1
Iteration 8800: Loss = -11159.686026936657
Iteration 8900: Loss = -11159.685974223625
Iteration 9000: Loss = -11159.702101350991
1
Iteration 9100: Loss = -11159.686008690076
Iteration 9200: Loss = -11159.685970181476
Iteration 9300: Loss = -11159.791300774012
1
Iteration 9400: Loss = -11159.685930921942
Iteration 9500: Loss = -11159.685959168624
Iteration 9600: Loss = -11159.709929009325
1
Iteration 9700: Loss = -11159.685930615835
Iteration 9800: Loss = -11159.685899574186
Iteration 9900: Loss = -11159.685949200644
Iteration 10000: Loss = -11159.68589249866
Iteration 10100: Loss = -11159.693289582827
1
Iteration 10200: Loss = -11159.695665485851
2
Iteration 10300: Loss = -11159.68608164445
3
Iteration 10400: Loss = -11159.690992515
4
Iteration 10500: Loss = -11159.690662512436
5
Iteration 10600: Loss = -11159.688414815693
6
Iteration 10700: Loss = -11159.707769333492
7
Iteration 10800: Loss = -11159.698279830278
8
Iteration 10900: Loss = -11159.689102556837
9
Iteration 11000: Loss = -11159.685955622608
Iteration 11100: Loss = -11159.697099426374
1
Iteration 11200: Loss = -11159.720705233107
2
Iteration 11300: Loss = -11159.68585154879
Iteration 11400: Loss = -11159.685928693709
Iteration 11500: Loss = -11159.688899135277
1
Iteration 11600: Loss = -11159.685781867829
Iteration 11700: Loss = -11159.687244064671
1
Iteration 11800: Loss = -11159.686432517588
2
Iteration 11900: Loss = -11159.685786736498
Iteration 12000: Loss = -11159.68572943541
Iteration 12100: Loss = -11159.686314846378
1
Iteration 12200: Loss = -11159.68572537163
Iteration 12300: Loss = -11159.690378718311
1
Iteration 12400: Loss = -11159.822337106687
2
Iteration 12500: Loss = -11159.686288254652
3
Iteration 12600: Loss = -11159.686204761983
4
Iteration 12700: Loss = -11159.700036574446
5
Iteration 12800: Loss = -11159.685710596526
Iteration 12900: Loss = -11159.687460044763
1
Iteration 13000: Loss = -11159.688226216298
2
Iteration 13100: Loss = -11159.697436649074
3
Iteration 13200: Loss = -11159.686286589029
4
Iteration 13300: Loss = -11159.686091701678
5
Iteration 13400: Loss = -11159.69060266459
6
Iteration 13500: Loss = -11159.696786024018
7
Iteration 13600: Loss = -11159.685730655285
Iteration 13700: Loss = -11159.703680361186
1
Iteration 13800: Loss = -11159.685665773337
Iteration 13900: Loss = -11159.687800383135
1
Iteration 14000: Loss = -11159.693773576008
2
Iteration 14100: Loss = -11159.689215983424
3
Iteration 14200: Loss = -11159.700203636585
4
Iteration 14300: Loss = -11159.685890944043
5
Iteration 14400: Loss = -11159.685817873464
6
Iteration 14500: Loss = -11159.686336298082
7
Iteration 14600: Loss = -11159.685882282405
8
Iteration 14700: Loss = -11159.686829119104
9
Iteration 14800: Loss = -11159.685999045618
10
Iteration 14900: Loss = -11159.686986941908
11
Iteration 15000: Loss = -11159.685800833286
12
Iteration 15100: Loss = -11159.6857223306
Iteration 15200: Loss = -11159.685798947206
Iteration 15300: Loss = -11159.685704412226
Iteration 15400: Loss = -11159.68571491487
Iteration 15500: Loss = -11159.685836028386
1
Iteration 15600: Loss = -11159.685650349174
Iteration 15700: Loss = -11159.6862668876
1
Iteration 15800: Loss = -11159.686108540698
2
Iteration 15900: Loss = -11159.68572669242
Iteration 16000: Loss = -11159.69435715839
1
Iteration 16100: Loss = -11159.685672266314
Iteration 16200: Loss = -11159.687433801175
1
Iteration 16300: Loss = -11159.687030553832
2
Iteration 16400: Loss = -11159.687540032617
3
Iteration 16500: Loss = -11159.685904506016
4
Iteration 16600: Loss = -11159.753438825228
5
Iteration 16700: Loss = -11159.685703770781
Iteration 16800: Loss = -11159.686268118247
1
Iteration 16900: Loss = -11159.702131681379
2
Iteration 17000: Loss = -11159.685931145521
3
Iteration 17100: Loss = -11159.685971691488
4
Iteration 17200: Loss = -11159.686693871174
5
Iteration 17300: Loss = -11159.685669692484
Iteration 17400: Loss = -11159.69230123621
1
Iteration 17500: Loss = -11159.685663956467
Iteration 17600: Loss = -11159.686141337621
1
Iteration 17700: Loss = -11159.687867573091
2
Iteration 17800: Loss = -11159.686209812244
3
Iteration 17900: Loss = -11159.691168787585
4
Iteration 18000: Loss = -11159.686291878716
5
Iteration 18100: Loss = -11159.767333282709
6
Iteration 18200: Loss = -11159.685655393403
Iteration 18300: Loss = -11159.685997592784
1
Iteration 18400: Loss = -11159.685791807691
2
Iteration 18500: Loss = -11159.685713377095
Iteration 18600: Loss = -11159.785401294987
1
Iteration 18700: Loss = -11159.685701941225
Iteration 18800: Loss = -11159.727032924324
1
Iteration 18900: Loss = -11159.685722690581
Iteration 19000: Loss = -11159.693693453773
1
Iteration 19100: Loss = -11159.887217578978
2
Iteration 19200: Loss = -11159.685963742812
3
Iteration 19300: Loss = -11159.685762324616
Iteration 19400: Loss = -11159.685841386863
Iteration 19500: Loss = -11159.685827499372
Iteration 19600: Loss = -11159.685868553994
Iteration 19700: Loss = -11159.68616202323
1
Iteration 19800: Loss = -11159.686070499693
2
Iteration 19900: Loss = -11159.685767595778
pi: tensor([[2.0466e-06, 1.0000e+00],
        [8.5342e-02, 9.1466e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0181, 0.9819], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2165, 0.1030],
         [0.6761, 0.1637]],

        [[0.6804, 0.1433],
         [0.5137, 0.6481]],

        [[0.5790, 0.2355],
         [0.5577, 0.6791]],

        [[0.6724, 0.2201],
         [0.6692, 0.6590]],

        [[0.5530, 0.1794],
         [0.6563, 0.7100]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.012298440577296121
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0022067846219673715
Average Adjusted Rand Index: -0.0031087715341019196
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24760.473985328783
Iteration 100: Loss = -11165.816226774028
Iteration 200: Loss = -11164.540503807346
Iteration 300: Loss = -11163.708667290035
Iteration 400: Loss = -11163.093007366098
Iteration 500: Loss = -11162.547604018348
Iteration 600: Loss = -11161.879916393775
Iteration 700: Loss = -11161.158785955657
Iteration 800: Loss = -11160.567865409512
Iteration 900: Loss = -11160.046156816103
Iteration 1000: Loss = -11159.610592595553
Iteration 1100: Loss = -11159.38552383391
Iteration 1200: Loss = -11159.26071692808
Iteration 1300: Loss = -11159.180685946088
Iteration 1400: Loss = -11159.121518816213
Iteration 1500: Loss = -11159.078549537937
Iteration 1600: Loss = -11159.048675303964
Iteration 1700: Loss = -11159.02941586785
Iteration 1800: Loss = -11159.014133619728
Iteration 1900: Loss = -11158.999969773664
Iteration 2000: Loss = -11158.986224193139
Iteration 2100: Loss = -11158.972544012007
Iteration 2200: Loss = -11158.958742165703
Iteration 2300: Loss = -11158.944292538272
Iteration 2400: Loss = -11158.928342073854
Iteration 2500: Loss = -11158.90939490216
Iteration 2600: Loss = -11158.880878921587
Iteration 2700: Loss = -11158.830951342123
Iteration 2800: Loss = -11158.722671669731
Iteration 2900: Loss = -11158.499328925831
Iteration 3000: Loss = -11158.295248363534
Iteration 3100: Loss = -11158.241660329466
Iteration 3200: Loss = -11158.227568084723
Iteration 3300: Loss = -11158.215171052549
Iteration 3400: Loss = -11158.19740248821
Iteration 3500: Loss = -11158.173984747018
Iteration 3600: Loss = -11158.04631565045
Iteration 3700: Loss = -11027.903583655405
Iteration 3800: Loss = -11027.50336542904
Iteration 3900: Loss = -11027.34636256452
Iteration 4000: Loss = -11027.294769949054
Iteration 4100: Loss = -11027.262711831338
Iteration 4200: Loss = -11027.271185720201
1
Iteration 4300: Loss = -11027.239100894873
Iteration 4400: Loss = -11027.233909888777
Iteration 4500: Loss = -11027.218148300075
Iteration 4600: Loss = -11027.21026126535
Iteration 4700: Loss = -11027.20795732088
Iteration 4800: Loss = -11027.208606231714
1
Iteration 4900: Loss = -11027.20376156551
Iteration 5000: Loss = -11027.189682916085
Iteration 5100: Loss = -11027.18891249341
Iteration 5200: Loss = -11027.187581681603
Iteration 5300: Loss = -11027.186737990847
Iteration 5400: Loss = -11027.185479639935
Iteration 5500: Loss = -11027.164565616453
Iteration 5600: Loss = -11027.163912164142
Iteration 5700: Loss = -11027.162681903088
Iteration 5800: Loss = -11027.161785448732
Iteration 5900: Loss = -11027.16551907816
1
Iteration 6000: Loss = -11027.17421196995
2
Iteration 6100: Loss = -11027.159919983482
Iteration 6200: Loss = -11027.15680395142
Iteration 6300: Loss = -11027.159571682507
1
Iteration 6400: Loss = -11027.156216340618
Iteration 6500: Loss = -11027.156507580155
1
Iteration 6600: Loss = -11027.155661616487
Iteration 6700: Loss = -11027.155765446447
1
Iteration 6800: Loss = -11027.150383673124
Iteration 6900: Loss = -11027.150442746632
Iteration 7000: Loss = -11027.149183100284
Iteration 7100: Loss = -11027.148853903433
Iteration 7200: Loss = -11027.148195753049
Iteration 7300: Loss = -11027.147332487197
Iteration 7400: Loss = -11027.147249106447
Iteration 7500: Loss = -11027.146955320772
Iteration 7600: Loss = -11027.14679098442
Iteration 7700: Loss = -11027.146293758575
Iteration 7800: Loss = -11027.14563979912
Iteration 7900: Loss = -11027.14279558096
Iteration 8000: Loss = -11027.165002733493
1
Iteration 8100: Loss = -11027.136021442004
Iteration 8200: Loss = -11027.135981503421
Iteration 8300: Loss = -11027.335570382218
1
Iteration 8400: Loss = -11027.135877617271
Iteration 8500: Loss = -11027.135833815164
Iteration 8600: Loss = -11027.136149602671
1
Iteration 8700: Loss = -11027.135803948684
Iteration 8800: Loss = -11027.135776460145
Iteration 8900: Loss = -11027.135923387228
1
Iteration 9000: Loss = -11027.135762784237
Iteration 9100: Loss = -11027.140567399962
1
Iteration 9200: Loss = -11027.13574596471
Iteration 9300: Loss = -11027.135922395835
1
Iteration 9400: Loss = -11027.140489888883
2
Iteration 9500: Loss = -11027.135850583487
3
Iteration 9600: Loss = -11027.138895934262
4
Iteration 9700: Loss = -11027.135678036586
Iteration 9800: Loss = -11027.137837267615
1
Iteration 9900: Loss = -11027.144073968926
2
Iteration 10000: Loss = -11027.13715416199
3
Iteration 10100: Loss = -11027.144842127844
4
Iteration 10200: Loss = -11027.135598005734
Iteration 10300: Loss = -11027.138709131024
1
Iteration 10400: Loss = -11027.13558397004
Iteration 10500: Loss = -11027.145587665347
1
Iteration 10600: Loss = -11027.141896286796
2
Iteration 10700: Loss = -11027.135452010672
Iteration 10800: Loss = -11027.164688145178
1
Iteration 10900: Loss = -11027.135356622535
Iteration 11000: Loss = -11027.137017267414
1
Iteration 11100: Loss = -11027.135351615221
Iteration 11200: Loss = -11027.135846224708
1
Iteration 11300: Loss = -11027.135306070873
Iteration 11400: Loss = -11027.13535318725
Iteration 11500: Loss = -11027.135579947793
1
Iteration 11600: Loss = -11027.135316436303
Iteration 11700: Loss = -11027.137101757413
1
Iteration 11800: Loss = -11027.135334382949
Iteration 11900: Loss = -11027.13923594456
1
Iteration 12000: Loss = -11027.145765087744
2
Iteration 12100: Loss = -11027.135361387342
Iteration 12200: Loss = -11027.135985758758
1
Iteration 12300: Loss = -11027.14170150442
2
Iteration 12400: Loss = -11027.288793796779
3
Iteration 12500: Loss = -11027.135581569908
4
Iteration 12600: Loss = -11027.136989610393
5
Iteration 12700: Loss = -11027.147057696855
6
Iteration 12800: Loss = -11027.148578590524
7
Iteration 12900: Loss = -11027.135318545954
Iteration 13000: Loss = -11027.136105591846
1
Iteration 13100: Loss = -11027.135294985595
Iteration 13200: Loss = -11027.13610769451
1
Iteration 13300: Loss = -11027.135356047538
Iteration 13400: Loss = -11027.13534906591
Iteration 13500: Loss = -11027.205546660207
1
Iteration 13600: Loss = -11027.146333792025
2
Iteration 13700: Loss = -11027.135445810707
Iteration 13800: Loss = -11027.135399369856
Iteration 13900: Loss = -11027.155567001346
1
Iteration 14000: Loss = -11027.135299887279
Iteration 14100: Loss = -11027.137745316408
1
Iteration 14200: Loss = -11027.13531298945
Iteration 14300: Loss = -11027.135347940193
Iteration 14400: Loss = -11027.135855630992
1
Iteration 14500: Loss = -11027.136509348591
2
Iteration 14600: Loss = -11027.173069960732
3
Iteration 14700: Loss = -11027.136560476723
4
Iteration 14800: Loss = -11027.13587400464
5
Iteration 14900: Loss = -11027.138435740097
6
Iteration 15000: Loss = -11027.138505446417
7
Iteration 15100: Loss = -11027.138607839559
8
Iteration 15200: Loss = -11027.141147955937
9
Iteration 15300: Loss = -11027.135436634882
Iteration 15400: Loss = -11027.13550109044
Iteration 15500: Loss = -11027.137897055924
1
Iteration 15600: Loss = -11027.13531274456
Iteration 15700: Loss = -11027.135301241518
Iteration 15800: Loss = -11027.135268145616
Iteration 15900: Loss = -11027.136119275681
1
Iteration 16000: Loss = -11027.135294803888
Iteration 16100: Loss = -11027.13680311852
1
Iteration 16200: Loss = -11027.136516729752
2
Iteration 16300: Loss = -11027.13529736339
Iteration 16400: Loss = -11027.136544844845
1
Iteration 16500: Loss = -11027.13528399891
Iteration 16600: Loss = -11027.155330627014
1
Iteration 16700: Loss = -11027.135285293994
Iteration 16800: Loss = -11027.141586207414
1
Iteration 16900: Loss = -11027.135297324952
Iteration 17000: Loss = -11027.143633504049
1
Iteration 17100: Loss = -11027.14789630007
2
Iteration 17200: Loss = -11027.136908152348
3
Iteration 17300: Loss = -11027.146809022082
4
Iteration 17400: Loss = -11027.139266449887
5
Iteration 17500: Loss = -11027.13533921366
Iteration 17600: Loss = -11027.151690110131
1
Iteration 17700: Loss = -11027.13528771529
Iteration 17800: Loss = -11027.139525607872
1
Iteration 17900: Loss = -11027.135256081881
Iteration 18000: Loss = -11027.214091136359
1
Iteration 18100: Loss = -11027.13526810341
Iteration 18200: Loss = -11027.135290571441
Iteration 18300: Loss = -11027.13541222008
1
Iteration 18400: Loss = -11027.135293979089
Iteration 18500: Loss = -11027.360040636266
1
Iteration 18600: Loss = -11027.135297465353
Iteration 18700: Loss = -11027.169937886056
1
Iteration 18800: Loss = -11027.135387498454
Iteration 18900: Loss = -11027.14908804539
1
Iteration 19000: Loss = -11027.135348144026
Iteration 19100: Loss = -11027.146787424212
1
Iteration 19200: Loss = -11027.149145089996
2
Iteration 19300: Loss = -11027.162940053386
3
Iteration 19400: Loss = -11027.135292262956
Iteration 19500: Loss = -11027.135467284641
1
Iteration 19600: Loss = -11027.13533730059
Iteration 19700: Loss = -11027.138782563907
1
Iteration 19800: Loss = -11027.13910869233
2
Iteration 19900: Loss = -11027.138599526506
3
pi: tensor([[0.7232, 0.2768],
        [0.2411, 0.7589]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4151, 0.5849], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2072, 0.0920],
         [0.6598, 0.2474]],

        [[0.6427, 0.0919],
         [0.6042, 0.6833]],

        [[0.5035, 0.1105],
         [0.7174, 0.6018]],

        [[0.5836, 0.1143],
         [0.5602, 0.5678]],

        [[0.5110, 0.1029],
         [0.5264, 0.6498]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 2
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.8984964594640596
Average Adjusted Rand Index: 0.8993873004369901
11062.587622567185
[-0.0022067846219673715, 0.8984964594640596] [-0.0031087715341019196, 0.8993873004369901] [11159.690416730893, 11027.152425274142]
-------------------------------------
This iteration is 61
True Objective function: Loss = -10757.289524718703
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24019.659800161473
Iteration 100: Loss = -10887.219391038854
Iteration 200: Loss = -10886.713372013452
Iteration 300: Loss = -10885.256337357729
Iteration 400: Loss = -10883.84525527963
Iteration 500: Loss = -10883.647445685807
Iteration 600: Loss = -10883.606207719798
Iteration 700: Loss = -10883.580198746611
Iteration 800: Loss = -10883.550638738428
Iteration 900: Loss = -10883.506012627464
Iteration 1000: Loss = -10883.425669173206
Iteration 1100: Loss = -10883.300055283049
Iteration 1200: Loss = -10883.213990040049
Iteration 1300: Loss = -10883.182859000495
Iteration 1400: Loss = -10883.172096015867
Iteration 1500: Loss = -10883.16732065264
Iteration 1600: Loss = -10883.164799040389
Iteration 1700: Loss = -10883.163309943084
Iteration 1800: Loss = -10883.162411923391
Iteration 1900: Loss = -10883.16181944803
Iteration 2000: Loss = -10883.161340878509
Iteration 2100: Loss = -10883.161010997792
Iteration 2200: Loss = -10883.160712275578
Iteration 2300: Loss = -10883.160455726858
Iteration 2400: Loss = -10883.160204915801
Iteration 2500: Loss = -10883.160002755356
Iteration 2600: Loss = -10883.159878944742
Iteration 2700: Loss = -10883.15969532873
Iteration 2800: Loss = -10883.159532310838
Iteration 2900: Loss = -10883.159456286081
Iteration 3000: Loss = -10883.159338040059
Iteration 3100: Loss = -10883.159233468774
Iteration 3200: Loss = -10883.15917074006
Iteration 3300: Loss = -10883.159089559857
Iteration 3400: Loss = -10883.159012275586
Iteration 3500: Loss = -10883.15894563618
Iteration 3600: Loss = -10883.158913529704
Iteration 3700: Loss = -10883.158954345952
Iteration 3800: Loss = -10883.158821011364
Iteration 3900: Loss = -10883.158776936889
Iteration 4000: Loss = -10883.158739999755
Iteration 4100: Loss = -10883.158718859804
Iteration 4200: Loss = -10883.158692368059
Iteration 4300: Loss = -10883.15869169133
Iteration 4400: Loss = -10883.158637493203
Iteration 4500: Loss = -10883.15865795865
Iteration 4600: Loss = -10883.158621934219
Iteration 4700: Loss = -10883.158569571673
Iteration 4800: Loss = -10883.15863834397
Iteration 4900: Loss = -10883.158605244531
Iteration 5000: Loss = -10883.158588864624
Iteration 5100: Loss = -10883.158568118457
Iteration 5200: Loss = -10883.15863940072
Iteration 5300: Loss = -10883.158559386377
Iteration 5400: Loss = -10883.159983067986
1
Iteration 5500: Loss = -10883.158543436553
Iteration 5600: Loss = -10883.158617499806
Iteration 5700: Loss = -10883.15850439925
Iteration 5800: Loss = -10883.158504730582
Iteration 5900: Loss = -10883.158473746504
Iteration 6000: Loss = -10883.158477912457
Iteration 6100: Loss = -10883.158458612665
Iteration 6200: Loss = -10883.158433785597
Iteration 6300: Loss = -10883.158468992202
Iteration 6400: Loss = -10883.158395607321
Iteration 6500: Loss = -10883.158435947327
Iteration 6600: Loss = -10883.158415444284
Iteration 6700: Loss = -10883.158426795895
Iteration 6800: Loss = -10883.161633473812
1
Iteration 6900: Loss = -10883.158372069292
Iteration 7000: Loss = -10883.159144063173
1
Iteration 7100: Loss = -10883.158304642207
Iteration 7200: Loss = -10883.158591867445
1
Iteration 7300: Loss = -10883.15826143169
Iteration 7400: Loss = -10883.163231771443
1
Iteration 7500: Loss = -10883.158153095503
Iteration 7600: Loss = -10883.159602433028
1
Iteration 7700: Loss = -10883.15801857218
Iteration 7800: Loss = -10883.157935981493
Iteration 7900: Loss = -10883.158362521366
1
Iteration 8000: Loss = -10883.158602467478
2
Iteration 8100: Loss = -10883.164798080557
3
Iteration 8200: Loss = -10883.156423128861
Iteration 8300: Loss = -10883.156087509391
Iteration 8400: Loss = -10883.155824279345
Iteration 8500: Loss = -10883.160742067314
1
Iteration 8600: Loss = -10883.154529106147
Iteration 8700: Loss = -10883.153349345379
Iteration 8800: Loss = -10883.153350776765
Iteration 8900: Loss = -10883.151484364416
Iteration 9000: Loss = -10883.15169912731
1
Iteration 9100: Loss = -10883.150573786115
Iteration 9200: Loss = -10883.15044373131
Iteration 9300: Loss = -10883.158307214604
1
Iteration 9400: Loss = -10883.153335721505
2
Iteration 9500: Loss = -10883.145009689786
Iteration 9600: Loss = -10883.205854690534
1
Iteration 9700: Loss = -10883.14284389895
Iteration 9800: Loss = -10880.604074123
Iteration 9900: Loss = -10879.978278996783
Iteration 10000: Loss = -10879.977143290096
Iteration 10100: Loss = -10879.977070072944
Iteration 10200: Loss = -10879.977034664185
Iteration 10300: Loss = -10879.99880190354
1
Iteration 10400: Loss = -10879.977010031096
Iteration 10500: Loss = -10879.977028594138
Iteration 10600: Loss = -10879.977032130084
Iteration 10700: Loss = -10879.977054819674
Iteration 10800: Loss = -10879.976967504517
Iteration 10900: Loss = -10879.976967942554
Iteration 11000: Loss = -10879.97752399474
1
Iteration 11100: Loss = -10879.97687895471
Iteration 11200: Loss = -10879.976903258732
Iteration 11300: Loss = -10879.97697527493
Iteration 11400: Loss = -10879.976900630418
Iteration 11500: Loss = -10879.977440056236
1
Iteration 11600: Loss = -10879.9769218851
Iteration 11700: Loss = -10879.976917518929
Iteration 11800: Loss = -10879.983906603204
1
Iteration 11900: Loss = -10879.97693027619
Iteration 12000: Loss = -10879.97691534036
Iteration 12100: Loss = -10879.981902178928
1
Iteration 12200: Loss = -10879.976902323171
Iteration 12300: Loss = -10879.976911868907
Iteration 12400: Loss = -10879.992478908423
1
Iteration 12500: Loss = -10879.976896232574
Iteration 12600: Loss = -10879.97690566374
Iteration 12700: Loss = -10880.212323114105
1
Iteration 12800: Loss = -10879.976891036747
Iteration 12900: Loss = -10880.002312685741
1
Iteration 13000: Loss = -10879.976887489103
Iteration 13100: Loss = -10879.976871205496
Iteration 13200: Loss = -10879.977119292604
1
Iteration 13300: Loss = -10879.97690661589
Iteration 13400: Loss = -10879.976911623566
Iteration 13500: Loss = -10879.977075237373
1
Iteration 13600: Loss = -10879.976880232609
Iteration 13700: Loss = -10879.976900053052
Iteration 13800: Loss = -10880.018106707868
1
Iteration 13900: Loss = -10879.976880217127
Iteration 14000: Loss = -10880.121696448141
1
Iteration 14100: Loss = -10879.976876941908
Iteration 14200: Loss = -10879.976892457094
Iteration 14300: Loss = -10879.977183557718
1
Iteration 14400: Loss = -10879.976887434053
Iteration 14500: Loss = -10879.97699329752
1
Iteration 14600: Loss = -10879.976908269986
Iteration 14700: Loss = -10879.976871009161
Iteration 14800: Loss = -10879.977576219235
1
Iteration 14900: Loss = -10879.976983508379
2
Iteration 15000: Loss = -10879.97785142264
3
Iteration 15100: Loss = -10879.976949645288
Iteration 15200: Loss = -10879.97688110869
Iteration 15300: Loss = -10879.977337021419
1
Iteration 15400: Loss = -10879.976878759688
Iteration 15500: Loss = -10879.977041910872
1
Iteration 15600: Loss = -10879.977060480709
2
Iteration 15700: Loss = -10879.976972037552
Iteration 15800: Loss = -10879.97859896498
1
Iteration 15900: Loss = -10879.98177855242
2
Iteration 16000: Loss = -10879.976912159951
Iteration 16100: Loss = -10880.026967529704
1
Iteration 16200: Loss = -10879.976871181298
Iteration 16300: Loss = -10879.98508605847
1
Iteration 16400: Loss = -10879.9768907851
Iteration 16500: Loss = -10879.97698079903
Iteration 16600: Loss = -10879.976874898019
Iteration 16700: Loss = -10879.976858078511
Iteration 16800: Loss = -10880.045196318542
1
Iteration 16900: Loss = -10879.976879385626
Iteration 17000: Loss = -10879.976876571169
Iteration 17100: Loss = -10879.976886058354
Iteration 17200: Loss = -10879.977035743932
1
Iteration 17300: Loss = -10879.976855678693
Iteration 17400: Loss = -10880.104592503301
1
Iteration 17500: Loss = -10879.976885066659
Iteration 17600: Loss = -10879.976881315408
Iteration 17700: Loss = -10880.047314510863
1
Iteration 17800: Loss = -10879.976888961457
Iteration 17900: Loss = -10879.976882012113
Iteration 18000: Loss = -10879.976886798731
Iteration 18100: Loss = -10879.976856888885
Iteration 18200: Loss = -10879.98239963484
1
Iteration 18300: Loss = -10879.976865496872
Iteration 18400: Loss = -10879.976922639747
Iteration 18500: Loss = -10879.977172241302
1
Iteration 18600: Loss = -10879.97696031865
Iteration 18700: Loss = -10879.977623609973
1
Iteration 18800: Loss = -10879.977000102044
Iteration 18900: Loss = -10879.97697821297
Iteration 19000: Loss = -10879.977807100311
1
Iteration 19100: Loss = -10879.976874677513
Iteration 19200: Loss = -10879.97687533109
Iteration 19300: Loss = -10879.977154335904
1
Iteration 19400: Loss = -10879.977030526596
2
Iteration 19500: Loss = -10879.977451308554
3
Iteration 19600: Loss = -10879.976908809398
Iteration 19700: Loss = -10879.97988813624
1
Iteration 19800: Loss = -10879.976870794313
Iteration 19900: Loss = -10879.977123098375
1
pi: tensor([[0.7735, 0.2265],
        [0.0129, 0.9871]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0172, 0.9828], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.5252, 0.1743],
         [0.7284, 0.1625]],

        [[0.5783, 0.0578],
         [0.7164, 0.5948]],

        [[0.6919, 0.1420],
         [0.5625, 0.6257]],

        [[0.5777, 0.1714],
         [0.7285, 0.6641]],

        [[0.5472, 0.0995],
         [0.5956, 0.5705]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.011530202595462608
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
Global Adjusted Rand Index: -0.00319257640898831
Average Adjusted Rand Index: -0.0018603906814836381
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20675.00627399806
Iteration 100: Loss = -10887.550832462777
Iteration 200: Loss = -10887.05828973057
Iteration 300: Loss = -10886.967999676932
Iteration 400: Loss = -10886.919089248142
Iteration 500: Loss = -10886.887450517734
Iteration 600: Loss = -10886.864587190075
Iteration 700: Loss = -10886.84666519327
Iteration 800: Loss = -10886.831390767044
Iteration 900: Loss = -10886.81710914715
Iteration 1000: Loss = -10886.802114678027
Iteration 1100: Loss = -10886.783461229763
Iteration 1200: Loss = -10886.755079424469
Iteration 1300: Loss = -10886.70490149229
Iteration 1400: Loss = -10886.628403063327
Iteration 1500: Loss = -10886.557159856382
Iteration 1600: Loss = -10886.505133384591
Iteration 1700: Loss = -10886.459679838208
Iteration 1800: Loss = -10886.412555299297
Iteration 1900: Loss = -10886.36162205743
Iteration 2000: Loss = -10886.308281247406
Iteration 2100: Loss = -10886.256529751183
Iteration 2200: Loss = -10886.210922501667
Iteration 2300: Loss = -10886.173018061321
Iteration 2400: Loss = -10886.141923818279
Iteration 2500: Loss = -10886.115721430986
Iteration 2600: Loss = -10886.092085988977
Iteration 2700: Loss = -10886.068101668412
Iteration 2800: Loss = -10886.038154648306
Iteration 2900: Loss = -10885.980458511378
Iteration 3000: Loss = -10885.781209073884
Iteration 3100: Loss = -10885.569972853275
Iteration 3200: Loss = -10885.472705707592
Iteration 3300: Loss = -10885.41597096945
Iteration 3400: Loss = -10885.377752312323
Iteration 3500: Loss = -10885.350421127881
Iteration 3600: Loss = -10885.329998670728
Iteration 3700: Loss = -10885.31421586675
Iteration 3800: Loss = -10885.301782945493
Iteration 3900: Loss = -10885.291617012612
Iteration 4000: Loss = -10885.283357644164
Iteration 4100: Loss = -10885.27635283944
Iteration 4200: Loss = -10885.270464108866
Iteration 4300: Loss = -10885.265407433
Iteration 4400: Loss = -10885.261026356276
Iteration 4500: Loss = -10885.257191841378
Iteration 4600: Loss = -10885.253891401424
Iteration 4700: Loss = -10885.250946065178
Iteration 4800: Loss = -10885.248312107024
Iteration 4900: Loss = -10885.24597460787
Iteration 5000: Loss = -10885.243926892485
Iteration 5100: Loss = -10885.242041471589
Iteration 5200: Loss = -10885.240367579763
Iteration 5300: Loss = -10885.23883432133
Iteration 5400: Loss = -10885.237402500443
Iteration 5500: Loss = -10885.236113545161
Iteration 5600: Loss = -10885.234965240734
Iteration 5700: Loss = -10885.233891306125
Iteration 5800: Loss = -10885.232954356808
Iteration 5900: Loss = -10885.232051738498
Iteration 6000: Loss = -10885.231183188635
Iteration 6100: Loss = -10885.230414739868
Iteration 6200: Loss = -10885.229689081149
Iteration 6300: Loss = -10885.229044512518
Iteration 6400: Loss = -10885.228443530397
Iteration 6500: Loss = -10885.227867731128
Iteration 6600: Loss = -10885.227296351482
Iteration 6700: Loss = -10885.226806813576
Iteration 6800: Loss = -10885.226317240993
Iteration 6900: Loss = -10885.225873862759
Iteration 7000: Loss = -10885.22547102565
Iteration 7100: Loss = -10885.225100453854
Iteration 7200: Loss = -10885.22473127779
Iteration 7300: Loss = -10885.224733188292
Iteration 7400: Loss = -10885.224029769694
Iteration 7500: Loss = -10885.223736057991
Iteration 7600: Loss = -10885.22343403141
Iteration 7700: Loss = -10885.223172849091
Iteration 7800: Loss = -10885.223052673917
Iteration 7900: Loss = -10885.222664970908
Iteration 8000: Loss = -10885.2224483961
Iteration 8100: Loss = -10885.222314893139
Iteration 8200: Loss = -10885.222099138771
Iteration 8300: Loss = -10885.221853942161
Iteration 8400: Loss = -10885.270251759566
1
Iteration 8500: Loss = -10885.221527052252
Iteration 8600: Loss = -10885.221375128032
Iteration 8700: Loss = -10885.221224982124
Iteration 8800: Loss = -10885.229578268329
1
Iteration 8900: Loss = -10885.220949399996
Iteration 9000: Loss = -10885.22081895043
Iteration 9100: Loss = -10885.295185575913
1
Iteration 9200: Loss = -10885.220610816756
Iteration 9300: Loss = -10885.220493950808
Iteration 9400: Loss = -10885.220390124518
Iteration 9500: Loss = -10885.220282945258
Iteration 9600: Loss = -10885.220167610158
Iteration 9700: Loss = -10885.220080320505
Iteration 9800: Loss = -10885.219989022353
Iteration 9900: Loss = -10885.231733607465
1
Iteration 10000: Loss = -10885.219824986521
Iteration 10100: Loss = -10885.219739642074
Iteration 10200: Loss = -10885.222659916963
1
Iteration 10300: Loss = -10885.219565878724
Iteration 10400: Loss = -10885.219537425894
Iteration 10500: Loss = -10885.219486220765
Iteration 10600: Loss = -10885.220038163967
1
Iteration 10700: Loss = -10885.219439757366
Iteration 10800: Loss = -10885.219305138846
Iteration 10900: Loss = -10885.219266096961
Iteration 11000: Loss = -10885.23056955835
1
Iteration 11100: Loss = -10885.219152780594
Iteration 11200: Loss = -10885.219131674337
Iteration 11300: Loss = -10885.219052865205
Iteration 11400: Loss = -10885.219461260356
1
Iteration 11500: Loss = -10885.219047990046
Iteration 11600: Loss = -10885.218985198862
Iteration 11700: Loss = -10885.219000057787
Iteration 11800: Loss = -10885.219023846801
Iteration 11900: Loss = -10885.218959201788
Iteration 12000: Loss = -10885.218924404247
Iteration 12100: Loss = -10885.232853120075
1
Iteration 12200: Loss = -10885.218821000895
Iteration 12300: Loss = -10885.218787842943
Iteration 12400: Loss = -10885.218780626103
Iteration 12500: Loss = -10885.224366752274
1
Iteration 12600: Loss = -10885.218740142596
Iteration 12700: Loss = -10885.218712344289
Iteration 12800: Loss = -10885.218731190836
Iteration 12900: Loss = -10885.221356448108
1
Iteration 13000: Loss = -10885.218683901487
Iteration 13100: Loss = -10885.218856191941
1
Iteration 13200: Loss = -10885.218642663773
Iteration 13300: Loss = -10885.219259944708
1
Iteration 13400: Loss = -10885.218630132595
Iteration 13500: Loss = -10885.218648106382
Iteration 13600: Loss = -10885.224597370056
1
Iteration 13700: Loss = -10885.224526259752
2
Iteration 13800: Loss = -10885.218969162983
3
Iteration 13900: Loss = -10885.23284447847
4
Iteration 14000: Loss = -10885.219040615171
5
Iteration 14100: Loss = -10885.218706779504
Iteration 14200: Loss = -10885.248235104957
1
Iteration 14300: Loss = -10885.218518090107
Iteration 14400: Loss = -10885.229719461411
1
Iteration 14500: Loss = -10885.218534315227
Iteration 14600: Loss = -10885.228717645188
1
Iteration 14700: Loss = -10885.219290463729
2
Iteration 14800: Loss = -10885.224979110895
3
Iteration 14900: Loss = -10885.21875856703
4
Iteration 15000: Loss = -10885.222040773257
5
Iteration 15100: Loss = -10885.219060329071
6
Iteration 15200: Loss = -10885.23659282983
7
Iteration 15300: Loss = -10885.218490226654
Iteration 15400: Loss = -10885.299992397402
1
Iteration 15500: Loss = -10885.21843929914
Iteration 15600: Loss = -10885.236845928915
1
Iteration 15700: Loss = -10885.218456086244
Iteration 15800: Loss = -10885.219815797373
1
Iteration 15900: Loss = -10885.21844521847
Iteration 16000: Loss = -10885.218516736631
Iteration 16100: Loss = -10885.218798586322
1
Iteration 16200: Loss = -10885.240338651645
2
Iteration 16300: Loss = -10885.218423975057
Iteration 16400: Loss = -10885.26207658153
1
Iteration 16500: Loss = -10885.218428163944
Iteration 16600: Loss = -10885.542581919179
1
Iteration 16700: Loss = -10885.218399910145
Iteration 16800: Loss = -10885.218430326462
Iteration 16900: Loss = -10885.218394409074
Iteration 17000: Loss = -10885.218411540309
Iteration 17100: Loss = -10885.22149441815
1
Iteration 17200: Loss = -10885.21838000785
Iteration 17300: Loss = -10885.218450485125
Iteration 17400: Loss = -10885.381232050184
1
Iteration 17500: Loss = -10885.218420442932
Iteration 17600: Loss = -10885.219970092545
1
Iteration 17700: Loss = -10885.219748136598
2
Iteration 17800: Loss = -10885.220909118181
3
Iteration 17900: Loss = -10885.218534066427
4
Iteration 18000: Loss = -10885.218413880875
Iteration 18100: Loss = -10885.218711468273
1
Iteration 18200: Loss = -10885.239254978684
2
Iteration 18300: Loss = -10885.218370238208
Iteration 18400: Loss = -10885.220232995487
1
Iteration 18500: Loss = -10885.218375463166
Iteration 18600: Loss = -10885.21869270487
1
Iteration 18700: Loss = -10885.218367695283
Iteration 18800: Loss = -10885.219499748066
1
Iteration 18900: Loss = -10885.21836103383
Iteration 19000: Loss = -10885.23177213491
1
Iteration 19100: Loss = -10885.218422411492
Iteration 19200: Loss = -10885.218538379931
1
Iteration 19300: Loss = -10885.21838728701
Iteration 19400: Loss = -10885.225627030713
1
Iteration 19500: Loss = -10885.218330493848
Iteration 19600: Loss = -10885.24958716952
1
Iteration 19700: Loss = -10885.218466248734
2
Iteration 19800: Loss = -10885.228343107441
3
Iteration 19900: Loss = -10885.22135736544
4
pi: tensor([[1.0000e+00, 2.7525e-08],
        [2.1017e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9794, 0.0206], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1595, 0.1210],
         [0.6549, 0.2094]],

        [[0.6841, 0.1686],
         [0.6943, 0.6706]],

        [[0.6099, 0.2599],
         [0.6730, 0.7100]],

        [[0.7302, 0.2064],
         [0.7013, 0.5013]],

        [[0.6780, 0.1568],
         [0.6727, 0.6201]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
Global Adjusted Rand Index: 0.00012612959590748072
Average Adjusted Rand Index: 0.002058281958142549
10757.289524718703
[-0.00319257640898831, 0.00012612959590748072] [-0.0018603906814836381, 0.002058281958142549] [10879.97757536175, 10885.223740060928]
-------------------------------------
This iteration is 62
True Objective function: Loss = -10858.056372567185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21555.52938700854
Iteration 100: Loss = -10934.386813084207
Iteration 200: Loss = -10933.629033323647
Iteration 300: Loss = -10933.325837501101
Iteration 400: Loss = -10933.174175448265
Iteration 500: Loss = -10933.078171464691
Iteration 600: Loss = -10932.995733893087
Iteration 700: Loss = -10932.911973065484
Iteration 800: Loss = -10932.835318085037
Iteration 900: Loss = -10932.76529701089
Iteration 1000: Loss = -10932.687850980958
Iteration 1100: Loss = -10932.591607525794
Iteration 1200: Loss = -10932.475803173727
Iteration 1300: Loss = -10932.346289931751
Iteration 1400: Loss = -10932.216975926283
Iteration 1500: Loss = -10932.095198531077
Iteration 1600: Loss = -10931.992111502987
Iteration 1700: Loss = -10931.921471116946
Iteration 1800: Loss = -10931.866145136839
Iteration 1900: Loss = -10931.797627512828
Iteration 2000: Loss = -10931.477195642241
Iteration 2100: Loss = -10930.992782707195
Iteration 2200: Loss = -10930.701101019014
Iteration 2300: Loss = -10930.390443839435
Iteration 2400: Loss = -10929.902899304236
Iteration 2500: Loss = -10929.241760295416
Iteration 2600: Loss = -10928.876025005806
Iteration 2700: Loss = -10928.762439758766
Iteration 2800: Loss = -10928.62740435469
Iteration 2900: Loss = -10928.139632305425
Iteration 3000: Loss = -10928.019784829146
Iteration 3100: Loss = -10928.003025356897
Iteration 3200: Loss = -10927.996860889072
Iteration 3300: Loss = -10927.993659747985
Iteration 3400: Loss = -10927.991713196356
Iteration 3500: Loss = -10927.990389765926
Iteration 3600: Loss = -10927.98942431406
Iteration 3700: Loss = -10927.988710606847
Iteration 3800: Loss = -10927.988146846206
Iteration 3900: Loss = -10927.987686099725
Iteration 4000: Loss = -10927.987295025327
Iteration 4100: Loss = -10927.986929899507
Iteration 4200: Loss = -10927.986633394294
Iteration 4300: Loss = -10927.986367892527
Iteration 4400: Loss = -10927.98613507532
Iteration 4500: Loss = -10927.985946762908
Iteration 4600: Loss = -10927.986678237961
1
Iteration 4700: Loss = -10927.9855854301
Iteration 4800: Loss = -10928.043025706042
1
Iteration 4900: Loss = -10927.985274355678
Iteration 5000: Loss = -10927.98518717248
Iteration 5100: Loss = -10927.985638363956
1
Iteration 5200: Loss = -10927.984964216565
Iteration 5300: Loss = -10927.984865727685
Iteration 5400: Loss = -10927.984790919423
Iteration 5500: Loss = -10927.984693988625
Iteration 5600: Loss = -10927.98455819677
Iteration 5700: Loss = -10927.984802849785
1
Iteration 5800: Loss = -10927.984439893722
Iteration 5900: Loss = -10927.984365235925
Iteration 6000: Loss = -10927.984408565322
Iteration 6100: Loss = -10927.984240641139
Iteration 6200: Loss = -10927.984189136412
Iteration 6300: Loss = -10927.984823695533
1
Iteration 6400: Loss = -10927.984077222967
Iteration 6500: Loss = -10927.984013287212
Iteration 6600: Loss = -10927.984068939817
Iteration 6700: Loss = -10927.98390058841
Iteration 6800: Loss = -10927.98387520865
Iteration 6900: Loss = -10927.983878458817
Iteration 7000: Loss = -10927.983774992697
Iteration 7100: Loss = -10927.983765315435
Iteration 7200: Loss = -10927.983855375574
Iteration 7300: Loss = -10927.983684957571
Iteration 7400: Loss = -10927.98365356216
Iteration 7500: Loss = -10927.98377569535
1
Iteration 7600: Loss = -10927.983572985007
Iteration 7700: Loss = -10927.983575776196
Iteration 7800: Loss = -10928.198994749837
1
Iteration 7900: Loss = -10927.983515306403
Iteration 8000: Loss = -10927.983462874254
Iteration 8100: Loss = -10927.983588671545
1
Iteration 8200: Loss = -10927.983414821925
Iteration 8300: Loss = -10927.983389902029
Iteration 8400: Loss = -10927.984462622107
1
Iteration 8500: Loss = -10927.983366506796
Iteration 8600: Loss = -10927.983345442697
Iteration 8700: Loss = -10927.983552541265
1
Iteration 8800: Loss = -10927.983329400957
Iteration 8900: Loss = -10927.983240211459
Iteration 9000: Loss = -10927.983918975193
1
Iteration 9100: Loss = -10927.98320620367
Iteration 9200: Loss = -10927.983178612327
Iteration 9300: Loss = -10928.11412023245
1
Iteration 9400: Loss = -10927.983138728683
Iteration 9500: Loss = -10927.983111666981
Iteration 9600: Loss = -10928.403234122023
1
Iteration 9700: Loss = -10927.983080657965
Iteration 9800: Loss = -10927.983098069186
Iteration 9900: Loss = -10927.986347994485
1
Iteration 10000: Loss = -10927.983035259098
Iteration 10100: Loss = -10927.983037791588
Iteration 10200: Loss = -10928.063506720051
1
Iteration 10300: Loss = -10927.982993459304
Iteration 10400: Loss = -10927.98298596155
Iteration 10500: Loss = -10928.301169380993
1
Iteration 10600: Loss = -10927.98295009314
Iteration 10700: Loss = -10927.98291985336
Iteration 10800: Loss = -10927.993947911304
1
Iteration 10900: Loss = -10927.982939261585
Iteration 11000: Loss = -10927.98294050883
Iteration 11100: Loss = -10927.995889395906
1
Iteration 11200: Loss = -10927.982892555558
Iteration 11300: Loss = -10927.9829031242
Iteration 11400: Loss = -10927.98292059504
Iteration 11500: Loss = -10928.069059314537
1
Iteration 11600: Loss = -10927.996519162909
2
Iteration 11700: Loss = -10927.982857028434
Iteration 11800: Loss = -10927.983525917794
1
Iteration 11900: Loss = -10927.982815038742
Iteration 12000: Loss = -10927.983250116575
1
Iteration 12100: Loss = -10927.982822508355
Iteration 12200: Loss = -10927.982947701135
1
Iteration 12300: Loss = -10927.98283698141
Iteration 12400: Loss = -10927.983339466948
1
Iteration 12500: Loss = -10927.984658753867
2
Iteration 12600: Loss = -10927.982877384997
Iteration 12700: Loss = -10927.984202310947
1
Iteration 12800: Loss = -10927.982801205788
Iteration 12900: Loss = -10927.984778022115
1
Iteration 13000: Loss = -10927.994816702078
2
Iteration 13100: Loss = -10927.985027758434
3
Iteration 13200: Loss = -10927.9829905853
4
Iteration 13300: Loss = -10927.982958174141
5
Iteration 13400: Loss = -10927.983569578366
6
Iteration 13500: Loss = -10927.983098318131
7
Iteration 13600: Loss = -10927.983166744663
8
Iteration 13700: Loss = -10927.999060963504
9
Iteration 13800: Loss = -10927.982800419059
Iteration 13900: Loss = -10927.99419456435
1
Iteration 14000: Loss = -10927.982762309177
Iteration 14100: Loss = -10928.029974546429
1
Iteration 14200: Loss = -10927.982962854045
2
Iteration 14300: Loss = -10927.982788650872
Iteration 14400: Loss = -10927.98854353135
1
Iteration 14500: Loss = -10927.982714837583
Iteration 14600: Loss = -10927.983028912891
1
Iteration 14700: Loss = -10927.982727384266
Iteration 14800: Loss = -10927.9845403459
1
Iteration 14900: Loss = -10927.983979781182
2
Iteration 15000: Loss = -10927.982718922267
Iteration 15100: Loss = -10927.984101916374
1
Iteration 15200: Loss = -10927.989161203506
2
Iteration 15300: Loss = -10927.982710022223
Iteration 15400: Loss = -10927.982996400597
1
Iteration 15500: Loss = -10927.983958098275
2
Iteration 15600: Loss = -10927.983009425556
3
Iteration 15700: Loss = -10927.983085537218
4
Iteration 15800: Loss = -10927.98269991476
Iteration 15900: Loss = -10927.983158620751
1
Iteration 16000: Loss = -10927.988431143805
2
Iteration 16100: Loss = -10927.982684630831
Iteration 16200: Loss = -10927.982951345793
1
Iteration 16300: Loss = -10927.98268882188
Iteration 16400: Loss = -10927.984086859387
1
Iteration 16500: Loss = -10927.982666367401
Iteration 16600: Loss = -10927.984224544083
1
Iteration 16700: Loss = -10927.983001630533
2
Iteration 16800: Loss = -10927.98340430908
3
Iteration 16900: Loss = -10928.091086634668
4
Iteration 17000: Loss = -10927.982673807788
Iteration 17100: Loss = -10927.98856369747
1
Iteration 17200: Loss = -10927.991892776492
2
Iteration 17300: Loss = -10928.016175963225
3
Iteration 17400: Loss = -10927.982730857071
Iteration 17500: Loss = -10927.98284968238
1
Iteration 17600: Loss = -10927.98272473299
Iteration 17700: Loss = -10927.982935437054
1
Iteration 17800: Loss = -10928.064704604134
2
Iteration 17900: Loss = -10927.982694407467
Iteration 18000: Loss = -10928.010134459759
1
Iteration 18100: Loss = -10927.982721006192
Iteration 18200: Loss = -10927.992523087629
1
Iteration 18300: Loss = -10927.982689552622
Iteration 18400: Loss = -10927.9833560974
1
Iteration 18500: Loss = -10928.013653346661
2
Iteration 18600: Loss = -10927.982678977069
Iteration 18700: Loss = -10928.017235729918
1
Iteration 18800: Loss = -10927.982675598263
Iteration 18900: Loss = -10927.988846242899
1
Iteration 19000: Loss = -10928.002624366363
2
Iteration 19100: Loss = -10927.982693197144
Iteration 19200: Loss = -10927.985048432252
1
Iteration 19300: Loss = -10927.982662109682
Iteration 19400: Loss = -10927.982961944821
1
Iteration 19500: Loss = -10927.982779724405
2
Iteration 19600: Loss = -10927.98290601299
3
Iteration 19700: Loss = -10927.997616465645
4
Iteration 19800: Loss = -10927.989231182759
5
Iteration 19900: Loss = -10928.045638094061
6
pi: tensor([[1.0000e+00, 4.4050e-06],
        [2.9513e-02, 9.7049e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0258, 0.9742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1321, 0.1106],
         [0.5331, 0.1665]],

        [[0.5140, 0.0767],
         [0.5082, 0.7039]],

        [[0.6605, 0.1489],
         [0.5422, 0.7177]],

        [[0.5682, 0.1384],
         [0.6427, 0.5834]],

        [[0.5893, 0.1413],
         [0.6251, 0.6540]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012598425196850394
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0035158395898187145
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
Global Adjusted Rand Index: -0.0021112816191000976
Average Adjusted Rand Index: -0.0005301806740898909
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23366.67742161988
Iteration 100: Loss = -10891.752699645644
Iteration 200: Loss = -10813.515691188448
Iteration 300: Loss = -10812.71249877363
Iteration 400: Loss = -10812.106374674724
Iteration 500: Loss = -10812.004935683555
Iteration 600: Loss = -10811.971941257472
Iteration 700: Loss = -10811.95573863474
Iteration 800: Loss = -10811.945930262204
Iteration 900: Loss = -10811.939386896252
Iteration 1000: Loss = -10811.934798264627
Iteration 1100: Loss = -10811.931430690995
Iteration 1200: Loss = -10811.928868553869
Iteration 1300: Loss = -10811.926888198599
Iteration 1400: Loss = -10811.925337961333
Iteration 1500: Loss = -10811.92411590582
Iteration 1600: Loss = -10811.923061552425
Iteration 1700: Loss = -10811.92224964585
Iteration 1800: Loss = -10811.92147835742
Iteration 1900: Loss = -10811.92093759797
Iteration 2000: Loss = -10811.920398657017
Iteration 2100: Loss = -10811.91991232186
Iteration 2200: Loss = -10811.919557577072
Iteration 2300: Loss = -10811.91916600483
Iteration 2400: Loss = -10811.918915784037
Iteration 2500: Loss = -10811.918602188598
Iteration 2600: Loss = -10811.918393217165
Iteration 2700: Loss = -10811.918179138747
Iteration 2800: Loss = -10811.917992232735
Iteration 2900: Loss = -10811.917874762028
Iteration 3000: Loss = -10811.921496338464
1
Iteration 3100: Loss = -10811.917512132322
Iteration 3200: Loss = -10811.917974622802
1
Iteration 3300: Loss = -10811.917339234755
Iteration 3400: Loss = -10811.917312006924
Iteration 3500: Loss = -10811.917172350986
Iteration 3600: Loss = -10811.91724932883
Iteration 3700: Loss = -10811.921216668614
1
Iteration 3800: Loss = -10811.927023170017
2
Iteration 3900: Loss = -10811.916996663875
Iteration 4000: Loss = -10811.916798875742
Iteration 4100: Loss = -10811.916701722965
Iteration 4200: Loss = -10811.91666572378
Iteration 4300: Loss = -10811.916618899297
Iteration 4400: Loss = -10811.91655783519
Iteration 4500: Loss = -10811.92593301004
1
Iteration 4600: Loss = -10811.916478293415
Iteration 4700: Loss = -10811.916427361995
Iteration 4800: Loss = -10811.91655847439
1
Iteration 4900: Loss = -10811.91636180678
Iteration 5000: Loss = -10811.917118698226
1
Iteration 5100: Loss = -10811.916306249426
Iteration 5200: Loss = -10811.917442607186
1
Iteration 5300: Loss = -10811.916306350584
Iteration 5400: Loss = -10811.919469114911
1
Iteration 5500: Loss = -10811.91658781817
2
Iteration 5600: Loss = -10811.917020536232
3
Iteration 5700: Loss = -10811.916330409897
Iteration 5800: Loss = -10811.916291166797
Iteration 5900: Loss = -10811.917985040289
1
Iteration 6000: Loss = -10811.91614060244
Iteration 6100: Loss = -10811.916150166417
Iteration 6200: Loss = -10811.928110152012
1
Iteration 6300: Loss = -10811.916112472609
Iteration 6400: Loss = -10811.916911397082
1
Iteration 6500: Loss = -10811.91605729732
Iteration 6600: Loss = -10811.916072197488
Iteration 6700: Loss = -10811.916067462256
Iteration 6800: Loss = -10811.916054496902
Iteration 6900: Loss = -10811.916999231315
1
Iteration 7000: Loss = -10811.916025033828
Iteration 7100: Loss = -10811.916032102825
Iteration 7200: Loss = -10811.916029100257
Iteration 7300: Loss = -10811.916011316609
Iteration 7400: Loss = -10811.916030275075
Iteration 7500: Loss = -10811.91615818873
1
Iteration 7600: Loss = -10811.91621990884
2
Iteration 7700: Loss = -10811.916011150584
Iteration 7800: Loss = -10811.915960439277
Iteration 7900: Loss = -10811.916656805051
1
Iteration 8000: Loss = -10811.916252224511
2
Iteration 8100: Loss = -10811.917093415725
3
Iteration 8200: Loss = -10811.915986115091
Iteration 8300: Loss = -10811.915969318776
Iteration 8400: Loss = -10811.916086321053
1
Iteration 8500: Loss = -10811.916794415556
2
Iteration 8600: Loss = -10811.916396014909
3
Iteration 8700: Loss = -10811.916013400516
Iteration 8800: Loss = -10811.916097771713
Iteration 8900: Loss = -10811.916969614673
1
Iteration 9000: Loss = -10811.917064698253
2
Iteration 9100: Loss = -10811.916455430312
3
Iteration 9200: Loss = -10811.91593245038
Iteration 9300: Loss = -10811.917965848459
1
Iteration 9400: Loss = -10811.915974542224
Iteration 9500: Loss = -10811.91634045983
1
Iteration 9600: Loss = -10811.91593114604
Iteration 9700: Loss = -10811.916037835173
1
Iteration 9800: Loss = -10811.915926593405
Iteration 9900: Loss = -10811.916285032783
1
Iteration 10000: Loss = -10811.915909715532
Iteration 10100: Loss = -10811.964844030668
1
Iteration 10200: Loss = -10811.915944853748
Iteration 10300: Loss = -10811.915896933813
Iteration 10400: Loss = -10811.91605171567
1
Iteration 10500: Loss = -10811.915894266966
Iteration 10600: Loss = -10811.91633469317
1
Iteration 10700: Loss = -10811.915901474778
Iteration 10800: Loss = -10811.979062976387
1
Iteration 10900: Loss = -10811.915920657966
Iteration 11000: Loss = -10811.915916090758
Iteration 11100: Loss = -10811.91803680564
1
Iteration 11200: Loss = -10811.915933249686
Iteration 11300: Loss = -10811.9159513609
Iteration 11400: Loss = -10811.91603388844
Iteration 11500: Loss = -10811.91594425827
Iteration 11600: Loss = -10812.09292954369
1
Iteration 11700: Loss = -10811.915925957539
Iteration 11800: Loss = -10811.915928768054
Iteration 11900: Loss = -10811.91612703989
1
Iteration 12000: Loss = -10811.988329305741
2
Iteration 12100: Loss = -10811.91594462364
Iteration 12200: Loss = -10811.933754797597
1
Iteration 12300: Loss = -10811.9201576974
2
Iteration 12400: Loss = -10811.917468283382
3
Iteration 12500: Loss = -10811.915972415021
Iteration 12600: Loss = -10811.940225997381
1
Iteration 12700: Loss = -10811.91591622239
Iteration 12800: Loss = -10811.940381910808
1
Iteration 12900: Loss = -10811.915947939726
Iteration 13000: Loss = -10811.935257211047
1
Iteration 13100: Loss = -10811.915927471211
Iteration 13200: Loss = -10811.915930842319
Iteration 13300: Loss = -10811.926453276195
1
Iteration 13400: Loss = -10811.91591300428
Iteration 13500: Loss = -10811.915910322004
Iteration 13600: Loss = -10811.91667848585
1
Iteration 13700: Loss = -10811.915945531142
Iteration 13800: Loss = -10811.917051790522
1
Iteration 13900: Loss = -10811.915922493856
Iteration 14000: Loss = -10811.91592446307
Iteration 14100: Loss = -10811.92242589008
1
Iteration 14200: Loss = -10811.915914354071
Iteration 14300: Loss = -10811.91593930287
Iteration 14400: Loss = -10811.916739584198
1
Iteration 14500: Loss = -10811.918322433161
2
Iteration 14600: Loss = -10811.91860344518
3
Iteration 14700: Loss = -10811.925874786557
4
Iteration 14800: Loss = -10811.915918384235
Iteration 14900: Loss = -10811.920324708393
1
Iteration 15000: Loss = -10811.915936396217
Iteration 15100: Loss = -10811.916360066854
1
Iteration 15200: Loss = -10811.9159409452
Iteration 15300: Loss = -10811.916259546044
1
Iteration 15400: Loss = -10811.91625938154
2
Iteration 15500: Loss = -10811.915955768265
Iteration 15600: Loss = -10811.91591846902
Iteration 15700: Loss = -10811.9159604063
Iteration 15800: Loss = -10811.915893989115
Iteration 15900: Loss = -10811.92076505137
1
Iteration 16000: Loss = -10811.915903488874
Iteration 16100: Loss = -10811.915977824925
Iteration 16200: Loss = -10811.91595242552
Iteration 16300: Loss = -10811.915906413347
Iteration 16400: Loss = -10811.979021725758
1
Iteration 16500: Loss = -10811.915883747677
Iteration 16600: Loss = -10811.916486109858
1
Iteration 16700: Loss = -10811.916218671087
2
Iteration 16800: Loss = -10811.917748111993
3
Iteration 16900: Loss = -10811.959641330466
4
Iteration 17000: Loss = -10811.915934907962
Iteration 17100: Loss = -10811.91600688112
Iteration 17200: Loss = -10811.921655559958
1
Iteration 17300: Loss = -10811.920030862842
2
Iteration 17400: Loss = -10811.929668404038
3
Iteration 17500: Loss = -10811.916503685288
4
Iteration 17600: Loss = -10811.915940228597
Iteration 17700: Loss = -10811.921078819552
1
Iteration 17800: Loss = -10811.915900507007
Iteration 17900: Loss = -10812.032209739307
1
Iteration 18000: Loss = -10811.915911664157
Iteration 18100: Loss = -10811.915930700749
Iteration 18200: Loss = -10811.915987550734
Iteration 18300: Loss = -10811.91590969697
Iteration 18400: Loss = -10811.91900860328
1
Iteration 18500: Loss = -10811.915881518575
Iteration 18600: Loss = -10811.919515001651
1
Iteration 18700: Loss = -10811.915935247825
Iteration 18800: Loss = -10811.915896821089
Iteration 18900: Loss = -10811.916180957196
1
Iteration 19000: Loss = -10811.91814723194
2
Iteration 19100: Loss = -10811.91835538359
3
Iteration 19200: Loss = -10811.982125006534
4
Iteration 19300: Loss = -10811.915922429313
Iteration 19400: Loss = -10811.918656324873
1
Iteration 19500: Loss = -10811.915910319933
Iteration 19600: Loss = -10811.91793200775
1
Iteration 19700: Loss = -10811.915913850746
Iteration 19800: Loss = -10811.922586557059
1
Iteration 19900: Loss = -10811.9159204453
pi: tensor([[0.7663, 0.2337],
        [0.2713, 0.7287]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5449, 0.4551], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.1073],
         [0.5141, 0.2548]],

        [[0.5551, 0.1019],
         [0.5915, 0.5291]],

        [[0.6783, 0.0886],
         [0.6246, 0.7253]],

        [[0.5377, 0.1017],
         [0.5768, 0.5045]],

        [[0.5607, 0.1074],
         [0.5246, 0.6812]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 85
Adjusted Rand Index: 0.48483637448226513
Global Adjusted Rand Index: 0.7952544090302294
Average Adjusted Rand Index: 0.8040362181162495
10858.056372567185
[-0.0021112816191000976, 0.7952544090302294] [-0.0005301806740898909, 0.8040362181162495] [10927.982876276126, 10811.997537742986]
-------------------------------------
This iteration is 63
True Objective function: Loss = -10842.897439165668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23443.929151335436
Iteration 100: Loss = -10957.416506031634
Iteration 200: Loss = -10956.214133016107
Iteration 300: Loss = -10956.093793713017
Iteration 400: Loss = -10956.04143111379
Iteration 500: Loss = -10956.008157503058
Iteration 600: Loss = -10955.980995494081
Iteration 700: Loss = -10955.958284592607
Iteration 800: Loss = -10955.9396658856
Iteration 900: Loss = -10955.922745954678
Iteration 1000: Loss = -10955.906807912552
Iteration 1100: Loss = -10955.891348495155
Iteration 1200: Loss = -10955.87561814313
Iteration 1300: Loss = -10955.85848337361
Iteration 1400: Loss = -10955.838225969292
Iteration 1500: Loss = -10955.810948747025
Iteration 1600: Loss = -10955.764568997292
Iteration 1700: Loss = -10955.68827339851
Iteration 1800: Loss = -10955.627283105045
Iteration 1900: Loss = -10955.574088282681
Iteration 2000: Loss = -10955.538329947089
Iteration 2100: Loss = -10955.518469719838
Iteration 2200: Loss = -10955.486372711292
Iteration 2300: Loss = -10955.35214457394
Iteration 2400: Loss = -10842.552109399561
Iteration 2500: Loss = -10829.151921895216
Iteration 2600: Loss = -10788.812506240338
Iteration 2700: Loss = -10788.628168209712
Iteration 2800: Loss = -10788.572122093847
Iteration 2900: Loss = -10788.549930299729
Iteration 3000: Loss = -10788.542968790776
Iteration 3100: Loss = -10788.511814368372
Iteration 3200: Loss = -10788.510208740181
Iteration 3300: Loss = -10788.505445967321
Iteration 3400: Loss = -10788.50318152707
Iteration 3500: Loss = -10788.499195420558
Iteration 3600: Loss = -10788.488847677065
Iteration 3700: Loss = -10788.483884682528
Iteration 3800: Loss = -10788.454253269965
Iteration 3900: Loss = -10788.453796499734
Iteration 4000: Loss = -10788.45403862665
1
Iteration 4100: Loss = -10788.450741740604
Iteration 4200: Loss = -10788.450015608887
Iteration 4300: Loss = -10788.449667479043
Iteration 4400: Loss = -10788.449922167625
1
Iteration 4500: Loss = -10788.449769194569
2
Iteration 4600: Loss = -10788.448680360389
Iteration 4700: Loss = -10788.447742178094
Iteration 4800: Loss = -10788.447544828134
Iteration 4900: Loss = -10788.447311570468
Iteration 5000: Loss = -10788.447274763266
Iteration 5100: Loss = -10788.476137866175
1
Iteration 5200: Loss = -10788.446838936508
Iteration 5300: Loss = -10788.447839224245
1
Iteration 5400: Loss = -10788.446195083447
Iteration 5500: Loss = -10788.44744771419
1
Iteration 5600: Loss = -10788.445616900712
Iteration 5700: Loss = -10788.444869568832
Iteration 5800: Loss = -10788.441248125862
Iteration 5900: Loss = -10788.4397057706
Iteration 6000: Loss = -10788.439545000907
Iteration 6100: Loss = -10788.439460785077
Iteration 6200: Loss = -10788.439488029322
Iteration 6300: Loss = -10788.439432533569
Iteration 6400: Loss = -10788.441027432636
1
Iteration 6500: Loss = -10788.43943268034
Iteration 6600: Loss = -10788.439410969859
Iteration 6700: Loss = -10788.439827459122
1
Iteration 6800: Loss = -10788.43934391111
Iteration 6900: Loss = -10788.4855323322
1
Iteration 7000: Loss = -10788.439222127452
Iteration 7100: Loss = -10788.439227760464
Iteration 7200: Loss = -10788.439868658914
1
Iteration 7300: Loss = -10788.439186708873
Iteration 7400: Loss = -10788.4392684626
Iteration 7500: Loss = -10788.439192380003
Iteration 7600: Loss = -10788.444143293276
1
Iteration 7700: Loss = -10788.440455725053
2
Iteration 7800: Loss = -10788.445508778548
3
Iteration 7900: Loss = -10788.529239525165
4
Iteration 8000: Loss = -10788.439106877304
Iteration 8100: Loss = -10788.439253169885
1
Iteration 8200: Loss = -10788.447591575248
2
Iteration 8300: Loss = -10788.43897650194
Iteration 8400: Loss = -10788.501195553492
1
Iteration 8500: Loss = -10788.438940766726
Iteration 8600: Loss = -10788.438899305287
Iteration 8700: Loss = -10788.439076393272
1
Iteration 8800: Loss = -10788.438701461231
Iteration 8900: Loss = -10788.452359488729
1
Iteration 9000: Loss = -10788.438686967347
Iteration 9100: Loss = -10788.43864952956
Iteration 9200: Loss = -10788.438683856364
Iteration 9300: Loss = -10788.438691658006
Iteration 9400: Loss = -10788.438666963126
Iteration 9500: Loss = -10788.46703042024
1
Iteration 9600: Loss = -10788.438665044521
Iteration 9700: Loss = -10788.4386745232
Iteration 9800: Loss = -10788.43867719435
Iteration 9900: Loss = -10788.477293065387
1
Iteration 10000: Loss = -10788.438638639725
Iteration 10100: Loss = -10788.43864442731
Iteration 10200: Loss = -10788.439675280126
1
Iteration 10300: Loss = -10788.438642400588
Iteration 10400: Loss = -10788.449770830688
1
Iteration 10500: Loss = -10788.438645739665
Iteration 10600: Loss = -10788.43863112207
Iteration 10700: Loss = -10788.537701633526
1
Iteration 10800: Loss = -10788.43862333717
Iteration 10900: Loss = -10788.43860890223
Iteration 11000: Loss = -10788.454239943592
1
Iteration 11100: Loss = -10788.438607148477
Iteration 11200: Loss = -10788.438598320608
Iteration 11300: Loss = -10788.440968552844
1
Iteration 11400: Loss = -10788.438620754236
Iteration 11500: Loss = -10788.438596781392
Iteration 11600: Loss = -10788.439090513088
1
Iteration 11700: Loss = -10788.438595271416
Iteration 11800: Loss = -10788.438588858076
Iteration 11900: Loss = -10788.43869412516
1
Iteration 12000: Loss = -10788.438559858436
Iteration 12100: Loss = -10788.439589024054
1
Iteration 12200: Loss = -10788.438622479607
Iteration 12300: Loss = -10788.438558185468
Iteration 12400: Loss = -10788.441703926099
1
Iteration 12500: Loss = -10788.43855956032
Iteration 12600: Loss = -10788.441879936125
1
Iteration 12700: Loss = -10788.43859900871
Iteration 12800: Loss = -10788.704334261454
1
Iteration 12900: Loss = -10788.438548610158
Iteration 13000: Loss = -10788.43854596583
Iteration 13100: Loss = -10788.44542068609
1
Iteration 13200: Loss = -10788.438559668206
Iteration 13300: Loss = -10788.441486256579
1
Iteration 13400: Loss = -10788.438542980966
Iteration 13500: Loss = -10788.438535355615
Iteration 13600: Loss = -10788.438850812096
1
Iteration 13700: Loss = -10788.438518028532
Iteration 13800: Loss = -10788.44294610508
1
Iteration 13900: Loss = -10788.438536704849
Iteration 14000: Loss = -10788.438568481493
Iteration 14100: Loss = -10788.4387226573
1
Iteration 14200: Loss = -10788.438578574167
Iteration 14300: Loss = -10788.44744173395
1
Iteration 14400: Loss = -10788.438555694402
Iteration 14500: Loss = -10788.43854234249
Iteration 14600: Loss = -10788.438539148823
Iteration 14700: Loss = -10788.438687454931
1
Iteration 14800: Loss = -10788.43854462324
Iteration 14900: Loss = -10788.438516012277
Iteration 15000: Loss = -10788.439041891264
1
Iteration 15100: Loss = -10788.438515006923
Iteration 15200: Loss = -10788.438532349448
Iteration 15300: Loss = -10788.438683685454
1
Iteration 15400: Loss = -10788.438528148425
Iteration 15500: Loss = -10788.438537299913
Iteration 15600: Loss = -10788.438998609048
1
Iteration 15700: Loss = -10788.43853134419
Iteration 15800: Loss = -10788.438535845307
Iteration 15900: Loss = -10788.438751342721
1
Iteration 16000: Loss = -10788.438540341185
Iteration 16100: Loss = -10788.455361336684
1
Iteration 16200: Loss = -10788.438547458121
Iteration 16300: Loss = -10788.441924811283
1
Iteration 16400: Loss = -10788.438559891747
Iteration 16500: Loss = -10788.639260395501
1
Iteration 16600: Loss = -10788.438552447435
Iteration 16700: Loss = -10788.438526678976
Iteration 16800: Loss = -10788.599868925461
1
Iteration 16900: Loss = -10788.43855606313
Iteration 17000: Loss = -10788.438586750222
Iteration 17100: Loss = -10788.481299871797
1
Iteration 17200: Loss = -10788.438670177748
Iteration 17300: Loss = -10788.438637069605
Iteration 17400: Loss = -10788.528487472502
1
Iteration 17500: Loss = -10788.43859457927
Iteration 17600: Loss = -10788.59296381198
1
Iteration 17700: Loss = -10788.438544217881
Iteration 17800: Loss = -10788.439259126955
1
Iteration 17900: Loss = -10788.438620088305
Iteration 18000: Loss = -10788.43862756515
Iteration 18100: Loss = -10788.438996111227
1
Iteration 18200: Loss = -10788.438476932146
Iteration 18300: Loss = -10788.439795567578
1
Iteration 18400: Loss = -10788.438531584008
Iteration 18500: Loss = -10788.438452328119
Iteration 18600: Loss = -10788.441258537045
1
Iteration 18700: Loss = -10788.438466316562
Iteration 18800: Loss = -10788.771752482751
1
Iteration 18900: Loss = -10788.438442998404
Iteration 19000: Loss = -10788.438495804345
Iteration 19100: Loss = -10788.438594579464
Iteration 19200: Loss = -10788.43844073554
Iteration 19300: Loss = -10788.575599516245
1
Iteration 19400: Loss = -10788.438466425036
Iteration 19500: Loss = -10788.438462456585
Iteration 19600: Loss = -10788.776994768434
1
Iteration 19700: Loss = -10788.438475486899
Iteration 19800: Loss = -10788.43845918243
Iteration 19900: Loss = -10788.438521140302
pi: tensor([[0.7806, 0.2194],
        [0.2500, 0.7500]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5248, 0.4752], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2518, 0.1009],
         [0.7097, 0.1939]],

        [[0.7292, 0.0899],
         [0.5677, 0.6400]],

        [[0.6550, 0.1037],
         [0.6825, 0.7151]],

        [[0.5493, 0.0958],
         [0.6303, 0.7218]],

        [[0.7145, 0.0988],
         [0.5078, 0.7300]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721128416103438
Global Adjusted Rand Index: 0.8387332280164854
Average Adjusted Rand Index: 0.8391063171852846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21278.223065086077
Iteration 100: Loss = -10956.959073674307
Iteration 200: Loss = -10956.398377423799
Iteration 300: Loss = -10956.253903834557
Iteration 400: Loss = -10956.183605762553
Iteration 500: Loss = -10956.130218001843
Iteration 600: Loss = -10956.080846761144
Iteration 700: Loss = -10956.029677040886
Iteration 800: Loss = -10955.96888253058
Iteration 900: Loss = -10955.887145368824
Iteration 1000: Loss = -10955.791722325417
Iteration 1100: Loss = -10955.715742222468
Iteration 1200: Loss = -10955.659484363427
Iteration 1300: Loss = -10955.609648046031
Iteration 1400: Loss = -10955.558155210236
Iteration 1500: Loss = -10955.499922006855
Iteration 1600: Loss = -10955.428699275664
Iteration 1700: Loss = -10955.329312217596
Iteration 1800: Loss = -10955.149755446606
Iteration 1900: Loss = -10954.215846169438
Iteration 2000: Loss = -10833.708248284725
Iteration 2100: Loss = -10829.585850375972
Iteration 2200: Loss = -10829.31479516346
Iteration 2300: Loss = -10829.243606696746
Iteration 2400: Loss = -10829.201535572709
Iteration 2500: Loss = -10829.170896756279
Iteration 2600: Loss = -10829.142033949442
Iteration 2700: Loss = -10829.127474008068
Iteration 2800: Loss = -10829.115475661509
Iteration 2900: Loss = -10829.10475216052
Iteration 3000: Loss = -10829.09423458958
Iteration 3100: Loss = -10829.081644283058
Iteration 3200: Loss = -10829.06528608723
Iteration 3300: Loss = -10829.05182317988
Iteration 3400: Loss = -10829.032244376895
Iteration 3500: Loss = -10828.998513015136
Iteration 3600: Loss = -10828.920348043863
Iteration 3700: Loss = -10828.688827791044
Iteration 3800: Loss = -10828.24276568073
Iteration 3900: Loss = -10825.880153373333
Iteration 4000: Loss = -10825.46913955294
Iteration 4100: Loss = -10825.455107017498
Iteration 4200: Loss = -10825.449237674009
Iteration 4300: Loss = -10825.44626408821
Iteration 4400: Loss = -10825.444316542154
Iteration 4500: Loss = -10825.443129560503
Iteration 4600: Loss = -10825.441892890114
Iteration 4700: Loss = -10825.44105976905
Iteration 4800: Loss = -10825.44027168823
Iteration 4900: Loss = -10825.439584124868
Iteration 5000: Loss = -10825.438972188023
Iteration 5100: Loss = -10825.438560824983
Iteration 5200: Loss = -10825.438172953442
Iteration 5300: Loss = -10825.43793869903
Iteration 5400: Loss = -10825.4376375343
Iteration 5500: Loss = -10825.43782797205
1
Iteration 5600: Loss = -10825.437248924685
Iteration 5700: Loss = -10825.437061851495
Iteration 5800: Loss = -10825.437048103719
Iteration 5900: Loss = -10825.444195210823
1
Iteration 6000: Loss = -10825.43665509632
Iteration 6100: Loss = -10825.436504735939
Iteration 6200: Loss = -10825.43766076439
1
Iteration 6300: Loss = -10825.437175472984
2
Iteration 6400: Loss = -10825.438046074793
3
Iteration 6500: Loss = -10825.449710299748
4
Iteration 6600: Loss = -10825.436031836856
Iteration 6700: Loss = -10825.436053200965
Iteration 6800: Loss = -10825.436615846407
1
Iteration 6900: Loss = -10825.435821884506
Iteration 7000: Loss = -10825.435754790475
Iteration 7100: Loss = -10825.435706693603
Iteration 7200: Loss = -10825.435580621208
Iteration 7300: Loss = -10825.435522456792
Iteration 7400: Loss = -10825.43577351951
1
Iteration 7500: Loss = -10825.435415912061
Iteration 7600: Loss = -10825.435381688674
Iteration 7700: Loss = -10825.435420673262
Iteration 7800: Loss = -10825.43537538442
Iteration 7900: Loss = -10825.435788067705
1
Iteration 8000: Loss = -10825.435418189758
Iteration 8100: Loss = -10825.435230439936
Iteration 8200: Loss = -10825.457745869831
1
Iteration 8300: Loss = -10825.435170713028
Iteration 8400: Loss = -10825.43606111696
1
Iteration 8500: Loss = -10825.435129743992
Iteration 8600: Loss = -10825.435750611734
1
Iteration 8700: Loss = -10825.453354373929
2
Iteration 8800: Loss = -10825.441097637324
3
Iteration 8900: Loss = -10825.435058697605
Iteration 9000: Loss = -10825.435580567773
1
Iteration 9100: Loss = -10825.435076920508
Iteration 9200: Loss = -10825.435101144694
Iteration 9300: Loss = -10825.438184795765
1
Iteration 9400: Loss = -10825.434991255515
Iteration 9500: Loss = -10825.435076701686
Iteration 9600: Loss = -10825.435045266991
Iteration 9700: Loss = -10825.43494411029
Iteration 9800: Loss = -10825.446184199025
1
Iteration 9900: Loss = -10825.434916733422
Iteration 10000: Loss = -10825.434882859508
Iteration 10100: Loss = -10825.443744851682
1
Iteration 10200: Loss = -10825.43488077122
Iteration 10300: Loss = -10825.434877258425
Iteration 10400: Loss = -10825.436629504728
1
Iteration 10500: Loss = -10825.434865628156
Iteration 10600: Loss = -10825.434852673257
Iteration 10700: Loss = -10825.44152427426
1
Iteration 10800: Loss = -10825.434830495313
Iteration 10900: Loss = -10825.434838038427
Iteration 11000: Loss = -10825.467966898275
1
Iteration 11100: Loss = -10825.43482028206
Iteration 11200: Loss = -10825.434810699287
Iteration 11300: Loss = -10825.442618732626
1
Iteration 11400: Loss = -10825.43479943177
Iteration 11500: Loss = -10825.434826194612
Iteration 11600: Loss = -10825.52370682849
1
Iteration 11700: Loss = -10825.434796908932
Iteration 11800: Loss = -10825.43467299748
Iteration 11900: Loss = -10825.446683043787
1
Iteration 12000: Loss = -10825.434356036412
Iteration 12100: Loss = -10825.434350965044
Iteration 12200: Loss = -10825.451567770047
1
Iteration 12300: Loss = -10825.43434027918
Iteration 12400: Loss = -10825.434367205155
Iteration 12500: Loss = -10825.470570983303
1
Iteration 12600: Loss = -10825.434305143717
Iteration 12700: Loss = -10825.434324258878
Iteration 12800: Loss = -10825.43503604406
1
Iteration 12900: Loss = -10825.434316546023
Iteration 13000: Loss = -10825.449750542342
1
Iteration 13100: Loss = -10825.434316314366
Iteration 13200: Loss = -10825.434268983594
Iteration 13300: Loss = -10825.434250630004
Iteration 13400: Loss = -10825.434373269181
1
Iteration 13500: Loss = -10825.434263599924
Iteration 13600: Loss = -10825.434291557409
Iteration 13700: Loss = -10825.436018062735
1
Iteration 13800: Loss = -10825.434289607398
Iteration 13900: Loss = -10825.435283597359
1
Iteration 14000: Loss = -10825.434279512217
Iteration 14100: Loss = -10825.52334531446
1
Iteration 14200: Loss = -10825.434256954728
Iteration 14300: Loss = -10825.43424682166
Iteration 14400: Loss = -10825.473039143786
1
Iteration 14500: Loss = -10825.434260587412
Iteration 14600: Loss = -10825.434281381213
Iteration 14700: Loss = -10825.43948522838
1
Iteration 14800: Loss = -10825.434250310318
Iteration 14900: Loss = -10825.434260829214
Iteration 15000: Loss = -10825.450523063872
1
Iteration 15100: Loss = -10825.434238373078
Iteration 15200: Loss = -10825.434252100395
Iteration 15300: Loss = -10825.462597762427
1
Iteration 15400: Loss = -10825.434248054446
Iteration 15500: Loss = -10825.434224582135
Iteration 15600: Loss = -10825.44163181466
1
Iteration 15700: Loss = -10825.434250674803
Iteration 15800: Loss = -10825.434248775056
Iteration 15900: Loss = -10825.435808107848
1
Iteration 16000: Loss = -10825.434231766816
Iteration 16100: Loss = -10825.434249512238
Iteration 16200: Loss = -10825.434440805931
1
Iteration 16300: Loss = -10825.434230805178
Iteration 16400: Loss = -10825.437300478978
1
Iteration 16500: Loss = -10825.434280085277
Iteration 16600: Loss = -10825.434242535328
Iteration 16700: Loss = -10825.434251855093
Iteration 16800: Loss = -10825.434284980134
Iteration 16900: Loss = -10825.43422313807
Iteration 17000: Loss = -10825.45634688924
1
Iteration 17100: Loss = -10825.434251522327
Iteration 17200: Loss = -10825.434276919405
Iteration 17300: Loss = -10825.44252970428
1
Iteration 17400: Loss = -10825.434235901188
Iteration 17500: Loss = -10825.434221946623
Iteration 17600: Loss = -10825.434881326399
1
Iteration 17700: Loss = -10825.434599252225
2
Iteration 17800: Loss = -10825.434558170153
3
Iteration 17900: Loss = -10825.470313636932
4
Iteration 18000: Loss = -10825.434245933964
Iteration 18100: Loss = -10825.435666055047
1
Iteration 18200: Loss = -10825.43422294545
Iteration 18300: Loss = -10825.44237239288
1
Iteration 18400: Loss = -10825.434248116433
Iteration 18500: Loss = -10825.43471004451
1
Iteration 18600: Loss = -10825.434297758893
Iteration 18700: Loss = -10825.434222070633
Iteration 18800: Loss = -10825.502923032385
1
Iteration 18900: Loss = -10825.43427482546
Iteration 19000: Loss = -10825.434215054256
Iteration 19100: Loss = -10825.434319344677
1
Iteration 19200: Loss = -10825.434300102861
Iteration 19300: Loss = -10825.434227040627
Iteration 19400: Loss = -10825.483841868514
1
Iteration 19500: Loss = -10825.434213566485
Iteration 19600: Loss = -10825.43420915517
Iteration 19700: Loss = -10825.497503142129
1
Iteration 19800: Loss = -10825.434208116561
Iteration 19900: Loss = -10825.434201082007
pi: tensor([[0.6466, 0.3534],
        [0.2439, 0.7561]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8631, 0.1369], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1870, 0.0952],
         [0.5413, 0.2555]],

        [[0.6801, 0.0911],
         [0.5417, 0.5531]],

        [[0.6442, 0.1039],
         [0.5033, 0.6262]],

        [[0.7002, 0.0946],
         [0.7286, 0.6139]],

        [[0.5448, 0.0988],
         [0.6184, 0.6718]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 33
Adjusted Rand Index: 0.10666666666666667
time is 1
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721016799725718
Global Adjusted Rand Index: 0.45048121631352284
Average Adjusted Rand Index: 0.7058509193947305
10842.897439165668
[0.8387332280164854, 0.45048121631352284] [0.8391063171852846, 0.7058509193947305] [10788.438464972662, 10825.4675646801]
-------------------------------------
This iteration is 64
True Objective function: Loss = -10707.716467245691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21783.62066590356
Iteration 100: Loss = -10780.906434528559
Iteration 200: Loss = -10780.333449916283
Iteration 300: Loss = -10780.206240281032
Iteration 400: Loss = -10780.133046528674
Iteration 500: Loss = -10780.06772088357
Iteration 600: Loss = -10779.995624913765
Iteration 700: Loss = -10779.945578273027
Iteration 800: Loss = -10779.927075799269
Iteration 900: Loss = -10779.91284792245
Iteration 1000: Loss = -10779.899517956463
Iteration 1100: Loss = -10779.886715468654
Iteration 1200: Loss = -10779.873960679068
Iteration 1300: Loss = -10779.85998581776
Iteration 1400: Loss = -10779.842549996805
Iteration 1500: Loss = -10779.816764988742
Iteration 1600: Loss = -10779.77055065335
Iteration 1700: Loss = -10779.680513593054
Iteration 1800: Loss = -10779.593894425672
Iteration 1900: Loss = -10779.547646477371
Iteration 2000: Loss = -10779.504244976615
Iteration 2100: Loss = -10779.463903694654
Iteration 2200: Loss = -10779.431606516458
Iteration 2300: Loss = -10779.40884413563
Iteration 2400: Loss = -10779.394853273625
Iteration 2500: Loss = -10779.386312235794
Iteration 2600: Loss = -10779.380683210129
Iteration 2700: Loss = -10779.376868418258
Iteration 2800: Loss = -10779.374120688166
Iteration 2900: Loss = -10779.372189256615
Iteration 3000: Loss = -10779.370818844693
Iteration 3100: Loss = -10779.369764275252
Iteration 3200: Loss = -10779.369084355572
Iteration 3300: Loss = -10779.36853546054
Iteration 3400: Loss = -10779.368157465171
Iteration 3500: Loss = -10779.36787720145
Iteration 3600: Loss = -10779.367652170753
Iteration 3700: Loss = -10779.367556756564
Iteration 3800: Loss = -10779.367420886778
Iteration 3900: Loss = -10779.367329043858
Iteration 4000: Loss = -10779.367298001414
Iteration 4100: Loss = -10779.367253604543
Iteration 4200: Loss = -10779.367167529714
Iteration 4300: Loss = -10779.36712281807
Iteration 4400: Loss = -10779.367079330079
Iteration 4500: Loss = -10779.367087095632
Iteration 4600: Loss = -10779.367172124956
Iteration 4700: Loss = -10779.367009579879
Iteration 4800: Loss = -10779.366990576542
Iteration 4900: Loss = -10779.366955934951
Iteration 5000: Loss = -10779.366921782845
Iteration 5100: Loss = -10779.3669297507
Iteration 5200: Loss = -10779.366865120592
Iteration 5300: Loss = -10779.369114351444
1
Iteration 5400: Loss = -10779.366819109962
Iteration 5500: Loss = -10779.371115505577
1
Iteration 5600: Loss = -10779.367030880578
2
Iteration 5700: Loss = -10779.366962830381
3
Iteration 5800: Loss = -10779.366657322456
Iteration 5900: Loss = -10779.366757989843
1
Iteration 6000: Loss = -10779.366583582378
Iteration 6100: Loss = -10779.36664021087
Iteration 6200: Loss = -10779.366511542594
Iteration 6300: Loss = -10779.366510349319
Iteration 6400: Loss = -10779.366827977048
1
Iteration 6500: Loss = -10779.366446807217
Iteration 6600: Loss = -10779.36750013434
1
Iteration 6700: Loss = -10779.367107396944
2
Iteration 6800: Loss = -10779.366475107394
Iteration 6900: Loss = -10779.366338222775
Iteration 7000: Loss = -10779.366619939789
1
Iteration 7100: Loss = -10779.366267593245
Iteration 7200: Loss = -10779.370649091628
1
Iteration 7300: Loss = -10779.368240831896
2
Iteration 7400: Loss = -10779.369413977178
3
Iteration 7500: Loss = -10779.36655368561
4
Iteration 7600: Loss = -10779.366273409103
Iteration 7700: Loss = -10779.366841604711
1
Iteration 7800: Loss = -10779.375109484146
2
Iteration 7900: Loss = -10779.366409462307
3
Iteration 8000: Loss = -10779.37705452784
4
Iteration 8100: Loss = -10779.366113133352
Iteration 8200: Loss = -10779.36636212187
1
Iteration 8300: Loss = -10779.375527835029
2
Iteration 8400: Loss = -10779.366088934334
Iteration 8500: Loss = -10779.366081911117
Iteration 8600: Loss = -10779.380243406826
1
Iteration 8700: Loss = -10779.366023425142
Iteration 8800: Loss = -10779.395774270566
1
Iteration 8900: Loss = -10779.36595737692
Iteration 9000: Loss = -10779.3857454805
1
Iteration 9100: Loss = -10779.365978117621
Iteration 9200: Loss = -10779.36593982417
Iteration 9300: Loss = -10779.366585875772
1
Iteration 9400: Loss = -10779.365954149918
Iteration 9500: Loss = -10779.365956038408
Iteration 9600: Loss = -10779.422609108686
1
Iteration 9700: Loss = -10779.3749580162
2
Iteration 9800: Loss = -10779.371032763827
3
Iteration 9900: Loss = -10779.411398917628
4
Iteration 10000: Loss = -10779.36717917053
5
Iteration 10100: Loss = -10779.366257291036
6
Iteration 10200: Loss = -10779.370088572425
7
Iteration 10300: Loss = -10779.43856796802
8
Iteration 10400: Loss = -10779.365946712229
Iteration 10500: Loss = -10779.371057895338
1
Iteration 10600: Loss = -10779.365874560883
Iteration 10700: Loss = -10779.366202290696
1
Iteration 10800: Loss = -10779.366973269465
2
Iteration 10900: Loss = -10779.365885172578
Iteration 11000: Loss = -10779.371618095352
1
Iteration 11100: Loss = -10779.365826302297
Iteration 11200: Loss = -10779.367555401748
1
Iteration 11300: Loss = -10779.36584208473
Iteration 11400: Loss = -10779.366354673797
1
Iteration 11500: Loss = -10779.36590419101
Iteration 11600: Loss = -10779.366098553879
1
Iteration 11700: Loss = -10779.616897158401
2
Iteration 11800: Loss = -10779.365785909968
Iteration 11900: Loss = -10779.371071028447
1
Iteration 12000: Loss = -10779.379903037747
2
Iteration 12100: Loss = -10779.405136311754
3
Iteration 12200: Loss = -10779.366069370193
4
Iteration 12300: Loss = -10779.36784578995
5
Iteration 12400: Loss = -10779.366826711712
6
Iteration 12500: Loss = -10779.365988761278
7
Iteration 12600: Loss = -10779.366201149747
8
Iteration 12700: Loss = -10779.367293648676
9
Iteration 12800: Loss = -10779.367232382874
10
Iteration 12900: Loss = -10779.40042948506
11
Iteration 13000: Loss = -10779.365851050998
Iteration 13100: Loss = -10779.365853621573
Iteration 13200: Loss = -10779.38264673065
1
Iteration 13300: Loss = -10779.365796327085
Iteration 13400: Loss = -10779.367502209412
1
Iteration 13500: Loss = -10779.36693000049
2
Iteration 13600: Loss = -10779.370478730214
3
Iteration 13700: Loss = -10779.383334677634
4
Iteration 13800: Loss = -10779.400813539369
5
Iteration 13900: Loss = -10779.365765318636
Iteration 14000: Loss = -10779.367029748088
1
Iteration 14100: Loss = -10779.365761614
Iteration 14200: Loss = -10779.365840019487
Iteration 14300: Loss = -10779.365775393002
Iteration 14400: Loss = -10779.36592698179
1
Iteration 14500: Loss = -10779.366593340512
2
Iteration 14600: Loss = -10779.365819294953
Iteration 14700: Loss = -10779.370573794762
1
Iteration 14800: Loss = -10779.366283797703
2
Iteration 14900: Loss = -10779.366616704268
3
Iteration 15000: Loss = -10779.365823244007
Iteration 15100: Loss = -10779.410147927198
1
Iteration 15200: Loss = -10779.377406621192
2
Iteration 15300: Loss = -10779.365771315863
Iteration 15400: Loss = -10779.366346117215
1
Iteration 15500: Loss = -10779.36771868609
2
Iteration 15600: Loss = -10779.384321271482
3
Iteration 15700: Loss = -10779.365752511621
Iteration 15800: Loss = -10779.387715761215
1
Iteration 15900: Loss = -10779.365734715631
Iteration 16000: Loss = -10779.366174477087
1
Iteration 16100: Loss = -10779.373028025682
2
Iteration 16200: Loss = -10779.375015735284
3
Iteration 16300: Loss = -10779.36601714683
4
Iteration 16400: Loss = -10779.366583053918
5
Iteration 16500: Loss = -10779.367244376519
6
Iteration 16600: Loss = -10779.372509367044
7
Iteration 16700: Loss = -10779.366025328813
8
Iteration 16800: Loss = -10779.365986172921
9
Iteration 16900: Loss = -10779.367624107648
10
Iteration 17000: Loss = -10779.374243562825
11
Iteration 17100: Loss = -10779.365786056203
Iteration 17200: Loss = -10779.368215548497
1
Iteration 17300: Loss = -10779.369301413337
2
Iteration 17400: Loss = -10779.36575871064
Iteration 17500: Loss = -10779.366573073807
1
Iteration 17600: Loss = -10779.365797889712
Iteration 17700: Loss = -10779.3661548402
1
Iteration 17800: Loss = -10779.402600822927
2
Iteration 17900: Loss = -10779.36575607454
Iteration 18000: Loss = -10779.385637627192
1
Iteration 18100: Loss = -10779.3748656256
2
Iteration 18200: Loss = -10779.36619672045
3
Iteration 18300: Loss = -10779.366916500674
4
Iteration 18400: Loss = -10779.365790848173
Iteration 18500: Loss = -10779.394638914444
1
Iteration 18600: Loss = -10779.581209245016
2
Iteration 18700: Loss = -10779.375606796697
3
Iteration 18800: Loss = -10779.367460680798
4
Iteration 18900: Loss = -10779.369887370647
5
Iteration 19000: Loss = -10779.366602580376
6
Iteration 19100: Loss = -10779.365883080884
Iteration 19200: Loss = -10779.366289096588
1
Iteration 19300: Loss = -10779.37487973231
2
Iteration 19400: Loss = -10779.36641205525
3
Iteration 19500: Loss = -10779.365754796978
Iteration 19600: Loss = -10779.374867497521
1
Iteration 19700: Loss = -10779.38631078775
2
Iteration 19800: Loss = -10779.366384516361
3
Iteration 19900: Loss = -10779.36877107008
4
pi: tensor([[5.1251e-06, 9.9999e-01],
        [2.7123e-02, 9.7288e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3348, 0.6652], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1687, 0.1629],
         [0.5804, 0.1572]],

        [[0.6718, 0.1255],
         [0.5318, 0.6408]],

        [[0.5278, 0.1952],
         [0.7246, 0.6002]],

        [[0.7055, 0.1590],
         [0.5647, 0.6508]],

        [[0.6440, 0.1009],
         [0.5588, 0.6923]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24094.931313154473
Iteration 100: Loss = -10781.23987074493
Iteration 200: Loss = -10780.464953496064
Iteration 300: Loss = -10780.328847042998
Iteration 400: Loss = -10780.262488179327
Iteration 500: Loss = -10780.222663041717
Iteration 600: Loss = -10780.195701162116
Iteration 700: Loss = -10780.175915919024
Iteration 800: Loss = -10780.160552317706
Iteration 900: Loss = -10780.148165997469
Iteration 1000: Loss = -10780.137736819714
Iteration 1100: Loss = -10780.128764911393
Iteration 1200: Loss = -10780.120753605539
Iteration 1300: Loss = -10780.113335722646
Iteration 1400: Loss = -10780.106180154466
Iteration 1500: Loss = -10780.09896876658
Iteration 1600: Loss = -10780.091293872296
Iteration 1700: Loss = -10780.082815957305
Iteration 1800: Loss = -10780.072263037348
Iteration 1900: Loss = -10780.047932945557
Iteration 2000: Loss = -10778.235893351131
Iteration 2100: Loss = -10776.968940404326
Iteration 2200: Loss = -10775.69742823273
Iteration 2300: Loss = -10775.145044948455
Iteration 2400: Loss = -10774.412057149357
Iteration 2500: Loss = -10774.089125164468
Iteration 2600: Loss = -10773.976365286291
Iteration 2700: Loss = -10773.918900777
Iteration 2800: Loss = -10773.884117203377
Iteration 2900: Loss = -10773.860878341964
Iteration 3000: Loss = -10773.844354455732
Iteration 3100: Loss = -10773.832078487667
Iteration 3200: Loss = -10773.822667443492
Iteration 3300: Loss = -10773.81518695733
Iteration 3400: Loss = -10773.809192250961
Iteration 3500: Loss = -10773.80424421088
Iteration 3600: Loss = -10773.80008708486
Iteration 3700: Loss = -10773.796541315685
Iteration 3800: Loss = -10773.793402531664
Iteration 3900: Loss = -10773.79035753568
Iteration 4000: Loss = -10773.785999315545
Iteration 4100: Loss = -10773.782047894547
Iteration 4200: Loss = -10773.77995185819
Iteration 4300: Loss = -10773.778296107703
Iteration 4400: Loss = -10773.776778172336
Iteration 4500: Loss = -10773.77555237525
Iteration 4600: Loss = -10773.774325521856
Iteration 4700: Loss = -10773.773271202219
Iteration 4800: Loss = -10773.772302441821
Iteration 4900: Loss = -10773.77140716665
Iteration 5000: Loss = -10773.77057957011
Iteration 5100: Loss = -10773.769820249829
Iteration 5200: Loss = -10773.769102456223
Iteration 5300: Loss = -10773.76845973437
Iteration 5400: Loss = -10773.767838906395
Iteration 5500: Loss = -10773.7672687397
Iteration 5600: Loss = -10773.766682555039
Iteration 5700: Loss = -10773.766187933263
Iteration 5800: Loss = -10773.765733104956
Iteration 5900: Loss = -10773.765243098424
Iteration 6000: Loss = -10773.764728684318
Iteration 6100: Loss = -10773.764332003118
Iteration 6200: Loss = -10773.763942010497
Iteration 6300: Loss = -10773.763475856174
Iteration 6400: Loss = -10773.763131804792
Iteration 6500: Loss = -10773.762692140894
Iteration 6600: Loss = -10773.76229425886
Iteration 6700: Loss = -10773.761872901574
Iteration 6800: Loss = -10773.761513400867
Iteration 6900: Loss = -10773.76108531754
Iteration 7000: Loss = -10773.760694301236
Iteration 7100: Loss = -10773.76029619061
Iteration 7200: Loss = -10773.759880956444
Iteration 7300: Loss = -10773.759490381746
Iteration 7400: Loss = -10773.75902269971
Iteration 7500: Loss = -10773.75858273912
Iteration 7600: Loss = -10773.758111698933
Iteration 7700: Loss = -10773.757617275684
Iteration 7800: Loss = -10773.757149476914
Iteration 7900: Loss = -10773.756627555093
Iteration 8000: Loss = -10773.75609118637
Iteration 8100: Loss = -10773.755550576043
Iteration 8200: Loss = -10773.754961846793
Iteration 8300: Loss = -10773.754386838596
Iteration 8400: Loss = -10773.753721345278
Iteration 8500: Loss = -10773.7531191084
Iteration 8600: Loss = -10773.752485426157
Iteration 8700: Loss = -10773.867631162118
1
Iteration 8800: Loss = -10773.751115626954
Iteration 8900: Loss = -10773.750384616502
Iteration 9000: Loss = -10773.749672847794
Iteration 9100: Loss = -10773.796174960884
1
Iteration 9200: Loss = -10773.748296925321
Iteration 9300: Loss = -10773.747526779121
Iteration 9400: Loss = -10773.74683482801
Iteration 9500: Loss = -10773.900745523939
1
Iteration 9600: Loss = -10773.74546927266
Iteration 9700: Loss = -10773.744800058019
Iteration 9800: Loss = -10773.744181845434
Iteration 9900: Loss = -10773.74353737273
Iteration 10000: Loss = -10773.743198107179
Iteration 10100: Loss = -10773.742411320116
Iteration 10200: Loss = -10773.741886960232
Iteration 10300: Loss = -10773.75451838077
1
Iteration 10400: Loss = -10773.74090161858
Iteration 10500: Loss = -10773.740412189758
Iteration 10600: Loss = -10773.740017973189
Iteration 10700: Loss = -10773.780860543146
1
Iteration 10800: Loss = -10773.73925251719
Iteration 10900: Loss = -10773.738901199666
Iteration 11000: Loss = -10773.73862689361
Iteration 11100: Loss = -10773.741415502971
1
Iteration 11200: Loss = -10773.738019695717
Iteration 11300: Loss = -10773.737773109002
Iteration 11400: Loss = -10773.79437531724
1
Iteration 11500: Loss = -10773.7373301885
Iteration 11600: Loss = -10773.737115762315
Iteration 11700: Loss = -10773.736904615136
Iteration 11800: Loss = -10773.848378543113
1
Iteration 11900: Loss = -10773.736557262708
Iteration 12000: Loss = -10773.736407518356
Iteration 12100: Loss = -10773.736245923745
Iteration 12200: Loss = -10773.754407588067
1
Iteration 12300: Loss = -10773.735981877386
Iteration 12400: Loss = -10773.735857917405
Iteration 12500: Loss = -10773.735746372044
Iteration 12600: Loss = -10773.73689026588
1
Iteration 12700: Loss = -10773.735565124958
Iteration 12800: Loss = -10773.735436233126
Iteration 12900: Loss = -10773.73537906586
Iteration 13000: Loss = -10773.735445634977
Iteration 13100: Loss = -10773.735241690481
Iteration 13200: Loss = -10773.735172701943
Iteration 13300: Loss = -10773.779784505627
1
Iteration 13400: Loss = -10773.735051081414
Iteration 13500: Loss = -10773.73501945422
Iteration 13600: Loss = -10773.734956347573
Iteration 13700: Loss = -10773.734980267756
Iteration 13800: Loss = -10773.734944375696
Iteration 13900: Loss = -10773.7348198624
Iteration 14000: Loss = -10773.73475462452
Iteration 14100: Loss = -10773.925776012078
1
Iteration 14200: Loss = -10773.734715023174
Iteration 14300: Loss = -10773.734638874288
Iteration 14400: Loss = -10773.734618817198
Iteration 14500: Loss = -10773.819044731774
1
Iteration 14600: Loss = -10773.734567119109
Iteration 14700: Loss = -10773.734519477586
Iteration 14800: Loss = -10773.734473777811
Iteration 14900: Loss = -10773.742563952615
1
Iteration 15000: Loss = -10773.73442219162
Iteration 15100: Loss = -10773.734397497734
Iteration 15200: Loss = -10773.734395907475
Iteration 15300: Loss = -10773.736062750275
1
Iteration 15400: Loss = -10773.734350361121
Iteration 15500: Loss = -10773.7343319607
Iteration 15600: Loss = -10773.772450373337
1
Iteration 15700: Loss = -10773.734287857014
Iteration 15800: Loss = -10773.757054165446
1
Iteration 15900: Loss = -10773.734240653836
Iteration 16000: Loss = -10773.734227148992
Iteration 16100: Loss = -10773.734367027628
1
Iteration 16200: Loss = -10773.734203249192
Iteration 16300: Loss = -10773.734194228819
Iteration 16400: Loss = -10773.73441238652
1
Iteration 16500: Loss = -10773.734164677304
Iteration 16600: Loss = -10773.734168049876
Iteration 16700: Loss = -10773.753840404883
1
Iteration 16800: Loss = -10773.73416868129
Iteration 16900: Loss = -10773.734158801723
Iteration 17000: Loss = -10774.047925841553
1
Iteration 17100: Loss = -10773.734168129504
Iteration 17200: Loss = -10773.734141782435
Iteration 17300: Loss = -10773.7579624161
1
Iteration 17400: Loss = -10773.734132655503
Iteration 17500: Loss = -10773.734108342764
Iteration 17600: Loss = -10773.73410270298
Iteration 17700: Loss = -10773.735517261444
1
Iteration 17800: Loss = -10773.734035826288
Iteration 17900: Loss = -10773.734241082306
1
Iteration 18000: Loss = -10773.734175838465
2
Iteration 18100: Loss = -10773.736783918297
3
Iteration 18200: Loss = -10773.733990620185
Iteration 18300: Loss = -10773.733990475448
Iteration 18400: Loss = -10773.734176151145
1
Iteration 18500: Loss = -10773.734022343928
Iteration 18600: Loss = -10773.794900126439
1
Iteration 18700: Loss = -10773.733953912313
Iteration 18800: Loss = -10773.76038751108
1
Iteration 18900: Loss = -10773.733976490335
Iteration 19000: Loss = -10773.734626319336
1
Iteration 19100: Loss = -10773.733964995232
Iteration 19200: Loss = -10773.734875823751
1
Iteration 19300: Loss = -10773.733947442508
Iteration 19400: Loss = -10773.733950430957
Iteration 19500: Loss = -10773.734246750502
1
Iteration 19600: Loss = -10773.733941654971
Iteration 19700: Loss = -10773.73393468832
Iteration 19800: Loss = -10773.826416543714
1
Iteration 19900: Loss = -10773.733943500423
pi: tensor([[1.0000e+00, 1.0446e-08],
        [1.4030e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9896, 0.0104], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.5781e-01, 3.1581e-02],
         [5.3857e-01, 1.3561e-04]],

        [[5.1511e-01, 2.4774e-01],
         [5.0272e-01, 5.5288e-01]],

        [[5.9534e-01, 1.4236e-01],
         [5.6817e-01, 6.1587e-01]],

        [[6.9900e-01, 1.2976e-01],
         [6.4384e-01, 5.0399e-01]],

        [[6.5504e-01, 1.7227e-01],
         [6.4430e-01, 6.2701e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: -0.0007688900089874237
Average Adjusted Rand Index: -0.0008407014075915633
10707.716467245691
[0.0, -0.0007688900089874237] [0.0, -0.0008407014075915633] [10779.366312569538, 10773.733945423424]
-------------------------------------
This iteration is 65
True Objective function: Loss = -10984.094335361695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20292.049826133818
Iteration 100: Loss = -11023.040074903813
Iteration 200: Loss = -11022.154954806225
Iteration 300: Loss = -11021.91268163197
Iteration 400: Loss = -11021.79469614117
Iteration 500: Loss = -11021.708801336363
Iteration 600: Loss = -11021.607712546816
Iteration 700: Loss = -11021.409372370155
Iteration 800: Loss = -11020.942744770515
Iteration 900: Loss = -11019.526493956684
Iteration 1000: Loss = -11018.142639882879
Iteration 1100: Loss = -11015.191877486552
Iteration 1200: Loss = -11014.589795777556
Iteration 1300: Loss = -11014.315079889442
Iteration 1400: Loss = -11013.983929798578
Iteration 1500: Loss = -11013.625545184703
Iteration 1600: Loss = -11013.433389220434
Iteration 1700: Loss = -11013.314858582677
Iteration 1800: Loss = -11013.188732864894
Iteration 1900: Loss = -10982.835000875704
Iteration 2000: Loss = -10975.205498360054
Iteration 2100: Loss = -10974.077162736201
Iteration 2200: Loss = -10973.423199402601
Iteration 2300: Loss = -10972.492686450918
Iteration 2400: Loss = -10958.00922813793
Iteration 2500: Loss = -10925.422640375054
Iteration 2600: Loss = -10916.351045384677
Iteration 2700: Loss = -10909.928444289664
Iteration 2800: Loss = -10909.342289475187
Iteration 2900: Loss = -10909.141150492474
Iteration 3000: Loss = -10909.068410279133
Iteration 3100: Loss = -10909.057286549954
Iteration 3200: Loss = -10909.052354530193
Iteration 3300: Loss = -10909.04630721324
Iteration 3400: Loss = -10909.043373864282
Iteration 3500: Loss = -10909.039490289993
Iteration 3600: Loss = -10909.037547876265
Iteration 3700: Loss = -10909.035389280092
Iteration 3800: Loss = -10909.033758323158
Iteration 3900: Loss = -10909.03943873741
1
Iteration 4000: Loss = -10909.03219433649
Iteration 4100: Loss = -10909.028098935994
Iteration 4200: Loss = -10909.02568579038
Iteration 4300: Loss = -10909.003240322612
Iteration 4400: Loss = -10909.002381849967
Iteration 4500: Loss = -10909.001975707904
Iteration 4600: Loss = -10909.001663855755
Iteration 4700: Loss = -10909.002077748188
1
Iteration 4800: Loss = -10909.000439037587
Iteration 4900: Loss = -10909.001030864396
1
Iteration 5000: Loss = -10908.999377516462
Iteration 5100: Loss = -10908.997629531395
Iteration 5200: Loss = -10908.994519944195
Iteration 5300: Loss = -10908.990906892253
Iteration 5400: Loss = -10908.99095858958
Iteration 5500: Loss = -10908.987303535394
Iteration 5600: Loss = -10908.984468274542
Iteration 5700: Loss = -10908.981640210746
Iteration 5800: Loss = -10908.917274951964
Iteration 5900: Loss = -10908.916321190953
Iteration 6000: Loss = -10908.913578702686
Iteration 6100: Loss = -10908.91008035013
Iteration 6200: Loss = -10908.909332437199
Iteration 6300: Loss = -10908.910194795928
1
Iteration 6400: Loss = -10908.893350923701
Iteration 6500: Loss = -10908.892833558888
Iteration 6600: Loss = -10908.892029275112
Iteration 6700: Loss = -10908.89171519372
Iteration 6800: Loss = -10908.909463225784
1
Iteration 6900: Loss = -10908.879966409722
Iteration 7000: Loss = -10908.877813399395
Iteration 7100: Loss = -10908.879712733293
1
Iteration 7200: Loss = -10908.877561665371
Iteration 7300: Loss = -10908.877428747084
Iteration 7400: Loss = -10908.877682629334
1
Iteration 7500: Loss = -10908.876666736098
Iteration 7600: Loss = -10908.876429346468
Iteration 7700: Loss = -10908.877185141288
1
Iteration 7800: Loss = -10908.875942558374
Iteration 7900: Loss = -10908.87561126866
Iteration 8000: Loss = -10908.875367804503
Iteration 8100: Loss = -10908.87580807266
1
Iteration 8200: Loss = -10908.874922432542
Iteration 8300: Loss = -10908.873418834
Iteration 8400: Loss = -10908.871048386338
Iteration 8500: Loss = -10908.870781829359
Iteration 8600: Loss = -10908.868867834593
Iteration 8700: Loss = -10908.873616505338
1
Iteration 8800: Loss = -10908.865645697086
Iteration 8900: Loss = -10908.865204911945
Iteration 9000: Loss = -10908.865591427031
1
Iteration 9100: Loss = -10908.86479199428
Iteration 9200: Loss = -10908.863100342545
Iteration 9300: Loss = -10908.862927235754
Iteration 9400: Loss = -10908.86283017183
Iteration 9500: Loss = -10908.865162794158
1
Iteration 9600: Loss = -10908.864909931599
2
Iteration 9700: Loss = -10908.851508370582
Iteration 9800: Loss = -10908.851301567565
Iteration 9900: Loss = -10908.898486962236
1
Iteration 10000: Loss = -10908.851131184789
Iteration 10100: Loss = -10908.85524530251
1
Iteration 10200: Loss = -10908.843648875509
Iteration 10300: Loss = -10908.907576481784
1
Iteration 10400: Loss = -10908.855283353188
2
Iteration 10500: Loss = -10908.9015725634
3
Iteration 10600: Loss = -10908.843027974654
Iteration 10700: Loss = -10908.842938702099
Iteration 10800: Loss = -10908.853870529372
1
Iteration 10900: Loss = -10908.842378304653
Iteration 11000: Loss = -10908.842336936214
Iteration 11100: Loss = -10908.93386990174
1
Iteration 11200: Loss = -10908.848160268126
2
Iteration 11300: Loss = -10908.841125233956
Iteration 11400: Loss = -10908.865765145207
1
Iteration 11500: Loss = -10908.849179597582
2
Iteration 11600: Loss = -10908.855072315953
3
Iteration 11700: Loss = -10908.874459523842
4
Iteration 11800: Loss = -10908.840941145
Iteration 11900: Loss = -10908.841201064773
1
Iteration 12000: Loss = -10908.840617311896
Iteration 12100: Loss = -10908.84072446276
1
Iteration 12200: Loss = -10908.84081417603
2
Iteration 12300: Loss = -10908.840630319122
Iteration 12400: Loss = -10908.86619291186
1
Iteration 12500: Loss = -10908.840571738005
Iteration 12600: Loss = -10908.840835249697
1
Iteration 12700: Loss = -10908.847432978333
2
Iteration 12800: Loss = -10908.848163238923
3
Iteration 12900: Loss = -10908.847566911725
4
Iteration 13000: Loss = -10908.850147213516
5
Iteration 13100: Loss = -10908.839773984433
Iteration 13200: Loss = -10908.840274453543
1
Iteration 13300: Loss = -10908.840553721215
2
Iteration 13400: Loss = -10908.850116706395
3
Iteration 13500: Loss = -10908.839823427716
Iteration 13600: Loss = -10908.839582986282
Iteration 13700: Loss = -10908.788184490573
Iteration 13800: Loss = -10908.78193733191
Iteration 13900: Loss = -10908.78589320383
1
Iteration 14000: Loss = -10908.785634728736
2
Iteration 14100: Loss = -10908.780918001332
Iteration 14200: Loss = -10908.801744862563
1
Iteration 14300: Loss = -10908.795017806324
2
Iteration 14400: Loss = -10908.862106855404
3
Iteration 14500: Loss = -10908.782567309041
4
Iteration 14600: Loss = -10908.78059884392
Iteration 14700: Loss = -10908.78240908689
1
Iteration 14800: Loss = -10908.78051688082
Iteration 14900: Loss = -10908.783131628954
1
Iteration 15000: Loss = -10908.78245695767
2
Iteration 15100: Loss = -10908.788148612744
3
Iteration 15200: Loss = -10908.780118389677
Iteration 15300: Loss = -10908.77943333403
Iteration 15400: Loss = -10908.778643745562
Iteration 15500: Loss = -10908.779080877417
1
Iteration 15600: Loss = -10908.87393940723
2
Iteration 15700: Loss = -10908.770677002016
Iteration 15800: Loss = -10908.772750349955
1
Iteration 15900: Loss = -10908.771805810318
2
Iteration 16000: Loss = -10908.77587598995
3
Iteration 16100: Loss = -10908.789884411615
4
Iteration 16200: Loss = -10908.769842815696
Iteration 16300: Loss = -10908.7704026832
1
Iteration 16400: Loss = -10908.768701878873
Iteration 16500: Loss = -10908.78143190111
1
Iteration 16600: Loss = -10908.772743486208
2
Iteration 16700: Loss = -10908.770683686895
3
Iteration 16800: Loss = -10908.768395117966
Iteration 16900: Loss = -10908.775643030836
1
Iteration 17000: Loss = -10908.7670149442
Iteration 17100: Loss = -10908.766996622826
Iteration 17200: Loss = -10908.766864243427
Iteration 17300: Loss = -10908.767046365647
1
Iteration 17400: Loss = -10908.7668720653
Iteration 17500: Loss = -10908.769315878204
1
Iteration 17600: Loss = -10908.767741321784
2
Iteration 17700: Loss = -10908.76678742325
Iteration 17800: Loss = -10908.774680127392
1
Iteration 17900: Loss = -10908.79209300215
2
Iteration 18000: Loss = -10908.76632306637
Iteration 18100: Loss = -10908.766610943872
1
Iteration 18200: Loss = -10908.772736217166
2
Iteration 18300: Loss = -10908.767999114883
3
Iteration 18400: Loss = -10908.766630917788
4
Iteration 18500: Loss = -10908.766824485965
5
Iteration 18600: Loss = -10908.774939052464
6
Iteration 18700: Loss = -10908.766205270336
Iteration 18800: Loss = -10908.766551010187
1
Iteration 18900: Loss = -10908.879366742995
2
Iteration 19000: Loss = -10908.817314616903
3
Iteration 19100: Loss = -10908.779860920935
4
Iteration 19200: Loss = -10908.767771169234
5
Iteration 19300: Loss = -10908.765906310768
Iteration 19400: Loss = -10908.767753054362
1
Iteration 19500: Loss = -10908.770218109947
2
Iteration 19600: Loss = -10908.76644796257
3
Iteration 19700: Loss = -10908.76620739653
4
Iteration 19800: Loss = -10908.768057934
5
Iteration 19900: Loss = -10908.841337924547
6
pi: tensor([[0.7764, 0.2236],
        [0.2456, 0.7544]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4990, 0.5010], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2064, 0.0961],
         [0.7037, 0.2423]],

        [[0.7137, 0.1066],
         [0.5852, 0.7293]],

        [[0.6692, 0.1135],
         [0.7206, 0.6969]],

        [[0.6898, 0.1053],
         [0.6669, 0.5010]],

        [[0.5701, 0.0935],
         [0.6314, 0.6644]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369432436752338
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 85
Adjusted Rand Index: 0.48482022644156053
time is 3
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7024841981478759
Global Adjusted Rand Index: 0.7185403803673669
Average Adjusted Rand Index: 0.7227833498038088
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21708.33896197475
Iteration 100: Loss = -11021.630347882057
Iteration 200: Loss = -11019.859415958586
Iteration 300: Loss = -11017.841829652873
Iteration 400: Loss = -11017.289817232266
Iteration 500: Loss = -11016.79016590391
Iteration 600: Loss = -11016.328232425147
Iteration 700: Loss = -11016.13255685093
Iteration 800: Loss = -11015.953688362584
Iteration 900: Loss = -11015.726821016922
Iteration 1000: Loss = -11014.988450467976
Iteration 1100: Loss = -11014.714612040263
Iteration 1200: Loss = -11014.617113241631
Iteration 1300: Loss = -11014.57969097444
Iteration 1400: Loss = -11014.548324341002
Iteration 1500: Loss = -11014.507033569045
Iteration 1600: Loss = -11014.480498285553
Iteration 1700: Loss = -11014.467254508823
Iteration 1800: Loss = -11014.45087261037
Iteration 1900: Loss = -11014.434899189419
Iteration 2000: Loss = -11014.42814182579
Iteration 2100: Loss = -11014.421907452972
Iteration 2200: Loss = -11014.415762316985
Iteration 2300: Loss = -11014.40929337222
Iteration 2400: Loss = -11014.40177315209
Iteration 2500: Loss = -11014.39446940513
Iteration 2600: Loss = -11014.387152687112
Iteration 2700: Loss = -11014.379071987398
Iteration 2800: Loss = -11014.370235040788
Iteration 2900: Loss = -11014.360767464223
Iteration 3000: Loss = -11014.350538650977
Iteration 3100: Loss = -11014.339667835848
Iteration 3200: Loss = -11014.328361971531
Iteration 3300: Loss = -11014.31684650453
Iteration 3400: Loss = -11014.305462262266
Iteration 3500: Loss = -11014.294524528774
Iteration 3600: Loss = -11014.284187031433
Iteration 3700: Loss = -11014.27485032409
Iteration 3800: Loss = -11014.266549265247
Iteration 3900: Loss = -11014.259282274617
Iteration 4000: Loss = -11014.253110738431
Iteration 4100: Loss = -11014.247937698472
Iteration 4200: Loss = -11014.243561016297
Iteration 4300: Loss = -11014.239840412736
Iteration 4400: Loss = -11014.236275603887
Iteration 4500: Loss = -11014.231726618907
Iteration 4600: Loss = -11014.224200474933
Iteration 4700: Loss = -11014.222151058631
Iteration 4800: Loss = -11014.22115661882
Iteration 4900: Loss = -11014.220454285778
Iteration 5000: Loss = -11014.219965511646
Iteration 5100: Loss = -11014.219583826141
Iteration 5200: Loss = -11014.219260064878
Iteration 5300: Loss = -11014.219036467131
Iteration 5400: Loss = -11014.218832748773
Iteration 5500: Loss = -11014.21867023964
Iteration 5600: Loss = -11014.21853776684
Iteration 5700: Loss = -11014.218371986884
Iteration 5800: Loss = -11014.218207473557
Iteration 5900: Loss = -11014.217976745786
Iteration 6000: Loss = -11014.217723287336
Iteration 6100: Loss = -11014.217625033436
Iteration 6200: Loss = -11014.217599721245
Iteration 6300: Loss = -11014.217526989383
Iteration 6400: Loss = -11014.217441249137
Iteration 6500: Loss = -11014.217423290103
Iteration 6600: Loss = -11014.21737167126
Iteration 6700: Loss = -11014.217406620483
Iteration 6800: Loss = -11014.21726453371
Iteration 6900: Loss = -11014.2172114293
Iteration 7000: Loss = -11014.217154202157
Iteration 7100: Loss = -11014.217171043681
Iteration 7200: Loss = -11014.217120639536
Iteration 7300: Loss = -11014.217071535377
Iteration 7400: Loss = -11014.217062726108
Iteration 7500: Loss = -11014.217196977195
1
Iteration 7600: Loss = -11014.217027392908
Iteration 7700: Loss = -11014.216962422028
Iteration 7800: Loss = -11014.22252945929
1
Iteration 7900: Loss = -11014.217228618993
2
Iteration 8000: Loss = -11014.217029018968
Iteration 8100: Loss = -11014.226581440307
1
Iteration 8200: Loss = -11014.216891254131
Iteration 8300: Loss = -11014.316988164937
1
Iteration 8400: Loss = -11014.216876408022
Iteration 8500: Loss = -11014.216847983205
Iteration 8600: Loss = -11014.220137584878
1
Iteration 8700: Loss = -11014.216822736584
Iteration 8800: Loss = -11014.216822934843
Iteration 8900: Loss = -11014.222262827367
1
Iteration 9000: Loss = -11014.216824498888
Iteration 9100: Loss = -11014.216788181684
Iteration 9200: Loss = -11014.216775446032
Iteration 9300: Loss = -11014.274057407301
1
Iteration 9400: Loss = -11014.216719166967
Iteration 9500: Loss = -11014.216772127143
Iteration 9600: Loss = -11014.216757756636
Iteration 9700: Loss = -11014.217037960674
1
Iteration 9800: Loss = -11014.216757900638
Iteration 9900: Loss = -11014.216767109034
Iteration 10000: Loss = -11014.458136683457
1
Iteration 10100: Loss = -11014.21673679629
Iteration 10200: Loss = -11014.216696632435
Iteration 10300: Loss = -11014.21669623854
Iteration 10400: Loss = -11014.21672694548
Iteration 10500: Loss = -11014.21687383773
1
Iteration 10600: Loss = -11014.216676664038
Iteration 10700: Loss = -11014.216710250908
Iteration 10800: Loss = -11014.815841854886
1
Iteration 10900: Loss = -11014.216681346894
Iteration 11000: Loss = -11014.216654562628
Iteration 11100: Loss = -11014.21666751386
Iteration 11200: Loss = -11014.221725355672
1
Iteration 11300: Loss = -11014.216664535224
Iteration 11400: Loss = -11014.216668844032
Iteration 11500: Loss = -11014.502299261767
1
Iteration 11600: Loss = -11014.216653557283
Iteration 11700: Loss = -11014.216619145134
Iteration 11800: Loss = -11014.216626458567
Iteration 11900: Loss = -11014.217586648958
1
Iteration 12000: Loss = -11014.216642759218
Iteration 12100: Loss = -11014.21663437455
Iteration 12200: Loss = -11014.246880119412
1
Iteration 12300: Loss = -11014.216597110757
Iteration 12400: Loss = -11014.216650864719
Iteration 12500: Loss = -11014.216645165985
Iteration 12600: Loss = -11014.217426253028
1
Iteration 12700: Loss = -11014.216601446895
Iteration 12800: Loss = -11014.216642119318
Iteration 12900: Loss = -11014.616566463093
1
Iteration 13000: Loss = -11014.216636626268
Iteration 13100: Loss = -11014.216610006495
Iteration 13200: Loss = -11014.21661963462
Iteration 13300: Loss = -11014.220201764687
1
Iteration 13400: Loss = -11014.21658824178
Iteration 13500: Loss = -11014.21662907867
Iteration 13600: Loss = -11014.21661029937
Iteration 13700: Loss = -11014.216872453566
1
Iteration 13800: Loss = -11014.216622307888
Iteration 13900: Loss = -11014.216595924547
Iteration 14000: Loss = -11014.21981048273
1
Iteration 14100: Loss = -11014.216594471749
Iteration 14200: Loss = -11014.216596838005
Iteration 14300: Loss = -11014.218933492677
1
Iteration 14400: Loss = -11014.216704535233
2
Iteration 14500: Loss = -11014.216589733547
Iteration 14600: Loss = -11014.395888224857
1
Iteration 14700: Loss = -11014.216581039389
Iteration 14800: Loss = -11014.524099291557
1
Iteration 14900: Loss = -11014.216587687511
Iteration 15000: Loss = -11014.21657954495
Iteration 15100: Loss = -11014.694137716173
1
Iteration 15200: Loss = -11014.216598443156
Iteration 15300: Loss = -11014.216575509774
Iteration 15400: Loss = -11014.274271058668
1
Iteration 15500: Loss = -11014.216648184314
Iteration 15600: Loss = -11014.217286808056
1
Iteration 15700: Loss = -11014.218183567371
2
Iteration 15800: Loss = -11014.21661423678
Iteration 15900: Loss = -11014.21764378212
1
Iteration 16000: Loss = -11014.216617014525
Iteration 16100: Loss = -11014.216641540046
Iteration 16200: Loss = -11014.219194866202
1
Iteration 16300: Loss = -11014.216604065987
Iteration 16400: Loss = -11014.223148748933
1
Iteration 16500: Loss = -11014.216644448537
Iteration 16600: Loss = -11014.222365326947
1
Iteration 16700: Loss = -11014.21741408729
2
Iteration 16800: Loss = -11014.217755141253
3
Iteration 16900: Loss = -11014.217404877116
4
Iteration 17000: Loss = -11014.216625867357
Iteration 17100: Loss = -11014.306121721278
1
Iteration 17200: Loss = -11014.216561053401
Iteration 17300: Loss = -11014.216800391041
1
Iteration 17400: Loss = -11014.216591695003
Iteration 17500: Loss = -11014.21658262345
Iteration 17600: Loss = -11014.47910124169
1
Iteration 17700: Loss = -11014.216586238104
Iteration 17800: Loss = -11014.216581052195
Iteration 17900: Loss = -11014.287444849273
1
Iteration 18000: Loss = -11014.216629734216
Iteration 18100: Loss = -11014.216782686475
1
Iteration 18200: Loss = -11014.216713579159
Iteration 18300: Loss = -11014.217069134926
1
Iteration 18400: Loss = -11014.216640708046
Iteration 18500: Loss = -11014.216813785699
1
Iteration 18600: Loss = -11014.216815170344
2
Iteration 18700: Loss = -11014.216694254766
Iteration 18800: Loss = -11014.216688043072
Iteration 18900: Loss = -11014.21680433896
1
Iteration 19000: Loss = -11014.216718664164
Iteration 19100: Loss = -11014.217717169638
1
Iteration 19200: Loss = -11014.244764322579
2
Iteration 19300: Loss = -11014.216647377554
Iteration 19400: Loss = -11014.2378931515
1
Iteration 19500: Loss = -11014.216619851777
Iteration 19600: Loss = -11014.243462174001
1
Iteration 19700: Loss = -11014.216617180622
Iteration 19800: Loss = -11014.2171519
1
Iteration 19900: Loss = -11014.216825678262
2
pi: tensor([[9.9295e-01, 7.0542e-03],
        [2.6546e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9979, 0.0021], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1625, 0.1708],
         [0.6210, 0.5635]],

        [[0.5790, 0.2285],
         [0.7040, 0.5764]],

        [[0.6107, 0.2515],
         [0.5415, 0.5894]],

        [[0.5164, 0.1825],
         [0.7036, 0.6811]],

        [[0.6488, 0.0559],
         [0.6130, 0.5413]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: -0.006096049957872825
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.012378759859606748
Global Adjusted Rand Index: 0.0006416349787499533
Average Adjusted Rand Index: 0.001456643053162954
10984.094335361695
[0.7185403803673669, 0.0006416349787499533] [0.7227833498038088, 0.001456643053162954] [10908.76614980233, 11014.327900355098]
-------------------------------------
This iteration is 66
True Objective function: Loss = -10817.466631436346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21682.18173142113
Iteration 100: Loss = -10866.25745477664
Iteration 200: Loss = -10865.446753622773
Iteration 300: Loss = -10865.212882380589
Iteration 400: Loss = -10865.003659245844
Iteration 500: Loss = -10864.749593915198
Iteration 600: Loss = -10864.557267995051
Iteration 700: Loss = -10864.405379560249
Iteration 800: Loss = -10864.302797410148
Iteration 900: Loss = -10864.258733973285
Iteration 1000: Loss = -10864.229186119057
Iteration 1100: Loss = -10864.206448480883
Iteration 1200: Loss = -10864.1876502472
Iteration 1300: Loss = -10864.170823450057
Iteration 1400: Loss = -10864.154303956202
Iteration 1500: Loss = -10864.135299861287
Iteration 1600: Loss = -10864.107576559407
Iteration 1700: Loss = -10864.052558951413
Iteration 1800: Loss = -10863.91197691475
Iteration 1900: Loss = -10863.605183184645
Iteration 2000: Loss = -10863.158835816454
Iteration 2100: Loss = -10862.855804337105
Iteration 2200: Loss = -10862.687150727448
Iteration 2300: Loss = -10862.586733657416
Iteration 2400: Loss = -10862.522831420594
Iteration 2500: Loss = -10862.479808436758
Iteration 2600: Loss = -10862.449420022345
Iteration 2700: Loss = -10862.42710206145
Iteration 2800: Loss = -10862.410132581188
Iteration 2900: Loss = -10862.396876707608
Iteration 3000: Loss = -10862.386310821532
Iteration 3100: Loss = -10862.37773802279
Iteration 3200: Loss = -10862.370596006578
Iteration 3300: Loss = -10862.364668462598
Iteration 3400: Loss = -10862.359635346324
Iteration 3500: Loss = -10862.355351046124
Iteration 3600: Loss = -10862.351601758937
Iteration 3700: Loss = -10862.348349017371
Iteration 3800: Loss = -10862.345461921657
Iteration 3900: Loss = -10862.342946017805
Iteration 4000: Loss = -10862.340729018682
Iteration 4100: Loss = -10862.33873616798
Iteration 4200: Loss = -10862.336892884005
Iteration 4300: Loss = -10862.335301967634
Iteration 4400: Loss = -10862.333852901837
Iteration 4500: Loss = -10862.332501295927
Iteration 4600: Loss = -10862.331311859121
Iteration 4700: Loss = -10862.330183818362
Iteration 4800: Loss = -10862.329195239328
Iteration 4900: Loss = -10862.32825533873
Iteration 5000: Loss = -10862.327445020303
Iteration 5100: Loss = -10862.326649012943
Iteration 5200: Loss = -10862.325927876405
Iteration 5300: Loss = -10862.325254297248
Iteration 5400: Loss = -10862.324608760016
Iteration 5500: Loss = -10862.32404667548
Iteration 5600: Loss = -10862.323550492456
Iteration 5700: Loss = -10862.323058039268
Iteration 5800: Loss = -10862.322588733372
Iteration 5900: Loss = -10862.32217588938
Iteration 6000: Loss = -10862.321752709766
Iteration 6100: Loss = -10862.32142402882
Iteration 6200: Loss = -10862.321063855517
Iteration 6300: Loss = -10862.3207190883
Iteration 6400: Loss = -10862.320437121924
Iteration 6500: Loss = -10862.320126150858
Iteration 6600: Loss = -10862.319880751407
Iteration 6700: Loss = -10862.319612830795
Iteration 6800: Loss = -10862.319379223776
Iteration 6900: Loss = -10862.319168656271
Iteration 7000: Loss = -10862.319008960276
Iteration 7100: Loss = -10862.31875283721
Iteration 7200: Loss = -10862.318590806604
Iteration 7300: Loss = -10862.318389619417
Iteration 7400: Loss = -10862.318285269264
Iteration 7500: Loss = -10862.318125384492
Iteration 7600: Loss = -10862.31795845455
Iteration 7700: Loss = -10862.317801655088
Iteration 7800: Loss = -10862.317662345873
Iteration 7900: Loss = -10862.31756189777
Iteration 8000: Loss = -10862.317460274697
Iteration 8100: Loss = -10862.317305560677
Iteration 8200: Loss = -10862.3172444697
Iteration 8300: Loss = -10862.317228153373
Iteration 8400: Loss = -10862.31707321321
Iteration 8500: Loss = -10862.316963886151
Iteration 8600: Loss = -10862.323598733692
1
Iteration 8700: Loss = -10862.316764680141
Iteration 8800: Loss = -10862.316741184364
Iteration 8900: Loss = -10862.331055978826
1
Iteration 9000: Loss = -10862.31658201015
Iteration 9100: Loss = -10862.316505105866
Iteration 9200: Loss = -10862.316441430345
Iteration 9300: Loss = -10862.325538337613
1
Iteration 9400: Loss = -10862.316344606861
Iteration 9500: Loss = -10862.316316039403
Iteration 9600: Loss = -10862.3162107206
Iteration 9700: Loss = -10862.317226979998
1
Iteration 9800: Loss = -10862.316136101157
Iteration 9900: Loss = -10862.316125813077
Iteration 10000: Loss = -10862.316062768681
Iteration 10100: Loss = -10862.317661617844
1
Iteration 10200: Loss = -10862.316005825778
Iteration 10300: Loss = -10862.315963175015
Iteration 10400: Loss = -10862.315921496676
Iteration 10500: Loss = -10862.327421253447
1
Iteration 10600: Loss = -10862.315878830766
Iteration 10700: Loss = -10862.3158300823
Iteration 10800: Loss = -10862.316672345574
1
Iteration 10900: Loss = -10862.315808112156
Iteration 11000: Loss = -10862.315788082855
Iteration 11100: Loss = -10862.315754953706
Iteration 11200: Loss = -10862.326093386162
1
Iteration 11300: Loss = -10862.315697137581
Iteration 11400: Loss = -10862.315677782824
Iteration 11500: Loss = -10862.315648249607
Iteration 11600: Loss = -10862.316560985955
1
Iteration 11700: Loss = -10862.315644870105
Iteration 11800: Loss = -10862.315610291698
Iteration 11900: Loss = -10862.331678206934
1
Iteration 12000: Loss = -10862.31552891658
Iteration 12100: Loss = -10862.315535606615
Iteration 12200: Loss = -10862.315520594579
Iteration 12300: Loss = -10862.328306726726
1
Iteration 12400: Loss = -10862.315471008282
Iteration 12500: Loss = -10862.315478656095
Iteration 12600: Loss = -10862.321414281889
1
Iteration 12700: Loss = -10862.315427144591
Iteration 12800: Loss = -10862.315444297024
Iteration 12900: Loss = -10862.315427578522
Iteration 13000: Loss = -10862.315430517021
Iteration 13100: Loss = -10862.315430151219
Iteration 13200: Loss = -10862.315369474536
Iteration 13300: Loss = -10862.319395692131
1
Iteration 13400: Loss = -10862.315394360747
Iteration 13500: Loss = -10862.315397149612
Iteration 13600: Loss = -10862.32392873932
1
Iteration 13700: Loss = -10862.315415981271
Iteration 13800: Loss = -10862.315422228348
Iteration 13900: Loss = -10862.315373885858
Iteration 14000: Loss = -10862.4119550727
1
Iteration 14100: Loss = -10862.31538770437
Iteration 14200: Loss = -10862.315362306512
Iteration 14300: Loss = -10862.31537759238
Iteration 14400: Loss = -10862.340463916382
1
Iteration 14500: Loss = -10862.31535865899
Iteration 14600: Loss = -10862.329289417736
1
Iteration 14700: Loss = -10862.315350894247
Iteration 14800: Loss = -10862.358506201177
1
Iteration 14900: Loss = -10862.3153422941
Iteration 15000: Loss = -10862.315338804572
Iteration 15100: Loss = -10862.316362609752
1
Iteration 15200: Loss = -10862.31533802606
Iteration 15300: Loss = -10862.315346750087
Iteration 15400: Loss = -10862.31630045114
1
Iteration 15500: Loss = -10862.315343679902
Iteration 15600: Loss = -10862.315338689255
Iteration 15700: Loss = -10862.332398836797
1
Iteration 15800: Loss = -10862.315340841495
Iteration 15900: Loss = -10862.315309492016
Iteration 16000: Loss = -10862.334545974478
1
Iteration 16100: Loss = -10862.3153185023
Iteration 16200: Loss = -10862.315277839527
Iteration 16300: Loss = -10862.347383826673
1
Iteration 16400: Loss = -10862.31531997437
Iteration 16500: Loss = -10862.315311532993
Iteration 16600: Loss = -10862.364937036515
1
Iteration 16700: Loss = -10862.315313878387
Iteration 16800: Loss = -10862.31532279672
Iteration 16900: Loss = -10862.321206027216
1
Iteration 17000: Loss = -10862.315309174332
Iteration 17100: Loss = -10862.474010081904
1
Iteration 17200: Loss = -10862.315289232813
Iteration 17300: Loss = -10862.315308156001
Iteration 17400: Loss = -10862.320372604889
1
Iteration 17500: Loss = -10862.315324875783
Iteration 17600: Loss = -10862.315310506016
Iteration 17700: Loss = -10862.317060528216
1
Iteration 17800: Loss = -10862.315301084222
Iteration 17900: Loss = -10862.315287415899
Iteration 18000: Loss = -10862.31533685848
Iteration 18100: Loss = -10862.315289166829
Iteration 18200: Loss = -10862.31527051035
Iteration 18300: Loss = -10862.315239736141
Iteration 18400: Loss = -10862.316900296688
1
Iteration 18500: Loss = -10862.315269466148
Iteration 18600: Loss = -10862.315267671105
Iteration 18700: Loss = -10862.509335286426
1
Iteration 18800: Loss = -10862.315285646546
Iteration 18900: Loss = -10862.327541278413
1
Iteration 19000: Loss = -10862.315288265108
Iteration 19100: Loss = -10862.317257278833
1
Iteration 19200: Loss = -10862.315335290805
Iteration 19300: Loss = -10862.315254268753
Iteration 19400: Loss = -10862.315610891372
1
Iteration 19500: Loss = -10862.315281729545
Iteration 19600: Loss = -10862.328057978371
1
Iteration 19700: Loss = -10862.315283563174
Iteration 19800: Loss = -10862.315271477682
Iteration 19900: Loss = -10862.319143962472
1
pi: tensor([[1.0000e+00, 9.6155e-07],
        [5.7581e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0120, 0.9880], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1775, 0.1431],
         [0.6485, 0.1590]],

        [[0.5309, 0.1948],
         [0.6487, 0.5405]],

        [[0.7050, 0.1073],
         [0.7140, 0.6820]],

        [[0.7266, 0.2004],
         [0.6832, 0.5601]],

        [[0.5063, 0.2842],
         [0.6536, 0.5211]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0014650707529385608
Average Adjusted Rand Index: -0.0020215742848337147
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21569.16206827525
Iteration 100: Loss = -10866.274555339345
Iteration 200: Loss = -10865.367818820025
Iteration 300: Loss = -10865.079336733294
Iteration 400: Loss = -10864.904548051405
Iteration 500: Loss = -10864.79165384888
Iteration 600: Loss = -10864.685247696003
Iteration 700: Loss = -10864.574114272013
Iteration 800: Loss = -10864.481837380683
Iteration 900: Loss = -10864.430950282444
Iteration 1000: Loss = -10864.396060277064
Iteration 1100: Loss = -10864.368173391349
Iteration 1200: Loss = -10864.344711864076
Iteration 1300: Loss = -10864.324295360282
Iteration 1400: Loss = -10864.306254849553
Iteration 1500: Loss = -10864.290232385485
Iteration 1600: Loss = -10864.276138141913
Iteration 1700: Loss = -10864.263789888226
Iteration 1800: Loss = -10864.252996873118
Iteration 1900: Loss = -10864.243553193948
Iteration 2000: Loss = -10864.235147814414
Iteration 2100: Loss = -10864.227626028252
Iteration 2200: Loss = -10864.220614720434
Iteration 2300: Loss = -10864.213884502664
Iteration 2400: Loss = -10864.207179245474
Iteration 2500: Loss = -10864.20025278702
Iteration 2600: Loss = -10864.19286247346
Iteration 2700: Loss = -10864.184728433504
Iteration 2800: Loss = -10864.17555519265
Iteration 2900: Loss = -10864.16468213381
Iteration 3000: Loss = -10864.150591036347
Iteration 3100: Loss = -10864.128039565767
Iteration 3200: Loss = -10864.071938898764
Iteration 3300: Loss = -10863.856638681473
Iteration 3400: Loss = -10863.421664386984
Iteration 3500: Loss = -10862.928109235341
Iteration 3600: Loss = -10862.684962517493
Iteration 3700: Loss = -10862.565021395789
Iteration 3800: Loss = -10862.497798427505
Iteration 3900: Loss = -10862.456127642914
Iteration 4000: Loss = -10862.428258947764
Iteration 4100: Loss = -10862.40852006206
Iteration 4200: Loss = -10862.393905640261
Iteration 4300: Loss = -10862.38269252223
Iteration 4400: Loss = -10862.373861023918
Iteration 4500: Loss = -10862.366710480941
Iteration 4600: Loss = -10862.360857746406
Iteration 4700: Loss = -10862.355963973343
Iteration 4800: Loss = -10862.351849193205
Iteration 4900: Loss = -10862.348261218538
Iteration 5000: Loss = -10862.34520420884
Iteration 5100: Loss = -10862.342541549384
Iteration 5200: Loss = -10862.340216907356
Iteration 5300: Loss = -10862.338117742673
Iteration 5400: Loss = -10862.336294448092
Iteration 5500: Loss = -10862.334631019517
Iteration 5600: Loss = -10862.33317181006
Iteration 5700: Loss = -10862.331843889986
Iteration 5800: Loss = -10862.330666620384
Iteration 5900: Loss = -10862.329551257904
Iteration 6000: Loss = -10862.328563780842
Iteration 6100: Loss = -10862.32768271773
Iteration 6200: Loss = -10862.326856075026
Iteration 6300: Loss = -10862.326136188447
Iteration 6400: Loss = -10862.325423853075
Iteration 6500: Loss = -10862.32478138963
Iteration 6600: Loss = -10862.324203571146
Iteration 6700: Loss = -10862.323609326928
Iteration 6800: Loss = -10862.323152743085
Iteration 6900: Loss = -10862.32272570797
Iteration 7000: Loss = -10862.322238725064
Iteration 7100: Loss = -10862.321899294477
Iteration 7200: Loss = -10862.321508329345
Iteration 7300: Loss = -10862.321138078276
Iteration 7400: Loss = -10862.320834617336
Iteration 7500: Loss = -10862.320519314375
Iteration 7600: Loss = -10862.320252334895
Iteration 7700: Loss = -10862.319980091295
Iteration 7800: Loss = -10862.320183627347
1
Iteration 7900: Loss = -10862.319511645914
Iteration 8000: Loss = -10862.319232497104
Iteration 8100: Loss = -10862.319048065472
Iteration 8200: Loss = -10862.318831876211
Iteration 8300: Loss = -10862.318835117469
Iteration 8400: Loss = -10862.318491224376
Iteration 8500: Loss = -10862.318366358697
Iteration 8600: Loss = -10862.318169332653
Iteration 8700: Loss = -10862.355484911406
1
Iteration 8800: Loss = -10862.317899719346
Iteration 8900: Loss = -10862.317779148852
Iteration 9000: Loss = -10862.317630932124
Iteration 9100: Loss = -10862.332664599859
1
Iteration 9200: Loss = -10862.317421692353
Iteration 9300: Loss = -10862.317327619918
Iteration 9400: Loss = -10862.317215186036
Iteration 9500: Loss = -10862.317113897321
Iteration 9600: Loss = -10862.317128768875
Iteration 9700: Loss = -10862.316955549388
Iteration 9800: Loss = -10862.316866643105
Iteration 9900: Loss = -10862.349652860723
1
Iteration 10000: Loss = -10862.316717932483
Iteration 10100: Loss = -10862.316647601985
Iteration 10200: Loss = -10862.316596040686
Iteration 10300: Loss = -10862.336970798215
1
Iteration 10400: Loss = -10862.31644062834
Iteration 10500: Loss = -10862.316411651413
Iteration 10600: Loss = -10862.316377332041
Iteration 10700: Loss = -10862.328824902032
1
Iteration 10800: Loss = -10862.316248786059
Iteration 10900: Loss = -10862.316193150475
Iteration 11000: Loss = -10862.316164365657
Iteration 11100: Loss = -10862.319443876157
1
Iteration 11200: Loss = -10862.316057809541
Iteration 11300: Loss = -10862.31601346735
Iteration 11400: Loss = -10862.316559640925
1
Iteration 11500: Loss = -10862.316025277556
Iteration 11600: Loss = -10862.315935029215
Iteration 11700: Loss = -10862.31593031686
Iteration 11800: Loss = -10862.315855449337
Iteration 11900: Loss = -10862.33412859999
1
Iteration 12000: Loss = -10862.315816649292
Iteration 12100: Loss = -10862.315791918445
Iteration 12200: Loss = -10862.315823082114
Iteration 12300: Loss = -10862.315732798736
Iteration 12400: Loss = -10862.316441138124
1
Iteration 12500: Loss = -10862.315707163758
Iteration 12600: Loss = -10862.315705273008
Iteration 12700: Loss = -10862.315660321712
Iteration 12800: Loss = -10862.316054020217
1
Iteration 12900: Loss = -10862.31560687523
Iteration 13000: Loss = -10862.315624408171
Iteration 13100: Loss = -10862.406991143342
1
Iteration 13200: Loss = -10862.315595050732
Iteration 13300: Loss = -10862.315606373524
Iteration 13400: Loss = -10862.315572531854
Iteration 13500: Loss = -10862.321395986028
1
Iteration 13600: Loss = -10862.315532170556
Iteration 13700: Loss = -10862.315535971227
Iteration 13800: Loss = -10862.315506978313
Iteration 13900: Loss = -10862.315517785903
Iteration 14000: Loss = -10862.315637405452
1
Iteration 14100: Loss = -10862.315455743632
Iteration 14200: Loss = -10862.315456996119
Iteration 14300: Loss = -10862.845072263837
1
Iteration 14400: Loss = -10862.315439292142
Iteration 14500: Loss = -10862.315436196059
Iteration 14600: Loss = -10862.315443309899
Iteration 14700: Loss = -10862.336111065604
1
Iteration 14800: Loss = -10862.315402761786
Iteration 14900: Loss = -10862.315385777752
Iteration 15000: Loss = -10862.315586334085
1
Iteration 15100: Loss = -10862.315443308256
Iteration 15200: Loss = -10862.315559989534
1
Iteration 15300: Loss = -10862.315464031806
Iteration 15400: Loss = -10862.315435801476
Iteration 15500: Loss = -10862.315904280716
1
Iteration 15600: Loss = -10862.315420694025
Iteration 15700: Loss = -10862.329790993343
1
Iteration 15800: Loss = -10862.315428953267
Iteration 15900: Loss = -10862.329486578948
1
Iteration 16000: Loss = -10862.315423259033
Iteration 16100: Loss = -10862.315397339175
Iteration 16200: Loss = -10862.315594689659
1
Iteration 16300: Loss = -10862.315392590217
Iteration 16400: Loss = -10862.33152590949
1
Iteration 16500: Loss = -10862.315463376015
Iteration 16600: Loss = -10862.315418625032
Iteration 16700: Loss = -10862.322607380574
1
Iteration 16800: Loss = -10862.31544719021
Iteration 16900: Loss = -10862.315402450591
Iteration 17000: Loss = -10862.315419244242
Iteration 17100: Loss = -10862.316722053707
1
Iteration 17200: Loss = -10862.31538285941
Iteration 17300: Loss = -10862.315390338172
Iteration 17400: Loss = -10862.321859246897
1
Iteration 17500: Loss = -10862.315405989839
Iteration 17600: Loss = -10862.315379732763
Iteration 17700: Loss = -10862.317253278072
1
Iteration 17800: Loss = -10862.315367852612
Iteration 17900: Loss = -10862.315374562237
Iteration 18000: Loss = -10862.315584604092
1
Iteration 18100: Loss = -10862.315368623218
Iteration 18200: Loss = -10862.31566867554
1
Iteration 18300: Loss = -10862.315419623994
Iteration 18400: Loss = -10862.315356877405
Iteration 18500: Loss = -10862.32464853146
1
Iteration 18600: Loss = -10862.315353939795
Iteration 18700: Loss = -10862.315373063115
Iteration 18800: Loss = -10862.362662485726
1
Iteration 18900: Loss = -10862.31534626019
Iteration 19000: Loss = -10862.31536707092
Iteration 19100: Loss = -10862.316085938468
1
Iteration 19200: Loss = -10862.315382473314
Iteration 19300: Loss = -10862.31533731727
Iteration 19400: Loss = -10862.315347680887
Iteration 19500: Loss = -10862.364410819559
1
Iteration 19600: Loss = -10862.31534402687
Iteration 19700: Loss = -10862.31531984163
Iteration 19800: Loss = -10862.317860156529
1
Iteration 19900: Loss = -10862.334710471787
2
pi: tensor([[1.0000e+00, 9.1052e-09],
        [1.6562e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9880, 0.0120], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1590, 0.1431],
         [0.6119, 0.1775]],

        [[0.6206, 0.1948],
         [0.7232, 0.6561]],

        [[0.5278, 0.1073],
         [0.6958, 0.6453]],

        [[0.5886, 0.2004],
         [0.5360, 0.6850]],

        [[0.7132, 0.2842],
         [0.5923, 0.5588]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0014650707529385608
Average Adjusted Rand Index: -0.0020215742848337147
10817.466631436346
[-0.0014650707529385608, -0.0014650707529385608] [-0.0020215742848337147, -0.0020215742848337147] [10862.315282178719, 10862.315320270221]
-------------------------------------
This iteration is 67
True Objective function: Loss = -10890.736060067185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23180.835319120062
Iteration 100: Loss = -10978.775535262128
Iteration 200: Loss = -10977.679450510894
Iteration 300: Loss = -10977.381183279686
Iteration 400: Loss = -10976.919727665885
Iteration 500: Loss = -10976.122009938126
Iteration 600: Loss = -10975.753957855488
Iteration 700: Loss = -10975.280249540723
Iteration 800: Loss = -10974.254909698133
Iteration 900: Loss = -10964.397485351781
Iteration 1000: Loss = -10945.820129939068
Iteration 1100: Loss = -10904.17126824361
Iteration 1200: Loss = -10849.908494874753
Iteration 1300: Loss = -10840.709127270105
Iteration 1400: Loss = -10838.686106426869
Iteration 1500: Loss = -10836.416078745264
Iteration 1600: Loss = -10836.246434059776
Iteration 1700: Loss = -10836.203840031343
Iteration 1800: Loss = -10836.177566078752
Iteration 1900: Loss = -10836.157985647083
Iteration 2000: Loss = -10836.140973367805
Iteration 2100: Loss = -10836.11346308522
Iteration 2200: Loss = -10836.084573412767
Iteration 2300: Loss = -10835.959374167303
Iteration 2400: Loss = -10835.8914030479
Iteration 2500: Loss = -10835.884648949805
Iteration 2600: Loss = -10835.877968309389
Iteration 2700: Loss = -10835.870536749797
Iteration 2800: Loss = -10835.866253106069
Iteration 2900: Loss = -10835.864433450308
Iteration 3000: Loss = -10835.857655652593
Iteration 3100: Loss = -10835.83673241942
Iteration 3200: Loss = -10835.8322525589
Iteration 3300: Loss = -10835.831912229489
Iteration 3400: Loss = -10835.829033578424
Iteration 3500: Loss = -10835.82639070348
Iteration 3600: Loss = -10835.824338738776
Iteration 3700: Loss = -10835.826636377635
1
Iteration 3800: Loss = -10835.818859797942
Iteration 3900: Loss = -10835.818673111475
Iteration 4000: Loss = -10835.814242286053
Iteration 4100: Loss = -10835.811822981943
Iteration 4200: Loss = -10835.813636214307
1
Iteration 4300: Loss = -10835.807390134643
Iteration 4400: Loss = -10835.801942346028
Iteration 4500: Loss = -10835.800306999645
Iteration 4600: Loss = -10835.798389999305
Iteration 4700: Loss = -10835.797684811461
Iteration 4800: Loss = -10835.797161505358
Iteration 4900: Loss = -10835.804996580111
1
Iteration 5000: Loss = -10835.79649577358
Iteration 5100: Loss = -10835.806478007062
1
Iteration 5200: Loss = -10835.79428398884
Iteration 5300: Loss = -10835.79231037036
Iteration 5400: Loss = -10835.792101464707
Iteration 5500: Loss = -10835.791367286949
Iteration 5600: Loss = -10835.790915456171
Iteration 5700: Loss = -10835.789653865835
Iteration 5800: Loss = -10835.787377472852
Iteration 5900: Loss = -10835.768258213382
Iteration 6000: Loss = -10835.75714328277
Iteration 6100: Loss = -10835.756745909577
Iteration 6200: Loss = -10835.756619375617
Iteration 6300: Loss = -10835.755865031657
Iteration 6400: Loss = -10835.755378404336
Iteration 6500: Loss = -10835.766881611797
1
Iteration 6600: Loss = -10835.75414312533
Iteration 6700: Loss = -10835.752664516749
Iteration 6800: Loss = -10835.751422261017
Iteration 6900: Loss = -10835.750914122507
Iteration 7000: Loss = -10835.759647439474
1
Iteration 7100: Loss = -10835.750367351766
Iteration 7200: Loss = -10835.751358660093
1
Iteration 7300: Loss = -10835.749565771239
Iteration 7400: Loss = -10835.74990476888
1
Iteration 7500: Loss = -10835.747386278003
Iteration 7600: Loss = -10835.740079262423
Iteration 7700: Loss = -10835.74146193223
1
Iteration 7800: Loss = -10835.738260927108
Iteration 7900: Loss = -10835.727896522605
Iteration 8000: Loss = -10835.727607158748
Iteration 8100: Loss = -10835.727141808664
Iteration 8200: Loss = -10835.726595689463
Iteration 8300: Loss = -10835.72884806022
1
Iteration 8400: Loss = -10835.725478283512
Iteration 8500: Loss = -10835.72535572239
Iteration 8600: Loss = -10835.733077374492
1
Iteration 8700: Loss = -10835.72492157118
Iteration 8800: Loss = -10835.724665926273
Iteration 8900: Loss = -10835.764986046606
1
Iteration 9000: Loss = -10835.724487099078
Iteration 9100: Loss = -10835.724282595103
Iteration 9200: Loss = -10835.723951459093
Iteration 9300: Loss = -10835.719144297194
Iteration 9400: Loss = -10835.71914482219
Iteration 9500: Loss = -10835.723440115979
1
Iteration 9600: Loss = -10835.718975999991
Iteration 9700: Loss = -10835.712092343409
Iteration 9800: Loss = -10835.712000251451
Iteration 9900: Loss = -10835.711620853219
Iteration 10000: Loss = -10835.718386827959
1
Iteration 10100: Loss = -10835.711571239463
Iteration 10200: Loss = -10835.712106894603
1
Iteration 10300: Loss = -10835.711535225017
Iteration 10400: Loss = -10835.854963744716
1
Iteration 10500: Loss = -10835.711511917292
Iteration 10600: Loss = -10835.711475349412
Iteration 10700: Loss = -10835.711473607327
Iteration 10800: Loss = -10835.711026820749
Iteration 10900: Loss = -10835.935525168723
1
Iteration 11000: Loss = -10835.71073790955
Iteration 11100: Loss = -10835.710678402042
Iteration 11200: Loss = -10835.710677154282
Iteration 11300: Loss = -10835.710701217198
Iteration 11400: Loss = -10835.710532998684
Iteration 11500: Loss = -10835.850222241364
1
Iteration 11600: Loss = -10835.710427973472
Iteration 11700: Loss = -10835.710577878806
1
Iteration 11800: Loss = -10835.711000537243
2
Iteration 11900: Loss = -10835.71042122463
Iteration 12000: Loss = -10835.71048114963
Iteration 12100: Loss = -10835.71042398731
Iteration 12200: Loss = -10835.710450783532
Iteration 12300: Loss = -10835.763038899637
1
Iteration 12400: Loss = -10835.710287545844
Iteration 12500: Loss = -10835.714327886919
1
Iteration 12600: Loss = -10835.542953973627
Iteration 12700: Loss = -10835.894767822121
1
Iteration 12800: Loss = -10835.542047006074
Iteration 12900: Loss = -10835.542016948579
Iteration 13000: Loss = -10835.547728891595
1
Iteration 13100: Loss = -10835.542007120746
Iteration 13200: Loss = -10835.564349619408
1
Iteration 13300: Loss = -10835.541989807927
Iteration 13400: Loss = -10835.542751568855
1
Iteration 13500: Loss = -10835.541466638879
Iteration 13600: Loss = -10835.541799057404
1
Iteration 13700: Loss = -10835.54141182851
Iteration 13800: Loss = -10835.5469995362
1
Iteration 13900: Loss = -10835.489496227598
Iteration 14000: Loss = -10835.482810075417
Iteration 14100: Loss = -10835.48320742737
1
Iteration 14200: Loss = -10835.488074315323
2
Iteration 14300: Loss = -10835.482784020369
Iteration 14400: Loss = -10835.482799336714
Iteration 14500: Loss = -10835.481664783909
Iteration 14600: Loss = -10835.482511374743
1
Iteration 14700: Loss = -10835.480149594892
Iteration 14800: Loss = -10835.480164606777
Iteration 14900: Loss = -10835.48028623382
1
Iteration 15000: Loss = -10835.483254213477
2
Iteration 15100: Loss = -10835.49362828301
3
Iteration 15200: Loss = -10835.480138306833
Iteration 15300: Loss = -10835.48055851584
1
Iteration 15400: Loss = -10835.480049135822
Iteration 15500: Loss = -10835.480094150256
Iteration 15600: Loss = -10835.480274518313
1
Iteration 15700: Loss = -10835.480148132978
Iteration 15800: Loss = -10835.495074087645
1
Iteration 15900: Loss = -10835.480059177236
Iteration 16000: Loss = -10835.47952027406
Iteration 16100: Loss = -10835.479645847961
1
Iteration 16200: Loss = -10835.479435033008
Iteration 16300: Loss = -10835.644188444716
1
Iteration 16400: Loss = -10835.479178381236
Iteration 16500: Loss = -10835.489770586708
1
Iteration 16600: Loss = -10835.479237488273
Iteration 16700: Loss = -10835.479300775394
Iteration 16800: Loss = -10835.483609493633
1
Iteration 16900: Loss = -10835.47916853586
Iteration 17000: Loss = -10835.479245762956
Iteration 17100: Loss = -10835.480239655933
1
Iteration 17200: Loss = -10835.474872169345
Iteration 17300: Loss = -10835.506788698696
1
Iteration 17400: Loss = -10835.475068576767
2
Iteration 17500: Loss = -10835.474872739833
Iteration 17600: Loss = -10835.506843176625
1
Iteration 17700: Loss = -10835.474807485576
Iteration 17800: Loss = -10835.53176463545
1
Iteration 17900: Loss = -10835.474729565742
Iteration 18000: Loss = -10835.535802483684
1
Iteration 18100: Loss = -10835.472019768193
Iteration 18200: Loss = -10835.47207654155
Iteration 18300: Loss = -10835.473116388926
1
Iteration 18400: Loss = -10835.471980737611
Iteration 18500: Loss = -10835.4719790062
Iteration 18600: Loss = -10835.472073544795
Iteration 18700: Loss = -10835.471954895078
Iteration 18800: Loss = -10835.47391722161
1
Iteration 18900: Loss = -10835.471960033214
Iteration 19000: Loss = -10835.474762994018
1
Iteration 19100: Loss = -10835.47273065116
2
Iteration 19200: Loss = -10835.472144924279
3
Iteration 19300: Loss = -10835.475055357
4
Iteration 19400: Loss = -10835.472041299092
Iteration 19500: Loss = -10835.471348196597
Iteration 19600: Loss = -10835.471281602779
Iteration 19700: Loss = -10835.471374072578
Iteration 19800: Loss = -10835.471291555552
Iteration 19900: Loss = -10835.475283283964
1
pi: tensor([[0.7636, 0.2364],
        [0.2154, 0.7846]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4246, 0.5754], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1885, 0.0967],
         [0.6824, 0.2491]],

        [[0.5730, 0.1135],
         [0.5547, 0.5649]],

        [[0.5202, 0.1009],
         [0.7029, 0.6777]],

        [[0.5282, 0.1022],
         [0.6369, 0.6579]],

        [[0.6089, 0.0924],
         [0.6174, 0.6762]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721052518156154
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.8241103530018717
Average Adjusted Rand Index: 0.8234258557121434
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23737.529493008293
Iteration 100: Loss = -10979.283178902926
Iteration 200: Loss = -10977.864387894455
Iteration 300: Loss = -10977.480913602352
Iteration 400: Loss = -10976.879697821152
Iteration 500: Loss = -10976.117792899342
Iteration 600: Loss = -10975.5111843157
Iteration 700: Loss = -10974.081102271006
Iteration 800: Loss = -10958.159608899921
Iteration 900: Loss = -10942.769335205126
Iteration 1000: Loss = -10925.440315375125
Iteration 1100: Loss = -10851.160030849953
Iteration 1200: Loss = -10839.432252786177
Iteration 1300: Loss = -10839.020278158501
Iteration 1400: Loss = -10836.978347412938
Iteration 1500: Loss = -10836.648567321903
Iteration 1600: Loss = -10836.553311213984
Iteration 1700: Loss = -10836.500888901865
Iteration 1800: Loss = -10836.468899105465
Iteration 1900: Loss = -10836.438167218488
Iteration 2000: Loss = -10836.398534485956
Iteration 2100: Loss = -10836.334641756177
Iteration 2200: Loss = -10836.324407663511
Iteration 2300: Loss = -10836.31384176919
Iteration 2400: Loss = -10836.302134716081
Iteration 2500: Loss = -10836.289060836863
Iteration 2600: Loss = -10836.273868234177
Iteration 2700: Loss = -10836.240522367638
Iteration 2800: Loss = -10836.172084564525
Iteration 2900: Loss = -10836.165889099086
Iteration 3000: Loss = -10836.162127577669
Iteration 3100: Loss = -10836.156777357972
Iteration 3200: Loss = -10836.148876810159
Iteration 3300: Loss = -10836.116333294553
Iteration 3400: Loss = -10836.11192300153
Iteration 3500: Loss = -10836.11052766803
Iteration 3600: Loss = -10836.109153626532
Iteration 3700: Loss = -10836.108077745375
Iteration 3800: Loss = -10836.10720699695
Iteration 3900: Loss = -10836.107836039217
1
Iteration 4000: Loss = -10836.10638445219
Iteration 4100: Loss = -10836.10519579214
Iteration 4200: Loss = -10836.104554184265
Iteration 4300: Loss = -10836.103851911432
Iteration 4400: Loss = -10836.103370100296
Iteration 4500: Loss = -10836.102333578534
Iteration 4600: Loss = -10836.101049752535
Iteration 4700: Loss = -10836.09868476303
Iteration 4800: Loss = -10836.0923133608
Iteration 4900: Loss = -10836.089956875421
Iteration 5000: Loss = -10836.073839088554
Iteration 5100: Loss = -10836.060079800165
Iteration 5200: Loss = -10836.056059105837
Iteration 5300: Loss = -10836.054275425215
Iteration 5400: Loss = -10836.046515261658
Iteration 5500: Loss = -10836.048481852713
1
Iteration 5600: Loss = -10836.03738840536
Iteration 5700: Loss = -10836.009803796853
Iteration 5800: Loss = -10835.961588015776
Iteration 5900: Loss = -10835.953352321689
Iteration 6000: Loss = -10835.93807319664
Iteration 6100: Loss = -10835.771886950519
Iteration 6200: Loss = -10835.770381355564
Iteration 6300: Loss = -10835.714275797
Iteration 6400: Loss = -10835.685384224651
Iteration 6500: Loss = -10835.684850386457
Iteration 6600: Loss = -10835.683063479368
Iteration 6700: Loss = -10835.68824450196
1
Iteration 6800: Loss = -10835.686200996977
2
Iteration 6900: Loss = -10835.677866247115
Iteration 7000: Loss = -10835.675469819547
Iteration 7100: Loss = -10835.682751866434
1
Iteration 7200: Loss = -10835.674330780874
Iteration 7300: Loss = -10835.672943942516
Iteration 7400: Loss = -10835.67077593093
Iteration 7500: Loss = -10835.634142130235
Iteration 7600: Loss = -10835.628832804858
Iteration 7700: Loss = -10835.628673178771
Iteration 7800: Loss = -10835.628226555416
Iteration 7900: Loss = -10835.627799652339
Iteration 8000: Loss = -10835.627479825576
Iteration 8100: Loss = -10835.627477608396
Iteration 8200: Loss = -10835.62724773045
Iteration 8300: Loss = -10835.627027037559
Iteration 8400: Loss = -10835.62364620058
Iteration 8500: Loss = -10835.622292470442
Iteration 8600: Loss = -10835.77324161636
1
Iteration 8700: Loss = -10835.619044486317
Iteration 8800: Loss = -10835.618965567315
Iteration 8900: Loss = -10835.632545761933
1
Iteration 9000: Loss = -10835.618509638567
Iteration 9100: Loss = -10835.607258980664
Iteration 9200: Loss = -10835.607352410581
Iteration 9300: Loss = -10835.606887426493
Iteration 9400: Loss = -10835.60553432004
Iteration 9500: Loss = -10835.585938969643
Iteration 9600: Loss = -10835.585803506445
Iteration 9700: Loss = -10835.58593329349
1
Iteration 9800: Loss = -10835.585148585802
Iteration 9900: Loss = -10835.583532436627
Iteration 10000: Loss = -10835.601259576408
1
Iteration 10100: Loss = -10835.627683623828
2
Iteration 10200: Loss = -10835.583428147827
Iteration 10300: Loss = -10835.583521436442
Iteration 10400: Loss = -10835.623381441308
1
Iteration 10500: Loss = -10835.609544475601
2
Iteration 10600: Loss = -10835.583244901518
Iteration 10700: Loss = -10835.583100166048
Iteration 10800: Loss = -10835.583888387371
1
Iteration 10900: Loss = -10835.602320597012
2
Iteration 11000: Loss = -10835.581959066461
Iteration 11100: Loss = -10835.655391708118
1
Iteration 11200: Loss = -10835.58158236189
Iteration 11300: Loss = -10835.580590539475
Iteration 11400: Loss = -10835.580192357118
Iteration 11500: Loss = -10835.578601201198
Iteration 11600: Loss = -10835.633599550782
1
Iteration 11700: Loss = -10835.578342731373
Iteration 11800: Loss = -10835.577045893322
Iteration 11900: Loss = -10835.57774922466
1
Iteration 12000: Loss = -10835.5769276833
Iteration 12100: Loss = -10835.604710031901
1
Iteration 12200: Loss = -10835.576575701818
Iteration 12300: Loss = -10835.576221217034
Iteration 12400: Loss = -10835.57574634874
Iteration 12500: Loss = -10835.603638555382
1
Iteration 12600: Loss = -10835.569745903182
Iteration 12700: Loss = -10835.8222239153
1
Iteration 12800: Loss = -10835.569766868175
Iteration 12900: Loss = -10835.570690277511
1
Iteration 13000: Loss = -10835.569550116234
Iteration 13100: Loss = -10835.62686421068
1
Iteration 13200: Loss = -10835.569205296737
Iteration 13300: Loss = -10835.56920079541
Iteration 13400: Loss = -10835.571025819225
1
Iteration 13500: Loss = -10835.568577791539
Iteration 13600: Loss = -10835.568454502438
Iteration 13700: Loss = -10835.568448998054
Iteration 13800: Loss = -10835.590339586819
1
Iteration 13900: Loss = -10835.56847266162
Iteration 14000: Loss = -10835.568308969694
Iteration 14100: Loss = -10835.568230803432
Iteration 14200: Loss = -10835.568663845146
1
Iteration 14300: Loss = -10835.663850325964
2
Iteration 14400: Loss = -10835.568115826049
Iteration 14500: Loss = -10835.57166769065
1
Iteration 14600: Loss = -10835.572602947348
2
Iteration 14700: Loss = -10835.588540886261
3
Iteration 14800: Loss = -10835.568085699328
Iteration 14900: Loss = -10835.580712480976
1
Iteration 15000: Loss = -10835.56774537714
Iteration 15100: Loss = -10835.56762218867
Iteration 15200: Loss = -10835.567090176057
Iteration 15300: Loss = -10835.558814625325
Iteration 15400: Loss = -10835.555654527498
Iteration 15500: Loss = -10835.55005163158
Iteration 15600: Loss = -10835.573783170237
1
Iteration 15700: Loss = -10835.549356243588
Iteration 15800: Loss = -10835.553017783579
1
Iteration 15900: Loss = -10835.550074608394
2
Iteration 16000: Loss = -10835.549289134275
Iteration 16100: Loss = -10835.613730910805
1
Iteration 16200: Loss = -10835.54642739024
Iteration 16300: Loss = -10835.651582124026
1
Iteration 16400: Loss = -10835.54556838462
Iteration 16500: Loss = -10835.54555707173
Iteration 16600: Loss = -10835.545627715646
Iteration 16700: Loss = -10835.54553642212
Iteration 16800: Loss = -10835.545893719074
1
Iteration 16900: Loss = -10835.54562679042
Iteration 17000: Loss = -10835.546820147149
1
Iteration 17100: Loss = -10835.69243136203
2
Iteration 17200: Loss = -10835.545524036012
Iteration 17300: Loss = -10835.54574420159
1
Iteration 17400: Loss = -10835.54551421582
Iteration 17500: Loss = -10835.539016275532
Iteration 17600: Loss = -10835.538345014586
Iteration 17700: Loss = -10835.542899753402
1
Iteration 17800: Loss = -10835.538352128326
Iteration 17900: Loss = -10835.554506438033
1
Iteration 18000: Loss = -10835.538355048438
Iteration 18100: Loss = -10835.555665708855
1
Iteration 18200: Loss = -10835.540736622226
2
Iteration 18300: Loss = -10835.541000318643
3
Iteration 18400: Loss = -10835.540361264428
4
Iteration 18500: Loss = -10835.54069936942
5
Iteration 18600: Loss = -10835.746187110108
6
Iteration 18700: Loss = -10835.538333588323
Iteration 18800: Loss = -10835.540565678855
1
Iteration 18900: Loss = -10835.538330262794
Iteration 19000: Loss = -10835.538279884762
Iteration 19100: Loss = -10835.53828172205
Iteration 19200: Loss = -10835.53886448026
1
Iteration 19300: Loss = -10835.537517619281
Iteration 19400: Loss = -10835.603007652018
1
Iteration 19500: Loss = -10835.53744229995
Iteration 19600: Loss = -10835.537577488078
1
Iteration 19700: Loss = -10835.53739910202
Iteration 19800: Loss = -10835.537366800165
Iteration 19900: Loss = -10835.537732053741
1
pi: tensor([[0.7847, 0.2153],
        [0.2361, 0.7639]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5760, 0.4240], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2491, 0.0966],
         [0.6254, 0.1885]],

        [[0.6053, 0.1135],
         [0.7085, 0.6620]],

        [[0.5335, 0.1009],
         [0.5344, 0.7259]],

        [[0.5965, 0.1022],
         [0.6863, 0.6380]],

        [[0.5606, 0.0924],
         [0.5187, 0.5020]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721052518156154
time is 1
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
time is 4
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
Global Adjusted Rand Index: 0.8241103530018717
Average Adjusted Rand Index: 0.8234258557121434
10890.736060067185
[0.8241103530018717, 0.8241103530018717] [0.8234258557121434, 0.8234258557121434] [10835.47125958065, 10835.53734397044]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11180.534908716018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21031.25521851958
Iteration 100: Loss = -11275.41050428225
Iteration 200: Loss = -11273.41376160519
Iteration 300: Loss = -11270.395997804584
Iteration 400: Loss = -11267.847996707216
Iteration 500: Loss = -11263.438230824115
Iteration 600: Loss = -11171.492838096434
Iteration 700: Loss = -11161.566302148598
Iteration 800: Loss = -11159.00893362818
Iteration 900: Loss = -11158.75905462169
Iteration 1000: Loss = -11158.679751283971
Iteration 1100: Loss = -11158.587518311335
Iteration 1200: Loss = -11158.408521303858
Iteration 1300: Loss = -11157.926340349488
Iteration 1400: Loss = -11147.778842852285
Iteration 1500: Loss = -11128.166185374354
Iteration 1600: Loss = -11128.012379505339
Iteration 1700: Loss = -11127.989108454154
Iteration 1800: Loss = -11127.960749302963
Iteration 1900: Loss = -11127.925004284558
Iteration 2000: Loss = -11127.911730714584
Iteration 2100: Loss = -11127.907312236954
Iteration 2200: Loss = -11127.904738966647
Iteration 2300: Loss = -11127.902780395576
Iteration 2400: Loss = -11127.901052834408
Iteration 2500: Loss = -11127.89923871689
Iteration 2600: Loss = -11127.897143415583
Iteration 2700: Loss = -11127.895628668857
Iteration 2800: Loss = -11127.894614712239
Iteration 2900: Loss = -11127.893936704577
Iteration 3000: Loss = -11127.893016486207
Iteration 3100: Loss = -11127.89238866866
Iteration 3200: Loss = -11127.8922875507
Iteration 3300: Loss = -11127.891719763686
Iteration 3400: Loss = -11127.891263725345
Iteration 3500: Loss = -11127.89053614507
Iteration 3600: Loss = -11127.889641178468
Iteration 3700: Loss = -11127.88403286078
Iteration 3800: Loss = -11127.883933994659
Iteration 3900: Loss = -11127.884625150107
1
Iteration 4000: Loss = -11127.883287129585
Iteration 4100: Loss = -11127.882988475722
Iteration 4200: Loss = -11127.88272340878
Iteration 4300: Loss = -11127.882185507226
Iteration 4400: Loss = -11127.874075801832
Iteration 4500: Loss = -11127.869627689168
Iteration 4600: Loss = -11127.856622227464
Iteration 4700: Loss = -11127.855833074711
Iteration 4800: Loss = -11127.855107009265
Iteration 4900: Loss = -11127.855063991548
Iteration 5000: Loss = -11127.855236183803
1
Iteration 5100: Loss = -11127.854856047166
Iteration 5200: Loss = -11127.854790720134
Iteration 5300: Loss = -11127.854656594829
Iteration 5400: Loss = -11127.854602045354
Iteration 5500: Loss = -11127.855519424922
1
Iteration 5600: Loss = -11127.844931568286
Iteration 5700: Loss = -11127.844036390481
Iteration 5800: Loss = -11127.843830352791
Iteration 5900: Loss = -11127.844606797522
1
Iteration 6000: Loss = -11127.843849090179
Iteration 6100: Loss = -11127.848903879127
1
Iteration 6200: Loss = -11127.84511315187
2
Iteration 6300: Loss = -11127.845804070457
3
Iteration 6400: Loss = -11127.843268604063
Iteration 6500: Loss = -11127.843153910051
Iteration 6600: Loss = -11127.843080532408
Iteration 6700: Loss = -11127.843157199477
Iteration 6800: Loss = -11127.842949556467
Iteration 6900: Loss = -11127.842700252386
Iteration 7000: Loss = -11127.840028163926
Iteration 7100: Loss = -11127.870773827952
1
Iteration 7200: Loss = -11127.839995077853
Iteration 7300: Loss = -11127.83996136703
Iteration 7400: Loss = -11128.034933732108
1
Iteration 7500: Loss = -11127.839907633479
Iteration 7600: Loss = -11127.839817541271
Iteration 7700: Loss = -11127.906878743193
1
Iteration 7800: Loss = -11127.839716301001
Iteration 7900: Loss = -11127.839650704796
Iteration 8000: Loss = -11128.090745038342
1
Iteration 8100: Loss = -11127.839622868136
Iteration 8200: Loss = -11127.839619913828
Iteration 8300: Loss = -11127.845108847108
1
Iteration 8400: Loss = -11127.83959439621
Iteration 8500: Loss = -11127.839584807476
Iteration 8600: Loss = -11127.839698448806
1
Iteration 8700: Loss = -11127.839546816718
Iteration 8800: Loss = -11127.839430861379
Iteration 8900: Loss = -11127.839224633455
Iteration 9000: Loss = -11127.839190316625
Iteration 9100: Loss = -11127.893885634856
1
Iteration 9200: Loss = -11127.839145927388
Iteration 9300: Loss = -11128.010767093243
1
Iteration 9400: Loss = -11127.83916195653
Iteration 9500: Loss = -11127.839122997633
Iteration 9600: Loss = -11127.839206651792
Iteration 9700: Loss = -11127.839114886941
Iteration 9800: Loss = -11127.839649870182
1
Iteration 9900: Loss = -11127.839095939487
Iteration 10000: Loss = -11127.999628805383
1
Iteration 10100: Loss = -11127.839089024726
Iteration 10200: Loss = -11127.839081534894
Iteration 10300: Loss = -11127.839837515516
1
Iteration 10400: Loss = -11127.839100577869
Iteration 10500: Loss = -11127.839597149628
1
Iteration 10600: Loss = -11127.839083738887
Iteration 10700: Loss = -11127.838891521962
Iteration 10800: Loss = -11127.83922227616
1
Iteration 10900: Loss = -11127.838814747442
Iteration 11000: Loss = -11128.031074195622
1
Iteration 11100: Loss = -11127.838834115364
Iteration 11200: Loss = -11127.838841307987
Iteration 11300: Loss = -11127.898295991585
1
Iteration 11400: Loss = -11127.838881134321
Iteration 11500: Loss = -11127.838836728177
Iteration 11600: Loss = -11127.852383735593
1
Iteration 11700: Loss = -11127.838827390588
Iteration 11800: Loss = -11127.83881670284
Iteration 11900: Loss = -11127.839751174335
1
Iteration 12000: Loss = -11127.8388245158
Iteration 12100: Loss = -11128.009076147751
1
Iteration 12200: Loss = -11127.838807162183
Iteration 12300: Loss = -11127.838807204484
Iteration 12400: Loss = -11127.875548073856
1
Iteration 12500: Loss = -11127.838791461107
Iteration 12600: Loss = -11127.838812722675
Iteration 12700: Loss = -11127.853617446044
1
Iteration 12800: Loss = -11127.838802260792
Iteration 12900: Loss = -11127.838815714656
Iteration 13000: Loss = -11127.838892206548
Iteration 13100: Loss = -11127.838729710535
Iteration 13200: Loss = -11127.868931582132
1
Iteration 13300: Loss = -11127.838708318577
Iteration 13400: Loss = -11127.838734452736
Iteration 13500: Loss = -11127.838914737691
1
Iteration 13600: Loss = -11127.838744236258
Iteration 13700: Loss = -11128.342491775817
1
Iteration 13800: Loss = -11127.83867412064
Iteration 13900: Loss = -11127.838654053281
Iteration 14000: Loss = -11127.838921764926
1
Iteration 14100: Loss = -11127.838730096102
Iteration 14200: Loss = -11127.838711656375
Iteration 14300: Loss = -11127.904796510304
1
Iteration 14400: Loss = -11127.838687775773
Iteration 14500: Loss = -11127.838689473458
Iteration 14600: Loss = -11128.027602835178
1
Iteration 14700: Loss = -11127.83870297605
Iteration 14800: Loss = -11127.838670291203
Iteration 14900: Loss = -11127.918070370259
1
Iteration 15000: Loss = -11127.838649825151
Iteration 15100: Loss = -11127.83865391377
Iteration 15200: Loss = -11128.081306819719
1
Iteration 15300: Loss = -11127.838710633456
Iteration 15400: Loss = -11127.83865281128
Iteration 15500: Loss = -11128.025740775567
1
Iteration 15600: Loss = -11127.838669448374
Iteration 15700: Loss = -11127.838664889992
Iteration 15800: Loss = -11128.615149240191
1
Iteration 15900: Loss = -11127.838699479575
Iteration 16000: Loss = -11127.838668203649
Iteration 16100: Loss = -11127.838659721225
Iteration 16200: Loss = -11127.84069193808
1
Iteration 16300: Loss = -11127.838663173561
Iteration 16400: Loss = -11127.83866305537
Iteration 16500: Loss = -11127.838880597816
1
Iteration 16600: Loss = -11127.83865557784
Iteration 16700: Loss = -11127.83865914417
Iteration 16800: Loss = -11127.838947518147
1
Iteration 16900: Loss = -11127.838585770205
Iteration 17000: Loss = -11127.838584092986
Iteration 17100: Loss = -11127.840922087607
1
Iteration 17200: Loss = -11127.838598892196
Iteration 17300: Loss = -11127.838595778754
Iteration 17400: Loss = -11127.839191951254
1
Iteration 17500: Loss = -11127.838589385101
Iteration 17600: Loss = -11127.840004624995
1
Iteration 17700: Loss = -11127.838600500634
Iteration 17800: Loss = -11127.8385439691
Iteration 17900: Loss = -11128.246864462008
1
Iteration 18000: Loss = -11127.838575728892
Iteration 18100: Loss = -11127.838546501112
Iteration 18200: Loss = -11127.860622856844
1
Iteration 18300: Loss = -11127.838533604563
Iteration 18400: Loss = -11127.838543416463
Iteration 18500: Loss = -11127.841807602668
1
Iteration 18600: Loss = -11127.83855113199
Iteration 18700: Loss = -11127.838546976542
Iteration 18800: Loss = -11127.839271001247
1
Iteration 18900: Loss = -11127.838548562951
Iteration 19000: Loss = -11128.006553614046
1
Iteration 19100: Loss = -11127.838542364443
Iteration 19200: Loss = -11127.838547309786
Iteration 19300: Loss = -11127.839142125944
1
Iteration 19400: Loss = -11127.838528260361
Iteration 19500: Loss = -11127.903305026946
1
Iteration 19600: Loss = -11127.83855912728
Iteration 19700: Loss = -11127.838538926339
Iteration 19800: Loss = -11127.83900884635
1
Iteration 19900: Loss = -11127.83855362477
pi: tensor([[0.8393, 0.1607],
        [0.2841, 0.7159]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4000, 0.6000], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2545, 0.1034],
         [0.5437, 0.2004]],

        [[0.6445, 0.0983],
         [0.6379, 0.6006]],

        [[0.5675, 0.0973],
         [0.6469, 0.6007]],

        [[0.7049, 0.1129],
         [0.7011, 0.5537]],

        [[0.5647, 0.1150],
         [0.6005, 0.6522]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448376182574403
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7719210443888802
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8075651037638424
Global Adjusted Rand Index: 0.8460875163269357
Average Adjusted Rand Index: 0.8455014734089954
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21067.377575429004
Iteration 100: Loss = -11275.781657603524
Iteration 200: Loss = -11274.102536993902
Iteration 300: Loss = -11272.816270985573
Iteration 400: Loss = -11268.620669822947
Iteration 500: Loss = -11263.315166162234
Iteration 600: Loss = -11190.907585941533
Iteration 700: Loss = -11159.553424205511
Iteration 800: Loss = -11158.99899448493
Iteration 900: Loss = -11158.849431991493
Iteration 1000: Loss = -11158.761994615837
Iteration 1100: Loss = -11158.696638700518
Iteration 1200: Loss = -11158.640240001221
Iteration 1300: Loss = -11158.578505296733
Iteration 1400: Loss = -11158.492514444994
Iteration 1500: Loss = -11158.341278962287
Iteration 1600: Loss = -11158.024223315355
Iteration 1700: Loss = -11157.713637363184
Iteration 1800: Loss = -11153.619493570135
Iteration 1900: Loss = -11129.18303638175
Iteration 2000: Loss = -11127.969938758912
Iteration 2100: Loss = -11127.908708932204
Iteration 2200: Loss = -11127.878731247589
Iteration 2300: Loss = -11127.869357136598
Iteration 2400: Loss = -11127.864479884018
Iteration 2500: Loss = -11127.859859334501
Iteration 2600: Loss = -11127.857014275292
Iteration 2700: Loss = -11127.854843097259
Iteration 2800: Loss = -11127.853435936137
Iteration 2900: Loss = -11127.851707395612
Iteration 3000: Loss = -11127.850577260644
Iteration 3100: Loss = -11127.849758385535
Iteration 3200: Loss = -11127.849426712464
Iteration 3300: Loss = -11127.847978922635
Iteration 3400: Loss = -11127.847283978916
Iteration 3500: Loss = -11127.847232406324
Iteration 3600: Loss = -11127.846187923304
Iteration 3700: Loss = -11127.84511672291
Iteration 3800: Loss = -11127.84326675206
Iteration 3900: Loss = -11127.846400586173
1
Iteration 4000: Loss = -11127.843533103862
2
Iteration 4100: Loss = -11127.841848166692
Iteration 4200: Loss = -11127.84159149459
Iteration 4300: Loss = -11127.842671218792
1
Iteration 4400: Loss = -11127.841157739465
Iteration 4500: Loss = -11127.840947614572
Iteration 4600: Loss = -11127.840813657325
Iteration 4700: Loss = -11127.840636590407
Iteration 4800: Loss = -11127.840603601977
Iteration 4900: Loss = -11127.840358182822
Iteration 5000: Loss = -11127.841026881053
1
Iteration 5100: Loss = -11127.840061758958
Iteration 5200: Loss = -11127.84286107304
1
Iteration 5300: Loss = -11127.839832344695
Iteration 5400: Loss = -11127.83973903677
Iteration 5500: Loss = -11127.83988057766
1
Iteration 5600: Loss = -11127.839751070182
Iteration 5700: Loss = -11127.839531151065
Iteration 5800: Loss = -11127.840763586468
1
Iteration 5900: Loss = -11127.841093616442
2
Iteration 6000: Loss = -11127.839346247858
Iteration 6100: Loss = -11127.839940971286
1
Iteration 6200: Loss = -11127.839264186845
Iteration 6300: Loss = -11127.83920683597
Iteration 6400: Loss = -11127.83925367704
Iteration 6500: Loss = -11127.840995078399
1
Iteration 6600: Loss = -11127.841165981648
2
Iteration 6700: Loss = -11127.86546355364
3
Iteration 6800: Loss = -11127.847939524272
4
Iteration 6900: Loss = -11127.839358374024
5
Iteration 7000: Loss = -11127.839028846325
Iteration 7100: Loss = -11127.875378553947
1
Iteration 7200: Loss = -11127.838919593692
Iteration 7300: Loss = -11127.838904873786
Iteration 7400: Loss = -11127.83911360624
1
Iteration 7500: Loss = -11127.83883341451
Iteration 7600: Loss = -11127.885300980925
1
Iteration 7700: Loss = -11127.83883219506
Iteration 7800: Loss = -11127.838823434478
Iteration 7900: Loss = -11127.839615123354
1
Iteration 8000: Loss = -11127.838802159591
Iteration 8100: Loss = -11127.83876761489
Iteration 8200: Loss = -11127.839077952836
1
Iteration 8300: Loss = -11127.838754591912
Iteration 8400: Loss = -11127.838734317453
Iteration 8500: Loss = -11127.838778721816
Iteration 8600: Loss = -11127.83870767229
Iteration 8700: Loss = -11128.086848310324
1
Iteration 8800: Loss = -11127.83870973092
Iteration 8900: Loss = -11127.838694408078
Iteration 9000: Loss = -11127.85376889388
1
Iteration 9100: Loss = -11127.838682580195
Iteration 9200: Loss = -11127.83868638749
Iteration 9300: Loss = -11127.838828877095
1
Iteration 9400: Loss = -11127.838637212883
Iteration 9500: Loss = -11127.857804424282
1
Iteration 9600: Loss = -11127.83862355935
Iteration 9700: Loss = -11127.851597568291
1
Iteration 9800: Loss = -11127.838639244588
Iteration 9900: Loss = -11127.838623927582
Iteration 10000: Loss = -11127.86908424419
1
Iteration 10100: Loss = -11127.838620091516
Iteration 10200: Loss = -11127.838598941618
Iteration 10300: Loss = -11127.84380721813
1
Iteration 10400: Loss = -11127.838620738139
Iteration 10500: Loss = -11127.83862480973
Iteration 10600: Loss = -11127.936550055238
1
Iteration 10700: Loss = -11127.838593862098
Iteration 10800: Loss = -11127.838620207607
Iteration 10900: Loss = -11127.84266410826
1
Iteration 11000: Loss = -11127.83861662476
Iteration 11100: Loss = -11128.321737736727
1
Iteration 11200: Loss = -11127.838588847311
Iteration 11300: Loss = -11127.838587667056
Iteration 11400: Loss = -11127.84592268836
1
Iteration 11500: Loss = -11127.838613705571
Iteration 11600: Loss = -11127.838577426295
Iteration 11700: Loss = -11127.838893605041
1
Iteration 11800: Loss = -11127.838553071108
Iteration 11900: Loss = -11127.838636167693
Iteration 12000: Loss = -11127.838659820422
Iteration 12100: Loss = -11127.838600610223
Iteration 12200: Loss = -11127.928267067187
1
Iteration 12300: Loss = -11127.838576706652
Iteration 12400: Loss = -11127.838563729098
Iteration 12500: Loss = -11127.838924415484
1
Iteration 12600: Loss = -11127.838576080016
Iteration 12700: Loss = -11127.838602130945
Iteration 12800: Loss = -11127.838602455993
Iteration 12900: Loss = -11127.838569539215
Iteration 13000: Loss = -11127.907350175967
1
Iteration 13100: Loss = -11127.838579352296
Iteration 13200: Loss = -11127.838557868936
Iteration 13300: Loss = -11127.843164799247
1
Iteration 13400: Loss = -11127.838571443493
Iteration 13500: Loss = -11127.83857791983
Iteration 13600: Loss = -11127.838742720929
1
Iteration 13700: Loss = -11127.838548432383
Iteration 13800: Loss = -11127.838751518542
1
Iteration 13900: Loss = -11127.838587842558
Iteration 14000: Loss = -11127.838544226284
Iteration 14100: Loss = -11127.838588819683
Iteration 14200: Loss = -11127.838617037622
Iteration 14300: Loss = -11127.838546959223
Iteration 14400: Loss = -11127.851020554683
1
Iteration 14500: Loss = -11127.838598919185
Iteration 14600: Loss = -11127.838569339956
Iteration 14700: Loss = -11127.838605837704
Iteration 14800: Loss = -11127.838558967802
Iteration 14900: Loss = -11127.841369853002
1
Iteration 15000: Loss = -11127.838560305367
Iteration 15100: Loss = -11127.838568091503
Iteration 15200: Loss = -11127.838816308005
1
Iteration 15300: Loss = -11127.838562419289
Iteration 15400: Loss = -11127.838573205452
Iteration 15500: Loss = -11127.838676874046
1
Iteration 15600: Loss = -11127.838551202372
Iteration 15700: Loss = -11127.88460817786
1
Iteration 15800: Loss = -11127.838582835577
Iteration 15900: Loss = -11127.838552562838
Iteration 16000: Loss = -11127.838775259288
1
Iteration 16100: Loss = -11127.838524453191
Iteration 16200: Loss = -11127.91003482497
1
Iteration 16300: Loss = -11127.838539272443
Iteration 16400: Loss = -11127.838546969773
Iteration 16500: Loss = -11127.839264378785
1
Iteration 16600: Loss = -11127.838542188005
Iteration 16700: Loss = -11127.88349468759
1
Iteration 16800: Loss = -11127.838557527248
Iteration 16900: Loss = -11127.838572116218
Iteration 17000: Loss = -11127.839224933614
1
Iteration 17100: Loss = -11127.838558114732
Iteration 17200: Loss = -11127.838596721958
Iteration 17300: Loss = -11127.838624233385
Iteration 17400: Loss = -11127.838544437964
Iteration 17500: Loss = -11127.881981589104
1
Iteration 17600: Loss = -11127.838542238573
Iteration 17700: Loss = -11127.838537495849
Iteration 17800: Loss = -11127.841266455216
1
Iteration 17900: Loss = -11127.838558562222
Iteration 18000: Loss = -11127.838539029673
Iteration 18100: Loss = -11127.839610991081
1
Iteration 18200: Loss = -11127.838523514241
Iteration 18300: Loss = -11127.838630512635
1
Iteration 18400: Loss = -11127.838535518234
Iteration 18500: Loss = -11127.838519649837
Iteration 18600: Loss = -11127.842796981862
1
Iteration 18700: Loss = -11127.8385257418
Iteration 18800: Loss = -11127.838530842559
Iteration 18900: Loss = -11127.838576951499
Iteration 19000: Loss = -11127.838525370833
Iteration 19100: Loss = -11127.885673034141
1
Iteration 19200: Loss = -11127.838540385575
Iteration 19300: Loss = -11127.838548682457
Iteration 19400: Loss = -11127.838988739575
1
Iteration 19500: Loss = -11127.838556498193
Iteration 19600: Loss = -11127.85147747214
1
Iteration 19700: Loss = -11127.8385461299
Iteration 19800: Loss = -11127.838528503135
Iteration 19900: Loss = -11127.844836097658
1
pi: tensor([[0.8405, 0.1595],
        [0.2848, 0.7152]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4023, 0.5977], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2538, 0.1037],
         [0.6086, 0.2008]],

        [[0.6611, 0.0991],
         [0.6741, 0.6681]],

        [[0.7139, 0.0976],
         [0.6392, 0.5909]],

        [[0.6126, 0.1125],
         [0.5073, 0.6550]],

        [[0.6386, 0.1154],
         [0.6748, 0.5381]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448376182574403
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7719210443888802
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8075651037638424
Global Adjusted Rand Index: 0.8460875163269357
Average Adjusted Rand Index: 0.8455014734089954
11180.534908716018
[0.8460875163269357, 0.8460875163269357] [0.8455014734089954, 0.8455014734089954] [11127.880388696049, 11127.838546693285]
-------------------------------------
This iteration is 69
True Objective function: Loss = -10879.624608799195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21243.169744795076
Iteration 100: Loss = -11001.232045947687
Iteration 200: Loss = -10999.972924503782
Iteration 300: Loss = -10999.138793125245
Iteration 400: Loss = -10998.43316666906
Iteration 500: Loss = -10997.981183439344
Iteration 600: Loss = -10997.680564545855
Iteration 700: Loss = -10997.458633421782
Iteration 800: Loss = -10997.299862411899
Iteration 900: Loss = -10997.181403437284
Iteration 1000: Loss = -10997.093169278336
Iteration 1100: Loss = -10997.016978659156
Iteration 1200: Loss = -10996.951072249583
Iteration 1300: Loss = -10996.888577856758
Iteration 1400: Loss = -10996.82194074109
Iteration 1500: Loss = -10996.778325435364
Iteration 1600: Loss = -10996.744538181585
Iteration 1700: Loss = -10996.718057774295
Iteration 1800: Loss = -10996.696280078813
Iteration 1900: Loss = -10996.677671754156
Iteration 2000: Loss = -10996.66166701121
Iteration 2100: Loss = -10996.647893087169
Iteration 2200: Loss = -10996.636111876212
Iteration 2300: Loss = -10996.626203847205
Iteration 2400: Loss = -10996.617869092404
Iteration 2500: Loss = -10996.61079147078
Iteration 2600: Loss = -10996.60474185881
Iteration 2700: Loss = -10996.599481192428
Iteration 2800: Loss = -10996.594773417246
Iteration 2900: Loss = -10996.590327659977
Iteration 3000: Loss = -10996.585866546508
Iteration 3100: Loss = -10996.581059964026
Iteration 3200: Loss = -10996.575833104642
Iteration 3300: Loss = -10996.570050910474
Iteration 3400: Loss = -10996.563760713932
Iteration 3500: Loss = -10996.557209908258
Iteration 3600: Loss = -10996.551272365408
Iteration 3700: Loss = -10996.546043167102
Iteration 3800: Loss = -10996.54147368297
Iteration 3900: Loss = -10996.537625090023
Iteration 4000: Loss = -10996.534438338213
Iteration 4100: Loss = -10996.53186986842
Iteration 4200: Loss = -10996.533962636298
1
Iteration 4300: Loss = -10996.52808768995
Iteration 4400: Loss = -10996.526727002716
Iteration 4500: Loss = -10996.525735058873
Iteration 4600: Loss = -10996.524880359555
Iteration 4700: Loss = -10996.530469840298
1
Iteration 4800: Loss = -10996.523823966028
Iteration 4900: Loss = -10996.523437960326
Iteration 5000: Loss = -10996.523821922554
1
Iteration 5100: Loss = -10996.522984833387
Iteration 5200: Loss = -10996.522863062464
Iteration 5300: Loss = -10996.522809116901
Iteration 5400: Loss = -10996.522644288129
Iteration 5500: Loss = -10996.522621840455
Iteration 5600: Loss = -10996.52268410008
Iteration 5700: Loss = -10996.522592038937
Iteration 5800: Loss = -10996.522569502133
Iteration 5900: Loss = -10996.522590157576
Iteration 6000: Loss = -10996.522520752082
Iteration 6100: Loss = -10996.522532999177
Iteration 6200: Loss = -10996.52270603628
1
Iteration 6300: Loss = -10996.522523164802
Iteration 6400: Loss = -10996.52252558394
Iteration 6500: Loss = -10996.522565013744
Iteration 6600: Loss = -10996.52255803003
Iteration 6700: Loss = -10996.523261231447
1
Iteration 6800: Loss = -10996.52256051163
Iteration 6900: Loss = -10996.52340650806
1
Iteration 7000: Loss = -10996.522856110982
2
Iteration 7100: Loss = -10996.522583665963
Iteration 7200: Loss = -10996.522753006613
1
Iteration 7300: Loss = -10996.522930407777
2
Iteration 7400: Loss = -10996.522830816764
3
Iteration 7500: Loss = -10996.522752343144
4
Iteration 7600: Loss = -10996.522568046688
Iteration 7700: Loss = -10996.522825639584
1
Iteration 7800: Loss = -10996.562806966873
2
Iteration 7900: Loss = -10996.522503638014
Iteration 8000: Loss = -10996.522966698016
1
Iteration 8100: Loss = -10996.52366822259
2
Iteration 8200: Loss = -10996.522549243029
Iteration 8300: Loss = -10996.540892131887
1
Iteration 8400: Loss = -10996.522569444705
Iteration 8500: Loss = -10996.522557748243
Iteration 8600: Loss = -10996.52251426781
Iteration 8700: Loss = -10996.522546717923
Iteration 8800: Loss = -10996.672220161527
1
Iteration 8900: Loss = -10996.522545901233
Iteration 9000: Loss = -10996.522556652502
Iteration 9100: Loss = -10996.580331369276
1
Iteration 9200: Loss = -10996.522535915848
Iteration 9300: Loss = -10996.522550714028
Iteration 9400: Loss = -10996.524247059888
1
Iteration 9500: Loss = -10996.522583420076
Iteration 9600: Loss = -10996.571192544801
1
Iteration 9700: Loss = -10996.522554973893
Iteration 9800: Loss = -10996.522531707924
Iteration 9900: Loss = -10996.67639180712
1
Iteration 10000: Loss = -10996.522566547103
Iteration 10100: Loss = -10996.52252800047
Iteration 10200: Loss = -10996.558993521656
1
Iteration 10300: Loss = -10996.52256884623
Iteration 10400: Loss = -10996.522678647778
1
Iteration 10500: Loss = -10996.52260745705
Iteration 10600: Loss = -10996.523302796195
1
Iteration 10700: Loss = -10996.522581134683
Iteration 10800: Loss = -10996.522522001476
Iteration 10900: Loss = -10996.524602040241
1
Iteration 11000: Loss = -10996.522554152012
Iteration 11100: Loss = -10996.53055554863
1
Iteration 11200: Loss = -10996.522533589447
Iteration 11300: Loss = -10996.523299657707
1
Iteration 11400: Loss = -10996.526174369621
2
Iteration 11500: Loss = -10996.545624650358
3
Iteration 11600: Loss = -10996.522572099413
Iteration 11700: Loss = -10996.522705604393
1
Iteration 11800: Loss = -10996.52669653813
2
Iteration 11900: Loss = -10996.552955775174
3
Iteration 12000: Loss = -10996.522620489695
Iteration 12100: Loss = -10996.522710299398
Iteration 12200: Loss = -10996.533210181615
1
Iteration 12300: Loss = -10996.688909292752
2
Iteration 12400: Loss = -10996.522570588482
Iteration 12500: Loss = -10996.522838365327
1
Iteration 12600: Loss = -10996.554440046071
2
Iteration 12700: Loss = -10996.52257153956
Iteration 12800: Loss = -10996.53143645818
1
Iteration 12900: Loss = -10996.569722322169
2
Iteration 13000: Loss = -10996.522542231603
Iteration 13100: Loss = -10996.52388129227
1
Iteration 13200: Loss = -10996.522601447203
Iteration 13300: Loss = -10996.52258330068
Iteration 13400: Loss = -10996.523187236502
1
Iteration 13500: Loss = -10996.558074463566
2
Iteration 13600: Loss = -10996.522681691335
Iteration 13700: Loss = -10996.522560938261
Iteration 13800: Loss = -10996.52518791933
1
Iteration 13900: Loss = -10996.524939537761
2
Iteration 14000: Loss = -10996.52670502228
3
Iteration 14100: Loss = -10996.529130105482
4
Iteration 14200: Loss = -10996.52802088927
5
Iteration 14300: Loss = -10996.526009926447
6
Iteration 14400: Loss = -10996.652311098866
7
Iteration 14500: Loss = -10996.543416991904
8
Iteration 14600: Loss = -10996.522652170224
Iteration 14700: Loss = -10996.522567392078
Iteration 14800: Loss = -10996.523162117308
1
Iteration 14900: Loss = -10996.52779055214
2
Iteration 15000: Loss = -10996.522675037786
3
Iteration 15100: Loss = -10996.522600105536
Iteration 15200: Loss = -10996.543203980265
1
Iteration 15300: Loss = -10996.522610460552
Iteration 15400: Loss = -10996.522661885117
Iteration 15500: Loss = -10996.523509174045
1
Iteration 15600: Loss = -10996.52934953103
2
Iteration 15700: Loss = -10996.529910502175
3
Iteration 15800: Loss = -10996.578642476738
4
Iteration 15900: Loss = -10996.522721430869
Iteration 16000: Loss = -10996.522655238716
Iteration 16100: Loss = -10996.55283183878
1
Iteration 16200: Loss = -10996.522572252945
Iteration 16300: Loss = -10996.523156013578
1
Iteration 16400: Loss = -10996.75685586175
2
Iteration 16500: Loss = -10996.522562024136
Iteration 16600: Loss = -10996.523129494231
1
Iteration 16700: Loss = -10996.534165499124
2
Iteration 16800: Loss = -10996.522658744543
Iteration 16900: Loss = -10996.522592471065
Iteration 17000: Loss = -10996.522957943953
1
Iteration 17100: Loss = -10996.540623528648
2
Iteration 17200: Loss = -10996.825498587079
3
Iteration 17300: Loss = -10996.522581276115
Iteration 17400: Loss = -10996.52412103354
1
Iteration 17500: Loss = -10996.522921607746
2
Iteration 17600: Loss = -10996.526095438257
3
Iteration 17700: Loss = -10996.544563089632
4
Iteration 17800: Loss = -10996.522536960081
Iteration 17900: Loss = -10996.523674703072
1
Iteration 18000: Loss = -10996.592982673666
2
Iteration 18100: Loss = -10996.5225489667
Iteration 18200: Loss = -10996.522682507044
1
Iteration 18300: Loss = -10996.5225570623
Iteration 18400: Loss = -10996.52267323236
1
Iteration 18500: Loss = -10996.526292513685
2
Iteration 18600: Loss = -10996.522662565014
3
Iteration 18700: Loss = -10996.522648484608
Iteration 18800: Loss = -10996.524524871544
1
Iteration 18900: Loss = -10996.52300483064
2
Iteration 19000: Loss = -10996.522583109287
Iteration 19100: Loss = -10996.522822243229
1
Iteration 19200: Loss = -10996.57219067069
2
Iteration 19300: Loss = -10996.55462556001
3
Iteration 19400: Loss = -10996.538497941634
4
Iteration 19500: Loss = -10996.526588324406
5
Iteration 19600: Loss = -10996.679999646598
6
Iteration 19700: Loss = -10996.522552033332
Iteration 19800: Loss = -10996.522766822389
1
Iteration 19900: Loss = -10996.522532376166
pi: tensor([[0.5530, 0.4470],
        [0.0892, 0.9108]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0583, 0.9417], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2595, 0.1899],
         [0.6763, 0.1534]],

        [[0.6932, 0.1827],
         [0.5371, 0.5010]],

        [[0.6480, 0.2002],
         [0.6356, 0.6048]],

        [[0.5524, 0.1813],
         [0.6997, 0.5493]],

        [[0.5413, 0.1888],
         [0.6779, 0.6359]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.01333633701284073
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.006767238045575733
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.019446650964606977
Global Adjusted Rand Index: 0.001834415285049805
Average Adjusted Rand Index: 0.002423285296597526
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24088.134420156995
Iteration 100: Loss = -10999.373607968359
Iteration 200: Loss = -10847.472563108066
Iteration 300: Loss = -10825.106129822449
Iteration 400: Loss = -10824.732809606108
Iteration 500: Loss = -10824.622640858799
Iteration 600: Loss = -10824.57050135015
Iteration 700: Loss = -10824.541455920167
Iteration 800: Loss = -10824.523274159797
Iteration 900: Loss = -10824.511182550434
Iteration 1000: Loss = -10824.502542648128
Iteration 1100: Loss = -10824.49625062725
Iteration 1200: Loss = -10824.491428732857
Iteration 1300: Loss = -10824.48776063947
Iteration 1400: Loss = -10824.484877232828
Iteration 1500: Loss = -10824.48251617389
Iteration 1600: Loss = -10824.480589444625
Iteration 1700: Loss = -10824.47897438581
Iteration 1800: Loss = -10824.477652250336
Iteration 1900: Loss = -10824.476521759912
Iteration 2000: Loss = -10824.475590891758
Iteration 2100: Loss = -10824.474692496096
Iteration 2200: Loss = -10824.473976957315
Iteration 2300: Loss = -10824.473351493047
Iteration 2400: Loss = -10824.472796412107
Iteration 2500: Loss = -10824.472277431123
Iteration 2600: Loss = -10824.471826248891
Iteration 2700: Loss = -10824.471418757934
Iteration 2800: Loss = -10824.47356349315
1
Iteration 2900: Loss = -10824.470760910099
Iteration 3000: Loss = -10824.47053814092
Iteration 3100: Loss = -10824.471954937993
1
Iteration 3200: Loss = -10824.470466220933
Iteration 3300: Loss = -10824.46973062684
Iteration 3400: Loss = -10824.469818245221
Iteration 3500: Loss = -10824.469321045059
Iteration 3600: Loss = -10824.494887137502
1
Iteration 3700: Loss = -10824.46893481694
Iteration 3800: Loss = -10824.468766642496
Iteration 3900: Loss = -10824.468613628105
Iteration 4000: Loss = -10824.468472456934
Iteration 4100: Loss = -10824.468460644817
Iteration 4200: Loss = -10824.468275960731
Iteration 4300: Loss = -10824.468152506377
Iteration 4400: Loss = -10824.46828056791
1
Iteration 4500: Loss = -10824.46802262275
Iteration 4600: Loss = -10824.467960292675
Iteration 4700: Loss = -10824.467953148518
Iteration 4800: Loss = -10824.467809690723
Iteration 4900: Loss = -10824.46840889349
1
Iteration 5000: Loss = -10824.467690894002
Iteration 5100: Loss = -10824.467667076311
Iteration 5200: Loss = -10824.46770280094
Iteration 5300: Loss = -10824.467595064718
Iteration 5400: Loss = -10824.468002454762
1
Iteration 5500: Loss = -10824.467525410697
Iteration 5600: Loss = -10824.467794981301
1
Iteration 5700: Loss = -10824.467895074034
2
Iteration 5800: Loss = -10824.478871900466
3
Iteration 5900: Loss = -10824.467393118979
Iteration 6000: Loss = -10824.468984781579
1
Iteration 6100: Loss = -10824.467332052784
Iteration 6200: Loss = -10824.467329524829
Iteration 6300: Loss = -10824.467389900976
Iteration 6400: Loss = -10824.467398120905
Iteration 6500: Loss = -10824.4675908375
1
Iteration 6600: Loss = -10824.467308340127
Iteration 6700: Loss = -10824.468008445987
1
Iteration 6800: Loss = -10824.469095770633
2
Iteration 6900: Loss = -10824.48674716134
3
Iteration 7000: Loss = -10824.467477367356
4
Iteration 7100: Loss = -10824.467139724184
Iteration 7200: Loss = -10824.467162695668
Iteration 7300: Loss = -10824.485724359025
1
Iteration 7400: Loss = -10824.467193196062
Iteration 7500: Loss = -10824.468766983491
1
Iteration 7600: Loss = -10824.469151816238
2
Iteration 7700: Loss = -10824.4676315685
3
Iteration 7800: Loss = -10824.467154697279
Iteration 7900: Loss = -10824.467190849688
Iteration 8000: Loss = -10824.46710461603
Iteration 8100: Loss = -10824.4670888657
Iteration 8200: Loss = -10824.46707375604
Iteration 8300: Loss = -10824.467345910803
1
Iteration 8400: Loss = -10824.46812208519
2
Iteration 8500: Loss = -10824.467555918205
3
Iteration 8600: Loss = -10824.467083077043
Iteration 8700: Loss = -10824.467153717744
Iteration 8800: Loss = -10824.467053214297
Iteration 8900: Loss = -10824.467189260866
1
Iteration 9000: Loss = -10824.467341536014
2
Iteration 9100: Loss = -10824.467084854674
Iteration 9200: Loss = -10824.467134307779
Iteration 9300: Loss = -10824.596986136676
1
Iteration 9400: Loss = -10824.467044806783
Iteration 9500: Loss = -10824.548711608582
1
Iteration 9600: Loss = -10824.467057421944
Iteration 9700: Loss = -10824.467044730969
Iteration 9800: Loss = -10824.467128093169
Iteration 9900: Loss = -10824.467038581908
Iteration 10000: Loss = -10824.468688463974
1
Iteration 10100: Loss = -10824.50523435932
2
Iteration 10200: Loss = -10824.47422756849
3
Iteration 10300: Loss = -10824.467427591957
4
Iteration 10400: Loss = -10824.747748261283
5
Iteration 10500: Loss = -10824.467149293727
6
Iteration 10600: Loss = -10824.47107209303
7
Iteration 10700: Loss = -10824.467047513892
Iteration 10800: Loss = -10824.467031567094
Iteration 10900: Loss = -10824.468930860809
1
Iteration 11000: Loss = -10824.467021388286
Iteration 11100: Loss = -10824.467242015096
1
Iteration 11200: Loss = -10824.467323960467
2
Iteration 11300: Loss = -10824.471694422033
3
Iteration 11400: Loss = -10824.4725402263
4
Iteration 11500: Loss = -10824.466999893242
Iteration 11600: Loss = -10824.467672290597
1
Iteration 11700: Loss = -10824.46703826958
Iteration 11800: Loss = -10824.467317190676
1
Iteration 11900: Loss = -10824.467520732951
2
Iteration 12000: Loss = -10824.471504497236
3
Iteration 12100: Loss = -10824.468132653248
4
Iteration 12200: Loss = -10824.46736370351
5
Iteration 12300: Loss = -10824.467283587574
6
Iteration 12400: Loss = -10824.468299892596
7
Iteration 12500: Loss = -10824.471980135093
8
Iteration 12600: Loss = -10824.47376367431
9
Iteration 12700: Loss = -10824.467484619214
10
Iteration 12800: Loss = -10824.472688163503
11
Iteration 12900: Loss = -10824.630389968406
12
Iteration 13000: Loss = -10824.4690796841
13
Iteration 13100: Loss = -10824.46755848337
14
Iteration 13200: Loss = -10824.48184496979
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[0.7136, 0.2864],
        [0.2777, 0.7223]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4507, 0.5493], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2133, 0.0989],
         [0.6947, 0.2457]],

        [[0.5200, 0.0866],
         [0.6032, 0.7115]],

        [[0.6046, 0.0945],
         [0.6451, 0.5285]],

        [[0.7016, 0.0858],
         [0.5181, 0.5904]],

        [[0.6541, 0.1062],
         [0.7191, 0.6581]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 92
Adjusted Rand Index: 0.7025982975809663
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6691246144255645
Global Adjusted Rand Index: 0.8096196414883815
Average Adjusted Rand Index: 0.8116354094657714
10879.624608799195
[0.001834415285049805, 0.8096196414883815] [0.002423285296597526, 0.8116354094657714] [10996.529135090266, 10824.48184496979]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11047.776767834708
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23046.504139668887
Iteration 100: Loss = -11113.299789069313
Iteration 200: Loss = -11112.279039968804
Iteration 300: Loss = -11111.242569784443
Iteration 400: Loss = -11110.23706839315
Iteration 500: Loss = -11109.369428490616
Iteration 600: Loss = -11109.117066262266
Iteration 700: Loss = -11108.989668734686
Iteration 800: Loss = -11108.895391721284
Iteration 900: Loss = -11108.822431856499
Iteration 1000: Loss = -11108.760626581927
Iteration 1100: Loss = -11108.650742616022
Iteration 1200: Loss = -11108.014106125318
Iteration 1300: Loss = -11102.991205391601
Iteration 1400: Loss = -11102.703937238291
Iteration 1500: Loss = -11102.657925169204
Iteration 1600: Loss = -11102.638093281712
Iteration 1700: Loss = -11102.626510964355
Iteration 1800: Loss = -11102.618747538265
Iteration 1900: Loss = -11102.613057748576
Iteration 2000: Loss = -11102.608684820687
Iteration 2100: Loss = -11102.605058511937
Iteration 2200: Loss = -11102.602005167619
Iteration 2300: Loss = -11102.599053280152
Iteration 2400: Loss = -11102.595744550134
Iteration 2500: Loss = -11102.589049685188
Iteration 2600: Loss = -11095.067299363103
Iteration 2700: Loss = -11071.92348203627
Iteration 2800: Loss = -11065.760878270148
Iteration 2900: Loss = -11064.946264098988
Iteration 3000: Loss = -11061.687501835966
Iteration 3100: Loss = -11061.465477778012
Iteration 3200: Loss = -11059.078903897986
Iteration 3300: Loss = -11054.636584342696
Iteration 3400: Loss = -11050.927783932271
Iteration 3500: Loss = -11038.038685924832
Iteration 3600: Loss = -11034.38615867787
Iteration 3700: Loss = -11031.629119913063
Iteration 3800: Loss = -11029.012535930124
Iteration 3900: Loss = -11026.23489390888
Iteration 4000: Loss = -11026.21850797454
Iteration 4100: Loss = -11026.054546929521
Iteration 4200: Loss = -11018.849544512224
Iteration 4300: Loss = -11018.836696085182
Iteration 4400: Loss = -11014.415449576443
Iteration 4500: Loss = -11014.224140221608
Iteration 4600: Loss = -11014.218656236018
Iteration 4700: Loss = -11010.812191810168
Iteration 4800: Loss = -11005.197328273514
Iteration 4900: Loss = -10996.03585691918
Iteration 5000: Loss = -10996.032979189602
Iteration 5100: Loss = -10996.031251994356
Iteration 5200: Loss = -10991.80065233547
Iteration 5300: Loss = -10991.786911132443
Iteration 5400: Loss = -10991.782020115006
Iteration 5500: Loss = -10991.784386588155
1
Iteration 5600: Loss = -10991.780822760424
Iteration 5700: Loss = -10988.724843209517
Iteration 5800: Loss = -10988.679928188654
Iteration 5900: Loss = -10988.636668368963
Iteration 6000: Loss = -10988.636664297392
Iteration 6100: Loss = -10988.632943045517
Iteration 6200: Loss = -10988.632715076006
Iteration 6300: Loss = -10988.63184725725
Iteration 6400: Loss = -10988.630227322394
Iteration 6500: Loss = -10988.631110534545
1
Iteration 6600: Loss = -10988.629631365297
Iteration 6700: Loss = -10988.628828623941
Iteration 6800: Loss = -10988.640849764392
1
Iteration 6900: Loss = -10988.571254365925
Iteration 7000: Loss = -10988.524364249066
Iteration 7100: Loss = -10988.45107234502
Iteration 7200: Loss = -10988.450563494833
Iteration 7300: Loss = -10988.466246928561
1
Iteration 7400: Loss = -10988.441644465564
Iteration 7500: Loss = -10988.189944258664
Iteration 7600: Loss = -10988.187160198591
Iteration 7700: Loss = -10988.178828576218
Iteration 7800: Loss = -10988.189058846563
1
Iteration 7900: Loss = -10988.177952483118
Iteration 8000: Loss = -10988.178193160906
1
Iteration 8100: Loss = -10988.17860885089
2
Iteration 8200: Loss = -10988.178830028251
3
Iteration 8300: Loss = -10988.177084297644
Iteration 8400: Loss = -10988.177439606296
1
Iteration 8500: Loss = -10988.178297110313
2
Iteration 8600: Loss = -10988.17711537149
Iteration 8700: Loss = -10988.102583049278
Iteration 8800: Loss = -10988.11479901447
1
Iteration 8900: Loss = -10988.101691898051
Iteration 9000: Loss = -10988.101632671027
Iteration 9100: Loss = -10988.103105954702
1
Iteration 9200: Loss = -10988.101479541758
Iteration 9300: Loss = -10988.121719174089
1
Iteration 9400: Loss = -10988.099626518331
Iteration 9500: Loss = -10988.084523207803
Iteration 9600: Loss = -10988.084879718324
1
Iteration 9700: Loss = -10988.084102015056
Iteration 9800: Loss = -10988.084082688412
Iteration 9900: Loss = -10988.084159186223
Iteration 10000: Loss = -10988.083017568582
Iteration 10100: Loss = -10988.074036441447
Iteration 10200: Loss = -10988.074510297662
1
Iteration 10300: Loss = -10988.071529223098
Iteration 10400: Loss = -10988.1057987138
1
Iteration 10500: Loss = -10988.071324598326
Iteration 10600: Loss = -10988.07131011781
Iteration 10700: Loss = -10988.071510579113
1
Iteration 10800: Loss = -10988.071293436433
Iteration 10900: Loss = -10988.071741595018
1
Iteration 11000: Loss = -10988.071325365085
Iteration 11100: Loss = -10988.071283316702
Iteration 11200: Loss = -10988.211155799163
1
Iteration 11300: Loss = -10988.07121827065
Iteration 11400: Loss = -10988.071048681828
Iteration 11500: Loss = -10988.072446972745
1
Iteration 11600: Loss = -10988.09497477332
2
Iteration 11700: Loss = -10988.07095575214
Iteration 11800: Loss = -10988.096879026883
1
Iteration 11900: Loss = -10988.071015739364
Iteration 12000: Loss = -10988.079078465804
1
Iteration 12100: Loss = -10988.144463816523
2
Iteration 12200: Loss = -10988.070887644735
Iteration 12300: Loss = -10988.160560289218
1
Iteration 12400: Loss = -10988.070919075584
Iteration 12500: Loss = -10988.073227417617
1
Iteration 12600: Loss = -10988.07091631566
Iteration 12700: Loss = -10988.072995722314
1
Iteration 12800: Loss = -10988.07088474556
Iteration 12900: Loss = -10988.078193431122
1
Iteration 13000: Loss = -10988.071464873085
2
Iteration 13100: Loss = -10988.070886815798
Iteration 13200: Loss = -10988.073457806977
1
Iteration 13300: Loss = -10988.07264431676
2
Iteration 13400: Loss = -10988.070839107864
Iteration 13500: Loss = -10988.071141508846
1
Iteration 13600: Loss = -10988.070806832127
Iteration 13700: Loss = -10988.075356250805
1
Iteration 13800: Loss = -10988.070829670569
Iteration 13900: Loss = -10988.219448345491
1
Iteration 14000: Loss = -10988.070861083623
Iteration 14100: Loss = -10988.07083108642
Iteration 14200: Loss = -10988.095108179716
1
Iteration 14300: Loss = -10988.070839174123
Iteration 14400: Loss = -10988.070845167455
Iteration 14500: Loss = -10988.071374459993
1
Iteration 14600: Loss = -10988.070827602563
Iteration 14700: Loss = -10988.073412236037
1
Iteration 14800: Loss = -10988.071411704599
2
Iteration 14900: Loss = -10988.149931804672
3
Iteration 15000: Loss = -10988.070793809198
Iteration 15100: Loss = -10988.070835806442
Iteration 15200: Loss = -10988.071056792549
1
Iteration 15300: Loss = -10988.070853870051
Iteration 15400: Loss = -10988.085206217202
1
Iteration 15500: Loss = -10988.070845760743
Iteration 15600: Loss = -10988.070898742775
Iteration 15700: Loss = -10988.09099250858
1
Iteration 15800: Loss = -10988.07084611216
Iteration 15900: Loss = -10988.07133203858
1
Iteration 16000: Loss = -10988.070828754926
Iteration 16100: Loss = -10988.070839770207
Iteration 16200: Loss = -10988.070834402703
Iteration 16300: Loss = -10988.071429560508
1
Iteration 16400: Loss = -10988.070803999735
Iteration 16500: Loss = -10988.070764233882
Iteration 16600: Loss = -10988.07106528559
1
Iteration 16700: Loss = -10988.070751366588
Iteration 16800: Loss = -10988.080058754458
1
Iteration 16900: Loss = -10988.070738491868
Iteration 17000: Loss = -10988.070751373505
Iteration 17100: Loss = -10988.080277822493
1
Iteration 17200: Loss = -10988.070726839276
Iteration 17300: Loss = -10988.070739829851
Iteration 17400: Loss = -10988.070942275877
1
Iteration 17500: Loss = -10988.070494611002
Iteration 17600: Loss = -10988.070891251726
1
Iteration 17700: Loss = -10988.070599596618
2
Iteration 17800: Loss = -10988.070505945137
Iteration 17900: Loss = -10988.184311249497
1
Iteration 18000: Loss = -10988.070489513446
Iteration 18100: Loss = -10988.070516950746
Iteration 18200: Loss = -10988.073962277387
1
Iteration 18300: Loss = -10988.07045562665
Iteration 18400: Loss = -10988.070506910472
Iteration 18500: Loss = -10988.07093789093
1
Iteration 18600: Loss = -10988.070499748448
Iteration 18700: Loss = -10988.08183011597
1
Iteration 18800: Loss = -10988.070492212511
Iteration 18900: Loss = -10988.070494885122
Iteration 19000: Loss = -10988.070590642212
Iteration 19100: Loss = -10988.070466921228
Iteration 19200: Loss = -10988.15103100718
1
Iteration 19300: Loss = -10988.070493063986
Iteration 19400: Loss = -10988.070512006048
Iteration 19500: Loss = -10988.07121030376
1
Iteration 19600: Loss = -10988.070484524875
Iteration 19700: Loss = -10988.070490940458
Iteration 19800: Loss = -10988.07076605063
1
Iteration 19900: Loss = -10988.07047914693
pi: tensor([[0.6640, 0.3360],
        [0.2434, 0.7566]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4649, 0.5351], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1963, 0.0991],
         [0.5880, 0.2449]],

        [[0.7130, 0.0976],
         [0.6296, 0.6361]],

        [[0.6616, 0.1212],
         [0.6145, 0.6884]],

        [[0.6088, 0.0928],
         [0.5806, 0.6586]],

        [[0.5545, 0.1032],
         [0.5928, 0.6758]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369635135591801
time is 1
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7720646154931399
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8822045595164437
time is 4
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.831347495867302
Average Adjusted Rand Index: 0.8313749327533827
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19718.7024016873
Iteration 100: Loss = -11112.405287940399
Iteration 200: Loss = -11111.688590438442
Iteration 300: Loss = -11110.560132572964
Iteration 400: Loss = -11108.518802764467
Iteration 500: Loss = -11107.181378921558
Iteration 600: Loss = -11105.962487394443
Iteration 700: Loss = -11105.037211628154
Iteration 800: Loss = -11096.25980234013
Iteration 900: Loss = -11028.424765795131
Iteration 1000: Loss = -11023.664605497424
Iteration 1100: Loss = -11023.272039094207
Iteration 1200: Loss = -11022.483254070796
Iteration 1300: Loss = -11022.28905049561
Iteration 1400: Loss = -11022.221244807473
Iteration 1500: Loss = -11022.18433359259
Iteration 1600: Loss = -11022.158474768585
Iteration 1700: Loss = -11022.135688767754
Iteration 1800: Loss = -11022.123090257744
Iteration 1900: Loss = -11022.113630067139
Iteration 2000: Loss = -11022.085954418999
Iteration 2100: Loss = -11022.019070605893
Iteration 2200: Loss = -11021.87984143118
Iteration 2300: Loss = -11021.86963851668
Iteration 2400: Loss = -11021.49724187813
Iteration 2500: Loss = -11021.494766813401
Iteration 2600: Loss = -11021.492499914906
Iteration 2700: Loss = -11021.490743914843
Iteration 2800: Loss = -11021.489282611828
Iteration 2900: Loss = -11021.487648543352
Iteration 3000: Loss = -11021.485880834656
Iteration 3100: Loss = -11021.48567605965
Iteration 3200: Loss = -11021.479714514271
Iteration 3300: Loss = -11021.241093908679
Iteration 3400: Loss = -11021.204217260603
Iteration 3500: Loss = -11021.200679107678
Iteration 3600: Loss = -11021.201619532594
1
Iteration 3700: Loss = -11021.19582357244
Iteration 3800: Loss = -11021.187357751684
Iteration 3900: Loss = -11021.182838580049
Iteration 4000: Loss = -11021.183178505129
1
Iteration 4100: Loss = -11021.181442003259
Iteration 4200: Loss = -11021.180993524373
Iteration 4300: Loss = -11021.18213388244
1
Iteration 4400: Loss = -11021.180014714164
Iteration 4500: Loss = -11021.179670953747
Iteration 4600: Loss = -11021.179318239105
Iteration 4700: Loss = -11021.179251450525
Iteration 4800: Loss = -11021.178944136129
Iteration 4900: Loss = -11021.182884434713
1
Iteration 5000: Loss = -11021.178594973702
Iteration 5100: Loss = -11021.179671444492
1
Iteration 5200: Loss = -11021.17825153349
Iteration 5300: Loss = -11021.177793491626
Iteration 5400: Loss = -11021.186351515113
1
Iteration 5500: Loss = -11021.199749733309
2
Iteration 5600: Loss = -11021.176814723569
Iteration 5700: Loss = -11021.17670028401
Iteration 5800: Loss = -11021.176591689107
Iteration 5900: Loss = -11021.17650540897
Iteration 6000: Loss = -11021.176481467755
Iteration 6100: Loss = -11021.176389972159
Iteration 6200: Loss = -11021.176314660155
Iteration 6300: Loss = -11021.176231382797
Iteration 6400: Loss = -11021.20247911095
1
Iteration 6500: Loss = -11021.175755201315
Iteration 6600: Loss = -11021.170477846019
Iteration 6700: Loss = -11021.16994615164
Iteration 6800: Loss = -11021.171284045955
1
Iteration 6900: Loss = -11021.169002865841
Iteration 7000: Loss = -11021.166749857668
Iteration 7100: Loss = -11021.165464326017
Iteration 7200: Loss = -11021.165511753261
Iteration 7300: Loss = -11021.165356477115
Iteration 7400: Loss = -11021.165347216165
Iteration 7500: Loss = -11021.165346408518
Iteration 7600: Loss = -11021.165630872954
1
Iteration 7700: Loss = -11021.165251827651
Iteration 7800: Loss = -11021.165250803406
Iteration 7900: Loss = -11021.165542334873
1
Iteration 8000: Loss = -11021.165164674154
Iteration 8100: Loss = -11021.165071354802
Iteration 8200: Loss = -11021.162732549743
Iteration 8300: Loss = -11021.160917759176
Iteration 8400: Loss = -11021.16085344048
Iteration 8500: Loss = -11021.160297010954
Iteration 8600: Loss = -11021.159643409621
Iteration 8700: Loss = -11021.16279542213
1
Iteration 8800: Loss = -11021.159625185937
Iteration 8900: Loss = -11021.159603567508
Iteration 9000: Loss = -11021.257867051674
1
Iteration 9100: Loss = -11021.159602650752
Iteration 9200: Loss = -11021.1595649632
Iteration 9300: Loss = -11021.15961254097
Iteration 9400: Loss = -11021.15955235622
Iteration 9500: Loss = -11021.311192457983
1
Iteration 9600: Loss = -11021.15953104591
Iteration 9700: Loss = -11021.159504738465
Iteration 9800: Loss = -11021.160038179512
1
Iteration 9900: Loss = -11021.159500156857
Iteration 10000: Loss = -11021.159535584986
Iteration 10100: Loss = -11021.159892990712
1
Iteration 10200: Loss = -11021.159520629448
Iteration 10300: Loss = -11021.159470434659
Iteration 10400: Loss = -11021.166679557235
1
Iteration 10500: Loss = -11021.159479152826
Iteration 10600: Loss = -11021.159468258034
Iteration 10700: Loss = -11021.192707869619
1
Iteration 10800: Loss = -11021.159437816501
Iteration 10900: Loss = -11021.159462397558
Iteration 11000: Loss = -11021.183575286985
1
Iteration 11100: Loss = -11021.159429469211
Iteration 11200: Loss = -11021.159346668956
Iteration 11300: Loss = -11021.166038652484
1
Iteration 11400: Loss = -11021.15883214836
Iteration 11500: Loss = -11021.158796969265
Iteration 11600: Loss = -11021.159100944713
1
Iteration 11700: Loss = -11021.158846711096
Iteration 11800: Loss = -11021.165225526785
1
Iteration 11900: Loss = -11021.434585168077
2
Iteration 12000: Loss = -11021.158836811295
Iteration 12100: Loss = -11021.536681476546
1
Iteration 12200: Loss = -11021.15884253805
Iteration 12300: Loss = -11021.15882754629
Iteration 12400: Loss = -11021.159387475125
1
Iteration 12500: Loss = -11021.15881840528
Iteration 12600: Loss = -11021.215068460897
1
Iteration 12700: Loss = -11021.158839139565
Iteration 12800: Loss = -11021.158776993241
Iteration 12900: Loss = -11021.185678228288
1
Iteration 13000: Loss = -11021.158810944433
Iteration 13100: Loss = -11021.158827744954
Iteration 13200: Loss = -11021.166070142668
1
Iteration 13300: Loss = -11021.158826118195
Iteration 13400: Loss = -11021.158858533
Iteration 13500: Loss = -11021.162121621812
1
Iteration 13600: Loss = -11021.161679748642
2
Iteration 13700: Loss = -11021.156676593186
Iteration 13800: Loss = -11021.157724487637
1
Iteration 13900: Loss = -11021.156773281427
Iteration 14000: Loss = -11021.156685408172
Iteration 14100: Loss = -11021.156589928232
Iteration 14200: Loss = -11021.157018924416
1
Iteration 14300: Loss = -11021.156606537244
Iteration 14400: Loss = -11021.28115407182
1
Iteration 14500: Loss = -11021.156609764736
Iteration 14600: Loss = -11021.156610226602
Iteration 14700: Loss = -11021.162710184084
1
Iteration 14800: Loss = -11021.156591702054
Iteration 14900: Loss = -11021.15659692615
Iteration 15000: Loss = -11021.166710014968
1
Iteration 15100: Loss = -11021.15659615524
Iteration 15200: Loss = -11021.156598446192
Iteration 15300: Loss = -11021.156981751185
1
Iteration 15400: Loss = -11021.156589912884
Iteration 15500: Loss = -11021.224300618065
1
Iteration 15600: Loss = -11021.1565956196
Iteration 15700: Loss = -11021.158784619332
1
Iteration 15800: Loss = -11021.156644308214
Iteration 15900: Loss = -11021.156576897161
Iteration 16000: Loss = -11021.156776895305
1
Iteration 16100: Loss = -11021.156583370444
Iteration 16200: Loss = -11021.180034261157
1
Iteration 16300: Loss = -11021.156572827387
Iteration 16400: Loss = -11021.157095988727
1
Iteration 16500: Loss = -11021.156566853722
Iteration 16600: Loss = -11021.157500230034
1
Iteration 16700: Loss = -11021.156590017279
Iteration 16800: Loss = -11021.156733108557
1
Iteration 16900: Loss = -11021.15663906102
Iteration 17000: Loss = -11021.15656816506
Iteration 17100: Loss = -11021.169245070758
1
Iteration 17200: Loss = -11021.156622991703
Iteration 17300: Loss = -11021.156577157226
Iteration 17400: Loss = -11021.15902225498
1
Iteration 17500: Loss = -11021.156617707036
Iteration 17600: Loss = -11021.156571710928
Iteration 17700: Loss = -11021.16552461557
1
Iteration 17800: Loss = -11021.156610780885
Iteration 17900: Loss = -11021.156596180073
Iteration 18000: Loss = -11021.156974626148
1
Iteration 18100: Loss = -11021.15851036574
2
Iteration 18200: Loss = -11021.180829304529
3
Iteration 18300: Loss = -11021.156591541121
Iteration 18400: Loss = -11021.194133695875
1
Iteration 18500: Loss = -11021.157218750113
2
Iteration 18600: Loss = -11021.15668897022
Iteration 18700: Loss = -11021.160027785485
1
Iteration 18800: Loss = -11021.15816591631
2
Iteration 18900: Loss = -11021.156632226264
Iteration 19000: Loss = -11021.47626644739
1
Iteration 19100: Loss = -11021.156580033878
Iteration 19200: Loss = -11021.157578378037
1
Iteration 19300: Loss = -11021.156695411872
2
Iteration 19400: Loss = -11021.157973766183
3
Iteration 19500: Loss = -11021.164929845369
4
Iteration 19600: Loss = -11021.156594532244
Iteration 19700: Loss = -11021.301192180332
1
Iteration 19800: Loss = -11021.156586822999
Iteration 19900: Loss = -11021.157191360153
1
pi: tensor([[0.7264, 0.2736],
        [0.4259, 0.5741]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0080, 0.9920], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2534, 0.3171],
         [0.6691, 0.1717]],

        [[0.5022, 0.0995],
         [0.7177, 0.6729]],

        [[0.7284, 0.1254],
         [0.5273, 0.5819]],

        [[0.6125, 0.0957],
         [0.5549, 0.5671]],

        [[0.6566, 0.1037],
         [0.5534, 0.5398]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 8
Adjusted Rand Index: 0.7025627025124112
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5115764431009991
Average Adjusted Rand Index: 0.6548070686880141
11047.776767834708
[0.831347495867302, 0.5115764431009991] [0.8313749327533827, 0.6548070686880141] [10988.071914563801, 11021.15666506328]
-------------------------------------
This iteration is 71
True Objective function: Loss = -10994.245702549195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24392.534541613444
Iteration 100: Loss = -11107.466620378518
Iteration 200: Loss = -11106.455709009124
Iteration 300: Loss = -11106.181823704701
Iteration 400: Loss = -11106.019608834544
Iteration 500: Loss = -11105.86875169593
Iteration 600: Loss = -11105.62773872693
Iteration 700: Loss = -11102.506380987383
Iteration 800: Loss = -11100.038866455137
Iteration 900: Loss = -11099.23113226557
Iteration 1000: Loss = -11098.677021681167
Iteration 1100: Loss = -11097.757069593652
Iteration 1200: Loss = -11097.256010853196
Iteration 1300: Loss = -11096.901917048022
Iteration 1400: Loss = -11096.606839755304
Iteration 1500: Loss = -11096.195121752571
Iteration 1600: Loss = -11095.146493564365
Iteration 1700: Loss = -10944.765017491454
Iteration 1800: Loss = -10943.974770074068
Iteration 1900: Loss = -10943.89495040693
Iteration 2000: Loss = -10943.781380521019
Iteration 2100: Loss = -10943.623100303172
Iteration 2200: Loss = -10943.613206856087
Iteration 2300: Loss = -10943.602020822575
Iteration 2400: Loss = -10943.579796392296
Iteration 2500: Loss = -10943.570879666344
Iteration 2600: Loss = -10943.566846713406
Iteration 2700: Loss = -10943.563278944599
Iteration 2800: Loss = -10943.558850291973
Iteration 2900: Loss = -10943.526484983695
Iteration 3000: Loss = -10943.52188315878
Iteration 3100: Loss = -10943.521910180789
Iteration 3200: Loss = -10943.51957163823
Iteration 3300: Loss = -10943.51770706357
Iteration 3400: Loss = -10943.514900072318
Iteration 3500: Loss = -10943.51039041931
Iteration 3600: Loss = -10943.509118493212
Iteration 3700: Loss = -10943.508413106287
Iteration 3800: Loss = -10943.510663048291
1
Iteration 3900: Loss = -10943.504072556228
Iteration 4000: Loss = -10943.483128423564
Iteration 4100: Loss = -10943.482476840512
Iteration 4200: Loss = -10943.482214759322
Iteration 4300: Loss = -10943.481849664327
Iteration 4400: Loss = -10943.481559505408
Iteration 4500: Loss = -10943.480833202722
Iteration 4600: Loss = -10943.48010374581
Iteration 4700: Loss = -10943.4800401554
Iteration 4800: Loss = -10943.482347117597
1
Iteration 4900: Loss = -10943.486386814233
2
Iteration 5000: Loss = -10943.489971501687
3
Iteration 5100: Loss = -10943.4882854803
4
Iteration 5200: Loss = -10943.477709764216
Iteration 5300: Loss = -10943.4774885469
Iteration 5400: Loss = -10943.47825672922
1
Iteration 5500: Loss = -10943.477730327355
2
Iteration 5600: Loss = -10943.477253845582
Iteration 5700: Loss = -10943.476556479574
Iteration 5800: Loss = -10943.463154955969
Iteration 5900: Loss = -10943.489260142645
1
Iteration 6000: Loss = -10943.461889453383
Iteration 6100: Loss = -10943.461935217741
Iteration 6200: Loss = -10943.461751765142
Iteration 6300: Loss = -10943.46205653825
1
Iteration 6400: Loss = -10943.461689582016
Iteration 6500: Loss = -10943.468397940958
1
Iteration 6600: Loss = -10943.462099293629
2
Iteration 6700: Loss = -10943.461803094284
3
Iteration 6800: Loss = -10943.46224685405
4
Iteration 6900: Loss = -10943.468006292704
5
Iteration 7000: Loss = -10943.461412171147
Iteration 7100: Loss = -10943.461023954093
Iteration 7200: Loss = -10943.461953959066
1
Iteration 7300: Loss = -10943.462089299306
2
Iteration 7400: Loss = -10943.460890026456
Iteration 7500: Loss = -10943.462246292554
1
Iteration 7600: Loss = -10943.470987297165
2
Iteration 7700: Loss = -10943.460967522413
Iteration 7800: Loss = -10943.46187028396
1
Iteration 7900: Loss = -10943.460692841387
Iteration 8000: Loss = -10943.460801465699
1
Iteration 8100: Loss = -10943.464939591506
2
Iteration 8200: Loss = -10943.460658248052
Iteration 8300: Loss = -10943.464340367787
1
Iteration 8400: Loss = -10943.460887211191
2
Iteration 8500: Loss = -10943.460979297979
3
Iteration 8600: Loss = -10943.472989239981
4
Iteration 8700: Loss = -10943.460567558295
Iteration 8800: Loss = -10943.461968627496
1
Iteration 8900: Loss = -10943.460964327045
2
Iteration 9000: Loss = -10943.460515682658
Iteration 9100: Loss = -10943.470574232648
1
Iteration 9200: Loss = -10943.460487958911
Iteration 9300: Loss = -10943.46070374786
1
Iteration 9400: Loss = -10943.460933644463
2
Iteration 9500: Loss = -10943.460468517118
Iteration 9600: Loss = -10943.464290604275
1
Iteration 9700: Loss = -10943.456971098936
Iteration 9800: Loss = -10943.457350268909
1
Iteration 9900: Loss = -10943.457134332246
2
Iteration 10000: Loss = -10943.457221038996
3
Iteration 10100: Loss = -10943.45803247246
4
Iteration 10200: Loss = -10943.491089197412
5
Iteration 10300: Loss = -10943.458612031385
6
Iteration 10400: Loss = -10943.456867666047
Iteration 10500: Loss = -10943.457186892269
1
Iteration 10600: Loss = -10943.457190922592
2
Iteration 10700: Loss = -10943.456875902362
Iteration 10800: Loss = -10943.458766895299
1
Iteration 10900: Loss = -10943.45722553664
2
Iteration 11000: Loss = -10943.456862277033
Iteration 11100: Loss = -10943.457036513362
1
Iteration 11200: Loss = -10943.598668142222
2
Iteration 11300: Loss = -10943.456805842492
Iteration 11400: Loss = -10943.462049431464
1
Iteration 11500: Loss = -10943.456810924137
Iteration 11600: Loss = -10943.476501726664
1
Iteration 11700: Loss = -10943.489674970237
2
Iteration 11800: Loss = -10943.456757843236
Iteration 11900: Loss = -10943.45660281977
Iteration 12000: Loss = -10943.46965742554
1
Iteration 12100: Loss = -10943.481373959306
2
Iteration 12200: Loss = -10943.463294535411
3
Iteration 12300: Loss = -10943.456599882105
Iteration 12400: Loss = -10943.45816627006
1
Iteration 12500: Loss = -10943.570938570721
2
Iteration 12600: Loss = -10943.513941509118
3
Iteration 12700: Loss = -10943.485490689363
4
Iteration 12800: Loss = -10943.457872511548
5
Iteration 12900: Loss = -10943.45650931447
Iteration 13000: Loss = -10943.456637938256
1
Iteration 13100: Loss = -10943.459696132979
2
Iteration 13200: Loss = -10943.56253603934
3
Iteration 13300: Loss = -10943.530660061193
4
Iteration 13400: Loss = -10943.462822670632
5
Iteration 13500: Loss = -10943.456225142347
Iteration 13600: Loss = -10943.456807365445
1
Iteration 13700: Loss = -10943.45613094777
Iteration 13800: Loss = -10943.466256247208
1
Iteration 13900: Loss = -10943.456136940751
Iteration 14000: Loss = -10943.47635090873
1
Iteration 14100: Loss = -10943.475007436695
2
Iteration 14200: Loss = -10943.463013930956
3
Iteration 14300: Loss = -10943.462468741665
4
Iteration 14400: Loss = -10943.468615976857
5
Iteration 14500: Loss = -10943.457591394872
6
Iteration 14600: Loss = -10943.46177052701
7
Iteration 14700: Loss = -10943.474542039537
8
Iteration 14800: Loss = -10943.461321444853
9
Iteration 14900: Loss = -10943.462029613633
10
Iteration 15000: Loss = -10943.456768142012
11
Iteration 15100: Loss = -10943.4565439351
12
Iteration 15200: Loss = -10943.456451995484
13
Iteration 15300: Loss = -10943.456471025376
14
Iteration 15400: Loss = -10943.456562169533
15
Stopping early at iteration 15400 due to no improvement.
pi: tensor([[0.7512, 0.2488],
        [0.2653, 0.7347]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5098, 0.4902], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2599, 0.1104],
         [0.6686, 0.2014]],

        [[0.6527, 0.1104],
         [0.5805, 0.6849]],

        [[0.6034, 0.0896],
         [0.5206, 0.5765]],

        [[0.5788, 0.1002],
         [0.5816, 0.5043]],

        [[0.6504, 0.0907],
         [0.6830, 0.6535]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.8609053245145319
Average Adjusted Rand Index: 0.8601970978534069
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22474.259101410353
Iteration 100: Loss = -11107.590361717117
Iteration 200: Loss = -11105.862902657771
Iteration 300: Loss = -11104.93424238615
Iteration 400: Loss = -11103.776267653884
Iteration 500: Loss = -11103.019336235451
Iteration 600: Loss = -11102.353996592858
Iteration 700: Loss = -11101.451101489356
Iteration 800: Loss = -11100.448128136944
Iteration 900: Loss = -11099.783994947742
Iteration 1000: Loss = -11099.27526007829
Iteration 1100: Loss = -11098.812275173466
Iteration 1200: Loss = -11098.345520330951
Iteration 1300: Loss = -11097.495553084022
Iteration 1400: Loss = -11096.955882268334
Iteration 1500: Loss = -11096.572524565581
Iteration 1600: Loss = -11093.068023463096
Iteration 1700: Loss = -11082.161126711775
Iteration 1800: Loss = -11047.251939333755
Iteration 1900: Loss = -10953.895319833942
Iteration 2000: Loss = -10945.46171945568
Iteration 2100: Loss = -10944.023845817968
Iteration 2200: Loss = -10943.856647894085
Iteration 2300: Loss = -10943.803066216518
Iteration 2400: Loss = -10943.770721559576
Iteration 2500: Loss = -10943.742754062983
Iteration 2600: Loss = -10943.714165111583
Iteration 2700: Loss = -10943.69635786136
Iteration 2800: Loss = -10943.683305362452
Iteration 2900: Loss = -10943.67098884652
Iteration 3000: Loss = -10943.64919057028
Iteration 3100: Loss = -10943.63800059873
Iteration 3200: Loss = -10943.632271746741
Iteration 3300: Loss = -10943.629724865987
Iteration 3400: Loss = -10943.623944422985
Iteration 3500: Loss = -10943.620710889534
Iteration 3600: Loss = -10943.617638773203
Iteration 3700: Loss = -10943.614028846654
Iteration 3800: Loss = -10943.608961062993
Iteration 3900: Loss = -10943.611597686195
1
Iteration 4000: Loss = -10943.611034608712
2
Iteration 4100: Loss = -10943.596610473158
Iteration 4200: Loss = -10943.591995525832
Iteration 4300: Loss = -10943.58772168574
Iteration 4400: Loss = -10943.581478718403
Iteration 4500: Loss = -10943.579747516953
Iteration 4600: Loss = -10943.57772631764
Iteration 4700: Loss = -10943.574038956396
Iteration 4800: Loss = -10943.573400946707
Iteration 4900: Loss = -10943.572523838899
Iteration 5000: Loss = -10943.571066188328
Iteration 5100: Loss = -10943.571069099124
Iteration 5200: Loss = -10943.570571722696
Iteration 5300: Loss = -10943.569619436155
Iteration 5400: Loss = -10943.569669362842
Iteration 5500: Loss = -10943.568468846242
Iteration 5600: Loss = -10943.56793389977
Iteration 5700: Loss = -10943.567206101156
Iteration 5800: Loss = -10943.565855233395
Iteration 5900: Loss = -10943.567845196312
1
Iteration 6000: Loss = -10943.561034109198
Iteration 6100: Loss = -10943.559322826517
Iteration 6200: Loss = -10943.565204769766
1
Iteration 6300: Loss = -10943.55708310881
Iteration 6400: Loss = -10943.55687812236
Iteration 6500: Loss = -10943.556186813763
Iteration 6600: Loss = -10943.540042490116
Iteration 6700: Loss = -10943.49021076954
Iteration 6800: Loss = -10943.489864479465
Iteration 6900: Loss = -10943.489730329304
Iteration 7000: Loss = -10943.489108208854
Iteration 7100: Loss = -10943.488709763442
Iteration 7200: Loss = -10943.488548918684
Iteration 7300: Loss = -10943.488475491711
Iteration 7400: Loss = -10943.4881341646
Iteration 7500: Loss = -10943.488095265493
Iteration 7600: Loss = -10943.511893148185
1
Iteration 7700: Loss = -10943.487716025202
Iteration 7800: Loss = -10943.487839782569
1
Iteration 7900: Loss = -10943.48778612252
Iteration 8000: Loss = -10943.486978753412
Iteration 8100: Loss = -10943.487672310215
1
Iteration 8200: Loss = -10943.486452578396
Iteration 8300: Loss = -10943.488001722199
1
Iteration 8400: Loss = -10943.486318202951
Iteration 8500: Loss = -10943.486098777676
Iteration 8600: Loss = -10943.48543544372
Iteration 8700: Loss = -10943.484522095368
Iteration 8800: Loss = -10943.483852657308
Iteration 8900: Loss = -10943.468917121214
Iteration 9000: Loss = -10943.461082938345
Iteration 9100: Loss = -10943.461731741294
1
Iteration 9200: Loss = -10943.461419259964
2
Iteration 9300: Loss = -10943.494740138478
3
Iteration 9400: Loss = -10943.460814232425
Iteration 9500: Loss = -10943.460448227113
Iteration 9600: Loss = -10943.460531713326
Iteration 9700: Loss = -10943.475054690618
1
Iteration 9800: Loss = -10943.45989235319
Iteration 9900: Loss = -10943.469599264816
1
Iteration 10000: Loss = -10943.460262997183
2
Iteration 10100: Loss = -10943.46705677278
3
Iteration 10200: Loss = -10943.45958921766
Iteration 10300: Loss = -10943.533022245249
1
Iteration 10400: Loss = -10943.4594689984
Iteration 10500: Loss = -10943.459301931789
Iteration 10600: Loss = -10943.468456842214
1
Iteration 10700: Loss = -10943.45787392407
Iteration 10800: Loss = -10943.457868107676
Iteration 10900: Loss = -10943.45796120221
Iteration 11000: Loss = -10943.461259335654
1
Iteration 11100: Loss = -10943.46281226714
2
Iteration 11200: Loss = -10943.461907312383
3
Iteration 11300: Loss = -10943.458478046914
4
Iteration 11400: Loss = -10943.457129342081
Iteration 11500: Loss = -10943.46262076711
1
Iteration 11600: Loss = -10943.457076458635
Iteration 11700: Loss = -10943.45783074238
1
Iteration 11800: Loss = -10943.457028654166
Iteration 11900: Loss = -10943.4652172104
1
Iteration 12000: Loss = -10943.456988911592
Iteration 12100: Loss = -10943.456993241678
Iteration 12200: Loss = -10943.456938471038
Iteration 12300: Loss = -10943.456884297582
Iteration 12400: Loss = -10943.518721157916
1
Iteration 12500: Loss = -10943.456836202804
Iteration 12600: Loss = -10943.470504375911
1
Iteration 12700: Loss = -10943.456785915145
Iteration 12800: Loss = -10943.461786175767
1
Iteration 12900: Loss = -10943.457087045928
2
Iteration 13000: Loss = -10943.464895766185
3
Iteration 13100: Loss = -10943.477037821061
4
Iteration 13200: Loss = -10943.458485473004
5
Iteration 13300: Loss = -10943.470556754597
6
Iteration 13400: Loss = -10943.4564831864
Iteration 13500: Loss = -10943.456516485567
Iteration 13600: Loss = -10943.561647233497
1
Iteration 13700: Loss = -10943.57076582151
2
Iteration 13800: Loss = -10943.460398790758
3
Iteration 13900: Loss = -10943.464445605105
4
Iteration 14000: Loss = -10943.501004230659
5
Iteration 14100: Loss = -10943.456539298095
Iteration 14200: Loss = -10943.457138738735
1
Iteration 14300: Loss = -10943.457580608747
2
Iteration 14400: Loss = -10943.456942812576
3
Iteration 14500: Loss = -10943.463249977218
4
Iteration 14600: Loss = -10943.482782964753
5
Iteration 14700: Loss = -10943.456395244257
Iteration 14800: Loss = -10943.456928183738
1
Iteration 14900: Loss = -10943.46984790993
2
Iteration 15000: Loss = -10943.486061682865
3
Iteration 15100: Loss = -10943.456928610638
4
Iteration 15200: Loss = -10943.462871120204
5
Iteration 15300: Loss = -10943.500284069818
6
Iteration 15400: Loss = -10943.468040151365
7
Iteration 15500: Loss = -10943.529521885488
8
Iteration 15600: Loss = -10943.46454566664
9
Iteration 15700: Loss = -10943.456340573555
Iteration 15800: Loss = -10943.45832692474
1
Iteration 15900: Loss = -10943.457836260766
2
Iteration 16000: Loss = -10943.456386253467
Iteration 16100: Loss = -10943.457054363427
1
Iteration 16200: Loss = -10943.45633748378
Iteration 16300: Loss = -10943.45664248107
1
Iteration 16400: Loss = -10943.465184819244
2
Iteration 16500: Loss = -10943.475389570549
3
Iteration 16600: Loss = -10943.50292981454
4
Iteration 16700: Loss = -10943.50673928358
5
Iteration 16800: Loss = -10943.599972066657
6
Iteration 16900: Loss = -10943.471124815598
7
Iteration 17000: Loss = -10943.46059674017
8
Iteration 17100: Loss = -10943.466708214675
9
Iteration 17200: Loss = -10943.46144443848
10
Iteration 17300: Loss = -10943.459621175263
11
Iteration 17400: Loss = -10943.515015692632
12
Iteration 17500: Loss = -10943.4563328249
Iteration 17600: Loss = -10943.456402847376
Iteration 17700: Loss = -10943.456929791853
1
Iteration 17800: Loss = -10943.456350852543
Iteration 17900: Loss = -10943.45644505847
Iteration 18000: Loss = -10943.456333183061
Iteration 18100: Loss = -10943.459385167049
1
Iteration 18200: Loss = -10943.45631077549
Iteration 18300: Loss = -10943.457442622312
1
Iteration 18400: Loss = -10943.45827758785
2
Iteration 18500: Loss = -10943.470945516778
3
Iteration 18600: Loss = -10943.456328187482
Iteration 18700: Loss = -10943.467660224498
1
Iteration 18800: Loss = -10943.45754524286
2
Iteration 18900: Loss = -10943.456591078706
3
Iteration 19000: Loss = -10943.456815678963
4
Iteration 19100: Loss = -10943.456384595902
Iteration 19200: Loss = -10943.457724083632
1
Iteration 19300: Loss = -10943.467470010237
2
Iteration 19400: Loss = -10943.459721897127
3
Iteration 19500: Loss = -10943.584770099837
4
Iteration 19600: Loss = -10943.456618074295
5
Iteration 19700: Loss = -10943.456662358412
6
Iteration 19800: Loss = -10943.456497440613
7
Iteration 19900: Loss = -10943.457976077474
8
pi: tensor([[0.7511, 0.2489],
        [0.2651, 0.7349]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5098, 0.4902], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2600, 0.1104],
         [0.6343, 0.2014]],

        [[0.6269, 0.1106],
         [0.7277, 0.6411]],

        [[0.7163, 0.0898],
         [0.6994, 0.7287]],

        [[0.6624, 0.1002],
         [0.6167, 0.5495]],

        [[0.6080, 0.0907],
         [0.6452, 0.6689]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824276204858761
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.8609053245145319
Average Adjusted Rand Index: 0.8601970978534069
10994.245702549195
[0.8609053245145319, 0.8609053245145319] [0.8601970978534069, 0.8601970978534069] [10943.456562169533, 10943.458392276534]
-------------------------------------
This iteration is 72
True Objective function: Loss = -10919.083777888682
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21971.708062339523
Iteration 100: Loss = -11008.326607022427
Iteration 200: Loss = -11007.142921635628
Iteration 300: Loss = -11003.982120305694
Iteration 400: Loss = -11003.505981369002
Iteration 500: Loss = -11003.368947379593
Iteration 600: Loss = -11003.212615590644
Iteration 700: Loss = -11002.989173732149
Iteration 800: Loss = -11002.812711745795
Iteration 900: Loss = -11002.68322014258
Iteration 1000: Loss = -11002.547113867564
Iteration 1100: Loss = -11002.417838272748
Iteration 1200: Loss = -11002.284825584205
Iteration 1300: Loss = -11002.14572791728
Iteration 1400: Loss = -11001.978955270188
Iteration 1500: Loss = -11001.813986948708
Iteration 1600: Loss = -11001.699495345922
Iteration 1700: Loss = -11001.606901547295
Iteration 1800: Loss = -11001.536386473213
Iteration 1900: Loss = -11001.469905962671
Iteration 2000: Loss = -11001.410627227913
Iteration 2100: Loss = -11001.38061975055
Iteration 2200: Loss = -11001.358837043119
Iteration 2300: Loss = -11001.345713613198
Iteration 2400: Loss = -11001.336266693477
Iteration 2500: Loss = -11001.32950222459
Iteration 2600: Loss = -11001.32414338336
Iteration 2700: Loss = -11001.319647125567
Iteration 2800: Loss = -11001.315848886676
Iteration 2900: Loss = -11001.312546175282
Iteration 3000: Loss = -11001.309746704394
Iteration 3100: Loss = -11001.30721026256
Iteration 3200: Loss = -11001.304893110393
Iteration 3300: Loss = -11001.302828090831
Iteration 3400: Loss = -11001.300948967424
Iteration 3500: Loss = -11001.299256317952
Iteration 3600: Loss = -11001.297708068378
Iteration 3700: Loss = -11001.296272081281
Iteration 3800: Loss = -11001.29492190063
Iteration 3900: Loss = -11001.29376972686
Iteration 4000: Loss = -11001.292687907775
Iteration 4100: Loss = -11001.291663475955
Iteration 4200: Loss = -11001.290710108191
Iteration 4300: Loss = -11001.28984021741
Iteration 4400: Loss = -11001.289042533777
Iteration 4500: Loss = -11001.288234981665
Iteration 4600: Loss = -11001.287532051101
Iteration 4700: Loss = -11001.286891203063
Iteration 4800: Loss = -11001.2862580906
Iteration 4900: Loss = -11001.28564036218
Iteration 5000: Loss = -11001.28510780836
Iteration 5100: Loss = -11001.28459448479
Iteration 5200: Loss = -11001.284111181021
Iteration 5300: Loss = -11001.283673569123
Iteration 5400: Loss = -11001.283244740094
Iteration 5500: Loss = -11001.282859058943
Iteration 5600: Loss = -11001.282457544787
Iteration 5700: Loss = -11001.282079901903
Iteration 5800: Loss = -11001.281842337505
Iteration 5900: Loss = -11001.281503101955
Iteration 6000: Loss = -11001.281175700704
Iteration 6100: Loss = -11001.280928817918
Iteration 6200: Loss = -11001.28064034803
Iteration 6300: Loss = -11001.280393418963
Iteration 6400: Loss = -11001.280135147194
Iteration 6500: Loss = -11001.280131112619
Iteration 6600: Loss = -11001.279725083306
Iteration 6700: Loss = -11001.279544252202
Iteration 6800: Loss = -11001.279349995346
Iteration 6900: Loss = -11001.279172034883
Iteration 7000: Loss = -11001.279021312233
Iteration 7100: Loss = -11001.2805639906
1
Iteration 7200: Loss = -11001.278711393958
Iteration 7300: Loss = -11001.278573272431
Iteration 7400: Loss = -11001.278655905915
Iteration 7500: Loss = -11001.27828791649
Iteration 7600: Loss = -11001.278894539018
1
Iteration 7700: Loss = -11001.278802497596
2
Iteration 7800: Loss = -11001.277964294
Iteration 7900: Loss = -11001.316819411968
1
Iteration 8000: Loss = -11001.277772324263
Iteration 8100: Loss = -11001.354970145036
1
Iteration 8200: Loss = -11001.277570651799
Iteration 8300: Loss = -11001.277511675851
Iteration 8400: Loss = -11001.277875774154
1
Iteration 8500: Loss = -11001.277364337848
Iteration 8600: Loss = -11001.27732368132
Iteration 8700: Loss = -11001.27742272287
Iteration 8800: Loss = -11001.27719012048
Iteration 8900: Loss = -11001.292997995153
1
Iteration 9000: Loss = -11001.27704403945
Iteration 9100: Loss = -11001.276976128844
Iteration 9200: Loss = -11001.283503525547
1
Iteration 9300: Loss = -11001.27689545426
Iteration 9400: Loss = -11001.27688544452
Iteration 9500: Loss = -11001.377995886123
1
Iteration 9600: Loss = -11001.276770523586
Iteration 9700: Loss = -11001.276757131965
Iteration 9800: Loss = -11001.277634078082
1
Iteration 9900: Loss = -11001.276689736671
Iteration 10000: Loss = -11001.276625845278
Iteration 10100: Loss = -11001.277054874403
1
Iteration 10200: Loss = -11001.276561578168
Iteration 10300: Loss = -11001.276563159388
Iteration 10400: Loss = -11001.277016595774
1
Iteration 10500: Loss = -11001.276499626847
Iteration 10600: Loss = -11001.276456284068
Iteration 10700: Loss = -11001.276560477378
1
Iteration 10800: Loss = -11001.276460599882
Iteration 10900: Loss = -11001.276383708735
Iteration 11000: Loss = -11001.276460868477
Iteration 11100: Loss = -11001.276388794317
Iteration 11200: Loss = -11001.467818268331
1
Iteration 11300: Loss = -11001.276335881212
Iteration 11400: Loss = -11001.276299206535
Iteration 11500: Loss = -11001.277386654678
1
Iteration 11600: Loss = -11001.276280554212
Iteration 11700: Loss = -11001.276256035238
Iteration 11800: Loss = -11001.276422782126
1
Iteration 11900: Loss = -11001.276257273294
Iteration 12000: Loss = -11001.279284513344
1
Iteration 12100: Loss = -11001.27624971265
Iteration 12200: Loss = -11001.456785323611
1
Iteration 12300: Loss = -11001.276217168732
Iteration 12400: Loss = -11001.27616565882
Iteration 12500: Loss = -11001.276240627183
Iteration 12600: Loss = -11001.276198280168
Iteration 12700: Loss = -11001.277747166198
1
Iteration 12800: Loss = -11001.276118276994
Iteration 12900: Loss = -11001.284010225143
1
Iteration 13000: Loss = -11001.276169497563
Iteration 13100: Loss = -11001.276122200357
Iteration 13200: Loss = -11001.276264105432
1
Iteration 13300: Loss = -11001.27609726541
Iteration 13400: Loss = -11001.34100233396
1
Iteration 13500: Loss = -11001.27609629414
Iteration 13600: Loss = -11001.276061216782
Iteration 13700: Loss = -11001.27689434292
1
Iteration 13800: Loss = -11001.276113434615
Iteration 13900: Loss = -11001.277989440223
1
Iteration 14000: Loss = -11001.276049198064
Iteration 14100: Loss = -11001.277405329873
1
Iteration 14200: Loss = -11001.280554258894
2
Iteration 14300: Loss = -11001.276119302785
Iteration 14400: Loss = -11001.296441565728
1
Iteration 14500: Loss = -11001.276033298323
Iteration 14600: Loss = -11001.276495416829
1
Iteration 14700: Loss = -11001.280290878041
2
Iteration 14800: Loss = -11001.276111819543
Iteration 14900: Loss = -11001.277245632446
1
Iteration 15000: Loss = -11001.342735123511
2
Iteration 15100: Loss = -11001.27604758784
Iteration 15200: Loss = -11001.327089626735
1
Iteration 15300: Loss = -11001.276047716268
Iteration 15400: Loss = -11001.29543194535
1
Iteration 15500: Loss = -11001.276027532933
Iteration 15600: Loss = -11001.276226860664
1
Iteration 15700: Loss = -11001.286065598506
2
Iteration 15800: Loss = -11001.309425997855
3
Iteration 15900: Loss = -11001.27602700244
Iteration 16000: Loss = -11001.27617539717
1
Iteration 16100: Loss = -11001.27605579209
Iteration 16200: Loss = -11001.276032740892
Iteration 16300: Loss = -11001.276090768964
Iteration 16400: Loss = -11001.276062832652
Iteration 16500: Loss = -11001.277672315216
1
Iteration 16600: Loss = -11001.276050344371
Iteration 16700: Loss = -11001.2761429178
Iteration 16800: Loss = -11001.276049461318
Iteration 16900: Loss = -11001.285304396777
1
Iteration 17000: Loss = -11001.276067989049
Iteration 17100: Loss = -11001.290016415714
1
Iteration 17200: Loss = -11001.27605706595
Iteration 17300: Loss = -11001.276053624137
Iteration 17400: Loss = -11001.288246121325
1
Iteration 17500: Loss = -11001.276004029813
Iteration 17600: Loss = -11001.277596684935
1
Iteration 17700: Loss = -11001.275991343296
Iteration 17800: Loss = -11001.276396305338
1
Iteration 17900: Loss = -11001.484332290465
2
Iteration 18000: Loss = -11001.276042627916
Iteration 18100: Loss = -11001.276424387179
1
Iteration 18200: Loss = -11001.293243432196
2
Iteration 18300: Loss = -11001.30253496238
3
Iteration 18400: Loss = -11001.276090441295
Iteration 18500: Loss = -11001.276103745346
Iteration 18600: Loss = -11001.278735791804
1
Iteration 18700: Loss = -11001.306427224632
2
Iteration 18800: Loss = -11001.276387173484
3
Iteration 18900: Loss = -11001.276059318592
Iteration 19000: Loss = -11001.278939078444
1
Iteration 19100: Loss = -11001.279764332145
2
Iteration 19200: Loss = -11001.276057721181
Iteration 19300: Loss = -11001.276511607348
1
Iteration 19400: Loss = -11001.278621324496
2
Iteration 19500: Loss = -11001.335398560568
3
Iteration 19600: Loss = -11001.276274922388
4
Iteration 19700: Loss = -11001.276034371764
Iteration 19800: Loss = -11001.277258673968
1
Iteration 19900: Loss = -11001.301518399398
2
pi: tensor([[1.0000e+00, 5.2447e-08],
        [4.9128e-01, 5.0872e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8124, 0.1876], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1578, 0.1942],
         [0.5704, 0.2625]],

        [[0.5831, 0.1941],
         [0.6028, 0.5899]],

        [[0.7247, 0.2100],
         [0.6996, 0.6786]],

        [[0.6258, 0.1317],
         [0.7210, 0.5214]],

        [[0.5834, 0.1965],
         [0.6216, 0.5224]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016333428560279379
Average Adjusted Rand Index: 0.004496774002246068
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19498.483838273885
Iteration 100: Loss = -11006.971944950317
Iteration 200: Loss = -11003.932813412517
Iteration 300: Loss = -11003.707670266049
Iteration 400: Loss = -11003.634580411228
Iteration 500: Loss = -11003.576178321373
Iteration 600: Loss = -11003.52371583686
Iteration 700: Loss = -11003.469990572288
Iteration 800: Loss = -11003.39263651179
Iteration 900: Loss = -11003.123737964448
Iteration 1000: Loss = -11002.609226373921
Iteration 1100: Loss = -11002.34705308259
Iteration 1200: Loss = -11002.107744971214
Iteration 1300: Loss = -11001.86530079662
Iteration 1400: Loss = -11001.658655855901
Iteration 1500: Loss = -11001.527659647452
Iteration 1600: Loss = -11001.450209309556
Iteration 1700: Loss = -11001.403817147422
Iteration 1800: Loss = -11001.377812804823
Iteration 1900: Loss = -11001.361966031993
Iteration 2000: Loss = -11001.35071903984
Iteration 2100: Loss = -11001.342045485382
Iteration 2200: Loss = -11001.335072767632
Iteration 2300: Loss = -11001.32917596625
Iteration 2400: Loss = -11001.324046175405
Iteration 2500: Loss = -11001.319569803374
Iteration 2600: Loss = -11001.315623659835
Iteration 2700: Loss = -11001.312126164712
Iteration 2800: Loss = -11001.30911060624
Iteration 2900: Loss = -11001.30635322999
Iteration 3000: Loss = -11001.303957992417
Iteration 3100: Loss = -11001.301787008133
Iteration 3200: Loss = -11001.299832373192
Iteration 3300: Loss = -11001.298090250235
Iteration 3400: Loss = -11001.29642569791
Iteration 3500: Loss = -11001.294972396068
Iteration 3600: Loss = -11001.293649638756
Iteration 3700: Loss = -11001.292410923954
Iteration 3800: Loss = -11001.29129952466
Iteration 3900: Loss = -11001.290287486521
Iteration 4000: Loss = -11001.289328913894
Iteration 4100: Loss = -11001.288447771158
Iteration 4200: Loss = -11001.287650412562
Iteration 4300: Loss = -11001.286845476865
Iteration 4400: Loss = -11001.286220522308
Iteration 4500: Loss = -11001.28554922265
Iteration 4600: Loss = -11001.284971817333
Iteration 4700: Loss = -11001.284414382211
Iteration 4800: Loss = -11001.283905662067
Iteration 4900: Loss = -11001.283450833871
Iteration 5000: Loss = -11001.282990018826
Iteration 5100: Loss = -11001.282610674665
Iteration 5200: Loss = -11001.282236634768
Iteration 5300: Loss = -11001.28183678414
Iteration 5400: Loss = -11001.281493080562
Iteration 5500: Loss = -11001.281396178805
Iteration 5600: Loss = -11001.280923159426
Iteration 5700: Loss = -11001.28063266917
Iteration 5800: Loss = -11001.280502131269
Iteration 5900: Loss = -11001.280140905063
Iteration 6000: Loss = -11001.279890298652
Iteration 6100: Loss = -11001.279713422427
Iteration 6200: Loss = -11001.279494852653
Iteration 6300: Loss = -11001.27931879026
Iteration 6400: Loss = -11001.279120836574
Iteration 6500: Loss = -11001.27895552036
Iteration 6600: Loss = -11001.27877081688
Iteration 6700: Loss = -11001.28002964812
1
Iteration 6800: Loss = -11001.278493769978
Iteration 6900: Loss = -11001.278358674172
Iteration 7000: Loss = -11001.2782359092
Iteration 7100: Loss = -11001.278102789585
Iteration 7200: Loss = -11001.280304838747
1
Iteration 7300: Loss = -11001.277909129867
Iteration 7400: Loss = -11001.277865440077
Iteration 7500: Loss = -11001.284184848255
1
Iteration 7600: Loss = -11001.277640997616
Iteration 7700: Loss = -11001.27931518443
1
Iteration 7800: Loss = -11001.277482993708
Iteration 7900: Loss = -11001.280551410677
1
Iteration 8000: Loss = -11001.277296337114
Iteration 8100: Loss = -11001.27722521599
Iteration 8200: Loss = -11001.283055736161
1
Iteration 8300: Loss = -11001.277100803543
Iteration 8400: Loss = -11001.277069377033
Iteration 8500: Loss = -11001.308346323945
1
Iteration 8600: Loss = -11001.276960544916
Iteration 8700: Loss = -11001.276913443799
Iteration 8800: Loss = -11001.278631850688
1
Iteration 8900: Loss = -11001.276839407612
Iteration 9000: Loss = -11001.276807363618
Iteration 9100: Loss = -11001.276844605361
Iteration 9200: Loss = -11001.276702620418
Iteration 9300: Loss = -11001.328924464988
1
Iteration 9400: Loss = -11001.2766293506
Iteration 9500: Loss = -11001.276639992335
Iteration 9600: Loss = -11001.282539066005
1
Iteration 9700: Loss = -11001.276563834865
Iteration 9800: Loss = -11001.276538532324
Iteration 9900: Loss = -11001.277075715938
1
Iteration 10000: Loss = -11001.276483149699
Iteration 10100: Loss = -11001.297247370738
1
Iteration 10200: Loss = -11001.276435067302
Iteration 10300: Loss = -11001.276402771164
Iteration 10400: Loss = -11001.282107066638
1
Iteration 10500: Loss = -11001.276393446025
Iteration 10600: Loss = -11001.279198169712
1
Iteration 10700: Loss = -11001.276314082132
Iteration 10800: Loss = -11001.279286354444
1
Iteration 10900: Loss = -11001.276353498726
Iteration 11000: Loss = -11001.276270974944
Iteration 11100: Loss = -11001.277811495276
1
Iteration 11200: Loss = -11001.276272278961
Iteration 11300: Loss = -11001.276383980916
1
Iteration 11400: Loss = -11001.276256469615
Iteration 11500: Loss = -11001.678643707077
1
Iteration 11600: Loss = -11001.276209324466
Iteration 11700: Loss = -11001.276209552641
Iteration 11800: Loss = -11001.276895020768
1
Iteration 11900: Loss = -11001.27618946508
Iteration 12000: Loss = -11001.28259403159
1
Iteration 12100: Loss = -11001.276142713152
Iteration 12200: Loss = -11001.363968042482
1
Iteration 12300: Loss = -11001.276196881032
Iteration 12400: Loss = -11001.34282962016
1
Iteration 12500: Loss = -11001.276133116737
Iteration 12600: Loss = -11001.293632167572
1
Iteration 12700: Loss = -11001.276139965872
Iteration 12800: Loss = -11001.46378778564
1
Iteration 12900: Loss = -11001.276124755674
Iteration 13000: Loss = -11001.276126005076
Iteration 13100: Loss = -11001.280706548336
1
Iteration 13200: Loss = -11001.276104154522
Iteration 13300: Loss = -11001.27638881998
1
Iteration 13400: Loss = -11001.276118868087
Iteration 13500: Loss = -11001.27737211204
1
Iteration 13600: Loss = -11001.276121262525
Iteration 13700: Loss = -11001.276235186118
1
Iteration 13800: Loss = -11001.311096233387
2
Iteration 13900: Loss = -11001.27607135641
Iteration 14000: Loss = -11001.311201952229
1
Iteration 14100: Loss = -11001.2760342178
Iteration 14200: Loss = -11001.278595726018
1
Iteration 14300: Loss = -11001.27607497537
Iteration 14400: Loss = -11001.276263402462
1
Iteration 14500: Loss = -11001.27701883708
2
Iteration 14600: Loss = -11001.276037924239
Iteration 14700: Loss = -11001.27746006578
1
Iteration 14800: Loss = -11001.301030297598
2
Iteration 14900: Loss = -11001.276071886008
Iteration 15000: Loss = -11001.276207319077
1
Iteration 15100: Loss = -11001.283053984576
2
Iteration 15200: Loss = -11001.276190319859
3
Iteration 15300: Loss = -11001.276098407146
Iteration 15400: Loss = -11001.276352570521
1
Iteration 15500: Loss = -11001.398288612516
2
Iteration 15600: Loss = -11001.276031500254
Iteration 15700: Loss = -11001.311764739728
1
Iteration 15800: Loss = -11001.276022737959
Iteration 15900: Loss = -11001.280376068462
1
Iteration 16000: Loss = -11001.276046723644
Iteration 16100: Loss = -11001.276176799458
1
Iteration 16200: Loss = -11001.276014647427
Iteration 16300: Loss = -11001.276885822756
1
Iteration 16400: Loss = -11001.276020051344
Iteration 16500: Loss = -11001.276760419723
1
Iteration 16600: Loss = -11001.280581100162
2
Iteration 16700: Loss = -11001.276024208635
Iteration 16800: Loss = -11001.276487085277
1
Iteration 16900: Loss = -11001.279462970515
2
Iteration 17000: Loss = -11001.276566541204
3
Iteration 17100: Loss = -11001.276022675886
Iteration 17200: Loss = -11001.276661497473
1
Iteration 17300: Loss = -11001.279118822687
2
Iteration 17400: Loss = -11001.277540391715
3
Iteration 17500: Loss = -11001.277365892553
4
Iteration 17600: Loss = -11001.317395610462
5
Iteration 17700: Loss = -11001.28376304056
6
Iteration 17800: Loss = -11001.276017959332
Iteration 17900: Loss = -11001.27611543464
Iteration 18000: Loss = -11001.277438578847
1
Iteration 18100: Loss = -11001.352536932796
2
Iteration 18200: Loss = -11001.276037832795
Iteration 18300: Loss = -11001.276243046888
1
Iteration 18400: Loss = -11001.285002751321
2
Iteration 18500: Loss = -11001.306313008194
3
Iteration 18600: Loss = -11001.276034339653
Iteration 18700: Loss = -11001.292263783207
1
Iteration 18800: Loss = -11001.27599891803
Iteration 18900: Loss = -11001.276580616825
1
Iteration 19000: Loss = -11001.288132233174
2
Iteration 19100: Loss = -11001.276022491907
Iteration 19200: Loss = -11001.278242813463
1
Iteration 19300: Loss = -11001.27600997971
Iteration 19400: Loss = -11001.277407548723
1
Iteration 19500: Loss = -11001.27600780383
Iteration 19600: Loss = -11001.276156293257
1
Iteration 19700: Loss = -11001.344988936647
2
Iteration 19800: Loss = -11001.276067328406
Iteration 19900: Loss = -11001.276015690832
pi: tensor([[1.0000e+00, 3.9151e-08],
        [4.8804e-01, 5.1196e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8134, 0.1866], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1577, 0.1940],
         [0.7097, 0.2621]],

        [[0.5227, 0.1931],
         [0.5517, 0.6301]],

        [[0.6866, 0.2095],
         [0.5465, 0.7102]],

        [[0.5012, 0.1313],
         [0.7117, 0.7301]],

        [[0.6649, 0.1956],
         [0.5373, 0.6720]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.01701445012943135
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 4.85601903559462e-05
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0016333428560279379
Average Adjusted Rand Index: 0.004496774002246068
10919.083777888682
[-0.0016333428560279379, -0.0016333428560279379] [0.004496774002246068, 0.004496774002246068] [11001.279061377085, 11001.281165364217]
-------------------------------------
This iteration is 73
True Objective function: Loss = -10883.416457319872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23226.265097434796
Iteration 100: Loss = -10954.040171468212
Iteration 200: Loss = -10953.34298158304
Iteration 300: Loss = -10953.15569197109
Iteration 400: Loss = -10953.058755536249
Iteration 500: Loss = -10952.997974089354
Iteration 600: Loss = -10952.95307735439
Iteration 700: Loss = -10952.911708301306
Iteration 800: Loss = -10952.851064561999
Iteration 900: Loss = -10952.434388914357
Iteration 1000: Loss = -10948.202750341992
Iteration 1100: Loss = -10947.981661212352
Iteration 1200: Loss = -10947.781923175202
Iteration 1300: Loss = -10947.0631870446
Iteration 1400: Loss = -10943.783228343153
Iteration 1500: Loss = -10941.766700674249
Iteration 1600: Loss = -10941.371176817034
Iteration 1700: Loss = -10940.912447903287
Iteration 1800: Loss = -10938.07801523124
Iteration 1900: Loss = -10937.932264870307
Iteration 2000: Loss = -10937.827075137264
Iteration 2100: Loss = -10936.619913790359
Iteration 2200: Loss = -10936.557610521022
Iteration 2300: Loss = -10936.361124768377
Iteration 2400: Loss = -10936.345812650092
Iteration 2500: Loss = -10936.333951394587
Iteration 2600: Loss = -10936.325595295515
Iteration 2700: Loss = -10936.318831862953
Iteration 2800: Loss = -10936.290746458773
Iteration 2900: Loss = -10936.22564609303
Iteration 3000: Loss = -10936.19043498577
Iteration 3100: Loss = -10933.135974101031
Iteration 3200: Loss = -10911.096011490605
Iteration 3300: Loss = -10883.464710661248
Iteration 3400: Loss = -10882.661279354377
Iteration 3500: Loss = -10879.223364401574
Iteration 3600: Loss = -10879.106319002729
Iteration 3700: Loss = -10879.064598941259
Iteration 3800: Loss = -10879.05369128711
Iteration 3900: Loss = -10879.06971987448
1
Iteration 4000: Loss = -10879.03369628083
Iteration 4100: Loss = -10879.02582356545
Iteration 4200: Loss = -10879.015999104326
Iteration 4300: Loss = -10878.9813224592
Iteration 4400: Loss = -10876.389210021987
Iteration 4500: Loss = -10874.080121411793
Iteration 4600: Loss = -10873.892577009512
Iteration 4700: Loss = -10873.8788042085
Iteration 4800: Loss = -10873.876747337441
Iteration 4900: Loss = -10873.818921892274
Iteration 5000: Loss = -10873.815038697141
Iteration 5100: Loss = -10873.781498862274
Iteration 5200: Loss = -10873.780990068413
Iteration 5300: Loss = -10873.780684196523
Iteration 5400: Loss = -10873.78025528276
Iteration 5500: Loss = -10873.779769729088
Iteration 5600: Loss = -10873.777052237592
Iteration 5700: Loss = -10873.732229136076
Iteration 5800: Loss = -10873.731404350054
Iteration 5900: Loss = -10873.728886538103
Iteration 6000: Loss = -10873.717686588154
Iteration 6100: Loss = -10873.712319748585
Iteration 6200: Loss = -10873.64816602917
Iteration 6300: Loss = -10873.638222045462
Iteration 6400: Loss = -10873.638500239189
1
Iteration 6500: Loss = -10873.638073759386
Iteration 6600: Loss = -10873.638022301653
Iteration 6700: Loss = -10873.637999233424
Iteration 6800: Loss = -10873.637932293252
Iteration 6900: Loss = -10873.6378941721
Iteration 7000: Loss = -10873.63784663605
Iteration 7100: Loss = -10873.637822259807
Iteration 7200: Loss = -10873.637671445686
Iteration 7300: Loss = -10873.642535958676
1
Iteration 7400: Loss = -10873.623703609552
Iteration 7500: Loss = -10873.623635739292
Iteration 7600: Loss = -10873.623629011616
Iteration 7700: Loss = -10873.623565707125
Iteration 7800: Loss = -10873.624048105106
1
Iteration 7900: Loss = -10873.622694100522
Iteration 8000: Loss = -10873.630631573995
1
Iteration 8100: Loss = -10873.618224641043
Iteration 8200: Loss = -10873.618556119924
1
Iteration 8300: Loss = -10873.6188893016
2
Iteration 8400: Loss = -10873.618998362685
3
Iteration 8500: Loss = -10873.618315495805
Iteration 8600: Loss = -10873.621615515953
1
Iteration 8700: Loss = -10873.617749105906
Iteration 8800: Loss = -10873.619504741371
1
Iteration 8900: Loss = -10873.617617525011
Iteration 9000: Loss = -10873.61761668074
Iteration 9100: Loss = -10873.617556881045
Iteration 9200: Loss = -10873.617590417634
Iteration 9300: Loss = -10873.617418509639
Iteration 9400: Loss = -10873.616935278511
Iteration 9500: Loss = -10873.616716617289
Iteration 9600: Loss = -10873.61815693084
1
Iteration 9700: Loss = -10873.6166843607
Iteration 9800: Loss = -10873.624168652097
1
Iteration 9900: Loss = -10873.616659006242
Iteration 10000: Loss = -10873.616683810182
Iteration 10100: Loss = -10873.616694843324
Iteration 10200: Loss = -10873.616660766353
Iteration 10300: Loss = -10873.692210234814
1
Iteration 10400: Loss = -10873.616657708364
Iteration 10500: Loss = -10873.616581307266
Iteration 10600: Loss = -10873.61624970829
Iteration 10700: Loss = -10873.613425490534
Iteration 10800: Loss = -10873.613457742955
Iteration 10900: Loss = -10873.613567690274
1
Iteration 11000: Loss = -10873.613179356167
Iteration 11100: Loss = -10873.612826757233
Iteration 11200: Loss = -10873.607790457576
Iteration 11300: Loss = -10873.607006533437
Iteration 11400: Loss = -10873.626614008326
1
Iteration 11500: Loss = -10873.606897927479
Iteration 11600: Loss = -10873.606858186968
Iteration 11700: Loss = -10873.607348685937
1
Iteration 11800: Loss = -10873.606599419725
Iteration 11900: Loss = -10873.606381891217
Iteration 12000: Loss = -10873.605654285471
Iteration 12100: Loss = -10873.58228068921
Iteration 12200: Loss = -10873.582225998907
Iteration 12300: Loss = -10873.582297998528
Iteration 12400: Loss = -10873.67141425392
1
Iteration 12500: Loss = -10873.582203868453
Iteration 12600: Loss = -10873.802976944748
1
Iteration 12700: Loss = -10873.581230538379
Iteration 12800: Loss = -10873.58124423639
Iteration 12900: Loss = -10873.582169823605
1
Iteration 13000: Loss = -10873.581194819488
Iteration 13100: Loss = -10873.581605854306
1
Iteration 13200: Loss = -10873.581115742656
Iteration 13300: Loss = -10873.584200060448
1
Iteration 13400: Loss = -10873.581129696451
Iteration 13500: Loss = -10873.729773533241
1
Iteration 13600: Loss = -10873.581155418107
Iteration 13700: Loss = -10873.581105200165
Iteration 13800: Loss = -10873.585883696836
1
Iteration 13900: Loss = -10873.58110916305
Iteration 14000: Loss = -10873.613822422505
1
Iteration 14100: Loss = -10873.580942058987
Iteration 14200: Loss = -10873.580895651474
Iteration 14300: Loss = -10873.583730149929
1
Iteration 14400: Loss = -10873.580882070266
Iteration 14500: Loss = -10873.58089651109
Iteration 14600: Loss = -10873.58301757793
1
Iteration 14700: Loss = -10873.580854341095
Iteration 14800: Loss = -10873.582367982082
1
Iteration 14900: Loss = -10873.58176801353
2
Iteration 15000: Loss = -10873.589428791931
3
Iteration 15100: Loss = -10873.5808872879
Iteration 15200: Loss = -10873.581738877652
1
Iteration 15300: Loss = -10873.580857004681
Iteration 15400: Loss = -10873.583690993844
1
Iteration 15500: Loss = -10873.58086609416
Iteration 15600: Loss = -10873.585013740154
1
Iteration 15700: Loss = -10873.580861815644
Iteration 15800: Loss = -10873.591015799535
1
Iteration 15900: Loss = -10873.580842514431
Iteration 16000: Loss = -10873.620441150215
1
Iteration 16100: Loss = -10873.58085052611
Iteration 16200: Loss = -10873.581562491585
1
Iteration 16300: Loss = -10873.580857038394
Iteration 16400: Loss = -10873.580825405174
Iteration 16500: Loss = -10873.580886356389
Iteration 16600: Loss = -10873.585088665464
1
Iteration 16700: Loss = -10873.581834781942
2
Iteration 16800: Loss = -10873.580909190901
Iteration 16900: Loss = -10873.59220839624
1
Iteration 17000: Loss = -10873.580853685851
Iteration 17100: Loss = -10873.582714705228
1
Iteration 17200: Loss = -10873.601921854315
2
Iteration 17300: Loss = -10873.580883592838
Iteration 17400: Loss = -10873.586246705387
1
Iteration 17500: Loss = -10873.580837932923
Iteration 17600: Loss = -10873.581315829555
1
Iteration 17700: Loss = -10873.580838952383
Iteration 17800: Loss = -10873.582934518654
1
Iteration 17900: Loss = -10873.580857307772
Iteration 18000: Loss = -10873.586787303211
1
Iteration 18100: Loss = -10873.580832834432
Iteration 18200: Loss = -10873.61031640296
1
Iteration 18300: Loss = -10873.580832654783
Iteration 18400: Loss = -10873.839415442546
1
Iteration 18500: Loss = -10873.580844241205
Iteration 18600: Loss = -10873.580842635392
Iteration 18700: Loss = -10873.582451489718
1
Iteration 18800: Loss = -10873.580830303654
Iteration 18900: Loss = -10873.582036775786
1
Iteration 19000: Loss = -10873.58082984754
Iteration 19100: Loss = -10873.595617080697
1
Iteration 19200: Loss = -10873.580816534957
Iteration 19300: Loss = -10873.580835647093
Iteration 19400: Loss = -10873.580881061833
Iteration 19500: Loss = -10873.580821138323
Iteration 19600: Loss = -10873.580987240308
1
Iteration 19700: Loss = -10873.595330322752
2
Iteration 19800: Loss = -10873.580825544548
Iteration 19900: Loss = -10873.582535795847
1
pi: tensor([[0.6604, 0.3396],
        [0.2765, 0.7235]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9351, 0.0649], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1815, 0.0897],
         [0.6230, 0.2582]],

        [[0.5696, 0.1067],
         [0.6943, 0.7148]],

        [[0.5878, 0.0995],
         [0.5369, 0.5124]],

        [[0.5441, 0.0997],
         [0.5559, 0.6202]],

        [[0.5208, 0.1039],
         [0.6624, 0.5104]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721463199647421
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448326530612245
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080875752406894
Global Adjusted Rand Index: 0.5407937788266778
Average Adjusted Rand Index: 0.6598001749499235
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20856.630218474635
Iteration 100: Loss = -10952.695780314492
Iteration 200: Loss = -10949.997649155865
Iteration 300: Loss = -10949.13606502857
Iteration 400: Loss = -10948.647858830998
Iteration 500: Loss = -10948.364240561838
Iteration 600: Loss = -10948.217755860574
Iteration 700: Loss = -10948.10146821879
Iteration 800: Loss = -10947.99848623023
Iteration 900: Loss = -10947.898231760277
Iteration 1000: Loss = -10947.778397534466
Iteration 1100: Loss = -10947.623549321173
Iteration 1200: Loss = -10947.227718961924
Iteration 1300: Loss = -10944.181431711144
Iteration 1400: Loss = -10942.78610487455
Iteration 1500: Loss = -10942.33768756334
Iteration 1600: Loss = -10942.169040383422
Iteration 1700: Loss = -10942.073439069938
Iteration 1800: Loss = -10941.858751250567
Iteration 1900: Loss = -10941.807063326693
Iteration 2000: Loss = -10941.652034711755
Iteration 2100: Loss = -10941.533753040605
Iteration 2200: Loss = -10941.507741250858
Iteration 2300: Loss = -10941.45455548681
Iteration 2400: Loss = -10941.395122113141
Iteration 2500: Loss = -10941.172522166231
Iteration 2600: Loss = -10939.903840397947
Iteration 2700: Loss = -10938.886472825014
Iteration 2800: Loss = -10938.82356674085
Iteration 2900: Loss = -10938.772275338817
Iteration 3000: Loss = -10937.258059406957
Iteration 3100: Loss = -10936.599215214475
Iteration 3200: Loss = -10936.50494867472
Iteration 3300: Loss = -10936.459641626792
Iteration 3400: Loss = -10936.426938461544
Iteration 3500: Loss = -10936.405603032737
Iteration 3600: Loss = -10936.387539397456
Iteration 3700: Loss = -10936.377144899689
Iteration 3800: Loss = -10936.360799477468
Iteration 3900: Loss = -10936.24384783062
Iteration 4000: Loss = -10936.23508714853
Iteration 4100: Loss = -10936.22777620959
Iteration 4200: Loss = -10936.21980278412
Iteration 4300: Loss = -10936.180774692826
Iteration 4400: Loss = -10935.280468189614
Iteration 4500: Loss = -10932.435981790857
Iteration 4600: Loss = -10928.513343624705
Iteration 4700: Loss = -10896.656177651392
Iteration 4800: Loss = -10882.02036713926
Iteration 4900: Loss = -10878.712740284767
Iteration 5000: Loss = -10878.323737889004
Iteration 5100: Loss = -10878.305098798231
Iteration 5200: Loss = -10878.295935216309
Iteration 5300: Loss = -10878.270328333327
Iteration 5400: Loss = -10878.240644357347
Iteration 5500: Loss = -10878.217865526016
Iteration 5600: Loss = -10877.248808111111
Iteration 5700: Loss = -10874.831066058767
Iteration 5800: Loss = -10874.728318914966
Iteration 5900: Loss = -10874.721566297829
Iteration 6000: Loss = -10874.735263441045
1
Iteration 6100: Loss = -10874.6658791764
Iteration 6200: Loss = -10874.624529146402
Iteration 6300: Loss = -10874.623231163687
Iteration 6400: Loss = -10874.620777651837
Iteration 6500: Loss = -10874.61620140919
Iteration 6600: Loss = -10874.599620087463
Iteration 6700: Loss = -10874.597249777582
Iteration 6800: Loss = -10874.59549517058
Iteration 6900: Loss = -10874.595097977146
Iteration 7000: Loss = -10874.567101071267
Iteration 7100: Loss = -10874.5466234595
Iteration 7200: Loss = -10874.542103157335
Iteration 7300: Loss = -10874.528453148863
Iteration 7400: Loss = -10874.518628890435
Iteration 7500: Loss = -10874.482427550016
Iteration 7600: Loss = -10874.496586437603
1
Iteration 7700: Loss = -10874.481747025426
Iteration 7800: Loss = -10874.501260379413
1
Iteration 7900: Loss = -10874.48105313404
Iteration 8000: Loss = -10874.480500015747
Iteration 8100: Loss = -10874.443733385244
Iteration 8200: Loss = -10874.44143059214
Iteration 8300: Loss = -10874.440582486337
Iteration 8400: Loss = -10874.426724986839
Iteration 8500: Loss = -10874.426535118275
Iteration 8600: Loss = -10874.511450425207
1
Iteration 8700: Loss = -10874.42648586803
Iteration 8800: Loss = -10874.426257089095
Iteration 8900: Loss = -10874.420604581712
Iteration 9000: Loss = -10874.417926129712
Iteration 9100: Loss = -10874.422323796703
1
Iteration 9200: Loss = -10874.417829934948
Iteration 9300: Loss = -10874.42910347807
1
Iteration 9400: Loss = -10874.417806826781
Iteration 9500: Loss = -10874.520420795516
1
Iteration 9600: Loss = -10874.417718478287
Iteration 9700: Loss = -10874.417525197778
Iteration 9800: Loss = -10874.411759554801
Iteration 9900: Loss = -10874.41096442146
Iteration 10000: Loss = -10874.811263728961
1
Iteration 10100: Loss = -10874.410935379974
Iteration 10200: Loss = -10874.410903272825
Iteration 10300: Loss = -10874.866597458393
1
Iteration 10400: Loss = -10874.410697128533
Iteration 10500: Loss = -10874.405789217099
Iteration 10600: Loss = -10874.44237391401
1
Iteration 10700: Loss = -10874.388610755948
Iteration 10800: Loss = -10874.388269256939
Iteration 10900: Loss = -10874.391702618017
1
Iteration 11000: Loss = -10874.387913842864
Iteration 11100: Loss = -10874.387620502412
Iteration 11200: Loss = -10874.395319176308
1
Iteration 11300: Loss = -10874.387619769617
Iteration 11400: Loss = -10874.387658081963
Iteration 11500: Loss = -10874.3886607787
1
Iteration 11600: Loss = -10874.387596605728
Iteration 11700: Loss = -10874.387581037936
Iteration 11800: Loss = -10874.387668572375
Iteration 11900: Loss = -10874.387611729562
Iteration 12000: Loss = -10874.439488667074
1
Iteration 12100: Loss = -10874.387118144505
Iteration 12200: Loss = -10874.387092870498
Iteration 12300: Loss = -10874.38761494481
1
Iteration 12400: Loss = -10874.387075283772
Iteration 12500: Loss = -10874.421091925882
1
Iteration 12600: Loss = -10874.386564095847
Iteration 12700: Loss = -10874.386485875391
Iteration 12800: Loss = -10874.391578282844
1
Iteration 12900: Loss = -10874.385155805641
Iteration 13000: Loss = -10874.38515611906
Iteration 13100: Loss = -10874.388956277307
1
Iteration 13200: Loss = -10874.385131447429
Iteration 13300: Loss = -10874.385161540293
Iteration 13400: Loss = -10874.386458502859
1
Iteration 13500: Loss = -10874.385152253157
Iteration 13600: Loss = -10874.386213869268
1
Iteration 13700: Loss = -10874.385172885513
Iteration 13800: Loss = -10874.385019194222
Iteration 13900: Loss = -10874.390185998647
1
Iteration 14000: Loss = -10874.38491877391
Iteration 14100: Loss = -10874.384853290994
Iteration 14200: Loss = -10874.38398115243
Iteration 14300: Loss = -10874.383330584165
Iteration 14400: Loss = -10874.390394607972
1
Iteration 14500: Loss = -10874.45300846563
2
Iteration 14600: Loss = -10874.400717987435
3
Iteration 14700: Loss = -10874.55631760948
4
Iteration 14800: Loss = -10874.383308625473
Iteration 14900: Loss = -10874.38403105051
1
Iteration 15000: Loss = -10874.383293631066
Iteration 15100: Loss = -10874.383298194303
Iteration 15200: Loss = -10874.383271585039
Iteration 15300: Loss = -10874.384057644129
1
Iteration 15400: Loss = -10874.383284970794
Iteration 15500: Loss = -10874.450541974358
1
Iteration 15600: Loss = -10874.383264112616
Iteration 15700: Loss = -10874.389507729958
1
Iteration 15800: Loss = -10874.383278140804
Iteration 15900: Loss = -10874.383284550358
Iteration 16000: Loss = -10874.383352282968
Iteration 16100: Loss = -10874.389342135582
1
Iteration 16200: Loss = -10874.384327710513
2
Iteration 16300: Loss = -10874.383549565882
3
Iteration 16400: Loss = -10874.383282396362
Iteration 16500: Loss = -10874.383202883413
Iteration 16600: Loss = -10874.383824104667
1
Iteration 16700: Loss = -10874.383186392308
Iteration 16800: Loss = -10874.38536120838
1
Iteration 16900: Loss = -10874.383159154446
Iteration 17000: Loss = -10874.388072557724
1
Iteration 17100: Loss = -10873.708602244962
Iteration 17200: Loss = -10873.650551823346
Iteration 17300: Loss = -10873.59186920587
Iteration 17400: Loss = -10873.590958189658
Iteration 17500: Loss = -10873.586897709112
Iteration 17600: Loss = -10873.587896191148
1
Iteration 17700: Loss = -10873.58590538135
Iteration 17800: Loss = -10873.94669528173
1
Iteration 17900: Loss = -10873.585844077417
Iteration 18000: Loss = -10873.58586772833
Iteration 18100: Loss = -10873.706657472074
1
Iteration 18200: Loss = -10873.584087544046
Iteration 18300: Loss = -10873.584071488189
Iteration 18400: Loss = -10873.86344459223
1
Iteration 18500: Loss = -10873.584038292227
Iteration 18600: Loss = -10873.58749731915
1
Iteration 18700: Loss = -10873.585737620144
2
Iteration 18800: Loss = -10873.584030970585
Iteration 18900: Loss = -10873.62943907953
1
Iteration 19000: Loss = -10873.584041991631
Iteration 19100: Loss = -10873.654358675623
1
Iteration 19200: Loss = -10873.584062506145
Iteration 19300: Loss = -10873.58460413982
1
Iteration 19400: Loss = -10873.584092798877
Iteration 19500: Loss = -10873.641606527563
1
Iteration 19600: Loss = -10873.584053699124
Iteration 19700: Loss = -10873.595798746337
1
Iteration 19800: Loss = -10873.584046432388
Iteration 19900: Loss = -10873.58403488151
pi: tensor([[0.7222, 0.2778],
        [0.3400, 0.6600]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0650, 0.9350], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2583, 0.0897],
         [0.5297, 0.1816]],

        [[0.5851, 0.1066],
         [0.6026, 0.5894]],

        [[0.7289, 0.0995],
         [0.5093, 0.6063]],

        [[0.6024, 0.0997],
         [0.6455, 0.5568]],

        [[0.6773, 0.1038],
         [0.5593, 0.5115]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721463199647421
time is 2
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080875752406894
Global Adjusted Rand Index: 0.5407937788266778
Average Adjusted Rand Index: 0.6598001749499235
10883.416457319872
[0.5407937788266778, 0.5407937788266778] [0.6598001749499235, 0.6598001749499235] [10873.580828355209, 10873.585431004922]
-------------------------------------
This iteration is 74
True Objective function: Loss = -10829.572161757842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21967.841764580597
Iteration 100: Loss = -10930.85721463982
Iteration 200: Loss = -10930.249231924921
Iteration 300: Loss = -10930.007823975988
Iteration 400: Loss = -10929.783855114985
Iteration 500: Loss = -10929.405725799374
Iteration 600: Loss = -10928.618880013795
Iteration 700: Loss = -10926.951084057206
Iteration 800: Loss = -10926.14608988714
Iteration 900: Loss = -10925.132232442578
Iteration 1000: Loss = -10924.552984197913
Iteration 1100: Loss = -10924.225227212137
Iteration 1200: Loss = -10924.002882760959
Iteration 1300: Loss = -10923.838934511356
Iteration 1400: Loss = -10923.712775762631
Iteration 1500: Loss = -10923.61410919092
Iteration 1600: Loss = -10923.535422190735
Iteration 1700: Loss = -10923.470244396103
Iteration 1800: Loss = -10923.414959125274
Iteration 1900: Loss = -10923.366983280086
Iteration 2000: Loss = -10923.324374146985
Iteration 2100: Loss = -10923.28556070484
Iteration 2200: Loss = -10923.249226276313
Iteration 2300: Loss = -10923.214259519531
Iteration 2400: Loss = -10923.179508785788
Iteration 2500: Loss = -10923.143636112472
Iteration 2600: Loss = -10923.105141898426
Iteration 2700: Loss = -10923.062786268594
Iteration 2800: Loss = -10923.015516578058
Iteration 2900: Loss = -10922.963685804125
Iteration 3000: Loss = -10922.90836142393
Iteration 3100: Loss = -10922.851599551761
Iteration 3200: Loss = -10922.795488038882
Iteration 3300: Loss = -10922.741484184715
Iteration 3400: Loss = -10922.690574246766
Iteration 3500: Loss = -10922.643162992405
Iteration 3600: Loss = -10922.599484973118
Iteration 3700: Loss = -10922.559375156761
Iteration 3800: Loss = -10922.522723462493
Iteration 3900: Loss = -10922.489225764333
Iteration 4000: Loss = -10922.458664331418
Iteration 4100: Loss = -10922.430776613863
Iteration 4200: Loss = -10922.40529768042
Iteration 4300: Loss = -10922.38203570657
Iteration 4400: Loss = -10922.360770181092
Iteration 4500: Loss = -10922.341266577347
Iteration 4600: Loss = -10922.323435135168
Iteration 4700: Loss = -10922.307032326935
Iteration 4800: Loss = -10922.291950726603
Iteration 4900: Loss = -10922.278128105103
Iteration 5000: Loss = -10922.265356741647
Iteration 5100: Loss = -10922.253576350986
Iteration 5200: Loss = -10922.242753113138
Iteration 5300: Loss = -10922.232709643
Iteration 5400: Loss = -10922.223387381731
Iteration 5500: Loss = -10922.214789583866
Iteration 5600: Loss = -10922.206845933722
Iteration 5700: Loss = -10922.199447198014
Iteration 5800: Loss = -10922.192573896802
Iteration 5900: Loss = -10922.1861933528
Iteration 6000: Loss = -10922.1802569091
Iteration 6100: Loss = -10922.174731477939
Iteration 6200: Loss = -10922.169556751094
Iteration 6300: Loss = -10922.164707284308
Iteration 6400: Loss = -10922.160274069905
Iteration 6500: Loss = -10922.156080049355
Iteration 6600: Loss = -10922.15218567722
Iteration 6700: Loss = -10922.148462757956
Iteration 6800: Loss = -10922.145029708334
Iteration 6900: Loss = -10922.141806788704
Iteration 7000: Loss = -10922.138840413396
Iteration 7100: Loss = -10922.135998169135
Iteration 7200: Loss = -10922.133351316315
Iteration 7300: Loss = -10922.245619240111
1
Iteration 7400: Loss = -10922.128503889668
Iteration 7500: Loss = -10922.126293184125
Iteration 7600: Loss = -10922.124270669396
Iteration 7700: Loss = -10922.125065640314
1
Iteration 7800: Loss = -10922.120464859348
Iteration 7900: Loss = -10922.11879376024
Iteration 8000: Loss = -10922.117105011412
Iteration 8100: Loss = -10922.115691417293
Iteration 8200: Loss = -10922.114155922
Iteration 8300: Loss = -10922.112792601869
Iteration 8400: Loss = -10922.112544105632
Iteration 8500: Loss = -10922.110390947288
Iteration 8600: Loss = -10922.10917774061
Iteration 8700: Loss = -10922.108111694304
Iteration 8800: Loss = -10922.108425416185
1
Iteration 8900: Loss = -10922.106118334148
Iteration 9000: Loss = -10922.105233828117
Iteration 9100: Loss = -10922.104818497028
Iteration 9200: Loss = -10922.103594924296
Iteration 9300: Loss = -10922.102811593473
Iteration 9400: Loss = -10922.10207853335
Iteration 9500: Loss = -10922.102174022973
Iteration 9600: Loss = -10922.100713211916
Iteration 9700: Loss = -10922.100137458412
Iteration 9800: Loss = -10922.09954148075
Iteration 9900: Loss = -10922.099201184665
Iteration 10000: Loss = -10922.098457606062
Iteration 10100: Loss = -10922.098018020526
Iteration 10200: Loss = -10922.276119522607
1
Iteration 10300: Loss = -10922.097063897274
Iteration 10400: Loss = -10922.096676403333
Iteration 10500: Loss = -10922.096253058568
Iteration 10600: Loss = -10922.100780299173
1
Iteration 10700: Loss = -10922.09554411122
Iteration 10800: Loss = -10922.095189682872
Iteration 10900: Loss = -10922.094851925907
Iteration 11000: Loss = -10922.377647842297
1
Iteration 11100: Loss = -10922.094278217011
Iteration 11200: Loss = -10922.094033984069
Iteration 11300: Loss = -10922.093706089798
Iteration 11400: Loss = -10922.100724389995
1
Iteration 11500: Loss = -10922.09323641181
Iteration 11600: Loss = -10922.093009570817
Iteration 11700: Loss = -10922.092795850484
Iteration 11800: Loss = -10922.094121197542
1
Iteration 11900: Loss = -10922.09239751143
Iteration 12000: Loss = -10922.092239478974
Iteration 12100: Loss = -10922.1055404008
1
Iteration 12200: Loss = -10922.091277826394
Iteration 12300: Loss = -10922.091003967309
Iteration 12400: Loss = -10922.090828098995
Iteration 12500: Loss = -10922.095798188471
1
Iteration 12600: Loss = -10922.090611225934
Iteration 12700: Loss = -10922.09047599507
Iteration 12800: Loss = -10922.0903587497
Iteration 12900: Loss = -10922.09368534008
1
Iteration 13000: Loss = -10922.090122831598
Iteration 13100: Loss = -10922.090019582798
Iteration 13200: Loss = -10922.08992738696
Iteration 13300: Loss = -10922.08982482229
Iteration 13400: Loss = -10922.089717202241
Iteration 13500: Loss = -10922.090316632053
1
Iteration 13600: Loss = -10922.091629553548
2
Iteration 13700: Loss = -10922.090540231868
3
Iteration 13800: Loss = -10922.089660387552
Iteration 13900: Loss = -10922.089500152957
Iteration 14000: Loss = -10922.278412417812
1
Iteration 14100: Loss = -10922.089298887198
Iteration 14200: Loss = -10922.170502542232
1
Iteration 14300: Loss = -10922.089152643695
Iteration 14400: Loss = -10922.092488159451
1
Iteration 14500: Loss = -10922.089110435554
Iteration 14600: Loss = -10922.1062697378
1
Iteration 14700: Loss = -10922.089038892698
Iteration 14800: Loss = -10922.094122913802
1
Iteration 14900: Loss = -10922.08951914627
2
Iteration 15000: Loss = -10922.088982519188
Iteration 15100: Loss = -10922.08914424034
1
Iteration 15200: Loss = -10922.089052926856
Iteration 15300: Loss = -10922.088971100118
Iteration 15400: Loss = -10922.103026047967
1
Iteration 15500: Loss = -10922.091287666406
2
Iteration 15600: Loss = -10922.088835445362
Iteration 15700: Loss = -10922.105790758811
1
Iteration 15800: Loss = -10922.08897537153
2
Iteration 15900: Loss = -10922.08920308349
3
Iteration 16000: Loss = -10922.280104854872
4
Iteration 16100: Loss = -10922.08871345749
Iteration 16200: Loss = -10922.08947612104
1
Iteration 16300: Loss = -10922.088663373921
Iteration 16400: Loss = -10922.088603103077
Iteration 16500: Loss = -10922.130555466534
1
Iteration 16600: Loss = -10922.088859920943
2
Iteration 16700: Loss = -10922.08848853662
Iteration 16800: Loss = -10922.341259520992
1
Iteration 16900: Loss = -10922.088425803953
Iteration 17000: Loss = -10922.113677278081
1
Iteration 17100: Loss = -10922.088562517798
2
Iteration 17200: Loss = -10922.091681266867
3
Iteration 17300: Loss = -10922.088553269858
4
Iteration 17400: Loss = -10922.089416555056
5
Iteration 17500: Loss = -10922.328680020313
6
Iteration 17600: Loss = -10922.088399763543
Iteration 17700: Loss = -10922.115889788722
1
Iteration 17800: Loss = -10922.088381833559
Iteration 17900: Loss = -10922.090240981937
1
Iteration 18000: Loss = -10922.088976043846
2
Iteration 18100: Loss = -10922.089341586276
3
Iteration 18200: Loss = -10922.09049943997
4
Iteration 18300: Loss = -10922.091919545135
5
Iteration 18400: Loss = -10922.089000560594
6
Iteration 18500: Loss = -10922.089421769502
7
Iteration 18600: Loss = -10922.088398517028
Iteration 18700: Loss = -10922.088878331948
1
Iteration 18800: Loss = -10922.088315158582
Iteration 18900: Loss = -10922.08952814519
1
Iteration 19000: Loss = -10922.088378304374
Iteration 19100: Loss = -10922.088949439178
1
Iteration 19200: Loss = -10922.089968386505
2
Iteration 19300: Loss = -10922.209113891695
3
Iteration 19400: Loss = -10922.08824808672
Iteration 19500: Loss = -10922.088241695275
Iteration 19600: Loss = -10922.088381286974
1
Iteration 19700: Loss = -10922.088241778694
Iteration 19800: Loss = -10922.089467390548
1
Iteration 19900: Loss = -10922.282713767278
2
pi: tensor([[1.0000e+00, 4.0031e-09],
        [8.0458e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9799, 0.0201], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.5892e-01, 2.3568e-01],
         [6.5652e-01, 1.2861e-05]],

        [[5.0572e-01, 2.6403e-01],
         [6.9917e-01, 5.3975e-01]],

        [[5.0710e-01, 1.8871e-01],
         [6.8639e-01, 7.1995e-01]],

        [[6.5668e-01, 1.4785e-01],
         [7.2987e-01, 6.6406e-01]],

        [[5.8142e-01, 2.4585e-01],
         [5.9121e-01, 5.4659e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: 0.004087827612858745
Average Adjusted Rand Index: 0.004250692246687353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21771.616973553722
Iteration 100: Loss = -10930.41836832503
Iteration 200: Loss = -10929.867718225198
Iteration 300: Loss = -10929.689150259765
Iteration 400: Loss = -10929.413318551575
Iteration 500: Loss = -10929.191174610622
Iteration 600: Loss = -10929.1601937685
Iteration 700: Loss = -10929.143086011978
Iteration 800: Loss = -10929.128841472437
Iteration 900: Loss = -10929.115095651998
Iteration 1000: Loss = -10929.098968969152
Iteration 1100: Loss = -10929.063508842359
Iteration 1200: Loss = -10927.783047023615
Iteration 1300: Loss = -10926.264684591382
Iteration 1400: Loss = -10925.784217875937
Iteration 1500: Loss = -10924.26654178193
Iteration 1600: Loss = -10923.75089100362
Iteration 1700: Loss = -10923.56660971619
Iteration 1800: Loss = -10923.45431781637
Iteration 1900: Loss = -10923.37261621998
Iteration 2000: Loss = -10923.308460765818
Iteration 2100: Loss = -10923.254155314138
Iteration 2200: Loss = -10923.206355188515
Iteration 2300: Loss = -10923.161207182138
Iteration 2400: Loss = -10923.115260357114
Iteration 2500: Loss = -10923.065505636774
Iteration 2600: Loss = -10923.009579219854
Iteration 2700: Loss = -10922.947271097679
Iteration 2800: Loss = -10922.881692565135
Iteration 2900: Loss = -10922.816557839198
Iteration 3000: Loss = -10922.754661310846
Iteration 3100: Loss = -10922.697443211078
Iteration 3200: Loss = -10922.645215600789
Iteration 3300: Loss = -10922.597950932788
Iteration 3400: Loss = -10922.555260235717
Iteration 3500: Loss = -10922.516827400208
Iteration 3600: Loss = -10922.482123766928
Iteration 3700: Loss = -10922.450794206477
Iteration 3800: Loss = -10922.4224858609
Iteration 3900: Loss = -10922.396810577877
Iteration 4000: Loss = -10922.373537192248
Iteration 4100: Loss = -10922.35233151235
Iteration 4200: Loss = -10922.333004337006
Iteration 4300: Loss = -10922.315392588094
Iteration 4400: Loss = -10922.299233953863
Iteration 4500: Loss = -10922.284518747305
Iteration 4600: Loss = -10922.270958191946
Iteration 4700: Loss = -10922.25849586346
Iteration 4800: Loss = -10922.247047023018
Iteration 4900: Loss = -10922.236505841713
Iteration 5000: Loss = -10922.22674762051
Iteration 5100: Loss = -10922.217757525616
Iteration 5200: Loss = -10922.209464284344
Iteration 5300: Loss = -10922.201735815057
Iteration 5400: Loss = -10922.194580419431
Iteration 5500: Loss = -10922.187992093131
Iteration 5600: Loss = -10922.181860571149
Iteration 5700: Loss = -10922.176139876257
Iteration 5800: Loss = -10922.170825221016
Iteration 5900: Loss = -10922.165878929816
Iteration 6000: Loss = -10922.161283911719
Iteration 6100: Loss = -10922.156960414997
Iteration 6200: Loss = -10922.152919604116
Iteration 6300: Loss = -10922.149197696295
Iteration 6400: Loss = -10922.145667230783
Iteration 6500: Loss = -10922.142410768924
Iteration 6600: Loss = -10922.13931601598
Iteration 6700: Loss = -10922.136447256144
Iteration 6800: Loss = -10922.133782301566
Iteration 6900: Loss = -10922.131221508995
Iteration 7000: Loss = -10922.128858372516
Iteration 7100: Loss = -10922.126603533206
Iteration 7200: Loss = -10922.124542826894
Iteration 7300: Loss = -10922.12256363778
Iteration 7400: Loss = -10922.120734903814
Iteration 7500: Loss = -10922.118992338861
Iteration 7600: Loss = -10922.117316590013
Iteration 7700: Loss = -10922.115760253568
Iteration 7800: Loss = -10922.122509029692
1
Iteration 7900: Loss = -10922.11297499196
Iteration 8000: Loss = -10922.111659191492
Iteration 8100: Loss = -10922.110401251895
Iteration 8200: Loss = -10922.112342106304
1
Iteration 8300: Loss = -10922.10822962244
Iteration 8400: Loss = -10922.107206697794
Iteration 8500: Loss = -10922.318351299531
1
Iteration 8600: Loss = -10922.105367958724
Iteration 8700: Loss = -10922.104447158626
Iteration 8800: Loss = -10922.103629976958
Iteration 8900: Loss = -10922.209521068273
1
Iteration 9000: Loss = -10922.10214937667
Iteration 9100: Loss = -10922.10146576586
Iteration 9200: Loss = -10922.100785951721
Iteration 9300: Loss = -10922.10345880165
1
Iteration 9400: Loss = -10922.09960650945
Iteration 9500: Loss = -10922.0990846184
Iteration 9600: Loss = -10922.09856207139
Iteration 9700: Loss = -10922.098214307634
Iteration 9800: Loss = -10922.097561005485
Iteration 9900: Loss = -10922.09711535462
Iteration 10000: Loss = -10922.096674224049
Iteration 10100: Loss = -10922.097734886958
1
Iteration 10200: Loss = -10922.095890418255
Iteration 10300: Loss = -10922.095553016887
Iteration 10400: Loss = -10922.095730024737
1
Iteration 10500: Loss = -10922.094910512702
Iteration 10600: Loss = -10922.094635401532
Iteration 10700: Loss = -10922.094342230732
Iteration 10800: Loss = -10922.184596880283
1
Iteration 10900: Loss = -10922.093798587579
Iteration 11000: Loss = -10922.093505038993
Iteration 11100: Loss = -10922.093266164185
Iteration 11200: Loss = -10922.175658029251
1
Iteration 11300: Loss = -10922.092810697088
Iteration 11400: Loss = -10922.092651246903
Iteration 11500: Loss = -10922.092477204369
Iteration 11600: Loss = -10922.14690048931
1
Iteration 11700: Loss = -10922.092128462498
Iteration 11800: Loss = -10922.091928672602
Iteration 11900: Loss = -10922.091774010441
Iteration 12000: Loss = -10922.156196250427
1
Iteration 12100: Loss = -10922.091483250582
Iteration 12200: Loss = -10922.091388872173
Iteration 12300: Loss = -10922.091227935238
Iteration 12400: Loss = -10922.091143045154
Iteration 12500: Loss = -10922.091275378392
1
Iteration 12600: Loss = -10922.090935848142
Iteration 12700: Loss = -10922.090839461163
Iteration 12800: Loss = -10922.100987118434
1
Iteration 12900: Loss = -10922.090652953324
Iteration 13000: Loss = -10922.090567252595
Iteration 13100: Loss = -10922.137188533483
1
Iteration 13200: Loss = -10922.090372688566
Iteration 13300: Loss = -10922.090305762647
Iteration 13400: Loss = -10922.09029239418
Iteration 13500: Loss = -10922.09025843377
Iteration 13600: Loss = -10922.090163031413
Iteration 13700: Loss = -10922.090115097257
Iteration 13800: Loss = -10922.091170155225
1
Iteration 13900: Loss = -10922.0921575915
2
Iteration 14000: Loss = -10922.089967925875
Iteration 14100: Loss = -10922.10405519529
1
Iteration 14200: Loss = -10922.089860592341
Iteration 14300: Loss = -10922.090341024636
1
Iteration 14400: Loss = -10922.089959814419
Iteration 14500: Loss = -10922.100085559692
1
Iteration 14600: Loss = -10922.089856883125
Iteration 14700: Loss = -10922.089733163011
Iteration 14800: Loss = -10922.16914168774
1
Iteration 14900: Loss = -10922.089705124292
Iteration 15000: Loss = -10922.099438973608
1
Iteration 15100: Loss = -10922.09066771257
2
Iteration 15200: Loss = -10922.145282681075
3
Iteration 15300: Loss = -10922.089520301783
Iteration 15400: Loss = -10922.089699529268
1
Iteration 15500: Loss = -10922.198552692982
2
Iteration 15600: Loss = -10922.089489163152
Iteration 15700: Loss = -10922.090156509139
1
Iteration 15800: Loss = -10922.089507203415
Iteration 15900: Loss = -10922.092781904053
1
Iteration 16000: Loss = -10922.091625183612
2
Iteration 16100: Loss = -10922.089399435221
Iteration 16200: Loss = -10922.089518000039
1
Iteration 16300: Loss = -10922.089771542653
2
Iteration 16400: Loss = -10922.089874130394
3
Iteration 16500: Loss = -10922.109802274343
4
Iteration 16600: Loss = -10922.090162889514
5
Iteration 16700: Loss = -10922.089539900233
6
Iteration 16800: Loss = -10922.090233668052
7
Iteration 16900: Loss = -10922.089332816
Iteration 17000: Loss = -10922.196654908561
1
Iteration 17100: Loss = -10922.089215403306
Iteration 17200: Loss = -10922.08960781727
1
Iteration 17300: Loss = -10922.383200069682
2
Iteration 17400: Loss = -10922.09006237198
3
Iteration 17500: Loss = -10922.111061262427
4
Iteration 17600: Loss = -10922.089138358908
Iteration 17700: Loss = -10922.096654494759
1
Iteration 17800: Loss = -10922.090194471024
2
Iteration 17900: Loss = -10922.08915905192
Iteration 18000: Loss = -10922.090484606588
1
Iteration 18100: Loss = -10922.08913049518
Iteration 18200: Loss = -10922.145524826394
1
Iteration 18300: Loss = -10922.089075119065
Iteration 18400: Loss = -10922.124926771936
1
Iteration 18500: Loss = -10922.090446387716
2
Iteration 18600: Loss = -10922.090375940219
3
Iteration 18700: Loss = -10922.08950001532
4
Iteration 18800: Loss = -10922.095673392349
5
Iteration 18900: Loss = -10922.089042947107
Iteration 19000: Loss = -10922.095774757505
1
Iteration 19100: Loss = -10922.08902581348
Iteration 19200: Loss = -10922.107541591731
1
Iteration 19300: Loss = -10922.089426054023
2
Iteration 19400: Loss = -10922.093237389689
3
Iteration 19500: Loss = -10922.092222452058
4
Iteration 19600: Loss = -10922.089022501483
Iteration 19700: Loss = -10922.090203655885
1
Iteration 19800: Loss = -10922.089881331143
2
Iteration 19900: Loss = -10922.090117717846
3
pi: tensor([[1.0000e+00, 1.9925e-09],
        [3.4783e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9799, 0.0201], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.5892e-01, 2.3574e-01],
         [6.2197e-01, 8.4507e-06]],

        [[5.3468e-01, 2.6458e-01],
         [5.1307e-01, 6.2671e-01]],

        [[7.1269e-01, 1.8883e-01],
         [6.5918e-01, 5.3437e-01]],

        [[5.5505e-01, 1.4727e-01],
         [6.0994e-01, 5.2203e-01]],

        [[7.1257e-01, 2.4647e-01],
         [5.1759e-01, 5.9130e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
Global Adjusted Rand Index: 0.004087827612858745
Average Adjusted Rand Index: 0.004250692246687353
10829.572161757842
[0.004087827612858745, 0.004087827612858745] [0.004250692246687353, 0.004250692246687353] [10922.088810822079, 10922.088979329823]
-------------------------------------
This iteration is 75
True Objective function: Loss = -10952.966487769521
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23369.875968252694
Iteration 100: Loss = -11049.839915697705
Iteration 200: Loss = -11047.320843503685
Iteration 300: Loss = -11046.397214502498
Iteration 400: Loss = -11045.535699054128
Iteration 500: Loss = -11045.046731100312
Iteration 600: Loss = -11044.702106416224
Iteration 700: Loss = -11044.366981437352
Iteration 800: Loss = -11044.008444240615
Iteration 900: Loss = -11043.594076915959
Iteration 1000: Loss = -11042.682063739916
Iteration 1100: Loss = -11041.73803482094
Iteration 1200: Loss = -11041.132242533879
Iteration 1300: Loss = -11039.61085334187
Iteration 1400: Loss = -11037.874595304518
Iteration 1500: Loss = -11037.262228557396
Iteration 1600: Loss = -11037.021152460773
Iteration 1700: Loss = -11036.77050583622
Iteration 1800: Loss = -11036.634605356627
Iteration 1900: Loss = -11036.566636713398
Iteration 2000: Loss = -11036.512540503165
Iteration 2100: Loss = -11036.465607340444
Iteration 2200: Loss = -11036.420316789417
Iteration 2300: Loss = -11036.368829337578
Iteration 2400: Loss = -11036.310217907878
Iteration 2500: Loss = -11036.25127694414
Iteration 2600: Loss = -11036.194226142487
Iteration 2700: Loss = -11036.145381326825
Iteration 2800: Loss = -11036.105830853425
Iteration 2900: Loss = -11036.071211937895
Iteration 3000: Loss = -11036.038582857776
Iteration 3100: Loss = -11036.009347859954
Iteration 3200: Loss = -11035.9884113001
Iteration 3300: Loss = -11035.971876837277
Iteration 3400: Loss = -11035.957210155184
Iteration 3500: Loss = -11035.943580041969
Iteration 3600: Loss = -11035.932196312897
Iteration 3700: Loss = -11035.922844159802
Iteration 3800: Loss = -11035.914608246616
Iteration 3900: Loss = -11035.90701123742
Iteration 4000: Loss = -11035.89974144149
Iteration 4100: Loss = -11035.893001653536
Iteration 4200: Loss = -11035.887103909103
Iteration 4300: Loss = -11035.881404500082
Iteration 4400: Loss = -11035.875820383571
Iteration 4500: Loss = -11035.870942266136
Iteration 4600: Loss = -11035.867032437702
Iteration 4700: Loss = -11035.863574866948
Iteration 4800: Loss = -11035.860488135722
Iteration 4900: Loss = -11035.857964286122
Iteration 5000: Loss = -11035.855878849212
Iteration 5100: Loss = -11035.854035947614
Iteration 5200: Loss = -11035.852337484263
Iteration 5300: Loss = -11035.850796480812
Iteration 5400: Loss = -11035.849451826633
Iteration 5500: Loss = -11035.848157633907
Iteration 5600: Loss = -11035.84709078855
Iteration 5700: Loss = -11035.846057363307
Iteration 5800: Loss = -11035.845094962413
Iteration 5900: Loss = -11035.844192005077
Iteration 6000: Loss = -11035.843332085784
Iteration 6100: Loss = -11035.84255296316
Iteration 6200: Loss = -11035.84182845701
Iteration 6300: Loss = -11035.841108987654
Iteration 6400: Loss = -11035.840486417761
Iteration 6500: Loss = -11035.839854435226
Iteration 6600: Loss = -11035.83929778936
Iteration 6700: Loss = -11035.838770300925
Iteration 6800: Loss = -11035.838259095577
Iteration 6900: Loss = -11035.837819213832
Iteration 7000: Loss = -11035.837444381641
Iteration 7100: Loss = -11035.837004756238
Iteration 7200: Loss = -11035.836792722132
Iteration 7300: Loss = -11035.836318031907
Iteration 7400: Loss = -11035.835992433198
Iteration 7500: Loss = -11035.835689922973
Iteration 7600: Loss = -11035.835400309206
Iteration 7700: Loss = -11035.84486794917
1
Iteration 7800: Loss = -11035.835946489626
2
Iteration 7900: Loss = -11035.849156523875
3
Iteration 8000: Loss = -11035.83440879517
Iteration 8100: Loss = -11035.836094527192
1
Iteration 8200: Loss = -11035.833988860208
Iteration 8300: Loss = -11036.262954682214
1
Iteration 8400: Loss = -11035.833620651012
Iteration 8500: Loss = -11035.833413605731
Iteration 8600: Loss = -11036.03248203771
1
Iteration 8700: Loss = -11035.833146575289
Iteration 8800: Loss = -11035.832964510657
Iteration 8900: Loss = -11035.9141293402
1
Iteration 9000: Loss = -11035.832698573831
Iteration 9100: Loss = -11035.832557319298
Iteration 9200: Loss = -11035.832477249425
Iteration 9300: Loss = -11035.832954423495
1
Iteration 9400: Loss = -11035.832253465624
Iteration 9500: Loss = -11035.832192858416
Iteration 9600: Loss = -11035.847863300034
1
Iteration 9700: Loss = -11035.831993584143
Iteration 9800: Loss = -11035.831890579213
Iteration 9900: Loss = -11035.83186481252
Iteration 10000: Loss = -11035.831799872725
Iteration 10100: Loss = -11035.831678414115
Iteration 10200: Loss = -11035.831626990663
Iteration 10300: Loss = -11035.834628535411
1
Iteration 10400: Loss = -11035.83149913662
Iteration 10500: Loss = -11035.881789206504
1
Iteration 10600: Loss = -11035.831358289019
Iteration 10700: Loss = -11035.831293042826
Iteration 10800: Loss = -11035.831427129482
1
Iteration 10900: Loss = -11035.831178409408
Iteration 11000: Loss = -11035.835893130437
1
Iteration 11100: Loss = -11035.831117898155
Iteration 11200: Loss = -11035.83108959577
Iteration 11300: Loss = -11035.83183141933
1
Iteration 11400: Loss = -11035.831009363195
Iteration 11500: Loss = -11035.830951093065
Iteration 11600: Loss = -11035.831376926837
1
Iteration 11700: Loss = -11035.830891110683
Iteration 11800: Loss = -11035.836460641167
1
Iteration 11900: Loss = -11035.830864808537
Iteration 12000: Loss = -11035.831439045473
1
Iteration 12100: Loss = -11035.831197759107
2
Iteration 12200: Loss = -11035.830781321007
Iteration 12300: Loss = -11035.840235213836
1
Iteration 12400: Loss = -11035.836670710782
2
Iteration 12500: Loss = -11035.83065783079
Iteration 12600: Loss = -11035.83208142169
1
Iteration 12700: Loss = -11035.848881608139
2
Iteration 12800: Loss = -11035.831890322259
3
Iteration 12900: Loss = -11035.831096200609
4
Iteration 13000: Loss = -11035.884405192937
5
Iteration 13100: Loss = -11035.831183528464
6
Iteration 13200: Loss = -11035.8371638247
7
Iteration 13300: Loss = -11035.830986922685
8
Iteration 13400: Loss = -11035.832928324184
9
Iteration 13500: Loss = -11035.876853547234
10
Iteration 13600: Loss = -11035.830505372647
Iteration 13700: Loss = -11035.83192363223
1
Iteration 13800: Loss = -11035.83050668309
Iteration 13900: Loss = -11035.830590351467
Iteration 14000: Loss = -11035.830469736988
Iteration 14100: Loss = -11035.830518096505
Iteration 14200: Loss = -11035.918086875048
1
Iteration 14300: Loss = -11035.830457593604
Iteration 14400: Loss = -11035.831865234584
1
Iteration 14500: Loss = -11035.83294944865
2
Iteration 14600: Loss = -11035.830962832682
3
Iteration 14700: Loss = -11035.831541413992
4
Iteration 14800: Loss = -11035.831444454969
5
Iteration 14900: Loss = -11035.835821810399
6
Iteration 15000: Loss = -11035.836979292202
7
Iteration 15100: Loss = -11035.830535026194
Iteration 15200: Loss = -11035.834236999463
1
Iteration 15300: Loss = -11035.834370340237
2
Iteration 15400: Loss = -11035.89146940365
3
Iteration 15500: Loss = -11035.846269538788
4
Iteration 15600: Loss = -11035.831498683945
5
Iteration 15700: Loss = -11035.830487386735
Iteration 15800: Loss = -11035.844621348124
1
Iteration 15900: Loss = -11035.833088620819
2
Iteration 16000: Loss = -11035.830428451023
Iteration 16100: Loss = -11035.830373390887
Iteration 16200: Loss = -11035.888026465414
1
Iteration 16300: Loss = -11035.830342698328
Iteration 16400: Loss = -11035.832431700155
1
Iteration 16500: Loss = -11035.83716858727
2
Iteration 16600: Loss = -11035.918151205378
3
Iteration 16700: Loss = -11035.830333908512
Iteration 16800: Loss = -11035.830327617607
Iteration 16900: Loss = -11035.830716593386
1
Iteration 17000: Loss = -11035.83043607216
2
Iteration 17100: Loss = -11035.830339566177
Iteration 17200: Loss = -11035.85135589206
1
Iteration 17300: Loss = -11035.832781569974
2
Iteration 17400: Loss = -11035.831646203904
3
Iteration 17500: Loss = -11035.873203920297
4
Iteration 17600: Loss = -11035.833724185706
5
Iteration 17700: Loss = -11035.830443755714
6
Iteration 17800: Loss = -11035.830698351696
7
Iteration 17900: Loss = -11036.044024915633
8
Iteration 18000: Loss = -11035.830264931164
Iteration 18100: Loss = -11035.83400775938
1
Iteration 18200: Loss = -11035.831457983435
2
Iteration 18300: Loss = -11035.83081522378
3
Iteration 18400: Loss = -11035.830372331588
4
Iteration 18500: Loss = -11035.830514117535
5
Iteration 18600: Loss = -11035.831999294074
6
Iteration 18700: Loss = -11035.848800560569
7
Iteration 18800: Loss = -11035.832086432501
8
Iteration 18900: Loss = -11035.83243009941
9
Iteration 19000: Loss = -11035.830412483121
10
Iteration 19100: Loss = -11035.844265452504
11
Iteration 19200: Loss = -11035.830776046674
12
Iteration 19300: Loss = -11035.830501814256
13
Iteration 19400: Loss = -11035.848369550951
14
Iteration 19500: Loss = -11035.83029268061
Iteration 19600: Loss = -11035.835392064964
1
Iteration 19700: Loss = -11035.830279096273
Iteration 19800: Loss = -11035.831003205834
1
Iteration 19900: Loss = -11035.831105317853
2
pi: tensor([[8.2107e-01, 1.7893e-01],
        [9.2401e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0869, 0.9131], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0895, 0.1029],
         [0.5844, 0.1642]],

        [[0.6232, 0.1287],
         [0.5501, 0.5272]],

        [[0.6078, 0.1945],
         [0.7208, 0.5877]],

        [[0.6120, 0.2816],
         [0.5777, 0.6752]],

        [[0.5235, 0.1817],
         [0.5119, 0.5305]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.020424516829035635
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.02216370547261358
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.003604415408875873
Global Adjusted Rand Index: 0.0005737982568541056
Average Adjusted Rand Index: 0.007356271104208306
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21080.645346061727
Iteration 100: Loss = -11047.615377475153
Iteration 200: Loss = -11045.19776924906
Iteration 300: Loss = -11044.470862714012
Iteration 400: Loss = -11043.866973910961
Iteration 500: Loss = -11043.440676172839
Iteration 600: Loss = -11043.217097049323
Iteration 700: Loss = -11043.038533800687
Iteration 800: Loss = -11042.928097060987
Iteration 900: Loss = -11042.854707165216
Iteration 1000: Loss = -11042.809107170353
Iteration 1100: Loss = -11042.77582957278
Iteration 1200: Loss = -11042.735018302747
Iteration 1300: Loss = -11042.60301124627
Iteration 1400: Loss = -11040.120833269386
Iteration 1500: Loss = -11036.730915906712
Iteration 1600: Loss = -11036.486535109589
Iteration 1700: Loss = -11036.348223927529
Iteration 1800: Loss = -11036.188619093924
Iteration 1900: Loss = -11036.058306605113
Iteration 2000: Loss = -11035.983883987468
Iteration 2100: Loss = -11035.941012376856
Iteration 2200: Loss = -11035.914533495135
Iteration 2300: Loss = -11035.89721834381
Iteration 2400: Loss = -11035.885319291816
Iteration 2500: Loss = -11035.87688838229
Iteration 2600: Loss = -11035.870458725996
Iteration 2700: Loss = -11035.865363329158
Iteration 2800: Loss = -11035.861214493783
Iteration 2900: Loss = -11035.857773700965
Iteration 3000: Loss = -11035.854874767943
Iteration 3100: Loss = -11035.85245081923
Iteration 3200: Loss = -11035.850364379281
Iteration 3300: Loss = -11035.848522524067
Iteration 3400: Loss = -11035.84692070205
Iteration 3500: Loss = -11035.84551140226
Iteration 3600: Loss = -11035.844236113473
Iteration 3700: Loss = -11035.84308579322
Iteration 3800: Loss = -11035.842148508027
Iteration 3900: Loss = -11035.841236880262
Iteration 4000: Loss = -11035.840446485245
Iteration 4100: Loss = -11035.83966808926
Iteration 4200: Loss = -11035.83906903522
Iteration 4300: Loss = -11035.838444486522
Iteration 4400: Loss = -11035.837874545765
Iteration 4500: Loss = -11035.83737705965
Iteration 4600: Loss = -11035.836932823642
Iteration 4700: Loss = -11035.836499336616
Iteration 4800: Loss = -11035.83614056495
Iteration 4900: Loss = -11035.835741802139
Iteration 5000: Loss = -11035.835420485026
Iteration 5100: Loss = -11035.838831717781
1
Iteration 5200: Loss = -11035.834816720586
Iteration 5300: Loss = -11035.834561183781
Iteration 5400: Loss = -11035.834288077634
Iteration 5500: Loss = -11035.834060433146
Iteration 5600: Loss = -11035.833874673457
Iteration 5700: Loss = -11035.8336553046
Iteration 5800: Loss = -11035.833543729672
Iteration 5900: Loss = -11035.83327238594
Iteration 6000: Loss = -11035.833128012462
Iteration 6100: Loss = -11035.832980710144
Iteration 6200: Loss = -11035.832826588223
Iteration 6300: Loss = -11035.832688348612
Iteration 6400: Loss = -11035.83251828892
Iteration 6500: Loss = -11035.83243451512
Iteration 6600: Loss = -11035.832335305286
Iteration 6700: Loss = -11035.832180200514
Iteration 6800: Loss = -11035.83261199021
1
Iteration 6900: Loss = -11035.83203253551
Iteration 7000: Loss = -11035.832658499769
1
Iteration 7100: Loss = -11035.831830162675
Iteration 7200: Loss = -11035.831732300574
Iteration 7300: Loss = -11035.831710274197
Iteration 7400: Loss = -11035.83157214759
Iteration 7500: Loss = -11035.83563225708
1
Iteration 7600: Loss = -11035.831468906425
Iteration 7700: Loss = -11035.831390177682
Iteration 7800: Loss = -11035.831346578114
Iteration 7900: Loss = -11035.844051410317
1
Iteration 8000: Loss = -11035.868975902858
2
Iteration 8100: Loss = -11035.842203593073
3
Iteration 8200: Loss = -11035.831148395437
Iteration 8300: Loss = -11035.831512365314
1
Iteration 8400: Loss = -11035.831061725114
Iteration 8500: Loss = -11035.833619806253
1
Iteration 8600: Loss = -11035.830970762747
Iteration 8700: Loss = -11035.830952089369
Iteration 8800: Loss = -11035.83092003456
Iteration 8900: Loss = -11035.830865893699
Iteration 9000: Loss = -11035.830831168862
Iteration 9100: Loss = -11035.831366728513
1
Iteration 9200: Loss = -11035.830796756636
Iteration 9300: Loss = -11035.830772753572
Iteration 9400: Loss = -11035.84856995678
1
Iteration 9500: Loss = -11035.830746287736
Iteration 9600: Loss = -11035.830716096476
Iteration 9700: Loss = -11035.840924937715
1
Iteration 9800: Loss = -11035.83064782927
Iteration 9900: Loss = -11035.83061882436
Iteration 10000: Loss = -11035.831641288858
1
Iteration 10100: Loss = -11035.830605256811
Iteration 10200: Loss = -11035.83064078503
Iteration 10300: Loss = -11035.830747770442
1
Iteration 10400: Loss = -11035.830532925422
Iteration 10500: Loss = -11035.83294253594
1
Iteration 10600: Loss = -11035.830522638385
Iteration 10700: Loss = -11035.830508646679
Iteration 10800: Loss = -11035.831865034332
1
Iteration 10900: Loss = -11035.83051790988
Iteration 11000: Loss = -11035.830461238209
Iteration 11100: Loss = -11035.840323489185
1
Iteration 11200: Loss = -11035.830806009993
2
Iteration 11300: Loss = -11035.831265889485
3
Iteration 11400: Loss = -11035.830593512732
4
Iteration 11500: Loss = -11035.832100460251
5
Iteration 11600: Loss = -11035.832152703804
6
Iteration 11700: Loss = -11035.920890689953
7
Iteration 11800: Loss = -11035.830416821187
Iteration 11900: Loss = -11035.832881492266
1
Iteration 12000: Loss = -11035.831347256793
2
Iteration 12100: Loss = -11035.830549927336
3
Iteration 12200: Loss = -11035.835145891007
4
Iteration 12300: Loss = -11035.83830042785
5
Iteration 12400: Loss = -11035.860917937967
6
Iteration 12500: Loss = -11035.835506736566
7
Iteration 12600: Loss = -11035.833548680373
8
Iteration 12700: Loss = -11035.838685904959
9
Iteration 12800: Loss = -11035.846375040215
10
Iteration 12900: Loss = -11035.830371250562
Iteration 13000: Loss = -11035.831408149292
1
Iteration 13100: Loss = -11035.830820378455
2
Iteration 13200: Loss = -11035.830546087667
3
Iteration 13300: Loss = -11035.830691085048
4
Iteration 13400: Loss = -11035.831324381621
5
Iteration 13500: Loss = -11035.830791849672
6
Iteration 13600: Loss = -11035.830871277421
7
Iteration 13700: Loss = -11035.83039018973
Iteration 13800: Loss = -11035.850211666077
1
Iteration 13900: Loss = -11035.841388042443
2
Iteration 14000: Loss = -11035.830582307193
3
Iteration 14100: Loss = -11035.830524043226
4
Iteration 14200: Loss = -11035.840070961533
5
Iteration 14300: Loss = -11035.8372630467
6
Iteration 14400: Loss = -11035.835084891965
7
Iteration 14500: Loss = -11035.831461718866
8
Iteration 14600: Loss = -11035.831235971957
9
Iteration 14700: Loss = -11035.91203015687
10
Iteration 14800: Loss = -11035.831095618747
11
Iteration 14900: Loss = -11035.831247326418
12
Iteration 15000: Loss = -11035.835866482996
13
Iteration 15100: Loss = -11035.830534072757
14
Iteration 15200: Loss = -11035.83135097621
15
Stopping early at iteration 15200 due to no improvement.
pi: tensor([[8.2133e-01, 1.7867e-01],
        [2.3324e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0868, 0.9132], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0894, 0.1029],
         [0.5568, 0.1642]],

        [[0.6145, 0.1288],
         [0.6144, 0.6161]],

        [[0.5525, 0.1943],
         [0.5309, 0.5056]],

        [[0.6284, 0.2816],
         [0.7084, 0.5158]],

        [[0.6309, 0.1817],
         [0.6307, 0.6801]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.020424516829035635
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.02216370547261358
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0006803382252891437
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0015221131464426677
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.003604415408875873
Global Adjusted Rand Index: 0.0005737982568541056
Average Adjusted Rand Index: 0.007356271104208306
10952.966487769521
[0.0005737982568541056, 0.0005737982568541056] [0.007356271104208306, 0.007356271104208306] [11035.83121630972, 11035.83135097621]
-------------------------------------
This iteration is 76
True Objective function: Loss = -10905.157832448027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22531.56002904112
Iteration 100: Loss = -11013.367073525924
Iteration 200: Loss = -11012.04225010298
Iteration 300: Loss = -11008.936219850979
Iteration 400: Loss = -11007.406505677958
Iteration 500: Loss = -11006.88218562967
Iteration 600: Loss = -11006.35057170597
Iteration 700: Loss = -11005.818640960742
Iteration 800: Loss = -11005.39491737524
Iteration 900: Loss = -11005.11719376924
Iteration 1000: Loss = -11004.938730326743
Iteration 1100: Loss = -11004.804049435945
Iteration 1200: Loss = -11004.684445828685
Iteration 1300: Loss = -11004.57040395728
Iteration 1400: Loss = -11004.463740360943
Iteration 1500: Loss = -11004.368109232926
Iteration 1600: Loss = -11004.287682903956
Iteration 1700: Loss = -11004.22465507352
Iteration 1800: Loss = -11004.176902775718
Iteration 1900: Loss = -11004.14119927996
Iteration 2000: Loss = -11004.1152519254
Iteration 2100: Loss = -11004.09715754912
Iteration 2200: Loss = -11004.085517235897
Iteration 2300: Loss = -11004.077260950207
Iteration 2400: Loss = -11004.071382167704
Iteration 2500: Loss = -11004.067329704154
Iteration 2600: Loss = -11004.064363164362
Iteration 2700: Loss = -11004.062061738161
Iteration 2800: Loss = -11004.059944475795
Iteration 2900: Loss = -11004.058056610245
Iteration 3000: Loss = -11004.05619847085
Iteration 3100: Loss = -11004.05443805146
Iteration 3200: Loss = -11004.052671616793
Iteration 3300: Loss = -11004.050791403088
Iteration 3400: Loss = -11004.048945744289
Iteration 3500: Loss = -11004.049599693337
1
Iteration 3600: Loss = -11004.045093027293
Iteration 3700: Loss = -11004.043016429214
Iteration 3800: Loss = -11004.040965698849
Iteration 3900: Loss = -11004.038446611125
Iteration 4000: Loss = -11004.035863603693
Iteration 4100: Loss = -11004.033033725202
Iteration 4200: Loss = -11004.029721081368
Iteration 4300: Loss = -11004.026491495573
Iteration 4400: Loss = -11004.020365800638
Iteration 4500: Loss = -11004.012029502597
Iteration 4600: Loss = -11003.993290400347
Iteration 4700: Loss = -11003.288749322135
Iteration 4800: Loss = -10912.284861439914
Iteration 4900: Loss = -10912.050304312921
Iteration 5000: Loss = -10911.893169276924
Iteration 5100: Loss = -10911.84633540221
Iteration 5200: Loss = -10911.834114311887
Iteration 5300: Loss = -10911.81910655934
Iteration 5400: Loss = -10911.809110211483
Iteration 5500: Loss = -10911.798970399697
Iteration 5600: Loss = -10911.7949869708
Iteration 5700: Loss = -10911.793184399143
Iteration 5800: Loss = -10911.796052593114
1
Iteration 5900: Loss = -10911.788034523383
Iteration 6000: Loss = -10911.78090174404
Iteration 6100: Loss = -10911.777392447437
Iteration 6200: Loss = -10911.769455177988
Iteration 6300: Loss = -10911.765040411485
Iteration 6400: Loss = -10911.763077941088
Iteration 6500: Loss = -10911.759446026148
Iteration 6600: Loss = -10911.756725769996
Iteration 6700: Loss = -10911.749489731435
Iteration 6800: Loss = -10911.748867435243
Iteration 6900: Loss = -10911.752674771828
1
Iteration 7000: Loss = -10911.745987849925
Iteration 7100: Loss = -10911.742477238688
Iteration 7200: Loss = -10911.732680213989
Iteration 7300: Loss = -10911.731877874483
Iteration 7400: Loss = -10911.744763166485
1
Iteration 7500: Loss = -10911.730392239317
Iteration 7600: Loss = -10911.728224367007
Iteration 7700: Loss = -10911.72760063628
Iteration 7800: Loss = -10911.716934383303
Iteration 7900: Loss = -10911.717107784198
1
Iteration 8000: Loss = -10911.716390142634
Iteration 8100: Loss = -10911.716250666006
Iteration 8200: Loss = -10911.730816971904
1
Iteration 8300: Loss = -10911.716052491814
Iteration 8400: Loss = -10911.716203975253
1
Iteration 8500: Loss = -10911.71580164333
Iteration 8600: Loss = -10911.715589642416
Iteration 8700: Loss = -10911.715122037338
Iteration 8800: Loss = -10911.719386989545
1
Iteration 8900: Loss = -10911.71503380325
Iteration 9000: Loss = -10911.71512414191
Iteration 9100: Loss = -10911.752234324757
1
Iteration 9200: Loss = -10911.71471487655
Iteration 9300: Loss = -10911.715693048582
1
Iteration 9400: Loss = -10911.71627571112
2
Iteration 9500: Loss = -10911.71372597488
Iteration 9600: Loss = -10911.719507427257
1
Iteration 9700: Loss = -10911.713093754439
Iteration 9800: Loss = -10911.712927846289
Iteration 9900: Loss = -10911.712855532764
Iteration 10000: Loss = -10911.713061751052
1
Iteration 10100: Loss = -10911.712757981144
Iteration 10200: Loss = -10911.713538506077
1
Iteration 10300: Loss = -10911.712649190762
Iteration 10400: Loss = -10911.719105871724
1
Iteration 10500: Loss = -10911.71520050484
2
Iteration 10600: Loss = -10911.712320196879
Iteration 10700: Loss = -10911.730834165794
1
Iteration 10800: Loss = -10911.711401570934
Iteration 10900: Loss = -10911.774864144843
1
Iteration 11000: Loss = -10911.711270445243
Iteration 11100: Loss = -10911.725868900136
1
Iteration 11200: Loss = -10911.711239241988
Iteration 11300: Loss = -10911.711246444314
Iteration 11400: Loss = -10911.711472689118
1
Iteration 11500: Loss = -10911.71211292173
2
Iteration 11600: Loss = -10911.711272926914
Iteration 11700: Loss = -10911.716586960676
1
Iteration 11800: Loss = -10911.721115121592
2
Iteration 11900: Loss = -10911.711227873384
Iteration 12000: Loss = -10911.72020249212
1
Iteration 12100: Loss = -10911.711187519773
Iteration 12200: Loss = -10911.71174574811
1
Iteration 12300: Loss = -10911.711175767392
Iteration 12400: Loss = -10911.711868599388
1
Iteration 12500: Loss = -10911.711177740266
Iteration 12600: Loss = -10911.711111030343
Iteration 12700: Loss = -10911.722868762557
1
Iteration 12800: Loss = -10911.710923215192
Iteration 12900: Loss = -10911.710926247051
Iteration 13000: Loss = -10911.710938479939
Iteration 13100: Loss = -10911.711342351726
1
Iteration 13200: Loss = -10911.742666054946
2
Iteration 13300: Loss = -10911.71090826787
Iteration 13400: Loss = -10911.710974320244
Iteration 13500: Loss = -10911.71096951576
Iteration 13600: Loss = -10911.75306621377
1
Iteration 13700: Loss = -10911.710956040119
Iteration 13800: Loss = -10911.720765394784
1
Iteration 13900: Loss = -10911.71086783488
Iteration 14000: Loss = -10911.710890542723
Iteration 14100: Loss = -10911.738319119646
1
Iteration 14200: Loss = -10911.710907858946
Iteration 14300: Loss = -10911.71835540386
1
Iteration 14400: Loss = -10911.710874748893
Iteration 14500: Loss = -10911.71086484197
Iteration 14600: Loss = -10911.716327186567
1
Iteration 14700: Loss = -10911.710757547547
Iteration 14800: Loss = -10911.71176642214
1
Iteration 14900: Loss = -10911.710752215455
Iteration 15000: Loss = -10911.711243726984
1
Iteration 15100: Loss = -10911.71098534453
2
Iteration 15200: Loss = -10911.715184745803
3
Iteration 15300: Loss = -10911.710727388781
Iteration 15400: Loss = -10911.711276401253
1
Iteration 15500: Loss = -10911.713814185281
2
Iteration 15600: Loss = -10911.713793160057
3
Iteration 15700: Loss = -10911.710935870156
4
Iteration 15800: Loss = -10911.711381851486
5
Iteration 15900: Loss = -10911.71592558917
6
Iteration 16000: Loss = -10911.710763406012
Iteration 16100: Loss = -10911.711063556884
1
Iteration 16200: Loss = -10911.710909342402
2
Iteration 16300: Loss = -10911.736914716568
3
Iteration 16400: Loss = -10911.713613456282
4
Iteration 16500: Loss = -10911.710773214261
Iteration 16600: Loss = -10911.7109781094
1
Iteration 16700: Loss = -10911.710706982793
Iteration 16800: Loss = -10911.711065642692
1
Iteration 16900: Loss = -10911.710725352217
Iteration 17000: Loss = -10911.711334961354
1
Iteration 17100: Loss = -10911.710727837386
Iteration 17200: Loss = -10911.720820305987
1
Iteration 17300: Loss = -10911.710723867187
Iteration 17400: Loss = -10911.713735241055
1
Iteration 17500: Loss = -10911.710736249204
Iteration 17600: Loss = -10911.716563150318
1
Iteration 17700: Loss = -10911.710744900267
Iteration 17800: Loss = -10911.710935665033
1
Iteration 17900: Loss = -10911.710775476826
Iteration 18000: Loss = -10911.71078031437
Iteration 18100: Loss = -10911.710727571339
Iteration 18200: Loss = -10911.711395628055
1
Iteration 18300: Loss = -10911.710728781827
Iteration 18400: Loss = -10911.727066864372
1
Iteration 18500: Loss = -10911.710739678765
Iteration 18600: Loss = -10911.710981062228
1
Iteration 18700: Loss = -10911.710786264886
Iteration 18800: Loss = -10911.71247841886
1
Iteration 18900: Loss = -10911.721254315167
2
Iteration 19000: Loss = -10911.71118738384
3
Iteration 19100: Loss = -10911.713948717557
4
Iteration 19200: Loss = -10911.710916690356
5
Iteration 19300: Loss = -10911.711557820714
6
Iteration 19400: Loss = -10911.710843670766
Iteration 19500: Loss = -10911.711017693586
1
Iteration 19600: Loss = -10911.710887475616
Iteration 19700: Loss = -10911.742876094007
1
Iteration 19800: Loss = -10911.717107873174
2
Iteration 19900: Loss = -10912.061265633058
3
pi: tensor([[0.6481, 0.3519],
        [0.2180, 0.7820]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9725, 0.0275], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1717, 0.2776],
         [0.5750, 0.2534]],

        [[0.5938, 0.1208],
         [0.5112, 0.5479]],

        [[0.5276, 0.0994],
         [0.6276, 0.6011]],

        [[0.6242, 0.0959],
         [0.6060, 0.5266]],

        [[0.6914, 0.0978],
         [0.5496, 0.6328]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.573395733075917
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7720905172413793
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844845420066659
Global Adjusted Rand Index: 0.4666152615344382
Average Adjusted Rand Index: 0.6129971268157396
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24989.064124818207
Iteration 100: Loss = -11012.74526489528
Iteration 200: Loss = -11008.14273286584
Iteration 300: Loss = -11006.84913140983
Iteration 400: Loss = -11006.110973792815
Iteration 500: Loss = -11005.570495527709
Iteration 600: Loss = -11005.255266319795
Iteration 700: Loss = -11005.061868270208
Iteration 800: Loss = -11004.912842520096
Iteration 900: Loss = -11004.774391077935
Iteration 1000: Loss = -11004.63847659374
Iteration 1100: Loss = -11004.505564288409
Iteration 1200: Loss = -11004.3818660813
Iteration 1300: Loss = -11004.276193059271
Iteration 1400: Loss = -11004.195021858268
Iteration 1500: Loss = -11004.13888612708
Iteration 1600: Loss = -11004.10368925953
Iteration 1700: Loss = -11004.083047185088
Iteration 1800: Loss = -11004.071651134058
Iteration 1900: Loss = -11004.065752619244
Iteration 2000: Loss = -11004.062389022465
Iteration 2100: Loss = -11004.059960844845
Iteration 2200: Loss = -11004.057868578007
Iteration 2300: Loss = -11004.055901151192
Iteration 2400: Loss = -11004.05394854285
Iteration 2500: Loss = -11004.051946078795
Iteration 2600: Loss = -11004.049917586486
Iteration 2700: Loss = -11004.047707952965
Iteration 2800: Loss = -11004.04540084938
Iteration 2900: Loss = -11004.042927493327
Iteration 3000: Loss = -11004.040235927036
Iteration 3100: Loss = -11004.037252552955
Iteration 3200: Loss = -11004.035443166162
Iteration 3300: Loss = -11004.030042787346
Iteration 3400: Loss = -11004.025240447669
Iteration 3500: Loss = -11004.018634736985
Iteration 3600: Loss = -11004.006852587965
Iteration 3700: Loss = -11003.964222056731
Iteration 3800: Loss = -10913.723299230747
Iteration 3900: Loss = -10912.049907604944
Iteration 4000: Loss = -10911.918296991933
Iteration 4100: Loss = -10911.856851726658
Iteration 4200: Loss = -10911.83485201405
Iteration 4300: Loss = -10911.829443582721
Iteration 4400: Loss = -10911.802964622917
Iteration 4500: Loss = -10911.79951453035
Iteration 4600: Loss = -10911.793655317639
Iteration 4700: Loss = -10911.783855824642
Iteration 4800: Loss = -10911.77702111649
Iteration 4900: Loss = -10911.772308728327
Iteration 5000: Loss = -10911.762840634874
Iteration 5100: Loss = -10911.761268055265
Iteration 5200: Loss = -10911.758529393712
Iteration 5300: Loss = -10911.754073451895
Iteration 5400: Loss = -10911.753340107269
Iteration 5500: Loss = -10911.795350552098
1
Iteration 5600: Loss = -10911.748319920534
Iteration 5700: Loss = -10911.741750808756
Iteration 5800: Loss = -10911.744886742845
1
Iteration 5900: Loss = -10911.740488380858
Iteration 6000: Loss = -10911.740070097134
Iteration 6100: Loss = -10911.739693935146
Iteration 6200: Loss = -10911.739423232453
Iteration 6300: Loss = -10911.743515595223
1
Iteration 6400: Loss = -10911.738388369331
Iteration 6500: Loss = -10911.730971452544
Iteration 6600: Loss = -10911.716112503043
Iteration 6700: Loss = -10911.715317376626
Iteration 6800: Loss = -10911.746517448464
1
Iteration 6900: Loss = -10911.715081265116
Iteration 7000: Loss = -10911.714998923164
Iteration 7100: Loss = -10911.714926727595
Iteration 7200: Loss = -10911.714784651582
Iteration 7300: Loss = -10911.71640955012
1
Iteration 7400: Loss = -10911.714533459524
Iteration 7500: Loss = -10911.714477907684
Iteration 7600: Loss = -10911.714515750975
Iteration 7700: Loss = -10911.715272499443
1
Iteration 7800: Loss = -10911.714503618716
Iteration 7900: Loss = -10911.714355565437
Iteration 8000: Loss = -10911.715051060452
1
Iteration 8100: Loss = -10911.714206585451
Iteration 8200: Loss = -10911.71404587353
Iteration 8300: Loss = -10911.721438363416
1
Iteration 8400: Loss = -10911.713298066396
Iteration 8500: Loss = -10911.717980718317
1
Iteration 8600: Loss = -10911.713435478021
2
Iteration 8700: Loss = -10911.713163512673
Iteration 8800: Loss = -10911.86913250653
1
Iteration 8900: Loss = -10911.712898720096
Iteration 9000: Loss = -10911.712650672054
Iteration 9100: Loss = -10911.712612892554
Iteration 9200: Loss = -10911.71255526946
Iteration 9300: Loss = -10911.712563101948
Iteration 9400: Loss = -10911.712479386864
Iteration 9500: Loss = -10911.716939935795
1
Iteration 9600: Loss = -10911.711848525847
Iteration 9700: Loss = -10911.718179967698
1
Iteration 9800: Loss = -10911.711320796463
Iteration 9900: Loss = -10911.715324020535
1
Iteration 10000: Loss = -10911.711063533017
Iteration 10100: Loss = -10911.711365843521
1
Iteration 10200: Loss = -10911.711012356085
Iteration 10300: Loss = -10911.798523232112
1
Iteration 10400: Loss = -10911.710960504526
Iteration 10500: Loss = -10911.71874693011
1
Iteration 10600: Loss = -10911.710937144138
Iteration 10700: Loss = -10911.71148486037
1
Iteration 10800: Loss = -10911.988677912344
2
Iteration 10900: Loss = -10911.710917374558
Iteration 11000: Loss = -10911.729158347065
1
Iteration 11100: Loss = -10911.894546440193
2
Iteration 11200: Loss = -10911.71112834739
3
Iteration 11300: Loss = -10911.711205249285
4
Iteration 11400: Loss = -10911.711662997996
5
Iteration 11500: Loss = -10911.719230680026
6
Iteration 11600: Loss = -10911.710937201753
Iteration 11700: Loss = -10911.944103326152
1
Iteration 11800: Loss = -10911.710870836943
Iteration 11900: Loss = -10911.729014817247
1
Iteration 12000: Loss = -10911.716916508505
2
Iteration 12100: Loss = -10911.71818170373
3
Iteration 12200: Loss = -10911.712000737796
4
Iteration 12300: Loss = -10911.712112157864
5
Iteration 12400: Loss = -10911.841594016432
6
Iteration 12500: Loss = -10911.710738575428
Iteration 12600: Loss = -10911.711193341303
1
Iteration 12700: Loss = -10911.996601338802
2
Iteration 12800: Loss = -10911.710735064818
Iteration 12900: Loss = -10911.824490121317
1
Iteration 13000: Loss = -10911.710742802017
Iteration 13100: Loss = -10911.802033021164
1
Iteration 13200: Loss = -10911.710711751502
Iteration 13300: Loss = -10911.712675061437
1
Iteration 13400: Loss = -10911.710738427457
Iteration 13500: Loss = -10911.710756731525
Iteration 13600: Loss = -10911.711518688446
1
Iteration 13700: Loss = -10911.710718701193
Iteration 13800: Loss = -10911.72887951308
1
Iteration 13900: Loss = -10911.71071813936
Iteration 14000: Loss = -10911.712172029813
1
Iteration 14100: Loss = -10911.71074861815
Iteration 14200: Loss = -10911.71074793784
Iteration 14300: Loss = -10911.712826626797
1
Iteration 14400: Loss = -10911.71079015296
Iteration 14500: Loss = -10911.712373767921
1
Iteration 14600: Loss = -10911.710772390627
Iteration 14700: Loss = -10911.71103428598
1
Iteration 14800: Loss = -10911.776129990063
2
Iteration 14900: Loss = -10911.71093464288
3
Iteration 15000: Loss = -10911.731647751321
4
Iteration 15100: Loss = -10911.710762806495
Iteration 15200: Loss = -10911.710784947161
Iteration 15300: Loss = -10911.71079142007
Iteration 15400: Loss = -10911.711010957157
1
Iteration 15500: Loss = -10911.710829843134
Iteration 15600: Loss = -10911.715105262025
1
Iteration 15700: Loss = -10911.71667703092
2
Iteration 15800: Loss = -10911.714525839681
3
Iteration 15900: Loss = -10911.710928174716
Iteration 16000: Loss = -10911.710824613952
Iteration 16100: Loss = -10911.7219423715
1
Iteration 16200: Loss = -10911.711019332693
2
Iteration 16300: Loss = -10911.710988654797
3
Iteration 16400: Loss = -10911.72194814965
4
Iteration 16500: Loss = -10911.712156609112
5
Iteration 16600: Loss = -10911.720124619993
6
Iteration 16700: Loss = -10911.711085371036
7
Iteration 16800: Loss = -10911.727227274663
8
Iteration 16900: Loss = -10911.710817371333
Iteration 17000: Loss = -10911.713752279056
1
Iteration 17100: Loss = -10911.71105171099
2
Iteration 17200: Loss = -10911.720065622823
3
Iteration 17300: Loss = -10911.710966324747
4
Iteration 17400: Loss = -10911.720250143935
5
Iteration 17500: Loss = -10911.710942310856
6
Iteration 17600: Loss = -10912.082431289671
7
Iteration 17700: Loss = -10911.710765101869
Iteration 17800: Loss = -10911.710729698056
Iteration 17900: Loss = -10911.710903592055
1
Iteration 18000: Loss = -10911.710720576371
Iteration 18100: Loss = -10911.713644629976
1
Iteration 18200: Loss = -10911.710714749343
Iteration 18300: Loss = -10911.711423597533
1
Iteration 18400: Loss = -10911.712886639794
2
Iteration 18500: Loss = -10911.77280802907
3
Iteration 18600: Loss = -10911.719193796414
4
Iteration 18700: Loss = -10911.710798851042
Iteration 18800: Loss = -10911.710890840794
Iteration 18900: Loss = -10911.797228053465
1
Iteration 19000: Loss = -10911.710762702494
Iteration 19100: Loss = -10911.710784474613
Iteration 19200: Loss = -10911.719749019867
1
Iteration 19300: Loss = -10911.711557296134
2
Iteration 19400: Loss = -10911.710768577732
Iteration 19500: Loss = -10911.711945836432
1
Iteration 19600: Loss = -10911.711823879261
2
Iteration 19700: Loss = -10911.712637180344
3
Iteration 19800: Loss = -10911.711517745438
4
Iteration 19900: Loss = -10911.719904757449
5
pi: tensor([[0.7849, 0.2151],
        [0.3516, 0.6484]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0275, 0.9725], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2533, 0.2774],
         [0.5996, 0.1717]],

        [[0.5779, 0.1207],
         [0.7120, 0.5297]],

        [[0.6581, 0.0995],
         [0.5535, 0.5412]],

        [[0.7251, 0.0960],
         [0.7128, 0.6267]],

        [[0.5324, 0.0977],
         [0.5088, 0.6751]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.573395733075917
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7720905172413793
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.844845420066659
Global Adjusted Rand Index: 0.4666152615344382
Average Adjusted Rand Index: 0.6129971268157396
10905.157832448027
[0.4666152615344382, 0.4666152615344382] [0.6129971268157396, 0.6129971268157396] [10911.71319154271, 10911.71104438917]
-------------------------------------
This iteration is 77
True Objective function: Loss = -10950.91155398354
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20938.338168458537
Iteration 100: Loss = -11007.469407100172
Iteration 200: Loss = -11006.865436955875
Iteration 300: Loss = -11006.6505459119
Iteration 400: Loss = -11006.273691547667
Iteration 500: Loss = -11005.184881763113
Iteration 600: Loss = -11004.27356336205
Iteration 700: Loss = -11003.153934900625
Iteration 800: Loss = -11002.230568204188
Iteration 900: Loss = -11001.68557519407
Iteration 1000: Loss = -11001.408096869154
Iteration 1100: Loss = -11001.26380844182
Iteration 1200: Loss = -11001.184759864636
Iteration 1300: Loss = -11001.139435437204
Iteration 1400: Loss = -11001.107375370839
Iteration 1500: Loss = -11001.082512281433
Iteration 1600: Loss = -11001.062469101193
Iteration 1700: Loss = -11001.046485976238
Iteration 1800: Loss = -11001.033808759023
Iteration 1900: Loss = -11001.02386288015
Iteration 2000: Loss = -11001.016239867318
Iteration 2100: Loss = -11001.010540419931
Iteration 2200: Loss = -11001.006297128808
Iteration 2300: Loss = -11001.002854942999
Iteration 2400: Loss = -11001.000075355205
Iteration 2500: Loss = -11000.997664628654
Iteration 2600: Loss = -11000.99564079114
Iteration 2700: Loss = -11000.99399213923
Iteration 2800: Loss = -11000.992624566145
Iteration 2900: Loss = -11000.991484133701
Iteration 3000: Loss = -11000.990606941386
Iteration 3100: Loss = -11000.989883567761
Iteration 3200: Loss = -11000.989323725762
Iteration 3300: Loss = -11000.98889762956
Iteration 3400: Loss = -11000.988515314364
Iteration 3500: Loss = -11000.988196655433
Iteration 3600: Loss = -11000.987904906011
Iteration 3700: Loss = -11000.987692672834
Iteration 3800: Loss = -11000.987456242754
Iteration 3900: Loss = -11000.987271812093
Iteration 4000: Loss = -11000.987170351946
Iteration 4100: Loss = -11000.98701810157
Iteration 4200: Loss = -11000.986923458202
Iteration 4300: Loss = -11000.986809618587
Iteration 4400: Loss = -11000.986739147414
Iteration 4500: Loss = -11000.986687392742
Iteration 4600: Loss = -11000.986639659315
Iteration 4700: Loss = -11000.986592795774
Iteration 4800: Loss = -11000.98664530399
Iteration 4900: Loss = -11000.98654986349
Iteration 5000: Loss = -11000.986480899333
Iteration 5100: Loss = -11000.98651204531
Iteration 5200: Loss = -11000.986456467506
Iteration 5300: Loss = -11000.987058947401
1
Iteration 5400: Loss = -11000.986461434924
Iteration 5500: Loss = -11000.986466351273
Iteration 5600: Loss = -11000.986461953478
Iteration 5700: Loss = -11000.986408726894
Iteration 5800: Loss = -11000.986403442774
Iteration 5900: Loss = -11000.986411330345
Iteration 6000: Loss = -11000.987167577621
1
Iteration 6100: Loss = -11000.986410682402
Iteration 6200: Loss = -11000.992695996805
1
Iteration 6300: Loss = -11000.986370493714
Iteration 6400: Loss = -11000.986361879226
Iteration 6500: Loss = -11000.98638161227
Iteration 6600: Loss = -11000.98637650545
Iteration 6700: Loss = -11000.986875840468
1
Iteration 6800: Loss = -11000.986394400376
Iteration 6900: Loss = -11000.987487865752
1
Iteration 7000: Loss = -11000.98636250755
Iteration 7100: Loss = -11000.98768670548
1
Iteration 7200: Loss = -11000.986351692045
Iteration 7300: Loss = -11000.987153808934
1
Iteration 7400: Loss = -11000.986357551226
Iteration 7500: Loss = -11000.99219827979
1
Iteration 7600: Loss = -11000.986365985496
Iteration 7700: Loss = -11000.987349931333
1
Iteration 7800: Loss = -11000.989873018802
2
Iteration 7900: Loss = -11000.986388244073
Iteration 8000: Loss = -11001.02031369461
1
Iteration 8100: Loss = -11000.986378332536
Iteration 8200: Loss = -11000.989908715119
1
Iteration 8300: Loss = -11000.986369326258
Iteration 8400: Loss = -11000.986332506454
Iteration 8500: Loss = -11000.987057297683
1
Iteration 8600: Loss = -11000.986362611971
Iteration 8700: Loss = -11000.986371915289
Iteration 8800: Loss = -11000.986482657483
1
Iteration 8900: Loss = -11000.986392918434
Iteration 9000: Loss = -11000.986365501105
Iteration 9100: Loss = -11000.98672884923
1
Iteration 9200: Loss = -11000.98635745906
Iteration 9300: Loss = -11000.988844305097
1
Iteration 9400: Loss = -11000.986377238705
Iteration 9500: Loss = -11000.986346435617
Iteration 9600: Loss = -11001.171208059657
1
Iteration 9700: Loss = -11000.9863434328
Iteration 9800: Loss = -11000.986355387133
Iteration 9900: Loss = -11000.986432169782
Iteration 10000: Loss = -11000.986446174731
Iteration 10100: Loss = -11000.98637157457
Iteration 10200: Loss = -11000.986301194323
Iteration 10300: Loss = -11000.98657647684
1
Iteration 10400: Loss = -11000.986360857225
Iteration 10500: Loss = -11001.006370236566
1
Iteration 10600: Loss = -11000.986325341566
Iteration 10700: Loss = -11000.988073423472
1
Iteration 10800: Loss = -11000.986347293936
Iteration 10900: Loss = -11001.069548768875
1
Iteration 11000: Loss = -11000.986337269173
Iteration 11100: Loss = -11000.986362860396
Iteration 11200: Loss = -11000.986507103822
1
Iteration 11300: Loss = -11000.986358618802
Iteration 11400: Loss = -11001.000398708145
1
Iteration 11500: Loss = -11000.98636649918
Iteration 11600: Loss = -11000.986364609516
Iteration 11700: Loss = -11000.98671222437
1
Iteration 11800: Loss = -11000.986360430248
Iteration 11900: Loss = -11001.125513614725
1
Iteration 12000: Loss = -11000.986369765016
Iteration 12100: Loss = -11000.986347669052
Iteration 12200: Loss = -11000.9871803504
1
Iteration 12300: Loss = -11000.98633663254
Iteration 12400: Loss = -11001.484652785748
1
Iteration 12500: Loss = -11000.98637205286
Iteration 12600: Loss = -11000.98633320752
Iteration 12700: Loss = -11001.035493846624
1
Iteration 12800: Loss = -11000.986366766068
Iteration 12900: Loss = -11000.986358591339
Iteration 13000: Loss = -11000.987563690973
1
Iteration 13100: Loss = -11000.986346034113
Iteration 13200: Loss = -11001.00753583799
1
Iteration 13300: Loss = -11000.986333387285
Iteration 13400: Loss = -11001.064837360158
1
Iteration 13500: Loss = -11000.98639448018
Iteration 13600: Loss = -11000.986334239988
Iteration 13700: Loss = -11000.986506080748
1
Iteration 13800: Loss = -11000.986323621193
Iteration 13900: Loss = -11001.168032226637
1
Iteration 14000: Loss = -11000.98632135091
Iteration 14100: Loss = -11000.986359389399
Iteration 14200: Loss = -11000.996441794017
1
Iteration 14300: Loss = -11000.986348781222
Iteration 14400: Loss = -11001.076974085465
1
Iteration 14500: Loss = -11000.996991008727
2
Iteration 14600: Loss = -11001.004481282882
3
Iteration 14700: Loss = -11000.9867231303
4
Iteration 14800: Loss = -11000.986507543732
5
Iteration 14900: Loss = -11000.987414222875
6
Iteration 15000: Loss = -11000.986444997412
Iteration 15100: Loss = -11001.013298243071
1
Iteration 15200: Loss = -11000.98776673859
2
Iteration 15300: Loss = -11000.986644512357
3
Iteration 15400: Loss = -11001.096632185332
4
Iteration 15500: Loss = -11000.986386960818
Iteration 15600: Loss = -11000.988916053662
1
Iteration 15700: Loss = -11000.986497199863
2
Iteration 15800: Loss = -11000.986493038205
3
Iteration 15900: Loss = -11000.987504687459
4
Iteration 16000: Loss = -11000.992533254148
5
Iteration 16100: Loss = -11000.99339638475
6
Iteration 16200: Loss = -11001.01040061922
7
Iteration 16300: Loss = -11000.98637452533
Iteration 16400: Loss = -11001.089775837323
1
Iteration 16500: Loss = -11000.986388845711
Iteration 16600: Loss = -11000.994475990276
1
Iteration 16700: Loss = -11001.048746430737
2
Iteration 16800: Loss = -11001.005942000958
3
Iteration 16900: Loss = -11000.986639425646
4
Iteration 17000: Loss = -11000.986406793314
Iteration 17100: Loss = -11000.988069610883
1
Iteration 17200: Loss = -11000.987385255785
2
Iteration 17300: Loss = -11001.195255020637
3
Iteration 17400: Loss = -11000.990309830866
4
Iteration 17500: Loss = -11000.986440397764
Iteration 17600: Loss = -11001.009125730767
1
Iteration 17700: Loss = -11000.987060486077
2
Iteration 17800: Loss = -11000.986371238616
Iteration 17900: Loss = -11000.986731939707
1
Iteration 18000: Loss = -11001.004271656464
2
Iteration 18100: Loss = -11000.993820798798
3
Iteration 18200: Loss = -11000.986415863426
Iteration 18300: Loss = -11000.987124811012
1
Iteration 18400: Loss = -11001.007962239742
2
Iteration 18500: Loss = -11001.00722013628
3
Iteration 18600: Loss = -11000.986704698616
4
Iteration 18700: Loss = -11000.989488123114
5
Iteration 18800: Loss = -11000.995680560667
6
Iteration 18900: Loss = -11000.986391831255
Iteration 19000: Loss = -11000.986666092595
1
Iteration 19100: Loss = -11000.989720213312
2
Iteration 19200: Loss = -11000.986530860606
3
Iteration 19300: Loss = -11000.986472391185
Iteration 19400: Loss = -11000.988961032175
1
Iteration 19500: Loss = -11000.9863694242
Iteration 19600: Loss = -11000.986347782316
Iteration 19700: Loss = -11001.008810761343
1
Iteration 19800: Loss = -11000.986365959274
Iteration 19900: Loss = -11000.986344876646
pi: tensor([[0.8362, 0.1638],
        [0.0437, 0.9563]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0078, 0.9922], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2301, 0.1609],
         [0.5903, 0.1559]],

        [[0.5087, 0.2411],
         [0.6627, 0.5780]],

        [[0.6557, 0.2105],
         [0.6487, 0.5456]],

        [[0.6612, 0.1751],
         [0.6134, 0.6669]],

        [[0.7293, 0.2128],
         [0.6627, 0.6011]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0013070675007002147
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0015169269193791859
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0006438680677883739
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.004682108332720131
Global Adjusted Rand Index: -5.070534640350624e-05
Average Adjusted Rand Index: -0.0010232233963659069
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22182.29617313331
Iteration 100: Loss = -11008.76565897223
Iteration 200: Loss = -11007.64159695559
Iteration 300: Loss = -11007.327626205997
Iteration 400: Loss = -11007.138949823744
Iteration 500: Loss = -11006.967701636453
Iteration 600: Loss = -11006.803439656154
Iteration 700: Loss = -11006.668873103737
Iteration 800: Loss = -11006.547826684055
Iteration 900: Loss = -11006.367689216882
Iteration 1000: Loss = -11005.484573313237
Iteration 1100: Loss = -11003.415631047974
Iteration 1200: Loss = -11002.371085842591
Iteration 1300: Loss = -11001.856007370789
Iteration 1400: Loss = -11001.500928101726
Iteration 1500: Loss = -11001.31618861209
Iteration 1600: Loss = -11001.219175903118
Iteration 1700: Loss = -11001.161710204704
Iteration 1800: Loss = -11001.124697478625
Iteration 1900: Loss = -11001.098220808899
Iteration 2000: Loss = -11001.078562349478
Iteration 2100: Loss = -11001.063540387753
Iteration 2200: Loss = -11001.051342209767
Iteration 2300: Loss = -11001.041189443667
Iteration 2400: Loss = -11001.032501634487
Iteration 2500: Loss = -11001.025466884374
Iteration 2600: Loss = -11001.019776098288
Iteration 2700: Loss = -11001.015264651154
Iteration 2800: Loss = -11001.0116548459
Iteration 2900: Loss = -11001.008796995546
Iteration 3000: Loss = -11001.00635254117
Iteration 3100: Loss = -11001.004318433483
Iteration 3200: Loss = -11001.00263150128
Iteration 3300: Loss = -11001.001149796231
Iteration 3400: Loss = -11000.99992013287
Iteration 3500: Loss = -11000.998817956535
Iteration 3600: Loss = -11000.997815845853
Iteration 3700: Loss = -11000.996856054582
Iteration 3800: Loss = -11000.995956447088
Iteration 3900: Loss = -11000.995040682903
Iteration 4000: Loss = -11000.994121002357
Iteration 4100: Loss = -11000.993227757825
Iteration 4200: Loss = -11000.99234086121
Iteration 4300: Loss = -11000.99145766357
Iteration 4400: Loss = -11000.9905963154
Iteration 4500: Loss = -11000.989782054749
Iteration 4600: Loss = -11000.989191839939
Iteration 4700: Loss = -11000.988702498524
Iteration 4800: Loss = -11000.98835349753
Iteration 4900: Loss = -11000.987998220828
Iteration 5000: Loss = -11000.989604675526
1
Iteration 5100: Loss = -11000.987623800722
Iteration 5200: Loss = -11000.987456849389
Iteration 5300: Loss = -11000.987838783676
1
Iteration 5400: Loss = -11000.987216870915
Iteration 5500: Loss = -11000.987159793303
Iteration 5600: Loss = -11000.987210684954
Iteration 5700: Loss = -11000.987030980148
Iteration 5800: Loss = -11000.986912022168
Iteration 5900: Loss = -11000.986894187046
Iteration 6000: Loss = -11000.9868342739
Iteration 6100: Loss = -11000.994674616506
1
Iteration 6200: Loss = -11000.986767073024
Iteration 6300: Loss = -11000.986695400961
Iteration 6400: Loss = -11000.986690369597
Iteration 6500: Loss = -11000.986670445545
Iteration 6600: Loss = -11000.99574518569
1
Iteration 6700: Loss = -11000.986606083005
Iteration 6800: Loss = -11000.986588828624
Iteration 6900: Loss = -11000.986596611927
Iteration 7000: Loss = -11000.98656907448
Iteration 7100: Loss = -11000.986644887675
Iteration 7200: Loss = -11000.986584388123
Iteration 7300: Loss = -11000.98652579131
Iteration 7400: Loss = -11000.986521335106
Iteration 7500: Loss = -11000.986531212184
Iteration 7600: Loss = -11000.986501716132
Iteration 7700: Loss = -11000.986489161998
Iteration 7800: Loss = -11000.990575761296
1
Iteration 7900: Loss = -11001.037934894804
2
Iteration 8000: Loss = -11000.986452011683
Iteration 8100: Loss = -11000.986478870373
Iteration 8200: Loss = -11000.986556767051
Iteration 8300: Loss = -11000.986498971082
Iteration 8400: Loss = -11000.986421705511
Iteration 8500: Loss = -11000.987391097047
1
Iteration 8600: Loss = -11000.986407878618
Iteration 8700: Loss = -11000.993552698375
1
Iteration 8800: Loss = -11000.986435223744
Iteration 8900: Loss = -11000.986381732575
Iteration 9000: Loss = -11001.004526147939
1
Iteration 9100: Loss = -11000.986428545768
Iteration 9200: Loss = -11000.986386862096
Iteration 9300: Loss = -11000.98648381586
Iteration 9400: Loss = -11000.986382486995
Iteration 9500: Loss = -11000.986393972073
Iteration 9600: Loss = -11000.98652515228
1
Iteration 9700: Loss = -11000.986388621452
Iteration 9800: Loss = -11000.989699506152
1
Iteration 9900: Loss = -11000.98640726068
Iteration 10000: Loss = -11000.986369284252
Iteration 10100: Loss = -11001.057284596829
1
Iteration 10200: Loss = -11000.986347657768
Iteration 10300: Loss = -11000.986453547444
1
Iteration 10400: Loss = -11000.98639976146
Iteration 10500: Loss = -11000.986412830409
Iteration 10600: Loss = -11001.021181024793
1
Iteration 10700: Loss = -11000.986339841023
Iteration 10800: Loss = -11001.189971607813
1
Iteration 10900: Loss = -11000.986376993178
Iteration 11000: Loss = -11000.986346170503
Iteration 11100: Loss = -11000.989845924263
1
Iteration 11200: Loss = -11000.986384897597
Iteration 11300: Loss = -11001.15950679073
1
Iteration 11400: Loss = -11000.98638149184
Iteration 11500: Loss = -11000.986336600157
Iteration 11600: Loss = -11000.98672052606
1
Iteration 11700: Loss = -11000.986381677729
Iteration 11800: Loss = -11000.988353531257
1
Iteration 11900: Loss = -11000.986423808796
Iteration 12000: Loss = -11001.09749908487
1
Iteration 12100: Loss = -11000.98636250676
Iteration 12200: Loss = -11001.05145395346
1
Iteration 12300: Loss = -11000.986366304218
Iteration 12400: Loss = -11000.98638616159
Iteration 12500: Loss = -11000.986425274992
Iteration 12600: Loss = -11000.986348874436
Iteration 12700: Loss = -11000.987813730117
1
Iteration 12800: Loss = -11000.986382943649
Iteration 12900: Loss = -11000.991836885427
1
Iteration 13000: Loss = -11000.986337331224
Iteration 13100: Loss = -11000.98632868779
Iteration 13200: Loss = -11000.98713305898
1
Iteration 13300: Loss = -11000.986337031456
Iteration 13400: Loss = -11000.99826053903
1
Iteration 13500: Loss = -11000.986366163312
Iteration 13600: Loss = -11001.051507668759
1
Iteration 13700: Loss = -11000.98637803275
Iteration 13800: Loss = -11000.986344877463
Iteration 13900: Loss = -11000.986546259322
1
Iteration 14000: Loss = -11001.253909653338
2
Iteration 14100: Loss = -11000.986381644048
Iteration 14200: Loss = -11000.991453124794
1
Iteration 14300: Loss = -11000.986398382147
Iteration 14400: Loss = -11000.98630933857
Iteration 14500: Loss = -11000.98900035528
1
Iteration 14600: Loss = -11000.986332539955
Iteration 14700: Loss = -11001.37023156463
1
Iteration 14800: Loss = -11000.986382426676
Iteration 14900: Loss = -11000.986376328676
Iteration 15000: Loss = -11000.987142821838
1
Iteration 15100: Loss = -11000.986554388535
2
Iteration 15200: Loss = -11000.9867544247
3
Iteration 15300: Loss = -11000.986499621653
4
Iteration 15400: Loss = -11000.986802718831
5
Iteration 15500: Loss = -11000.986637099519
6
Iteration 15600: Loss = -11000.987181580194
7
Iteration 15700: Loss = -11000.987636370326
8
Iteration 15800: Loss = -11000.986389267036
Iteration 15900: Loss = -11000.989785674232
1
Iteration 16000: Loss = -11001.05837206503
2
Iteration 16100: Loss = -11000.98637940613
Iteration 16200: Loss = -11000.986470913056
Iteration 16300: Loss = -11000.990563067484
1
Iteration 16400: Loss = -11000.986533113066
Iteration 16500: Loss = -11001.215603287857
1
Iteration 16600: Loss = -11000.98636888982
Iteration 16700: Loss = -11001.008689295053
1
Iteration 16800: Loss = -11000.986358734519
Iteration 16900: Loss = -11001.0545174309
1
Iteration 17000: Loss = -11000.986394553061
Iteration 17100: Loss = -11000.987603904576
1
Iteration 17200: Loss = -11001.003212323158
2
Iteration 17300: Loss = -11000.98659516459
3
Iteration 17400: Loss = -11001.084085847877
4
Iteration 17500: Loss = -11000.986418257082
Iteration 17600: Loss = -11000.986412850685
Iteration 17700: Loss = -11001.032334645302
1
Iteration 17800: Loss = -11000.98638411885
Iteration 17900: Loss = -11000.986454204914
Iteration 18000: Loss = -11000.993878569629
1
Iteration 18100: Loss = -11000.986362016396
Iteration 18200: Loss = -11001.011123633873
1
Iteration 18300: Loss = -11000.986452866659
Iteration 18400: Loss = -11000.9863462474
Iteration 18500: Loss = -11000.987896308012
1
Iteration 18600: Loss = -11000.986324755591
Iteration 18700: Loss = -11000.991104727436
1
Iteration 18800: Loss = -11000.986341502776
Iteration 18900: Loss = -11000.987790264799
1
Iteration 19000: Loss = -11000.995002827798
2
Iteration 19100: Loss = -11000.986382467978
Iteration 19200: Loss = -11000.987122709234
1
Iteration 19300: Loss = -11000.987005188974
2
Iteration 19400: Loss = -11000.986577594253
3
Iteration 19500: Loss = -11000.987540770573
4
Iteration 19600: Loss = -11001.058861097947
5
Iteration 19700: Loss = -11001.021794067143
6
Iteration 19800: Loss = -11001.01367945824
7
Iteration 19900: Loss = -11000.989011348636
8
pi: tensor([[0.9561, 0.0439],
        [0.1626, 0.8374]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9926, 0.0074], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1559, 0.1626],
         [0.5926, 0.2313]],

        [[0.6389, 0.2413],
         [0.6839, 0.6148]],

        [[0.7240, 0.2099],
         [0.7173, 0.6173]],

        [[0.5657, 0.1746],
         [0.5256, 0.7232]],

        [[0.5480, 0.2120],
         [0.5664, 0.5155]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0015169269193791859
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.0006438680677883739
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.004682108332720131
Global Adjusted Rand Index: -5.070534640350624e-05
Average Adjusted Rand Index: -0.0010232233963659069
10950.91155398354
[-5.070534640350624e-05, -5.070534640350624e-05] [-0.0010232233963659069, -0.0010232233963659069] [11001.029693720398, 11000.98702591672]
-------------------------------------
This iteration is 78
True Objective function: Loss = -10863.963373367535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21908.791103468884
Iteration 100: Loss = -10954.682564669798
Iteration 200: Loss = -10953.330863077394
Iteration 300: Loss = -10952.959389094809
Iteration 400: Loss = -10952.819709778865
Iteration 500: Loss = -10952.746871866662
Iteration 600: Loss = -10952.700669446993
Iteration 700: Loss = -10952.66217784399
Iteration 800: Loss = -10952.623740840772
Iteration 900: Loss = -10952.58241839855
Iteration 1000: Loss = -10952.535189271193
Iteration 1100: Loss = -10952.477988546492
Iteration 1200: Loss = -10952.40500239979
Iteration 1300: Loss = -10952.306165490365
Iteration 1400: Loss = -10952.163962293738
Iteration 1500: Loss = -10951.955751058904
Iteration 1600: Loss = -10951.684249039516
Iteration 1700: Loss = -10951.400467343292
Iteration 1800: Loss = -10951.15666471176
Iteration 1900: Loss = -10950.932554958647
Iteration 2000: Loss = -10950.733197313793
Iteration 2100: Loss = -10950.563248617877
Iteration 2200: Loss = -10950.325342362528
Iteration 2300: Loss = -10949.812369280355
Iteration 2400: Loss = -10949.244697884185
Iteration 2500: Loss = -10948.851511591512
Iteration 2600: Loss = -10948.60559173621
Iteration 2700: Loss = -10948.2806563103
Iteration 2800: Loss = -10947.887109129068
Iteration 2900: Loss = -10947.705966992758
Iteration 3000: Loss = -10947.542786228743
Iteration 3100: Loss = -10947.374402740943
Iteration 3200: Loss = -10947.207884208368
Iteration 3300: Loss = -10947.093071870004
Iteration 3400: Loss = -10947.046558814653
Iteration 3500: Loss = -10947.017177226617
Iteration 3600: Loss = -10946.996018537038
Iteration 3700: Loss = -10946.97970794394
Iteration 3800: Loss = -10946.967134002107
Iteration 3900: Loss = -10946.957461296912
Iteration 4000: Loss = -10946.949767999047
Iteration 4100: Loss = -10946.94350564908
Iteration 4200: Loss = -10946.938242802256
Iteration 4300: Loss = -10946.933735235418
Iteration 4400: Loss = -10946.929754619097
Iteration 4500: Loss = -10946.9262664731
Iteration 4600: Loss = -10946.923270233265
Iteration 4700: Loss = -10946.92071576131
Iteration 4800: Loss = -10946.918482946901
Iteration 4900: Loss = -10946.916481836379
Iteration 5000: Loss = -10946.914722465433
Iteration 5100: Loss = -10946.913067728234
Iteration 5200: Loss = -10946.91157187503
Iteration 5300: Loss = -10946.910193250551
Iteration 5400: Loss = -10946.908936939699
Iteration 5500: Loss = -10946.907824499363
Iteration 5600: Loss = -10946.907466655282
Iteration 5700: Loss = -10946.905773250668
Iteration 5800: Loss = -10946.904892922998
Iteration 5900: Loss = -10946.904209797922
Iteration 6000: Loss = -10946.903391159562
Iteration 6100: Loss = -10946.902743838351
Iteration 6200: Loss = -10946.90212949513
Iteration 6300: Loss = -10946.901606421277
Iteration 6400: Loss = -10946.901075859123
Iteration 6500: Loss = -10946.900609638496
Iteration 6600: Loss = -10946.900187389352
Iteration 6700: Loss = -10946.900372900574
1
Iteration 6800: Loss = -10946.899394659431
Iteration 6900: Loss = -10946.899044670237
Iteration 7000: Loss = -10946.89869760959
Iteration 7100: Loss = -10946.89928629959
1
Iteration 7200: Loss = -10946.898086769386
Iteration 7300: Loss = -10946.897820594391
Iteration 7400: Loss = -10946.897551850234
Iteration 7500: Loss = -10946.897337232313
Iteration 7600: Loss = -10946.897061948921
Iteration 7700: Loss = -10946.89947086669
1
Iteration 7800: Loss = -10946.92392652629
2
Iteration 7900: Loss = -10946.896490738613
Iteration 8000: Loss = -10946.896324967514
Iteration 8100: Loss = -10946.89615515208
Iteration 8200: Loss = -10946.895990178018
Iteration 8300: Loss = -10946.896139381406
1
Iteration 8400: Loss = -10946.89573559499
Iteration 8500: Loss = -10946.895600519141
Iteration 8600: Loss = -10946.896214086635
1
Iteration 8700: Loss = -10946.895360990335
Iteration 8800: Loss = -10946.89528377143
Iteration 8900: Loss = -10946.895124560322
Iteration 9000: Loss = -10946.895053311024
Iteration 9100: Loss = -10946.894946752702
Iteration 9200: Loss = -10946.902358986616
1
Iteration 9300: Loss = -10946.894797610144
Iteration 9400: Loss = -10946.894675306115
Iteration 9500: Loss = -10946.894612905156
Iteration 9600: Loss = -10946.894563750546
Iteration 9700: Loss = -10946.894517245655
Iteration 9800: Loss = -10946.894422688474
Iteration 9900: Loss = -10946.894358737063
Iteration 10000: Loss = -10946.894291597933
Iteration 10100: Loss = -10946.894272511643
Iteration 10200: Loss = -10946.894179420427
Iteration 10300: Loss = -10946.894158853433
Iteration 10400: Loss = -10947.433538950843
1
Iteration 10500: Loss = -10946.894083493937
Iteration 10600: Loss = -10946.894002229341
Iteration 10700: Loss = -10946.893978435623
Iteration 10800: Loss = -10946.894063133017
Iteration 10900: Loss = -10946.893902001719
Iteration 11000: Loss = -10946.893860719289
Iteration 11100: Loss = -10946.895263532422
1
Iteration 11200: Loss = -10946.893770380768
Iteration 11300: Loss = -10946.893744441388
Iteration 11400: Loss = -10946.894049010258
1
Iteration 11500: Loss = -10946.893894963008
2
Iteration 11600: Loss = -10946.90106974341
3
Iteration 11700: Loss = -10946.893713950505
Iteration 11800: Loss = -10946.893980736068
1
Iteration 11900: Loss = -10946.893971409723
2
Iteration 12000: Loss = -10946.89371511477
Iteration 12100: Loss = -10946.9357796952
1
Iteration 12200: Loss = -10946.893707744397
Iteration 12300: Loss = -10946.893647479576
Iteration 12400: Loss = -10946.900923074512
1
Iteration 12500: Loss = -10946.89353041315
Iteration 12600: Loss = -10946.893971736337
1
Iteration 12700: Loss = -10946.894214215949
2
Iteration 12800: Loss = -10946.894161100656
3
Iteration 12900: Loss = -10946.893914984443
4
Iteration 13000: Loss = -10946.893470941483
Iteration 13100: Loss = -10946.893466609945
Iteration 13200: Loss = -10946.893556338764
Iteration 13300: Loss = -10946.893414379058
Iteration 13400: Loss = -10946.893426090503
Iteration 13500: Loss = -10946.901489638003
1
Iteration 13600: Loss = -10946.893979699866
2
Iteration 13700: Loss = -10946.897098530566
3
Iteration 13800: Loss = -10946.893405452822
Iteration 13900: Loss = -10946.893399800469
Iteration 14000: Loss = -10946.89340167819
Iteration 14100: Loss = -10946.893390344567
Iteration 14200: Loss = -10947.098006101327
1
Iteration 14300: Loss = -10946.893350260958
Iteration 14400: Loss = -10946.89698331933
1
Iteration 14500: Loss = -10946.911264244607
2
Iteration 14600: Loss = -10946.89992979177
3
Iteration 14700: Loss = -10946.893341507981
Iteration 14800: Loss = -10946.912570666516
1
Iteration 14900: Loss = -10946.895191443256
2
Iteration 15000: Loss = -10946.897685907217
3
Iteration 15100: Loss = -10946.893836205574
4
Iteration 15200: Loss = -10946.920690124682
5
Iteration 15300: Loss = -10946.893289449785
Iteration 15400: Loss = -10946.89330979872
Iteration 15500: Loss = -10946.89474271418
1
Iteration 15600: Loss = -10946.893983350361
2
Iteration 15700: Loss = -10946.90427931105
3
Iteration 15800: Loss = -10946.893293109291
Iteration 15900: Loss = -10946.898877166715
1
Iteration 16000: Loss = -10946.89327047335
Iteration 16100: Loss = -10946.8944203336
1
Iteration 16200: Loss = -10946.893865470613
2
Iteration 16300: Loss = -10946.89349554196
3
Iteration 16400: Loss = -10946.992907343547
4
Iteration 16500: Loss = -10946.893288735175
Iteration 16600: Loss = -10946.900398913089
1
Iteration 16700: Loss = -10946.893294290167
Iteration 16800: Loss = -10946.895886669747
1
Iteration 16900: Loss = -10946.893541564965
2
Iteration 17000: Loss = -10946.902107331027
3
Iteration 17100: Loss = -10946.901148605899
4
Iteration 17200: Loss = -10946.893335062032
Iteration 17300: Loss = -10946.893586185433
1
Iteration 17400: Loss = -10946.893272483467
Iteration 17500: Loss = -10946.893301400083
Iteration 17600: Loss = -10946.926108476022
1
Iteration 17700: Loss = -10946.893271772382
Iteration 17800: Loss = -10947.200658717682
1
Iteration 17900: Loss = -10946.89322491528
Iteration 18000: Loss = -10946.893254793224
Iteration 18100: Loss = -10946.893504506372
1
Iteration 18200: Loss = -10946.904305544303
2
Iteration 18300: Loss = -10947.068840460899
3
Iteration 18400: Loss = -10946.893271828965
Iteration 18500: Loss = -10946.981522690456
1
Iteration 18600: Loss = -10946.894905415575
2
Iteration 18700: Loss = -10946.89326930037
Iteration 18800: Loss = -10946.893279668227
Iteration 18900: Loss = -10946.897275679747
1
Iteration 19000: Loss = -10946.893255401346
Iteration 19100: Loss = -10946.893472521955
1
Iteration 19200: Loss = -10946.893317136384
Iteration 19300: Loss = -10946.893269601454
Iteration 19400: Loss = -10946.89322987458
Iteration 19500: Loss = -10946.89326794798
Iteration 19600: Loss = -10946.893190042187
Iteration 19700: Loss = -10946.919113586528
1
Iteration 19800: Loss = -10946.893235251187
Iteration 19900: Loss = -10946.893228824541
pi: tensor([[1.0000e+00, 9.8929e-08],
        [1.3720e-01, 8.6280e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9008, 0.0992], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1678, 0.1073],
         [0.7269, 0.1851]],

        [[0.6677, 0.0970],
         [0.5749, 0.5030]],

        [[0.6384, 0.1466],
         [0.7120, 0.5617]],

        [[0.7101, 0.1587],
         [0.7254, 0.5951]],

        [[0.5122, 0.1238],
         [0.5956, 0.6570]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.02918749315918129
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.002427152448917675
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.01204793216819519
Global Adjusted Rand Index: 0.010194133736120313
Average Adjusted Rand Index: 0.0075389249767384165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22659.88644433799
Iteration 100: Loss = -10954.436636477967
Iteration 200: Loss = -10953.117164522902
Iteration 300: Loss = -10952.835467585632
Iteration 400: Loss = -10952.727981759012
Iteration 500: Loss = -10952.667522845584
Iteration 600: Loss = -10952.62025854249
Iteration 700: Loss = -10952.572460194915
Iteration 800: Loss = -10952.518627596635
Iteration 900: Loss = -10952.453863700413
Iteration 1000: Loss = -10952.373794499003
Iteration 1100: Loss = -10952.275820788458
Iteration 1200: Loss = -10952.161119698612
Iteration 1300: Loss = -10952.034364163954
Iteration 1400: Loss = -10951.904047300151
Iteration 1500: Loss = -10951.783814796769
Iteration 1600: Loss = -10951.682629732777
Iteration 1700: Loss = -10951.597872678001
Iteration 1800: Loss = -10951.523811166226
Iteration 1900: Loss = -10951.456786834944
Iteration 2000: Loss = -10951.393172615495
Iteration 2100: Loss = -10951.329843221916
Iteration 2200: Loss = -10951.268109391602
Iteration 2300: Loss = -10951.214426639635
Iteration 2400: Loss = -10951.175461790303
Iteration 2500: Loss = -10951.151118939228
Iteration 2600: Loss = -10951.135938404954
Iteration 2700: Loss = -10951.126061423181
Iteration 2800: Loss = -10951.118972940174
Iteration 2900: Loss = -10951.112716948766
Iteration 3000: Loss = -10951.105557826468
Iteration 3100: Loss = -10951.096050304688
Iteration 3200: Loss = -10951.081550719075
Iteration 3300: Loss = -10951.05624267811
Iteration 3400: Loss = -10951.004811147723
Iteration 3500: Loss = -10950.892946285498
Iteration 3600: Loss = -10950.714109726585
Iteration 3700: Loss = -10950.536906389045
Iteration 3800: Loss = -10950.396760494927
Iteration 3900: Loss = -10950.302052220411
Iteration 4000: Loss = -10950.241809046196
Iteration 4100: Loss = -10950.202729226843
Iteration 4200: Loss = -10950.17642506503
Iteration 4300: Loss = -10950.15766333728
Iteration 4400: Loss = -10950.1437266414
Iteration 4500: Loss = -10950.13301916237
Iteration 4600: Loss = -10950.124459060333
Iteration 4700: Loss = -10950.117598529989
Iteration 4800: Loss = -10950.111982770202
Iteration 4900: Loss = -10950.107320586889
Iteration 5000: Loss = -10950.103413704343
Iteration 5100: Loss = -10950.100105550238
Iteration 5200: Loss = -10950.097256013183
Iteration 5300: Loss = -10950.094790151734
Iteration 5400: Loss = -10950.092624776054
Iteration 5500: Loss = -10950.090769222043
Iteration 5600: Loss = -10950.08906512587
Iteration 5700: Loss = -10950.087552824758
Iteration 5800: Loss = -10950.086152512744
Iteration 5900: Loss = -10950.085353052855
Iteration 6000: Loss = -10950.08366640206
Iteration 6100: Loss = -10950.082479785191
Iteration 6200: Loss = -10950.0813529974
Iteration 6300: Loss = -10950.080373497685
Iteration 6400: Loss = -10950.079477599553
Iteration 6500: Loss = -10950.078666283174
Iteration 6600: Loss = -10950.077922194252
Iteration 6700: Loss = -10950.077187667926
Iteration 6800: Loss = -10950.076522067306
Iteration 6900: Loss = -10950.075840566444
Iteration 7000: Loss = -10950.07516199058
Iteration 7100: Loss = -10950.074328330915
Iteration 7200: Loss = -10950.073175780077
Iteration 7300: Loss = -10950.07223424797
Iteration 7400: Loss = -10950.071714254014
Iteration 7500: Loss = -10950.073543991148
1
Iteration 7600: Loss = -10950.071562369823
Iteration 7700: Loss = -10950.07153198473
Iteration 7800: Loss = -10950.071812583554
1
Iteration 7900: Loss = -10950.072061275134
2
Iteration 8000: Loss = -10950.069928752833
Iteration 8100: Loss = -10950.071039318947
1
Iteration 8200: Loss = -10950.069493201485
Iteration 8300: Loss = -10950.074664158255
1
Iteration 8400: Loss = -10950.069150640074
Iteration 8500: Loss = -10950.068952473946
Iteration 8600: Loss = -10950.068796747826
Iteration 8700: Loss = -10950.068656127774
Iteration 8800: Loss = -10950.068499764124
Iteration 8900: Loss = -10950.068539308397
Iteration 9000: Loss = -10950.068232284217
Iteration 9100: Loss = -10950.068078669436
Iteration 9200: Loss = -10950.068007992213
Iteration 9300: Loss = -10950.067828065
Iteration 9400: Loss = -10950.06773544392
Iteration 9500: Loss = -10950.070906794712
1
Iteration 9600: Loss = -10950.067584359333
Iteration 9700: Loss = -10950.067441377194
Iteration 9800: Loss = -10950.06738006072
Iteration 9900: Loss = -10950.06730643011
Iteration 10000: Loss = -10950.067254684192
Iteration 10100: Loss = -10950.067921753582
1
Iteration 10200: Loss = -10950.067095397908
Iteration 10300: Loss = -10950.067057302238
Iteration 10400: Loss = -10950.067336521783
1
Iteration 10500: Loss = -10950.066951936204
Iteration 10600: Loss = -10950.068469204836
1
Iteration 10700: Loss = -10950.077684474696
2
Iteration 10800: Loss = -10950.066760188956
Iteration 10900: Loss = -10950.067253581237
1
Iteration 11000: Loss = -10950.06669383734
Iteration 11100: Loss = -10950.06668174221
Iteration 11200: Loss = -10950.084554899044
1
Iteration 11300: Loss = -10950.066777181879
Iteration 11400: Loss = -10950.068548520952
1
Iteration 11500: Loss = -10950.06658569922
Iteration 11600: Loss = -10950.066530312337
Iteration 11700: Loss = -10950.081958973977
1
Iteration 11800: Loss = -10950.066456492164
Iteration 11900: Loss = -10950.070160349218
1
Iteration 12000: Loss = -10950.066465964746
Iteration 12100: Loss = -10950.068799110193
1
Iteration 12200: Loss = -10950.066396118922
Iteration 12300: Loss = -10950.066411543388
Iteration 12400: Loss = -10950.088349773116
1
Iteration 12500: Loss = -10950.072234453686
2
Iteration 12600: Loss = -10950.066435510445
Iteration 12700: Loss = -10950.068128602425
1
Iteration 12800: Loss = -10950.133897503752
2
Iteration 12900: Loss = -10950.066312596004
Iteration 13000: Loss = -10950.066671865156
1
Iteration 13100: Loss = -10950.073279810766
2
Iteration 13200: Loss = -10950.066432475583
3
Iteration 13300: Loss = -10950.066415190176
4
Iteration 13400: Loss = -10950.066579215234
5
Iteration 13500: Loss = -10950.074275200624
6
Iteration 13600: Loss = -10950.06679712757
7
Iteration 13700: Loss = -10950.066231677267
Iteration 13800: Loss = -10950.070631322242
1
Iteration 13900: Loss = -10950.068970374365
2
Iteration 14000: Loss = -10950.076118642524
3
Iteration 14100: Loss = -10950.066185882471
Iteration 14200: Loss = -10950.067178061949
1
Iteration 14300: Loss = -10950.070383804506
2
Iteration 14400: Loss = -10950.07029137012
3
Iteration 14500: Loss = -10950.067289950093
4
Iteration 14600: Loss = -10950.066322295152
5
Iteration 14700: Loss = -10950.066181439875
Iteration 14800: Loss = -10950.11168644438
1
Iteration 14900: Loss = -10950.066145102417
Iteration 15000: Loss = -10950.067500978372
1
Iteration 15100: Loss = -10950.066204775168
Iteration 15200: Loss = -10950.14075254001
1
Iteration 15300: Loss = -10950.066051543927
Iteration 15400: Loss = -10950.081765726585
1
Iteration 15500: Loss = -10950.066729561668
2
Iteration 15600: Loss = -10950.067168531441
3
Iteration 15700: Loss = -10950.115849434846
4
Iteration 15800: Loss = -10950.075209784418
5
Iteration 15900: Loss = -10950.071605519153
6
Iteration 16000: Loss = -10950.086606643401
7
Iteration 16100: Loss = -10950.06691983607
8
Iteration 16200: Loss = -10950.065869816099
Iteration 16300: Loss = -10950.07909277909
1
Iteration 16400: Loss = -10950.076026895087
2
Iteration 16500: Loss = -10950.068731107669
3
Iteration 16600: Loss = -10950.065115414716
Iteration 16700: Loss = -10950.06778484913
1
Iteration 16800: Loss = -10950.065728333215
2
Iteration 16900: Loss = -10950.071251512532
3
Iteration 17000: Loss = -10950.069588401248
4
Iteration 17100: Loss = -10950.074155321267
5
Iteration 17200: Loss = -10950.065096129656
Iteration 17300: Loss = -10950.065145258839
Iteration 17400: Loss = -10950.065711003364
1
Iteration 17500: Loss = -10950.068719752695
2
Iteration 17600: Loss = -10950.083764664543
3
Iteration 17700: Loss = -10950.065095281827
Iteration 17800: Loss = -10950.06532002082
1
Iteration 17900: Loss = -10950.065176020087
Iteration 18000: Loss = -10950.06513068104
Iteration 18100: Loss = -10950.065098216035
Iteration 18200: Loss = -10950.06515848223
Iteration 18300: Loss = -10950.065838442604
1
Iteration 18400: Loss = -10950.065157598772
Iteration 18500: Loss = -10950.065479342278
1
Iteration 18600: Loss = -10950.066645749626
2
Iteration 18700: Loss = -10950.095875700837
3
Iteration 18800: Loss = -10950.065065879564
Iteration 18900: Loss = -10950.065148161466
Iteration 19000: Loss = -10950.066794823315
1
Iteration 19100: Loss = -10950.06543797267
2
Iteration 19200: Loss = -10950.065363399655
3
Iteration 19300: Loss = -10950.151872635673
4
Iteration 19400: Loss = -10950.072915104076
5
Iteration 19500: Loss = -10950.218915787338
6
Iteration 19600: Loss = -10950.06512692122
Iteration 19700: Loss = -10950.06527226589
1
Iteration 19800: Loss = -10950.072631870577
2
Iteration 19900: Loss = -10950.076404925538
3
pi: tensor([[9.7327e-01, 2.6734e-02],
        [2.4730e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9671, 0.0329], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.2245],
         [0.5062, 0.1829]],

        [[0.6104, 0.1108],
         [0.6229, 0.5990]],

        [[0.6191, 0.1511],
         [0.6169, 0.6038]],

        [[0.6801, 0.1738],
         [0.6842, 0.5382]],

        [[0.6747, 0.1866],
         [0.6347, 0.5535]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.003937327268695544
Global Adjusted Rand Index: -0.00027388137537946147
Average Adjusted Rand Index: -0.0010544943943382689
10863.963373367535
[0.010194133736120313, -0.00027388137537946147] [0.0075389249767384165, -0.0010544943943382689] [10946.893465203659, 10950.144658076304]
-------------------------------------
This iteration is 79
True Objective function: Loss = -10778.992875480853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23568.44028303192
Iteration 100: Loss = -10854.364493346759
Iteration 200: Loss = -10853.415376657813
Iteration 300: Loss = -10851.856845030705
Iteration 400: Loss = -10849.861349228213
Iteration 500: Loss = -10848.35008428167
Iteration 600: Loss = -10846.952555309894
Iteration 700: Loss = -10846.429771434108
Iteration 800: Loss = -10846.075537556411
Iteration 900: Loss = -10845.956709800581
Iteration 1000: Loss = -10845.762033059083
Iteration 1100: Loss = -10845.59615693371
Iteration 1200: Loss = -10845.42367463526
Iteration 1300: Loss = -10845.289416083635
Iteration 1400: Loss = -10845.19934685941
Iteration 1500: Loss = -10845.119903431845
Iteration 1600: Loss = -10845.036937827832
Iteration 1700: Loss = -10844.925050552743
Iteration 1800: Loss = -10844.639823908621
Iteration 1900: Loss = -10812.875505229546
Iteration 2000: Loss = -10784.431628038757
Iteration 2100: Loss = -10780.143996030803
Iteration 2200: Loss = -10776.708109661226
Iteration 2300: Loss = -10776.479956835636
Iteration 2400: Loss = -10776.346949782872
Iteration 2500: Loss = -10775.858147988522
Iteration 2600: Loss = -10775.31716442416
Iteration 2700: Loss = -10741.679391882712
Iteration 2800: Loss = -10741.553196138397
Iteration 2900: Loss = -10741.531164811573
Iteration 3000: Loss = -10741.52177301779
Iteration 3100: Loss = -10741.515923478013
Iteration 3200: Loss = -10741.510868169618
Iteration 3300: Loss = -10741.502947218696
Iteration 3400: Loss = -10741.498645721525
Iteration 3500: Loss = -10741.483961502425
Iteration 3600: Loss = -10741.47971032384
Iteration 3700: Loss = -10741.481879186815
1
Iteration 3800: Loss = -10741.478255220614
Iteration 3900: Loss = -10741.47733365628
Iteration 4000: Loss = -10741.4751340456
Iteration 4100: Loss = -10741.473885178826
Iteration 4200: Loss = -10741.46957937866
Iteration 4300: Loss = -10741.462296320342
Iteration 4400: Loss = -10741.459389515476
Iteration 4500: Loss = -10741.457022577333
Iteration 4600: Loss = -10741.444754946111
Iteration 4700: Loss = -10741.443412344279
Iteration 4800: Loss = -10741.442640794345
Iteration 4900: Loss = -10741.442465729999
Iteration 5000: Loss = -10741.442042037783
Iteration 5100: Loss = -10741.430295159633
Iteration 5200: Loss = -10741.41668399657
Iteration 5300: Loss = -10741.416237269852
Iteration 5400: Loss = -10741.414268636045
Iteration 5500: Loss = -10741.413708596248
Iteration 5600: Loss = -10741.413621508951
Iteration 5700: Loss = -10741.413751854283
1
Iteration 5800: Loss = -10741.413479205832
Iteration 5900: Loss = -10741.4134144321
Iteration 6000: Loss = -10741.417728544315
1
Iteration 6100: Loss = -10741.413319874811
Iteration 6200: Loss = -10741.413302643254
Iteration 6300: Loss = -10741.413416852658
1
Iteration 6400: Loss = -10741.413179487834
Iteration 6500: Loss = -10741.41309624794
Iteration 6600: Loss = -10741.414021539567
1
Iteration 6700: Loss = -10741.41298851163
Iteration 6800: Loss = -10741.412961270406
Iteration 6900: Loss = -10741.41831492097
1
Iteration 7000: Loss = -10741.41287814206
Iteration 7100: Loss = -10741.41279090927
Iteration 7200: Loss = -10741.412155533353
Iteration 7300: Loss = -10741.409845815382
Iteration 7400: Loss = -10741.410570947808
1
Iteration 7500: Loss = -10741.43109926548
2
Iteration 7600: Loss = -10741.417913248111
3
Iteration 7700: Loss = -10741.409592075623
Iteration 7800: Loss = -10741.412150949194
1
Iteration 7900: Loss = -10741.408667290754
Iteration 8000: Loss = -10741.408576053429
Iteration 8100: Loss = -10741.419769505275
1
Iteration 8200: Loss = -10741.408403606532
Iteration 8300: Loss = -10741.410236441006
1
Iteration 8400: Loss = -10741.40798605422
Iteration 8500: Loss = -10741.42092635695
1
Iteration 8600: Loss = -10741.35972257364
Iteration 8700: Loss = -10741.392045574617
1
Iteration 8800: Loss = -10741.359428553538
Iteration 8900: Loss = -10741.359391357466
Iteration 9000: Loss = -10741.360114381232
1
Iteration 9100: Loss = -10741.359311388249
Iteration 9200: Loss = -10741.359254941506
Iteration 9300: Loss = -10741.35926859389
Iteration 9400: Loss = -10741.365411005987
1
Iteration 9500: Loss = -10741.361156743242
2
Iteration 9600: Loss = -10741.363986194016
3
Iteration 9700: Loss = -10741.42541110195
4
Iteration 9800: Loss = -10741.359171414202
Iteration 9900: Loss = -10741.360334976094
1
Iteration 10000: Loss = -10741.462053273668
2
Iteration 10100: Loss = -10741.359154535665
Iteration 10200: Loss = -10741.360795685103
1
Iteration 10300: Loss = -10741.359749692814
2
Iteration 10400: Loss = -10741.430239050793
3
Iteration 10500: Loss = -10741.37165179426
4
Iteration 10600: Loss = -10741.359037487855
Iteration 10700: Loss = -10741.359667089106
1
Iteration 10800: Loss = -10741.481889408755
2
Iteration 10900: Loss = -10741.359016508784
Iteration 11000: Loss = -10741.362708554785
1
Iteration 11100: Loss = -10741.365839784583
2
Iteration 11200: Loss = -10741.35903453696
Iteration 11300: Loss = -10741.375788817664
1
Iteration 11400: Loss = -10741.359044709563
Iteration 11500: Loss = -10741.613574111467
1
Iteration 11600: Loss = -10741.35903411024
Iteration 11700: Loss = -10741.35905610708
Iteration 11800: Loss = -10741.359037635853
Iteration 11900: Loss = -10741.359028700736
Iteration 12000: Loss = -10741.38351500697
1
Iteration 12100: Loss = -10741.35901810462
Iteration 12200: Loss = -10741.358979452873
Iteration 12300: Loss = -10741.359097011287
1
Iteration 12400: Loss = -10741.35898361331
Iteration 12500: Loss = -10741.361649239232
1
Iteration 12600: Loss = -10741.359010542357
Iteration 12700: Loss = -10741.374521422475
1
Iteration 12800: Loss = -10741.360324960298
2
Iteration 12900: Loss = -10741.359157297682
3
Iteration 13000: Loss = -10741.380421315233
4
Iteration 13100: Loss = -10741.359020574011
Iteration 13200: Loss = -10741.360897796518
1
Iteration 13300: Loss = -10741.359396062278
2
Iteration 13400: Loss = -10741.36392114311
3
Iteration 13500: Loss = -10741.359494098246
4
Iteration 13600: Loss = -10741.359160409562
5
Iteration 13700: Loss = -10741.3611168844
6
Iteration 13800: Loss = -10741.375167232214
7
Iteration 13900: Loss = -10741.361965076954
8
Iteration 14000: Loss = -10741.371303166938
9
Iteration 14100: Loss = -10741.359258792972
10
Iteration 14200: Loss = -10741.360633268147
11
Iteration 14300: Loss = -10741.36165192282
12
Iteration 14400: Loss = -10741.399960326058
13
Iteration 14500: Loss = -10741.359014910568
Iteration 14600: Loss = -10741.361711003903
1
Iteration 14700: Loss = -10741.359009024773
Iteration 14800: Loss = -10741.359658848725
1
Iteration 14900: Loss = -10741.365951603371
2
Iteration 15000: Loss = -10741.40197281791
3
Iteration 15100: Loss = -10741.44966551762
4
Iteration 15200: Loss = -10741.358993175552
Iteration 15300: Loss = -10741.362248990363
1
Iteration 15400: Loss = -10741.359015064241
Iteration 15500: Loss = -10741.362011534911
1
Iteration 15600: Loss = -10741.359004487997
Iteration 15700: Loss = -10741.36242088357
1
Iteration 15800: Loss = -10741.35901861734
Iteration 15900: Loss = -10741.361076834004
1
Iteration 16000: Loss = -10741.365907649295
2
Iteration 16100: Loss = -10741.360563256798
3
Iteration 16200: Loss = -10741.368334038718
4
Iteration 16300: Loss = -10741.363744584989
5
Iteration 16400: Loss = -10741.359026363201
Iteration 16500: Loss = -10741.380401684128
1
Iteration 16600: Loss = -10741.35894591709
Iteration 16700: Loss = -10741.359079926382
1
Iteration 16800: Loss = -10741.359042687174
Iteration 16900: Loss = -10741.365242405036
1
Iteration 17000: Loss = -10741.358884873307
Iteration 17100: Loss = -10741.358967684051
Iteration 17200: Loss = -10741.35887027958
Iteration 17300: Loss = -10741.359218944455
1
Iteration 17400: Loss = -10741.35888330698
Iteration 17500: Loss = -10741.35985662841
1
Iteration 17600: Loss = -10741.358727248957
Iteration 17700: Loss = -10741.357302528013
Iteration 17800: Loss = -10741.357375751799
Iteration 17900: Loss = -10741.35924126983
1
Iteration 18000: Loss = -10741.409347816587
2
Iteration 18100: Loss = -10741.357166782773
Iteration 18200: Loss = -10741.363760290586
1
Iteration 18300: Loss = -10741.357237105774
Iteration 18400: Loss = -10741.357349819995
1
Iteration 18500: Loss = -10741.358016516895
2
Iteration 18600: Loss = -10741.372392622778
3
Iteration 18700: Loss = -10741.35889742418
4
Iteration 18800: Loss = -10741.365226792735
5
Iteration 18900: Loss = -10741.357177345939
Iteration 19000: Loss = -10741.357479474953
1
Iteration 19100: Loss = -10741.357245505606
Iteration 19200: Loss = -10741.358635408877
1
Iteration 19300: Loss = -10741.357165504614
Iteration 19400: Loss = -10741.359672037677
1
Iteration 19500: Loss = -10741.357292925912
2
Iteration 19600: Loss = -10741.357466456328
3
Iteration 19700: Loss = -10741.459833606175
4
Iteration 19800: Loss = -10741.357238044771
Iteration 19900: Loss = -10741.383983267277
1
pi: tensor([[0.7265, 0.2735],
        [0.2214, 0.7786]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5765, 0.4235], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1936, 0.1015],
         [0.6985, 0.2444]],

        [[0.7208, 0.1000],
         [0.5393, 0.6523]],

        [[0.5117, 0.1044],
         [0.6122, 0.5491]],

        [[0.6955, 0.0995],
         [0.5486, 0.6971]],

        [[0.5149, 0.0937],
         [0.5436, 0.5313]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026076198471212
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.8314063469727304
Average Adjusted Rand Index: 0.8320532351200096
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22695.25808269871
Iteration 100: Loss = -10854.681409604811
Iteration 200: Loss = -10853.570276871866
Iteration 300: Loss = -10852.625242372866
Iteration 400: Loss = -10850.905288945349
Iteration 500: Loss = -10847.93805122304
Iteration 600: Loss = -10846.224956885942
Iteration 700: Loss = -10844.055263165144
Iteration 800: Loss = -10820.89387703598
Iteration 900: Loss = -10790.962768367113
Iteration 1000: Loss = -10778.271083555162
Iteration 1100: Loss = -10776.646559787789
Iteration 1200: Loss = -10776.388361570502
Iteration 1300: Loss = -10776.28407790848
Iteration 1400: Loss = -10776.191171363547
Iteration 1500: Loss = -10776.041387213665
Iteration 1600: Loss = -10775.731674238912
Iteration 1700: Loss = -10774.980146662003
Iteration 1800: Loss = -10774.35075112632
Iteration 1900: Loss = -10772.72376349605
Iteration 2000: Loss = -10772.619090631071
Iteration 2100: Loss = -10763.754264948207
Iteration 2200: Loss = -10741.997680072986
Iteration 2300: Loss = -10741.469858131843
Iteration 2400: Loss = -10741.419700945633
Iteration 2500: Loss = -10741.401559115342
Iteration 2600: Loss = -10741.393270928964
Iteration 2700: Loss = -10741.385884963027
Iteration 2800: Loss = -10741.381339470225
Iteration 2900: Loss = -10741.37846894206
Iteration 3000: Loss = -10741.374463105658
Iteration 3100: Loss = -10741.372136110167
Iteration 3200: Loss = -10741.37021839847
Iteration 3300: Loss = -10741.36928170177
Iteration 3400: Loss = -10741.367912401895
Iteration 3500: Loss = -10741.367075468777
Iteration 3600: Loss = -10741.366461293996
Iteration 3700: Loss = -10741.365742879854
Iteration 3800: Loss = -10741.365218519026
Iteration 3900: Loss = -10741.366083208199
1
Iteration 4000: Loss = -10741.3643616115
Iteration 4100: Loss = -10741.379758776164
1
Iteration 4200: Loss = -10741.363634863412
Iteration 4300: Loss = -10741.368976628932
1
Iteration 4400: Loss = -10741.363114262087
Iteration 4500: Loss = -10741.363004620262
Iteration 4600: Loss = -10741.3626223682
Iteration 4700: Loss = -10741.363845176103
1
Iteration 4800: Loss = -10741.362153598826
Iteration 4900: Loss = -10741.362213865163
Iteration 5000: Loss = -10741.362076479776
Iteration 5100: Loss = -10741.361502748327
Iteration 5200: Loss = -10741.36130746634
Iteration 5300: Loss = -10741.362471335682
1
Iteration 5400: Loss = -10741.359068665857
Iteration 5500: Loss = -10741.358430188453
Iteration 5600: Loss = -10741.358297120194
Iteration 5700: Loss = -10741.358349169652
Iteration 5800: Loss = -10741.359469858728
1
Iteration 5900: Loss = -10741.361851926617
2
Iteration 6000: Loss = -10741.358379388685
Iteration 6100: Loss = -10741.361175210537
1
Iteration 6200: Loss = -10741.35793531869
Iteration 6300: Loss = -10741.35896411481
1
Iteration 6400: Loss = -10741.359384992415
2
Iteration 6500: Loss = -10741.359325674417
3
Iteration 6600: Loss = -10741.359451393013
4
Iteration 6700: Loss = -10741.357681480778
Iteration 6800: Loss = -10741.373904375576
1
Iteration 6900: Loss = -10741.357615857632
Iteration 7000: Loss = -10741.358084786209
1
Iteration 7100: Loss = -10741.358084207914
2
Iteration 7200: Loss = -10741.357553150725
Iteration 7300: Loss = -10741.357542826967
Iteration 7400: Loss = -10741.3604931097
1
Iteration 7500: Loss = -10741.357466234364
Iteration 7600: Loss = -10741.358349442658
1
Iteration 7700: Loss = -10741.357408924905
Iteration 7800: Loss = -10741.366778010863
1
Iteration 7900: Loss = -10741.36895978343
2
Iteration 8000: Loss = -10741.363549403097
3
Iteration 8100: Loss = -10741.3576376454
4
Iteration 8200: Loss = -10741.357345324735
Iteration 8300: Loss = -10741.35803247266
1
Iteration 8400: Loss = -10741.357252791147
Iteration 8500: Loss = -10741.357912654837
1
Iteration 8600: Loss = -10741.35724422742
Iteration 8700: Loss = -10741.549602507112
1
Iteration 8800: Loss = -10741.357147196086
Iteration 8900: Loss = -10741.357154790352
Iteration 9000: Loss = -10741.365929716221
1
Iteration 9100: Loss = -10741.357093783781
Iteration 9200: Loss = -10741.441261724924
1
Iteration 9300: Loss = -10741.357104631752
Iteration 9400: Loss = -10741.357101275371
Iteration 9500: Loss = -10741.357270573386
1
Iteration 9600: Loss = -10741.357083393565
Iteration 9700: Loss = -10741.357089461759
Iteration 9800: Loss = -10741.3571240592
Iteration 9900: Loss = -10741.35705185152
Iteration 10000: Loss = -10741.362796864894
1
Iteration 10100: Loss = -10741.382786638762
2
Iteration 10200: Loss = -10741.357023670242
Iteration 10300: Loss = -10741.357663977511
1
Iteration 10400: Loss = -10741.376609871179
2
Iteration 10500: Loss = -10741.357215297581
3
Iteration 10600: Loss = -10741.357063699856
Iteration 10700: Loss = -10741.357031427047
Iteration 10800: Loss = -10741.357143517704
1
Iteration 10900: Loss = -10741.357039494174
Iteration 11000: Loss = -10741.357875215324
1
Iteration 11100: Loss = -10741.360453630843
2
Iteration 11200: Loss = -10741.467120774843
3
Iteration 11300: Loss = -10741.35891460737
4
Iteration 11400: Loss = -10741.357259864655
5
Iteration 11500: Loss = -10741.35708891632
Iteration 11600: Loss = -10741.357234211555
1
Iteration 11700: Loss = -10741.357885649479
2
Iteration 11800: Loss = -10741.358440233798
3
Iteration 11900: Loss = -10741.357147364412
Iteration 12000: Loss = -10741.357241348405
Iteration 12100: Loss = -10741.35702813657
Iteration 12200: Loss = -10741.35994406472
1
Iteration 12300: Loss = -10741.357061335782
Iteration 12400: Loss = -10741.357594576464
1
Iteration 12500: Loss = -10741.366183172506
2
Iteration 12600: Loss = -10741.372182704941
3
Iteration 12700: Loss = -10741.358431744015
4
Iteration 12800: Loss = -10741.364353983803
5
Iteration 12900: Loss = -10741.364197458315
6
Iteration 13000: Loss = -10741.358980168148
7
Iteration 13100: Loss = -10741.359327385839
8
Iteration 13200: Loss = -10741.356891980064
Iteration 13300: Loss = -10741.356936049893
Iteration 13400: Loss = -10741.381087240383
1
Iteration 13500: Loss = -10741.356975883573
Iteration 13600: Loss = -10741.356995764441
Iteration 13700: Loss = -10741.37282241949
1
Iteration 13800: Loss = -10741.356849170645
Iteration 13900: Loss = -10741.358762109516
1
Iteration 14000: Loss = -10741.36606787493
2
Iteration 14100: Loss = -10741.356879672241
Iteration 14200: Loss = -10741.35685812879
Iteration 14300: Loss = -10741.360555341606
1
Iteration 14400: Loss = -10741.356883467373
Iteration 14500: Loss = -10741.357353691697
1
Iteration 14600: Loss = -10741.357085682928
2
Iteration 14700: Loss = -10741.356946385209
Iteration 14800: Loss = -10741.358433125968
1
Iteration 14900: Loss = -10741.44583642862
2
Iteration 15000: Loss = -10741.356852385456
Iteration 15100: Loss = -10741.357114401975
1
Iteration 15200: Loss = -10741.357224853778
2
Iteration 15300: Loss = -10741.356872931385
Iteration 15400: Loss = -10741.35687072405
Iteration 15500: Loss = -10741.356946909395
Iteration 15600: Loss = -10741.356837813953
Iteration 15700: Loss = -10741.358857879162
1
Iteration 15800: Loss = -10741.366015864623
2
Iteration 15900: Loss = -10741.363371959076
3
Iteration 16000: Loss = -10741.47064207102
4
Iteration 16100: Loss = -10741.35684765885
Iteration 16200: Loss = -10741.357169731189
1
Iteration 16300: Loss = -10741.35793741196
2
Iteration 16400: Loss = -10741.361564882254
3
Iteration 16500: Loss = -10741.362493314638
4
Iteration 16600: Loss = -10741.559673066966
5
Iteration 16700: Loss = -10741.356893401162
Iteration 16800: Loss = -10741.357699287462
1
Iteration 16900: Loss = -10741.357285141827
2
Iteration 17000: Loss = -10741.357050937737
3
Iteration 17100: Loss = -10741.358511716317
4
Iteration 17200: Loss = -10741.362417674072
5
Iteration 17300: Loss = -10741.381458112157
6
Iteration 17400: Loss = -10741.356887048216
Iteration 17500: Loss = -10741.385075775535
1
Iteration 17600: Loss = -10741.357715585025
2
Iteration 17700: Loss = -10741.356877394348
Iteration 17800: Loss = -10741.357037534804
1
Iteration 17900: Loss = -10741.357077040644
2
Iteration 18000: Loss = -10741.358280982446
3
Iteration 18100: Loss = -10741.364690340142
4
Iteration 18200: Loss = -10741.357914360404
5
Iteration 18300: Loss = -10741.356926921935
Iteration 18400: Loss = -10741.433968402878
1
Iteration 18500: Loss = -10741.356849258073
Iteration 18600: Loss = -10741.361970219627
1
Iteration 18700: Loss = -10741.356876731748
Iteration 18800: Loss = -10741.356962054215
Iteration 18900: Loss = -10741.357073823614
1
Iteration 19000: Loss = -10741.36335842032
2
Iteration 19100: Loss = -10741.362830595954
3
Iteration 19200: Loss = -10741.35692812847
Iteration 19300: Loss = -10741.35688954784
Iteration 19400: Loss = -10741.365126926137
1
Iteration 19500: Loss = -10741.357006696837
2
Iteration 19600: Loss = -10741.362051306112
3
Iteration 19700: Loss = -10741.356839321614
Iteration 19800: Loss = -10741.361329263906
1
Iteration 19900: Loss = -10741.356831019748
pi: tensor([[0.7275, 0.2725],
        [0.2215, 0.7785]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5763, 0.4237], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1934, 0.1013],
         [0.6385, 0.2447]],

        [[0.5127, 0.1004],
         [0.5194, 0.5679]],

        [[0.6820, 0.1047],
         [0.7243, 0.5081]],

        [[0.5958, 0.0999],
         [0.6410, 0.6688]],

        [[0.7105, 0.0942],
         [0.5913, 0.6049]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
time is 1
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026076198471212
time is 4
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.8314063469727304
Average Adjusted Rand Index: 0.8320532351200096
10778.992875480853
[0.8314063469727304, 0.8314063469727304] [0.8320532351200096, 0.8320532351200096] [10741.358876669288, 10741.361523680878]
-------------------------------------
This iteration is 80
True Objective function: Loss = -10848.523292585178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22365.884819971074
Iteration 100: Loss = -10923.745085518907
Iteration 200: Loss = -10922.858931534189
Iteration 300: Loss = -10922.277689546463
Iteration 400: Loss = -10922.106536800322
Iteration 500: Loss = -10922.011398230012
Iteration 600: Loss = -10921.9421397638
Iteration 700: Loss = -10921.886776426429
Iteration 800: Loss = -10921.840783774342
Iteration 900: Loss = -10921.80054884729
Iteration 1000: Loss = -10921.759277849183
Iteration 1100: Loss = -10921.686917178076
Iteration 1200: Loss = -10921.241096736892
Iteration 1300: Loss = -10921.026719552865
Iteration 1400: Loss = -10920.812103578912
Iteration 1500: Loss = -10920.628304366626
Iteration 1600: Loss = -10920.520097564848
Iteration 1700: Loss = -10920.449892741968
Iteration 1800: Loss = -10920.404290280781
Iteration 1900: Loss = -10920.37383986567
Iteration 2000: Loss = -10920.352756255921
Iteration 2100: Loss = -10920.337606593574
Iteration 2200: Loss = -10920.326333221903
Iteration 2300: Loss = -10920.317689165904
Iteration 2400: Loss = -10920.310924627656
Iteration 2500: Loss = -10920.30555097967
Iteration 2600: Loss = -10920.301148408329
Iteration 2700: Loss = -10920.297559218903
Iteration 2800: Loss = -10920.294474145252
Iteration 2900: Loss = -10920.291917533345
Iteration 3000: Loss = -10920.28975777985
Iteration 3100: Loss = -10920.287882990184
Iteration 3200: Loss = -10920.286198021939
Iteration 3300: Loss = -10920.28475276651
Iteration 3400: Loss = -10920.283525643556
Iteration 3500: Loss = -10920.28242909411
Iteration 3600: Loss = -10920.281392026918
Iteration 3700: Loss = -10920.280534988768
Iteration 3800: Loss = -10920.279739282636
Iteration 3900: Loss = -10920.279030991429
Iteration 4000: Loss = -10920.278381614244
Iteration 4100: Loss = -10920.27776445986
Iteration 4200: Loss = -10920.277262302616
Iteration 4300: Loss = -10920.276764664819
Iteration 4400: Loss = -10920.27631748422
Iteration 4500: Loss = -10920.275871298114
Iteration 4600: Loss = -10920.27552794068
Iteration 4700: Loss = -10920.275175721614
Iteration 4800: Loss = -10920.274829928216
Iteration 4900: Loss = -10920.274560249029
Iteration 5000: Loss = -10920.274262856461
Iteration 5100: Loss = -10920.274030902858
Iteration 5200: Loss = -10920.273778323646
Iteration 5300: Loss = -10920.273537431514
Iteration 5400: Loss = -10920.273349103196
Iteration 5500: Loss = -10920.273160877168
Iteration 5600: Loss = -10920.272976172471
Iteration 5700: Loss = -10920.27282543667
Iteration 5800: Loss = -10920.272636037551
Iteration 5900: Loss = -10920.272508188144
Iteration 6000: Loss = -10920.272375419136
Iteration 6100: Loss = -10920.27223592258
Iteration 6200: Loss = -10920.272118701985
Iteration 6300: Loss = -10920.271999547833
Iteration 6400: Loss = -10920.271928670403
Iteration 6500: Loss = -10920.271840171683
Iteration 6600: Loss = -10920.271698210598
Iteration 6700: Loss = -10920.272019279206
1
Iteration 6800: Loss = -10920.271546377602
Iteration 6900: Loss = -10920.271507143556
Iteration 7000: Loss = -10920.271411872462
Iteration 7100: Loss = -10920.271305002665
Iteration 7200: Loss = -10920.2712299581
Iteration 7300: Loss = -10920.271156563766
Iteration 7400: Loss = -10920.271115360332
Iteration 7500: Loss = -10920.271041157243
Iteration 7600: Loss = -10920.27098693462
Iteration 7700: Loss = -10920.270966841994
Iteration 7800: Loss = -10920.270933712334
Iteration 7900: Loss = -10920.270876123177
Iteration 8000: Loss = -10920.270808756297
Iteration 8100: Loss = -10920.270779082377
Iteration 8200: Loss = -10920.271352392758
1
Iteration 8300: Loss = -10920.281233810194
2
Iteration 8400: Loss = -10920.360603817908
3
Iteration 8500: Loss = -10920.272156494253
4
Iteration 8600: Loss = -10920.270665133461
Iteration 8700: Loss = -10920.410066887887
1
Iteration 8800: Loss = -10920.270561216063
Iteration 8900: Loss = -10920.2707262037
1
Iteration 9000: Loss = -10920.270543506611
Iteration 9100: Loss = -10920.270474181561
Iteration 9200: Loss = -10920.270620040119
1
Iteration 9300: Loss = -10920.27046380183
Iteration 9400: Loss = -10920.270898642455
1
Iteration 9500: Loss = -10920.270405565987
Iteration 9600: Loss = -10920.270390280046
Iteration 9700: Loss = -10920.293906991943
1
Iteration 9800: Loss = -10920.2703184099
Iteration 9900: Loss = -10920.270323121727
Iteration 10000: Loss = -10920.27234318753
1
Iteration 10100: Loss = -10920.27028360972
Iteration 10200: Loss = -10920.270267716616
Iteration 10300: Loss = -10920.270266101654
Iteration 10400: Loss = -10920.271199254155
1
Iteration 10500: Loss = -10920.27026180412
Iteration 10600: Loss = -10920.270221895556
Iteration 10700: Loss = -10920.27985999176
1
Iteration 10800: Loss = -10920.27024390479
Iteration 10900: Loss = -10920.270221843219
Iteration 11000: Loss = -10920.368800647699
1
Iteration 11100: Loss = -10920.270205631854
Iteration 11200: Loss = -10920.270196464106
Iteration 11300: Loss = -10920.329952980654
1
Iteration 11400: Loss = -10920.270151964525
Iteration 11500: Loss = -10920.27018004416
Iteration 11600: Loss = -10920.307056439848
1
Iteration 11700: Loss = -10920.270201949685
Iteration 11800: Loss = -10920.270143544123
Iteration 11900: Loss = -10920.28614620768
1
Iteration 12000: Loss = -10920.270132356665
Iteration 12100: Loss = -10920.270146765823
Iteration 12200: Loss = -10920.271321504133
1
Iteration 12300: Loss = -10920.270145490038
Iteration 12400: Loss = -10920.288913501507
1
Iteration 12500: Loss = -10920.270123823975
Iteration 12600: Loss = -10920.272304166465
1
Iteration 12700: Loss = -10920.270099093372
Iteration 12800: Loss = -10920.270753246865
1
Iteration 12900: Loss = -10920.270152449586
Iteration 13000: Loss = -10920.27282971312
1
Iteration 13100: Loss = -10920.270127077394
Iteration 13200: Loss = -10920.315450347036
1
Iteration 13300: Loss = -10920.270123444157
Iteration 13400: Loss = -10920.271321801609
1
Iteration 13500: Loss = -10920.270099946645
Iteration 13600: Loss = -10920.29537770926
1
Iteration 13700: Loss = -10920.270091180684
Iteration 13800: Loss = -10920.449695482515
1
Iteration 13900: Loss = -10920.270109015744
Iteration 14000: Loss = -10920.271245832066
1
Iteration 14100: Loss = -10920.270074747328
Iteration 14200: Loss = -10920.273969085463
1
Iteration 14300: Loss = -10920.270081754352
Iteration 14400: Loss = -10920.28017388409
1
Iteration 14500: Loss = -10920.270123756974
Iteration 14600: Loss = -10920.270557314445
1
Iteration 14700: Loss = -10920.270093588999
Iteration 14800: Loss = -10920.270885573525
1
Iteration 14900: Loss = -10920.270071815861
Iteration 15000: Loss = -10920.273362340733
1
Iteration 15100: Loss = -10920.271203156928
2
Iteration 15200: Loss = -10920.270118034585
Iteration 15300: Loss = -10920.270089644668
Iteration 15400: Loss = -10920.270089069716
Iteration 15500: Loss = -10920.270728670685
1
Iteration 15600: Loss = -10920.274191270548
2
Iteration 15700: Loss = -10920.272727967615
3
Iteration 15800: Loss = -10920.270079777682
Iteration 15900: Loss = -10920.270101160982
Iteration 16000: Loss = -10920.270100855118
Iteration 16100: Loss = -10920.27406390114
1
Iteration 16200: Loss = -10920.270111719456
Iteration 16300: Loss = -10920.270958349882
1
Iteration 16400: Loss = -10920.27011373876
Iteration 16500: Loss = -10920.27758648014
1
Iteration 16600: Loss = -10920.270427257177
2
Iteration 16700: Loss = -10920.270377140318
3
Iteration 16800: Loss = -10920.298490637364
4
Iteration 16900: Loss = -10920.279490415387
5
Iteration 17000: Loss = -10920.270113644097
Iteration 17100: Loss = -10920.270396523802
1
Iteration 17200: Loss = -10920.326934406
2
Iteration 17300: Loss = -10920.270046007281
Iteration 17400: Loss = -10920.29584455406
1
Iteration 17500: Loss = -10920.270392766986
2
Iteration 17600: Loss = -10920.270326974287
3
Iteration 17700: Loss = -10920.5151197123
4
Iteration 17800: Loss = -10920.270064228536
Iteration 17900: Loss = -10920.273793733371
1
Iteration 18000: Loss = -10920.437037228501
2
Iteration 18100: Loss = -10920.27009969001
Iteration 18200: Loss = -10920.270818267094
1
Iteration 18300: Loss = -10920.274903896083
2
Iteration 18400: Loss = -10920.27006401581
Iteration 18500: Loss = -10920.270460178765
1
Iteration 18600: Loss = -10920.330867986544
2
Iteration 18700: Loss = -10920.270065408164
Iteration 18800: Loss = -10920.270971274793
1
Iteration 18900: Loss = -10920.270093669888
Iteration 19000: Loss = -10920.270307059738
1
Iteration 19100: Loss = -10920.271577439145
2
Iteration 19200: Loss = -10920.270051688802
Iteration 19300: Loss = -10920.270134956012
Iteration 19400: Loss = -10920.270062529386
Iteration 19500: Loss = -10920.270354593253
1
Iteration 19600: Loss = -10920.27005379462
Iteration 19700: Loss = -10920.270119316021
Iteration 19800: Loss = -10920.270088178793
Iteration 19900: Loss = -10920.270093417617
pi: tensor([[5.9174e-01, 4.0826e-01],
        [1.8395e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0485, 0.9515], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2143, 0.2390],
         [0.5133, 0.1601]],

        [[0.6882, 0.1846],
         [0.6011, 0.6133]],

        [[0.6252, 0.2358],
         [0.7234, 0.5866]],

        [[0.5913, 0.2114],
         [0.5374, 0.6844]],

        [[0.5059, 0.1269],
         [0.5025, 0.5558]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [28:13:26<6:49:43, 1293.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [28:35:12<6:29:15, 1297.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [28:49:08<5:28:24, 1159.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 84%|████████▍ | 84/100 [29:07:40<5:05:20, 1145.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [29:29:15<4:57:32, 1190.16s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [29:50:47<4:44:48, 1220.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [30:07:41<4:11:03, 1158.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [30:29:11<3:59:36, 1198.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [30:50:48<3:45:05, 1227.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [31:12:27<3:28:11, 1249.13s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [31:34:12<3:09:53, 1265.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [31:56:00<2:50:28, 1278.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [32:17:33<2:29:38, 1282.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [32:37:21<2:05:25, 1254.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [32:58:23<1:44:43, 1256.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [33:20:07<1:24:43, 1270.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [33:38:34<1:01:04, 1221.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [34:00:05<41:25, 1242.57s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [34:17:35<19:44, 1184.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [34:39:05<00:00, 1216.32s/it]100%|██████████| 100/100 [34:39:05<00:00, 1247.45s/it]
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
Global Adjusted Rand Index: 0.004067292625778128
Average Adjusted Rand Index: 0.0036590790532969723
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24067.640967218045
Iteration 100: Loss = -10925.43516214501
Iteration 200: Loss = -10923.763586794283
Iteration 300: Loss = -10923.120317802586
Iteration 400: Loss = -10922.514507475329
Iteration 500: Loss = -10922.345708604522
Iteration 600: Loss = -10922.240018548926
Iteration 700: Loss = -10922.154541263439
Iteration 800: Loss = -10922.075909612991
Iteration 900: Loss = -10921.99928917342
Iteration 1000: Loss = -10921.923811767363
Iteration 1100: Loss = -10921.848544406554
Iteration 1200: Loss = -10921.741324847793
Iteration 1300: Loss = -10921.37796082501
Iteration 1400: Loss = -10921.206948788453
Iteration 1500: Loss = -10921.099219312036
Iteration 1600: Loss = -10920.998150616275
Iteration 1700: Loss = -10920.85407594206
Iteration 1800: Loss = -10920.66718493648
Iteration 1900: Loss = -10920.550123943744
Iteration 2000: Loss = -10920.488811952366
Iteration 2100: Loss = -10920.448290152268
Iteration 2200: Loss = -10920.420240200889
Iteration 2300: Loss = -10920.399823479787
Iteration 2400: Loss = -10920.38325787471
Iteration 2500: Loss = -10920.369089021862
Iteration 2600: Loss = -10920.35743115184
Iteration 2700: Loss = -10920.348105599658
Iteration 2800: Loss = -10920.34042335846
Iteration 2900: Loss = -10920.33400854795
Iteration 3000: Loss = -10920.328482257357
Iteration 3100: Loss = -10920.323592499564
Iteration 3200: Loss = -10920.319187803894
Iteration 3300: Loss = -10920.315282773652
Iteration 3400: Loss = -10920.311704624006
Iteration 3500: Loss = -10920.30857173005
Iteration 3600: Loss = -10920.305683179451
Iteration 3700: Loss = -10920.303125646898
Iteration 3800: Loss = -10920.300812986157
Iteration 3900: Loss = -10920.298708837692
Iteration 4000: Loss = -10920.296673728726
Iteration 4100: Loss = -10920.294914957127
Iteration 4200: Loss = -10920.293268719266
Iteration 4300: Loss = -10920.291745240977
Iteration 4400: Loss = -10920.290344314282
Iteration 4500: Loss = -10920.289076985013
Iteration 4600: Loss = -10920.287902543296
Iteration 4700: Loss = -10920.28681095911
Iteration 4800: Loss = -10920.285811193682
Iteration 4900: Loss = -10920.284872787082
Iteration 5000: Loss = -10920.284018532566
Iteration 5100: Loss = -10920.283184777514
Iteration 5200: Loss = -10920.282428817452
Iteration 5300: Loss = -10920.281738645863
Iteration 5400: Loss = -10920.28111372577
Iteration 5500: Loss = -10920.280451097615
Iteration 5600: Loss = -10920.279902705688
Iteration 5700: Loss = -10920.279310402799
Iteration 5800: Loss = -10920.278894679452
Iteration 5900: Loss = -10920.278377505165
Iteration 6000: Loss = -10920.277932979227
Iteration 6100: Loss = -10920.277512700768
Iteration 6200: Loss = -10920.277106321266
Iteration 6300: Loss = -10920.276711105844
Iteration 6400: Loss = -10920.276401938207
Iteration 6500: Loss = -10920.276068684301
Iteration 6600: Loss = -10920.275735625055
Iteration 6700: Loss = -10920.275429349707
Iteration 6800: Loss = -10920.275157578573
Iteration 6900: Loss = -10920.274895236125
Iteration 7000: Loss = -10920.274629845762
Iteration 7100: Loss = -10920.27437909431
Iteration 7200: Loss = -10920.276194135738
1
Iteration 7300: Loss = -10920.273955944112
Iteration 7400: Loss = -10920.27377602345
Iteration 7500: Loss = -10920.27359895844
Iteration 7600: Loss = -10920.27341017143
Iteration 7700: Loss = -10920.273244339636
Iteration 7800: Loss = -10920.273077679774
Iteration 7900: Loss = -10920.27290112597
Iteration 8000: Loss = -10920.272823879983
Iteration 8100: Loss = -10920.272619339314
Iteration 8200: Loss = -10920.272551154783
Iteration 8300: Loss = -10920.272368196791
Iteration 8400: Loss = -10920.272390106025
Iteration 8500: Loss = -10920.272752980387
1
Iteration 8600: Loss = -10920.28159207998
2
Iteration 8700: Loss = -10920.273471581719
3
Iteration 8800: Loss = -10920.271851641835
Iteration 8900: Loss = -10920.30444736933
1
Iteration 9000: Loss = -10920.271683798172
Iteration 9100: Loss = -10920.282152312206
1
Iteration 9200: Loss = -10920.27154251323
Iteration 9300: Loss = -10920.271471975244
Iteration 9400: Loss = -10920.276922235409
1
Iteration 9500: Loss = -10920.27129591193
Iteration 9600: Loss = -10920.271405630181
1
Iteration 9700: Loss = -10920.2718190045
2
Iteration 9800: Loss = -10920.271131776193
Iteration 9900: Loss = -10920.271063516546
Iteration 10000: Loss = -10920.287105087742
1
Iteration 10100: Loss = -10920.270991564654
Iteration 10200: Loss = -10920.270923250746
Iteration 10300: Loss = -10920.270894967613
Iteration 10400: Loss = -10920.27099400805
Iteration 10500: Loss = -10920.27079501073
Iteration 10600: Loss = -10920.270706460116
Iteration 10700: Loss = -10920.274307951342
1
Iteration 10800: Loss = -10920.270708114116
Iteration 10900: Loss = -10920.270669778674
Iteration 11000: Loss = -10920.290911331394
1
Iteration 11100: Loss = -10920.270593977117
Iteration 11200: Loss = -10920.270571164676
Iteration 11300: Loss = -10920.643205011404
1
Iteration 11400: Loss = -10920.270512782292
Iteration 11500: Loss = -10920.270532052855
Iteration 11600: Loss = -10920.381723570294
1
Iteration 11700: Loss = -10920.270454684154
Iteration 11800: Loss = -10920.270455077376
Iteration 11900: Loss = -10920.295384772222
1
Iteration 12000: Loss = -10920.270383925916
Iteration 12100: Loss = -10920.275364450219
1
Iteration 12200: Loss = -10920.271858690729
2
Iteration 12300: Loss = -10920.270373120025
Iteration 12400: Loss = -10920.270639559385
1
Iteration 12500: Loss = -10920.358524783216
2
Iteration 12600: Loss = -10920.270312993109
Iteration 12700: Loss = -10920.327938645087
1
Iteration 12800: Loss = -10920.27027155269
Iteration 12900: Loss = -10920.280387551507
1
Iteration 13000: Loss = -10920.27030324863
Iteration 13100: Loss = -10920.27047675544
1
Iteration 13200: Loss = -10920.277086531356
2
Iteration 13300: Loss = -10920.270673000014
3
Iteration 13400: Loss = -10920.48832004062
4
Iteration 13500: Loss = -10920.270211967292
Iteration 13600: Loss = -10920.383452211978
1
Iteration 13700: Loss = -10920.27019523808
Iteration 13800: Loss = -10920.27103963891
1
Iteration 13900: Loss = -10920.27029517868
Iteration 14000: Loss = -10920.27025522154
Iteration 14100: Loss = -10920.270497029947
1
Iteration 14200: Loss = -10920.27021903182
Iteration 14300: Loss = -10920.270234201864
Iteration 14400: Loss = -10920.270168045867
Iteration 14500: Loss = -10920.27016227796
Iteration 14600: Loss = -10920.270139249496
Iteration 14700: Loss = -10920.270140849208
Iteration 14800: Loss = -10920.270351766067
1
Iteration 14900: Loss = -10920.270610027563
2
Iteration 15000: Loss = -10920.270124478595
Iteration 15100: Loss = -10920.271543911325
1
Iteration 15200: Loss = -10920.270182320613
Iteration 15300: Loss = -10920.271316314078
1
Iteration 15400: Loss = -10920.270335854158
2
Iteration 15500: Loss = -10920.280115604786
3
Iteration 15600: Loss = -10920.270228699253
Iteration 15700: Loss = -10920.270390622602
1
Iteration 15800: Loss = -10920.273250296194
2
Iteration 15900: Loss = -10920.27201789708
3
Iteration 16000: Loss = -10920.270288859267
Iteration 16100: Loss = -10920.27014547881
Iteration 16200: Loss = -10920.27010002907
Iteration 16300: Loss = -10920.271563376842
1
Iteration 16400: Loss = -10920.270615493737
2
Iteration 16500: Loss = -10920.274474050291
3
Iteration 16600: Loss = -10920.27131708886
4
Iteration 16700: Loss = -10920.2700875554
Iteration 16800: Loss = -10920.27770344462
1
Iteration 16900: Loss = -10920.274529281854
2
Iteration 17000: Loss = -10920.270637103156
3
Iteration 17100: Loss = -10920.270099762616
Iteration 17200: Loss = -10920.420428872036
1
Iteration 17300: Loss = -10920.270089438447
Iteration 17400: Loss = -10920.27226755439
1
Iteration 17500: Loss = -10920.2700908034
Iteration 17600: Loss = -10920.271004781564
1
Iteration 17700: Loss = -10920.270026015538
Iteration 17800: Loss = -10920.270129453895
1
Iteration 17900: Loss = -10920.285233857718
2
Iteration 18000: Loss = -10920.288955586755
3
Iteration 18100: Loss = -10920.301236644835
4
Iteration 18200: Loss = -10920.28418870733
5
Iteration 18300: Loss = -10920.270062104402
Iteration 18400: Loss = -10920.270162396619
1
Iteration 18500: Loss = -10920.275350937945
2
Iteration 18600: Loss = -10920.270080143358
Iteration 18700: Loss = -10920.30804333952
1
Iteration 18800: Loss = -10920.270045974588
Iteration 18900: Loss = -10920.279206435805
1
Iteration 19000: Loss = -10920.270047515309
Iteration 19100: Loss = -10920.27743545552
1
Iteration 19200: Loss = -10920.274332068471
2
Iteration 19300: Loss = -10920.291990236508
3
Iteration 19400: Loss = -10920.272153829974
4
Iteration 19500: Loss = -10920.270111495645
Iteration 19600: Loss = -10920.351976177015
1
Iteration 19700: Loss = -10920.270082804982
Iteration 19800: Loss = -10920.271402915045
1
Iteration 19900: Loss = -10920.270051665215
pi: tensor([[5.8721e-01, 4.1279e-01],
        [6.2294e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0502, 0.9498], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2153, 0.2369],
         [0.6204, 0.1584]],

        [[0.5860, 0.1841],
         [0.5057, 0.5176]],

        [[0.6708, 0.2336],
         [0.6458, 0.5519]],

        [[0.6039, 0.2110],
         [0.6264, 0.6434]],

        [[0.5567, 0.1282],
         [0.5503, 0.7165]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
Global Adjusted Rand Index: 0.004067292625778128
Average Adjusted Rand Index: 0.0036590790532969723
10848.523292585178
[0.004067292625778128, 0.004067292625778128] [0.0036590790532969723, 0.0036590790532969723] [10920.40164296331, 10920.272846681244]
-------------------------------------
This iteration is 81
True Objective function: Loss = -10961.74010998401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23764.78647112191
Iteration 100: Loss = -11057.817322997405
Iteration 200: Loss = -11056.974247790738
Iteration 300: Loss = -11056.734463844348
Iteration 400: Loss = -11056.481076818174
Iteration 500: Loss = -11055.89956735547
Iteration 600: Loss = -11052.341211990622
Iteration 700: Loss = -11050.630889607426
Iteration 800: Loss = -11048.30822868854
Iteration 900: Loss = -10994.150689549562
Iteration 1000: Loss = -10987.896480155085
Iteration 1100: Loss = -10976.517680171719
Iteration 1200: Loss = -10960.886850303637
Iteration 1300: Loss = -10951.291001155876
Iteration 1400: Loss = -10939.949596130253
Iteration 1500: Loss = -10926.758650622647
Iteration 1600: Loss = -10917.458148675021
Iteration 1700: Loss = -10916.997423130588
Iteration 1800: Loss = -10916.968981522461
Iteration 1900: Loss = -10916.377863368492
Iteration 2000: Loss = -10915.345186031842
Iteration 2100: Loss = -10915.33132121504
Iteration 2200: Loss = -10915.32956617041
Iteration 2300: Loss = -10915.328651039323
Iteration 2400: Loss = -10915.32775979569
Iteration 2500: Loss = -10915.32682871226
Iteration 2600: Loss = -10915.325551801778
Iteration 2700: Loss = -10915.324460200527
Iteration 2800: Loss = -10915.323944499021
Iteration 2900: Loss = -10915.323510506314
Iteration 3000: Loss = -10915.327327495846
1
Iteration 3100: Loss = -10915.322752602578
Iteration 3200: Loss = -10915.322430423905
Iteration 3300: Loss = -10915.322782117331
1
Iteration 3400: Loss = -10915.319289564444
Iteration 3500: Loss = -10915.254050484904
Iteration 3600: Loss = -10915.254267525039
1
Iteration 3700: Loss = -10915.253723154772
Iteration 3800: Loss = -10915.25354895397
Iteration 3900: Loss = -10915.265012965849
1
Iteration 4000: Loss = -10915.25320983173
Iteration 4100: Loss = -10915.252948869835
Iteration 4200: Loss = -10915.255077399706
1
Iteration 4300: Loss = -10915.252519881245
Iteration 4400: Loss = -10915.252205211795
Iteration 4500: Loss = -10915.253044929308
1
Iteration 4600: Loss = -10915.251788324927
Iteration 4700: Loss = -10915.251513730916
Iteration 4800: Loss = -10915.25060624133
Iteration 4900: Loss = -10915.251124471672
1
Iteration 5000: Loss = -10915.249229599018
Iteration 5100: Loss = -10915.252663458892
1
Iteration 5200: Loss = -10915.248886149917
Iteration 5300: Loss = -10915.250554695062
1
Iteration 5400: Loss = -10915.246406881743
Iteration 5500: Loss = -10915.246039967247
Iteration 5600: Loss = -10915.244853803068
Iteration 5700: Loss = -10915.244190009029
Iteration 5800: Loss = -10915.243109213856
Iteration 5900: Loss = -10915.242841482914
Iteration 6000: Loss = -10915.246714187942
1
Iteration 6100: Loss = -10915.243218435062
2
Iteration 6200: Loss = -10915.242712715492
Iteration 6300: Loss = -10915.242569782857
Iteration 6400: Loss = -10915.241956298529
Iteration 6500: Loss = -10915.240013684957
Iteration 6600: Loss = -10915.23941175249
Iteration 6700: Loss = -10915.247392842422
1
Iteration 6800: Loss = -10915.238587283402
Iteration 6900: Loss = -10915.23977856395
1
Iteration 7000: Loss = -10915.245143080441
2
Iteration 7100: Loss = -10915.243954454185
3
Iteration 7200: Loss = -10915.239780426717
4
Iteration 7300: Loss = -10915.2384557526
Iteration 7400: Loss = -10915.23838163082
Iteration 7500: Loss = -10915.24067856557
1
Iteration 7600: Loss = -10915.239642292276
2
Iteration 7700: Loss = -10915.238514382934
3
Iteration 7800: Loss = -10915.238492915822
4
Iteration 7900: Loss = -10915.240856849827
5
Iteration 8000: Loss = -10915.238602554138
6
Iteration 8100: Loss = -10915.238407707806
Iteration 8200: Loss = -10915.238283375062
Iteration 8300: Loss = -10912.765421829705
Iteration 8400: Loss = -10912.75590840504
Iteration 8500: Loss = -10912.75559818334
Iteration 8600: Loss = -10912.755919763478
1
Iteration 8700: Loss = -10912.755633267803
Iteration 8800: Loss = -10912.755538047448
Iteration 8900: Loss = -10912.77079740044
1
Iteration 9000: Loss = -10912.755551179856
Iteration 9100: Loss = -10912.77574970083
1
Iteration 9200: Loss = -10912.755470660437
Iteration 9300: Loss = -10912.755990484075
1
Iteration 9400: Loss = -10912.755482028937
Iteration 9500: Loss = -10912.755457595389
Iteration 9600: Loss = -10912.75583167582
1
Iteration 9700: Loss = -10912.754851816848
Iteration 9800: Loss = -10912.735967807805
Iteration 9900: Loss = -10912.737410937722
1
Iteration 10000: Loss = -10912.735779745277
Iteration 10100: Loss = -10912.736313472175
1
Iteration 10200: Loss = -10912.734647862057
Iteration 10300: Loss = -10912.734624191187
Iteration 10400: Loss = -10912.736239509592
1
Iteration 10500: Loss = -10912.734544876013
Iteration 10600: Loss = -10912.734554724335
Iteration 10700: Loss = -10912.734294867187
Iteration 10800: Loss = -10912.80516903793
1
Iteration 10900: Loss = -10912.686410797003
Iteration 11000: Loss = -10912.689962979468
1
Iteration 11100: Loss = -10912.686387045085
Iteration 11200: Loss = -10912.686948686545
1
Iteration 11300: Loss = -10912.689623167853
2
Iteration 11400: Loss = -10912.686422494495
Iteration 11500: Loss = -10912.689274133314
1
Iteration 11600: Loss = -10912.68642645306
Iteration 11700: Loss = -10912.686476668121
Iteration 11800: Loss = -10912.68644053211
Iteration 11900: Loss = -10912.700082462357
1
Iteration 12000: Loss = -10912.68814342602
2
Iteration 12100: Loss = -10912.686470716551
Iteration 12200: Loss = -10912.72219418651
1
Iteration 12300: Loss = -10912.693891390489
2
Iteration 12400: Loss = -10912.689337217806
3
Iteration 12500: Loss = -10912.693816136592
4
Iteration 12600: Loss = -10912.734730196727
5
Iteration 12700: Loss = -10912.686730181043
6
Iteration 12800: Loss = -10912.686483062173
Iteration 12900: Loss = -10912.688526667618
1
Iteration 13000: Loss = -10912.686435816111
Iteration 13100: Loss = -10912.687938928795
1
Iteration 13200: Loss = -10912.686426756054
Iteration 13300: Loss = -10912.68675987289
1
Iteration 13400: Loss = -10912.686408452242
Iteration 13500: Loss = -10912.689538475446
1
Iteration 13600: Loss = -10912.686397299909
Iteration 13700: Loss = -10912.713384895349
1
Iteration 13800: Loss = -10912.68641873944
Iteration 13900: Loss = -10912.698895895817
1
Iteration 14000: Loss = -10912.70058304488
2
Iteration 14100: Loss = -10912.76889699907
3
Iteration 14200: Loss = -10912.686373780063
Iteration 14300: Loss = -10912.691232425788
1
Iteration 14400: Loss = -10912.703838528043
2
Iteration 14500: Loss = -10912.793028065533
3
Iteration 14600: Loss = -10912.698624408187
4
Iteration 14700: Loss = -10912.686065349286
Iteration 14800: Loss = -10912.687610859268
1
Iteration 14900: Loss = -10912.718527434088
2
Iteration 15000: Loss = -10912.687650967355
3
Iteration 15100: Loss = -10912.686955033334
4
Iteration 15200: Loss = -10912.686079038247
Iteration 15300: Loss = -10912.763285255398
1
Iteration 15400: Loss = -10912.686015967265
Iteration 15500: Loss = -10912.687076626313
1
Iteration 15600: Loss = -10912.686037510626
Iteration 15700: Loss = -10912.686130650289
Iteration 15800: Loss = -10912.696574480791
1
Iteration 15900: Loss = -10912.686027794907
Iteration 16000: Loss = -10912.68666542965
1
Iteration 16100: Loss = -10912.699559595301
2
Iteration 16200: Loss = -10912.698631385336
3
Iteration 16300: Loss = -10912.686191044431
4
Iteration 16400: Loss = -10912.686062993125
Iteration 16500: Loss = -10912.68866165786
1
Iteration 16600: Loss = -10912.685699249138
Iteration 16700: Loss = -10912.685757472227
Iteration 16800: Loss = -10912.685685985489
Iteration 16900: Loss = -10912.685791228885
1
Iteration 17000: Loss = -10912.685683079497
Iteration 17100: Loss = -10912.686189399796
1
Iteration 17200: Loss = -10912.688196880586
2
Iteration 17300: Loss = -10912.690474256524
3
Iteration 17400: Loss = -10912.690280577901
4
Iteration 17500: Loss = -10912.68570858097
Iteration 17600: Loss = -10912.692107543844
1
Iteration 17700: Loss = -10912.685655163246
Iteration 17800: Loss = -10912.689480876305
1
Iteration 17900: Loss = -10912.685692407003
Iteration 18000: Loss = -10912.686116106393
1
Iteration 18100: Loss = -10912.687205341359
2
Iteration 18200: Loss = -10912.705837951884
3
Iteration 18300: Loss = -10912.685682521605
Iteration 18400: Loss = -10912.6883159752
1
Iteration 18500: Loss = -10912.685695381228
Iteration 18600: Loss = -10912.694380623507
1
Iteration 18700: Loss = -10912.685680043733
Iteration 18800: Loss = -10912.686989968422
1
Iteration 18900: Loss = -10912.69465476739
2
Iteration 19000: Loss = -10912.685700776592
Iteration 19100: Loss = -10912.691600181577
1
Iteration 19200: Loss = -10912.685663058637
Iteration 19300: Loss = -10912.686361495505
1
Iteration 19400: Loss = -10912.687957872695
2
Iteration 19500: Loss = -10912.68571625954
Iteration 19600: Loss = -10912.691065127918
1
Iteration 19700: Loss = -10912.705843904017
2
Iteration 19800: Loss = -10912.68568149628
Iteration 19900: Loss = -10912.686103122509
1
pi: tensor([[0.7836, 0.2164],
        [0.1877, 0.8123]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4479, 0.5521], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1963, 0.1022],
         [0.6894, 0.2472]],

        [[0.5982, 0.1113],
         [0.5451, 0.5344]],

        [[0.5166, 0.0941],
         [0.6191, 0.5959]],

        [[0.6338, 0.1003],
         [0.6002, 0.5743]],

        [[0.7200, 0.1038],
         [0.6043, 0.5940]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 92
Adjusted Rand Index: 0.7026262626262626
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026076198471212
Global Adjusted Rand Index: 0.8096143695178135
Average Adjusted Rand Index: 0.8106614486682083
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22908.033236519095
Iteration 100: Loss = -11058.346387947957
Iteration 200: Loss = -11057.756041644447
Iteration 300: Loss = -11057.534676337427
Iteration 400: Loss = -11057.37112791517
Iteration 500: Loss = -11057.23295959252
Iteration 600: Loss = -11057.10900727134
Iteration 700: Loss = -11057.006616532059
Iteration 800: Loss = -11056.93487264046
Iteration 900: Loss = -11056.876841167967
Iteration 1000: Loss = -11056.818601028473
Iteration 1100: Loss = -11056.753902068516
Iteration 1200: Loss = -11056.679502322175
Iteration 1300: Loss = -11056.59607848122
Iteration 1400: Loss = -11056.508041256375
Iteration 1500: Loss = -11056.4183254703
Iteration 1600: Loss = -11056.322123395286
Iteration 1700: Loss = -11056.181678660114
Iteration 1800: Loss = -11055.543389356955
Iteration 1900: Loss = -11053.611856875532
Iteration 2000: Loss = -11052.622463455864
Iteration 2100: Loss = -11052.0322297933
Iteration 2200: Loss = -11050.931543384797
Iteration 2300: Loss = -11050.019619202818
Iteration 2400: Loss = -11049.5929187287
Iteration 2500: Loss = -11049.428302883844
Iteration 2600: Loss = -11049.329724043339
Iteration 2700: Loss = -11049.245468476285
Iteration 2800: Loss = -11049.151466864747
Iteration 2900: Loss = -11049.056218622238
Iteration 3000: Loss = -11048.987950502495
Iteration 3100: Loss = -11048.947072611087
Iteration 3200: Loss = -11048.921350263636
Iteration 3300: Loss = -11048.903617125046
Iteration 3400: Loss = -11048.890630882419
Iteration 3500: Loss = -11048.880696135648
Iteration 3600: Loss = -11048.872717587425
Iteration 3700: Loss = -11048.866232278202
Iteration 3800: Loss = -11048.86078436259
Iteration 3900: Loss = -11048.856127674135
Iteration 4000: Loss = -11048.852087050751
Iteration 4100: Loss = -11048.848734826443
Iteration 4200: Loss = -11048.84584283199
Iteration 4300: Loss = -11048.843308561693
Iteration 4400: Loss = -11048.841080717733
Iteration 4500: Loss = -11048.839134203776
Iteration 4600: Loss = -11048.837343200335
Iteration 4700: Loss = -11048.835760812106
Iteration 4800: Loss = -11048.83435060421
Iteration 4900: Loss = -11048.833012503907
Iteration 5000: Loss = -11048.831867200664
Iteration 5100: Loss = -11048.830729095003
Iteration 5200: Loss = -11048.82970063279
Iteration 5300: Loss = -11048.828742829874
Iteration 5400: Loss = -11048.829687380492
1
Iteration 5500: Loss = -11048.827043740703
Iteration 5600: Loss = -11048.82633489506
Iteration 5700: Loss = -11048.82569716296
Iteration 5800: Loss = -11048.825027441955
Iteration 5900: Loss = -11048.82456362647
Iteration 6000: Loss = -11048.823918791706
Iteration 6100: Loss = -11048.823381942142
Iteration 6200: Loss = -11048.8229710813
Iteration 6300: Loss = -11048.822453478928
Iteration 6400: Loss = -11048.822227516335
Iteration 6500: Loss = -11048.821645538352
Iteration 6600: Loss = -11048.821317671203
Iteration 6700: Loss = -11048.821052636544
Iteration 6800: Loss = -11048.820676616526
Iteration 6900: Loss = -11048.820379132148
Iteration 7000: Loss = -11048.820094995945
Iteration 7100: Loss = -11048.819878135022
Iteration 7200: Loss = -11048.819611281226
Iteration 7300: Loss = -11048.819457169178
Iteration 7400: Loss = -11048.819216212338
Iteration 7500: Loss = -11048.819012158254
Iteration 7600: Loss = -11048.818829131293
Iteration 7700: Loss = -11048.81866760615
Iteration 7800: Loss = -11048.81848706897
Iteration 7900: Loss = -11048.818346701883
Iteration 8000: Loss = -11048.818761350847
1
Iteration 8100: Loss = -11048.818506002837
2
Iteration 8200: Loss = -11048.82019622948
3
Iteration 8300: Loss = -11048.817762556706
Iteration 8400: Loss = -11048.817592584215
Iteration 8500: Loss = -11048.817464364862
Iteration 8600: Loss = -11048.817453260715
Iteration 8700: Loss = -11048.81719592324
Iteration 8800: Loss = -11048.817443760166
1
Iteration 8900: Loss = -11048.817009873928
Iteration 9000: Loss = -11048.820556140025
1
Iteration 9100: Loss = -11048.816790473853
Iteration 9200: Loss = -11048.816658275891
Iteration 9300: Loss = -11048.816576959378
Iteration 9400: Loss = -11048.817633895073
1
Iteration 9500: Loss = -11048.81647178773
Iteration 9600: Loss = -11048.816400946995
Iteration 9700: Loss = -11048.846643892759
1
Iteration 9800: Loss = -11048.8162641956
Iteration 9900: Loss = -11048.816222908465
Iteration 10000: Loss = -11048.819277346418
1
Iteration 10100: Loss = -11048.816132600283
Iteration 10200: Loss = -11048.816073902906
Iteration 10300: Loss = -11048.816007657364
Iteration 10400: Loss = -11048.815987354055
Iteration 10500: Loss = -11048.815939002305
Iteration 10600: Loss = -11048.815859276832
Iteration 10700: Loss = -11048.81732113253
1
Iteration 10800: Loss = -11048.815792515734
Iteration 10900: Loss = -11048.815771487838
Iteration 11000: Loss = -11048.855509413097
1
Iteration 11100: Loss = -11048.815700556792
Iteration 11200: Loss = -11048.815646953564
Iteration 11300: Loss = -11048.815638294782
Iteration 11400: Loss = -11048.81561093459
Iteration 11500: Loss = -11048.815547797221
Iteration 11600: Loss = -11048.816267827777
1
Iteration 11700: Loss = -11048.815504247048
Iteration 11800: Loss = -11048.815942361454
1
Iteration 11900: Loss = -11048.815462536799
Iteration 12000: Loss = -11048.853732225158
1
Iteration 12100: Loss = -11048.81542151691
Iteration 12200: Loss = -11048.81575433729
1
Iteration 12300: Loss = -11048.815433960654
Iteration 12400: Loss = -11048.815350800414
Iteration 12500: Loss = -11048.870138421695
1
Iteration 12600: Loss = -11048.815319852418
Iteration 12700: Loss = -11048.81529294345
Iteration 12800: Loss = -11048.828788148674
1
Iteration 12900: Loss = -11048.81522970052
Iteration 13000: Loss = -11048.847877163806
1
Iteration 13100: Loss = -11048.81518924656
Iteration 13200: Loss = -11048.8155892502
1
Iteration 13300: Loss = -11048.829210708196
2
Iteration 13400: Loss = -11048.815107356952
Iteration 13500: Loss = -11048.815770448311
1
Iteration 13600: Loss = -11048.81509257713
Iteration 13700: Loss = -11048.820750591582
1
Iteration 13800: Loss = -11048.815072578096
Iteration 13900: Loss = -11048.878876417575
1
Iteration 14000: Loss = -11048.815080219385
Iteration 14100: Loss = -11048.817181825754
1
Iteration 14200: Loss = -11048.815051777257
Iteration 14300: Loss = -11048.815207725494
1
Iteration 14400: Loss = -11048.837422699746
2
Iteration 14500: Loss = -11048.815031443542
Iteration 14600: Loss = -11048.829664581235
1
Iteration 14700: Loss = -11048.815060491057
Iteration 14800: Loss = -11048.820772137628
1
Iteration 14900: Loss = -11048.828105626426
2
Iteration 15000: Loss = -11048.815020960585
Iteration 15100: Loss = -11048.815258804721
1
Iteration 15200: Loss = -11048.854397968847
2
Iteration 15300: Loss = -11048.814999034752
Iteration 15400: Loss = -11048.82908880323
1
Iteration 15500: Loss = -11048.815067299045
Iteration 15600: Loss = -11048.815114825085
Iteration 15700: Loss = -11048.830945338257
1
Iteration 15800: Loss = -11048.814990557514
Iteration 15900: Loss = -11048.852425108382
1
Iteration 16000: Loss = -11048.814975788173
Iteration 16100: Loss = -11048.830707121455
1
Iteration 16200: Loss = -11048.815013014213
Iteration 16300: Loss = -11048.814995354256
Iteration 16400: Loss = -11048.815432737574
1
Iteration 16500: Loss = -11048.815017144745
Iteration 16600: Loss = -11048.822679489209
1
Iteration 16700: Loss = -11048.814988263666
Iteration 16800: Loss = -11048.815019374502
Iteration 16900: Loss = -11048.814993585995
Iteration 17000: Loss = -11048.816348499055
1
Iteration 17100: Loss = -11048.815245276766
2
Iteration 17200: Loss = -11049.082158207624
3
Iteration 17300: Loss = -11048.815018812798
Iteration 17400: Loss = -11048.904238211668
1
Iteration 17500: Loss = -11048.81499535779
Iteration 17600: Loss = -11048.818030555898
1
Iteration 17700: Loss = -11048.814994126367
Iteration 17800: Loss = -11048.81609325673
1
Iteration 17900: Loss = -11048.814984970975
Iteration 18000: Loss = -11048.815385363787
1
Iteration 18100: Loss = -11048.814958243149
Iteration 18200: Loss = -11048.815189526948
1
Iteration 18300: Loss = -11048.814951127086
Iteration 18400: Loss = -11048.817313475924
1
Iteration 18500: Loss = -11048.8149675206
Iteration 18600: Loss = -11048.88301060624
1
Iteration 18700: Loss = -11048.815708996064
2
Iteration 18800: Loss = -11048.815023477611
Iteration 18900: Loss = -11048.8171260211
1
Iteration 19000: Loss = -11048.814995855657
Iteration 19100: Loss = -11048.81502245688
Iteration 19200: Loss = -11048.817075728566
1
Iteration 19300: Loss = -11048.814944548027
Iteration 19400: Loss = -11048.816496029734
1
Iteration 19500: Loss = -11048.979143116736
2
Iteration 19600: Loss = -11048.81495535747
Iteration 19700: Loss = -11048.816873159598
1
Iteration 19800: Loss = -11048.81496136865
Iteration 19900: Loss = -11048.815036118813
pi: tensor([[9.7432e-01, 2.5676e-02],
        [9.8855e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9901, 0.0099], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1688, 0.0635],
         [0.5046, 0.2527]],

        [[0.6278, 0.2173],
         [0.7136, 0.5871]],

        [[0.5366, 0.1545],
         [0.5692, 0.5962]],

        [[0.6801, 0.0956],
         [0.6624, 0.6350]],

        [[0.6432, 0.1076],
         [0.5746, 0.6463]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.022626262626262626
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.02397039968412133
time is 3
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.044214004900005696
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.010862123580924353
Global Adjusted Rand Index: 0.023910137937500155
Average Adjusted Rand Index: 0.021223447047151688
10961.74010998401
[0.8096143695178135, 0.023910137937500155] [0.8106614486682083, 0.021223447047151688] [10912.731195629745, 11048.815009714592]
-------------------------------------
This iteration is 82
True Objective function: Loss = -10828.985165600014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22785.31152890943
Iteration 100: Loss = -10996.23770636291
Iteration 200: Loss = -10994.368696279527
Iteration 300: Loss = -10993.825333858722
Iteration 400: Loss = -10993.526498808951
Iteration 500: Loss = -10993.276985100929
Iteration 600: Loss = -10993.012151716504
Iteration 700: Loss = -10992.720512239783
Iteration 800: Loss = -10992.437765303894
Iteration 900: Loss = -10992.203840633083
Iteration 1000: Loss = -10992.012528057034
Iteration 1100: Loss = -10991.843701615811
Iteration 1200: Loss = -10991.68878588396
Iteration 1300: Loss = -10991.54581345636
Iteration 1400: Loss = -10991.415647445441
Iteration 1500: Loss = -10991.296000939525
Iteration 1600: Loss = -10991.185292042086
Iteration 1700: Loss = -10991.08570028931
Iteration 1800: Loss = -10990.994374986121
Iteration 1900: Loss = -10990.904201138554
Iteration 2000: Loss = -10990.813452489107
Iteration 2100: Loss = -10990.72289150613
Iteration 2200: Loss = -10990.634093149527
Iteration 2300: Loss = -10990.544763893535
Iteration 2400: Loss = -10990.456067157129
Iteration 2500: Loss = -10990.369093770276
Iteration 2600: Loss = -10990.276723895624
Iteration 2700: Loss = -10990.186756797755
Iteration 2800: Loss = -10990.097365971493
Iteration 2900: Loss = -10989.993276230436
Iteration 3000: Loss = -10989.874825231078
Iteration 3100: Loss = -10989.74903021452
Iteration 3200: Loss = -10989.645425672741
Iteration 3300: Loss = -10989.589956434684
Iteration 3400: Loss = -10989.566961647248
Iteration 3500: Loss = -10989.555033478176
Iteration 3600: Loss = -10989.545210898477
Iteration 3700: Loss = -10989.537354182357
Iteration 3800: Loss = -10989.52157363879
Iteration 3900: Loss = -10989.504668754247
Iteration 4000: Loss = -10989.48379026178
Iteration 4100: Loss = -10989.457314717729
Iteration 4200: Loss = -10989.418198767065
Iteration 4300: Loss = -10989.343656875479
Iteration 4400: Loss = -10989.152878135164
Iteration 4500: Loss = -10988.808116405471
Iteration 4600: Loss = -10988.705370778516
Iteration 4700: Loss = -10988.677870544965
Iteration 4800: Loss = -10988.667195131446
Iteration 4900: Loss = -10988.662000756023
Iteration 5000: Loss = -10988.659169813922
Iteration 5100: Loss = -10988.657489632023
Iteration 5200: Loss = -10988.656437405556
Iteration 5300: Loss = -10988.655732287225
Iteration 5400: Loss = -10988.655346391899
Iteration 5500: Loss = -10988.6548839928
Iteration 5600: Loss = -10988.65465549852
Iteration 5700: Loss = -10988.65449107453
Iteration 5800: Loss = -10988.654398174063
Iteration 5900: Loss = -10988.654244744725
Iteration 6000: Loss = -10988.654159317262
Iteration 6100: Loss = -10988.654107907403
Iteration 6200: Loss = -10988.654006161512
Iteration 6300: Loss = -10988.653960834205
Iteration 6400: Loss = -10988.653903452061
Iteration 6500: Loss = -10988.65386077146
Iteration 6600: Loss = -10988.653773898463
Iteration 6700: Loss = -10988.664496168907
1
Iteration 6800: Loss = -10988.65370570375
Iteration 6900: Loss = -10988.653634625693
Iteration 7000: Loss = -10988.653544493844
Iteration 7100: Loss = -10988.653539911671
Iteration 7200: Loss = -10988.653595723123
Iteration 7300: Loss = -10988.653396498508
Iteration 7400: Loss = -10988.653374438742
Iteration 7500: Loss = -10988.653315719444
Iteration 7600: Loss = -10988.654479442881
1
Iteration 7700: Loss = -10988.65318532331
Iteration 7800: Loss = -10988.655439253616
1
Iteration 7900: Loss = -10988.653341484003
2
Iteration 8000: Loss = -10988.66247000359
3
Iteration 8100: Loss = -10988.653024148894
Iteration 8200: Loss = -10988.655239205682
1
Iteration 8300: Loss = -10988.652905104342
Iteration 8400: Loss = -10988.6532151679
1
Iteration 8500: Loss = -10988.73328078556
2
Iteration 8600: Loss = -10988.65273668668
Iteration 8700: Loss = -10988.65955139885
1
Iteration 8800: Loss = -10988.652695681052
Iteration 8900: Loss = -10988.65371907992
1
Iteration 9000: Loss = -10988.65262092053
Iteration 9100: Loss = -10988.652558293581
Iteration 9200: Loss = -10988.653405703442
1
Iteration 9300: Loss = -10988.65244888629
Iteration 9400: Loss = -10988.65255868935
1
Iteration 9500: Loss = -10988.652743847002
2
Iteration 9600: Loss = -10988.653471445577
3
Iteration 9700: Loss = -10988.657363558998
4
Iteration 9800: Loss = -10988.65226679764
Iteration 9900: Loss = -10988.659613020509
1
Iteration 10000: Loss = -10988.67019931805
2
Iteration 10100: Loss = -10988.684075265302
3
Iteration 10200: Loss = -10988.652220405456
Iteration 10300: Loss = -10988.652238974797
Iteration 10400: Loss = -10988.652619593553
1
Iteration 10500: Loss = -10988.652884861072
2
Iteration 10600: Loss = -10988.654549285491
3
Iteration 10700: Loss = -10988.652642935274
4
Iteration 10800: Loss = -10988.652085787802
Iteration 10900: Loss = -10988.656419489937
1
Iteration 11000: Loss = -10988.725104929756
2
Iteration 11100: Loss = -10988.656542921764
3
Iteration 11200: Loss = -10988.655354572658
4
Iteration 11300: Loss = -10988.651951680851
Iteration 11400: Loss = -10988.6528450283
1
Iteration 11500: Loss = -10988.652803726683
2
Iteration 11600: Loss = -10988.651950596375
Iteration 11700: Loss = -10988.655198451805
1
Iteration 11800: Loss = -10988.65231066511
2
Iteration 11900: Loss = -10988.652700960258
3
Iteration 12000: Loss = -10988.653411357187
4
Iteration 12100: Loss = -10988.656545225002
5
Iteration 12200: Loss = -10988.669607386068
6
Iteration 12300: Loss = -10988.654143446489
7
Iteration 12400: Loss = -10988.669026458447
8
Iteration 12500: Loss = -10988.654880278436
9
Iteration 12600: Loss = -10988.653935894761
10
Iteration 12700: Loss = -10988.65351280453
11
Iteration 12800: Loss = -10988.711035616929
12
Iteration 12900: Loss = -10988.654292349062
13
Iteration 13000: Loss = -10988.655330256472
14
Iteration 13100: Loss = -10988.730808637154
15
Stopping early at iteration 13100 due to no improvement.
pi: tensor([[2.7098e-04, 9.9973e-01],
        [9.3032e-01, 6.9683e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2790, 0.7210], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1858, 0.1613],
         [0.5711, 0.1475]],

        [[0.7285, 0.1592],
         [0.6536, 0.6136]],

        [[0.7118, 0.1647],
         [0.7169, 0.5674]],

        [[0.5559, 0.1593],
         [0.5219, 0.5454]],

        [[0.5507, 0.1687],
         [0.6496, 0.6046]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0035001589427644908
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.011830230715888271
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.006703298474032776
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.03833197691026588
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0007399479393771224
Global Adjusted Rand Index: -6.4088573591543165e-06
Average Adjusted Rand Index: 0.008139739629746802
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25607.134133951702
Iteration 100: Loss = -10995.884233759982
Iteration 200: Loss = -10994.654614108507
Iteration 300: Loss = -10994.330216210908
Iteration 400: Loss = -10994.176084957515
Iteration 500: Loss = -10994.085026497005
Iteration 600: Loss = -10994.02476057505
Iteration 700: Loss = -10993.981212827051
Iteration 800: Loss = -10993.947282730065
Iteration 900: Loss = -10993.918546635075
Iteration 1000: Loss = -10993.891776954448
Iteration 1100: Loss = -10993.862967249548
Iteration 1200: Loss = -10993.824439120994
Iteration 1300: Loss = -10993.747390288423
Iteration 1400: Loss = -10993.422305947595
Iteration 1500: Loss = -10992.619390000176
Iteration 1600: Loss = -10992.067757277826
Iteration 1700: Loss = -10991.684255426575
Iteration 1800: Loss = -10991.410051156707
Iteration 1900: Loss = -10991.17195330861
Iteration 2000: Loss = -10990.951205811509
Iteration 2100: Loss = -10990.743132087526
Iteration 2200: Loss = -10990.538668793459
Iteration 2300: Loss = -10990.33751212688
Iteration 2400: Loss = -10990.175025173978
Iteration 2500: Loss = -10989.987951576617
Iteration 2600: Loss = -10989.759149857156
Iteration 2700: Loss = -10989.609953118283
Iteration 2800: Loss = -10989.578223685285
Iteration 2900: Loss = -10989.567730577086
Iteration 3000: Loss = -10989.55993506956
Iteration 3100: Loss = -10989.55327051097
Iteration 3200: Loss = -10989.547009460548
Iteration 3300: Loss = -10989.54095090693
Iteration 3400: Loss = -10989.534712894756
Iteration 3500: Loss = -10989.527584601366
Iteration 3600: Loss = -10989.520925264102
Iteration 3700: Loss = -10989.507236015512
Iteration 3800: Loss = -10989.491640775248
Iteration 3900: Loss = -10989.47234919921
Iteration 4000: Loss = -10989.44089151131
Iteration 4100: Loss = -10989.381667295604
Iteration 4200: Loss = -10989.215070434588
Iteration 4300: Loss = -10988.75437602439
Iteration 4400: Loss = -10988.670417192821
Iteration 4500: Loss = -10988.659316689018
Iteration 4600: Loss = -10988.656167616578
Iteration 4700: Loss = -10988.654862356972
Iteration 4800: Loss = -10988.654235478589
Iteration 4900: Loss = -10988.653961424401
Iteration 5000: Loss = -10988.653817669994
Iteration 5100: Loss = -10988.653840980567
Iteration 5200: Loss = -10988.653578616346
Iteration 5300: Loss = -10988.653592863337
Iteration 5400: Loss = -10988.653577161116
Iteration 5500: Loss = -10988.653474805775
Iteration 5600: Loss = -10988.65337185064
Iteration 5700: Loss = -10988.653398719656
Iteration 5800: Loss = -10988.653316921116
Iteration 5900: Loss = -10988.653293611444
Iteration 6000: Loss = -10988.653264991235
Iteration 6100: Loss = -10988.653185170275
Iteration 6200: Loss = -10988.653239563562
Iteration 6300: Loss = -10988.653160939735
Iteration 6400: Loss = -10988.655306536828
1
Iteration 6500: Loss = -10988.65304106217
Iteration 6600: Loss = -10988.652977476999
Iteration 6700: Loss = -10988.652967767504
Iteration 6800: Loss = -10988.652906102738
Iteration 6900: Loss = -10988.65288607147
Iteration 7000: Loss = -10988.652800257249
Iteration 7100: Loss = -10988.670718339732
1
Iteration 7200: Loss = -10988.652743006694
Iteration 7300: Loss = -10988.652666499465
Iteration 7400: Loss = -10988.652730992693
Iteration 7500: Loss = -10988.652641785618
Iteration 7600: Loss = -10988.654275283152
1
Iteration 7700: Loss = -10988.652581292505
Iteration 7800: Loss = -10988.653789134458
1
Iteration 7900: Loss = -10988.652492093124
Iteration 8000: Loss = -10988.65244110485
Iteration 8100: Loss = -10988.652485096545
Iteration 8200: Loss = -10988.653261111287
1
Iteration 8300: Loss = -10988.65233180423
Iteration 8400: Loss = -10988.660420532366
1
Iteration 8500: Loss = -10988.652310351508
Iteration 8600: Loss = -10988.660102345031
1
Iteration 8700: Loss = -10988.653572749112
2
Iteration 8800: Loss = -10988.65469663572
3
Iteration 8900: Loss = -10988.653893429087
4
Iteration 9000: Loss = -10988.658482115196
5
Iteration 9100: Loss = -10988.654771563506
6
Iteration 9200: Loss = -10988.652069605685
Iteration 9300: Loss = -10988.65499840698
1
Iteration 9400: Loss = -10988.673371925643
2
Iteration 9500: Loss = -10988.671986033252
3
Iteration 9600: Loss = -10988.665794170924
4
Iteration 9700: Loss = -10988.660853374095
5
Iteration 9800: Loss = -10988.667577619759
6
Iteration 9900: Loss = -10988.715252757236
7
Iteration 10000: Loss = -10988.660294444491
8
Iteration 10100: Loss = -10988.669700137032
9
Iteration 10200: Loss = -10988.65189133616
Iteration 10300: Loss = -10988.652081236878
1
Iteration 10400: Loss = -10988.651820735082
Iteration 10500: Loss = -10988.651860880149
Iteration 10600: Loss = -10988.652266477471
1
Iteration 10700: Loss = -10988.651943027711
Iteration 10800: Loss = -10988.652301649714
1
Iteration 10900: Loss = -10988.65447869153
2
Iteration 11000: Loss = -10988.655505192426
3
Iteration 11100: Loss = -10988.652333661483
4
Iteration 11200: Loss = -10988.651999297892
Iteration 11300: Loss = -10988.655155996727
1
Iteration 11400: Loss = -10988.691131657144
2
Iteration 11500: Loss = -10988.725895921538
3
Iteration 11600: Loss = -10988.746096870876
4
Iteration 11700: Loss = -10988.653973507446
5
Iteration 11800: Loss = -10988.652859661917
6
Iteration 11900: Loss = -10988.656018946098
7
Iteration 12000: Loss = -10988.797857611276
8
Iteration 12100: Loss = -10988.669127684565
9
Iteration 12200: Loss = -10988.653659965275
10
Iteration 12300: Loss = -10988.663493383157
11
Iteration 12400: Loss = -10988.663572560814
12
Iteration 12500: Loss = -10988.657158239532
13
Iteration 12600: Loss = -10988.685054561416
14
Iteration 12700: Loss = -10988.666689872194
15
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[7.0587e-02, 9.2941e-01],
        [9.9982e-01, 1.8234e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7174, 0.2826], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1461, 0.1610],
         [0.6482, 0.1871]],

        [[0.5086, 0.1600],
         [0.6639, 0.6154]],

        [[0.5070, 0.1645],
         [0.7235, 0.5696]],

        [[0.6343, 0.1601],
         [0.6562, 0.5475]],

        [[0.5061, 0.1682],
         [0.6773, 0.5536]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0005004210860358107
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.011830230715888271
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.006703298474032776
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.03833197691026588
time is 4
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.005249744559409022
Global Adjusted Rand Index: 0.0007688179093023478
Average Adjusted Rand Index: 0.009641646525098916
10828.985165600014
[-6.4088573591543165e-06, 0.0007688179093023478] [0.008139739629746802, 0.009641646525098916] [10988.730808637154, 10988.666689872194]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11109.371720159359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24432.92526032967
Iteration 100: Loss = -11155.898295131727
Iteration 200: Loss = -11154.763076613828
Iteration 300: Loss = -11154.398201515058
Iteration 400: Loss = -11154.231157443099
Iteration 500: Loss = -11154.022056469877
Iteration 600: Loss = -11153.777832181073
Iteration 700: Loss = -11153.536513420113
Iteration 800: Loss = -11153.333117378437
Iteration 900: Loss = -11153.253355397708
Iteration 1000: Loss = -11153.22866462419
Iteration 1100: Loss = -11153.214270945904
Iteration 1200: Loss = -11153.204188803194
Iteration 1300: Loss = -11153.196886149917
Iteration 1400: Loss = -11153.1914415609
Iteration 1500: Loss = -11153.187312132115
Iteration 1600: Loss = -11153.184141896172
Iteration 1700: Loss = -11153.181586499031
Iteration 1800: Loss = -11153.179593658773
Iteration 1900: Loss = -11153.17793532589
Iteration 2000: Loss = -11153.176599188617
Iteration 2100: Loss = -11153.175413678877
Iteration 2200: Loss = -11153.174440591842
Iteration 2300: Loss = -11153.173581011559
Iteration 2400: Loss = -11153.172826619171
Iteration 2500: Loss = -11153.172164528654
Iteration 2600: Loss = -11153.171584460595
Iteration 2700: Loss = -11153.171077142742
Iteration 2800: Loss = -11153.170578037822
Iteration 2900: Loss = -11153.170178556236
Iteration 3000: Loss = -11153.169817363372
Iteration 3100: Loss = -11153.169461229028
Iteration 3200: Loss = -11153.169139014686
Iteration 3300: Loss = -11153.168866264376
Iteration 3400: Loss = -11153.168653162707
Iteration 3500: Loss = -11153.16839745418
Iteration 3600: Loss = -11153.16818334945
Iteration 3700: Loss = -11153.167988143214
Iteration 3800: Loss = -11153.177423238949
1
Iteration 3900: Loss = -11153.167617568997
Iteration 4000: Loss = -11153.167472446607
Iteration 4100: Loss = -11153.167697000905
1
Iteration 4200: Loss = -11153.167148456841
Iteration 4300: Loss = -11153.16704343867
Iteration 4400: Loss = -11153.16689401349
Iteration 4500: Loss = -11153.166790790176
Iteration 4600: Loss = -11153.179343015843
1
Iteration 4700: Loss = -11153.166597596693
Iteration 4800: Loss = -11153.16887484624
1
Iteration 4900: Loss = -11153.171678616487
2
Iteration 5000: Loss = -11153.167142266017
3
Iteration 5100: Loss = -11153.166787129867
4
Iteration 5200: Loss = -11153.166179036938
Iteration 5300: Loss = -11153.1661771077
Iteration 5400: Loss = -11153.167175327513
1
Iteration 5500: Loss = -11153.1660013801
Iteration 5600: Loss = -11153.166130361162
1
Iteration 5700: Loss = -11153.165885273616
Iteration 5800: Loss = -11153.170260883868
1
Iteration 5900: Loss = -11153.165787162285
Iteration 6000: Loss = -11153.165738494645
Iteration 6100: Loss = -11153.16578589112
Iteration 6200: Loss = -11153.165672479581
Iteration 6300: Loss = -11153.165640262634
Iteration 6400: Loss = -11153.166067205157
1
Iteration 6500: Loss = -11153.166300370398
2
Iteration 6600: Loss = -11153.16553750341
Iteration 6700: Loss = -11153.165889999947
1
Iteration 6800: Loss = -11153.165559400817
Iteration 6900: Loss = -11153.165418963028
Iteration 7000: Loss = -11153.178702398272
1
Iteration 7100: Loss = -11153.165378654
Iteration 7200: Loss = -11153.16572672265
1
Iteration 7300: Loss = -11153.165353051723
Iteration 7400: Loss = -11153.165383512373
Iteration 7500: Loss = -11153.165290051074
Iteration 7600: Loss = -11153.16528282649
Iteration 7700: Loss = -11153.175396045895
1
Iteration 7800: Loss = -11153.16526003281
Iteration 7900: Loss = -11153.165240319098
Iteration 8000: Loss = -11153.364496314236
1
Iteration 8100: Loss = -11153.165204166722
Iteration 8200: Loss = -11153.296228495565
1
Iteration 8300: Loss = -11153.1651866
Iteration 8400: Loss = -11153.165465248367
1
Iteration 8500: Loss = -11153.165271529579
Iteration 8600: Loss = -11153.16524508835
Iteration 8700: Loss = -11153.167008205202
1
Iteration 8800: Loss = -11153.165116521835
Iteration 8900: Loss = -11153.165714815977
1
Iteration 9000: Loss = -11153.165105127895
Iteration 9100: Loss = -11153.167470162776
1
Iteration 9200: Loss = -11153.165102063358
Iteration 9300: Loss = -11153.16830526564
1
Iteration 9400: Loss = -11153.16509383292
Iteration 9500: Loss = -11153.16520212418
1
Iteration 9600: Loss = -11153.16522729278
2
Iteration 9700: Loss = -11153.38971248857
3
Iteration 9800: Loss = -11153.165625903739
4
Iteration 9900: Loss = -11153.195531474015
5
Iteration 10000: Loss = -11153.165645412477
6
Iteration 10100: Loss = -11153.2360257987
7
Iteration 10200: Loss = -11153.165920091828
8
Iteration 10300: Loss = -11153.171736747327
9
Iteration 10400: Loss = -11153.168456084184
10
Iteration 10500: Loss = -11153.178404828674
11
Iteration 10600: Loss = -11153.16543039992
12
Iteration 10700: Loss = -11153.22484669052
13
Iteration 10800: Loss = -11153.165117342536
Iteration 10900: Loss = -11153.165895041482
1
Iteration 11000: Loss = -11153.165002601972
Iteration 11100: Loss = -11153.167210062173
1
Iteration 11200: Loss = -11153.223975898149
2
Iteration 11300: Loss = -11153.167477119025
3
Iteration 11400: Loss = -11153.168598835797
4
Iteration 11500: Loss = -11153.171670330736
5
Iteration 11600: Loss = -11153.166460465152
6
Iteration 11700: Loss = -11153.16863364075
7
Iteration 11800: Loss = -11153.166225947072
8
Iteration 11900: Loss = -11153.304326832842
9
Iteration 12000: Loss = -11153.16515086356
10
Iteration 12100: Loss = -11153.183097863724
11
Iteration 12200: Loss = -11153.166467876936
12
Iteration 12300: Loss = -11153.193108081621
13
Iteration 12400: Loss = -11153.164965639757
Iteration 12500: Loss = -11153.16513979661
1
Iteration 12600: Loss = -11153.16504705965
Iteration 12700: Loss = -11153.191642079015
1
Iteration 12800: Loss = -11153.166188644618
2
Iteration 12900: Loss = -11153.165500914314
3
Iteration 13000: Loss = -11153.165209393863
4
Iteration 13100: Loss = -11153.171318258626
5
Iteration 13200: Loss = -11153.16576899957
6
Iteration 13300: Loss = -11153.175341506763
7
Iteration 13400: Loss = -11153.237685561337
8
Iteration 13500: Loss = -11153.171267419766
9
Iteration 13600: Loss = -11153.214199432465
10
Iteration 13700: Loss = -11153.166095842273
11
Iteration 13800: Loss = -11153.169616598596
12
Iteration 13900: Loss = -11153.209458183068
13
Iteration 14000: Loss = -11153.17735006722
14
Iteration 14100: Loss = -11153.296146268769
15
Stopping early at iteration 14100 due to no improvement.
pi: tensor([[8.7047e-07, 1.0000e+00],
        [3.4077e-02, 9.6592e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1900, 0.8100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2410, 0.1969],
         [0.6777, 0.1627]],

        [[0.6559, 0.1049],
         [0.7241, 0.6351]],

        [[0.6224, 0.2116],
         [0.6073, 0.5100]],

        [[0.6920, 0.1594],
         [0.6042, 0.6218]],

        [[0.6713, 0.1776],
         [0.6381, 0.5409]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.004851708247904022
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00010001874489448532
Average Adjusted Rand Index: -0.00027458864189853154
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22589.275175321152
Iteration 100: Loss = -11158.325322549657
Iteration 200: Loss = -11156.812914943732
Iteration 300: Loss = -11155.993616927122
Iteration 400: Loss = -11155.113590750734
Iteration 500: Loss = -11154.626606832335
Iteration 600: Loss = -11154.389396766841
Iteration 700: Loss = -11154.215844323693
Iteration 800: Loss = -11154.022371824176
Iteration 900: Loss = -11153.25818459242
Iteration 1000: Loss = -11152.140574671253
Iteration 1100: Loss = -11151.564548929455
Iteration 1200: Loss = -11151.190367545023
Iteration 1300: Loss = -11150.937848390977
Iteration 1400: Loss = -11150.7139046772
Iteration 1500: Loss = -11150.484721733383
Iteration 1600: Loss = -11150.340393159164
Iteration 1700: Loss = -11150.214736272634
Iteration 1800: Loss = -11150.10275355588
Iteration 1900: Loss = -11150.045828367958
Iteration 2000: Loss = -11150.00349080116
Iteration 2100: Loss = -11149.965633784897
Iteration 2200: Loss = -11149.93908125894
Iteration 2300: Loss = -11149.917688317588
Iteration 2400: Loss = -11149.902553480562
Iteration 2500: Loss = -11149.887886325081
Iteration 2600: Loss = -11149.878461549637
Iteration 2700: Loss = -11149.873057447034
Iteration 2800: Loss = -11149.869141269546
Iteration 2900: Loss = -11149.86475506025
Iteration 3000: Loss = -11149.859680852656
Iteration 3100: Loss = -11149.855560728847
Iteration 3200: Loss = -11149.852378737138
Iteration 3300: Loss = -11149.84996066037
Iteration 3400: Loss = -11149.847979618153
Iteration 3500: Loss = -11149.846005035923
Iteration 3600: Loss = -11149.844006432286
Iteration 3700: Loss = -11149.841905886427
Iteration 3800: Loss = -11149.840422544707
Iteration 3900: Loss = -11149.839562608417
Iteration 4000: Loss = -11149.83887660102
Iteration 4100: Loss = -11149.838360207596
Iteration 4200: Loss = -11149.837783750245
Iteration 4300: Loss = -11149.837300921978
Iteration 4400: Loss = -11149.8368281295
Iteration 4500: Loss = -11149.836354718245
Iteration 4600: Loss = -11149.83588461339
Iteration 4700: Loss = -11149.83555458088
Iteration 4800: Loss = -11149.835065822313
Iteration 4900: Loss = -11149.834613162962
Iteration 5000: Loss = -11149.834244591497
Iteration 5100: Loss = -11149.833838411452
Iteration 5200: Loss = -11149.836827018557
1
Iteration 5300: Loss = -11149.836057259268
2
Iteration 5400: Loss = -11149.832715232384
Iteration 5500: Loss = -11149.832372106623
Iteration 5600: Loss = -11149.831965401107
Iteration 5700: Loss = -11149.831658019839
Iteration 5800: Loss = -11149.831344940372
Iteration 5900: Loss = -11149.831020441361
Iteration 6000: Loss = -11149.8307526644
Iteration 6100: Loss = -11149.830573496554
Iteration 6200: Loss = -11149.83014860331
Iteration 6300: Loss = -11149.82988903946
Iteration 6400: Loss = -11149.829634939346
Iteration 6500: Loss = -11149.830665543444
1
Iteration 6600: Loss = -11149.829122494642
Iteration 6700: Loss = -11149.828860184325
Iteration 6800: Loss = -11149.828676264833
Iteration 6900: Loss = -11149.828446271347
Iteration 7000: Loss = -11149.828261969737
Iteration 7100: Loss = -11149.828024849188
Iteration 7200: Loss = -11149.829826771962
1
Iteration 7300: Loss = -11149.82767036431
Iteration 7400: Loss = -11149.83862881725
1
Iteration 7500: Loss = -11149.827290374276
Iteration 7600: Loss = -11149.827357186765
Iteration 7700: Loss = -11149.826993725805
Iteration 7800: Loss = -11149.82688478533
Iteration 7900: Loss = -11149.82670656221
Iteration 8000: Loss = -11149.832097563774
1
Iteration 8100: Loss = -11149.826634488913
Iteration 8200: Loss = -11149.826437687741
Iteration 8300: Loss = -11149.826298741076
Iteration 8400: Loss = -11149.82629120653
Iteration 8500: Loss = -11149.827499805273
1
Iteration 8600: Loss = -11149.826080903713
Iteration 8700: Loss = -11149.825752634395
Iteration 8800: Loss = -11149.82625559882
1
Iteration 8900: Loss = -11149.825552241511
Iteration 9000: Loss = -11149.825576925337
Iteration 9100: Loss = -11149.825373757818
Iteration 9200: Loss = -11149.897078619684
1
Iteration 9300: Loss = -11149.825255028158
Iteration 9400: Loss = -11149.825153850474
Iteration 9500: Loss = -11149.834629833163
1
Iteration 9600: Loss = -11149.825057126796
Iteration 9700: Loss = -11149.824960439431
Iteration 9800: Loss = -11149.825124140514
1
Iteration 9900: Loss = -11149.824870300925
Iteration 10000: Loss = -11149.824980277996
1
Iteration 10100: Loss = -11149.824850605608
Iteration 10200: Loss = -11149.824688918761
Iteration 10300: Loss = -11149.84930815545
1
Iteration 10400: Loss = -11149.82466740434
Iteration 10500: Loss = -11149.824577307128
Iteration 10600: Loss = -11150.395919866618
1
Iteration 10700: Loss = -11149.8245307723
Iteration 10800: Loss = -11149.82443662219
Iteration 10900: Loss = -11149.833782222147
1
Iteration 11000: Loss = -11149.824398900262
Iteration 11100: Loss = -11149.852084244323
1
Iteration 11200: Loss = -11149.824337828266
Iteration 11300: Loss = -11149.898849977468
1
Iteration 11400: Loss = -11149.82427094269
Iteration 11500: Loss = -11149.824486248423
1
Iteration 11600: Loss = -11149.824270292625
Iteration 11700: Loss = -11149.82417860266
Iteration 11800: Loss = -11149.82464297727
1
Iteration 11900: Loss = -11149.824136579644
Iteration 12000: Loss = -11149.8258657543
1
Iteration 12100: Loss = -11149.824093739879
Iteration 12200: Loss = -11149.824602447412
1
Iteration 12300: Loss = -11149.82408804231
Iteration 12400: Loss = -11149.824712284151
1
Iteration 12500: Loss = -11149.824038169092
Iteration 12600: Loss = -11149.82420034023
1
Iteration 12700: Loss = -11149.824013286296
Iteration 12800: Loss = -11149.909830427026
1
Iteration 12900: Loss = -11149.823983841572
Iteration 13000: Loss = -11149.823993905471
Iteration 13100: Loss = -11149.824012670577
Iteration 13200: Loss = -11149.82394707259
Iteration 13300: Loss = -11149.824281467598
1
Iteration 13400: Loss = -11149.823920329616
Iteration 13500: Loss = -11149.824009924454
Iteration 13600: Loss = -11149.823949945023
Iteration 13700: Loss = -11149.823931545274
Iteration 13800: Loss = -11149.823921931578
Iteration 13900: Loss = -11149.823973756402
Iteration 14000: Loss = -11149.823884901305
Iteration 14100: Loss = -11149.824135154684
1
Iteration 14200: Loss = -11149.82385761313
Iteration 14300: Loss = -11149.824055575567
1
Iteration 14400: Loss = -11149.823846138832
Iteration 14500: Loss = -11149.82391556501
Iteration 14600: Loss = -11149.823865939456
Iteration 14700: Loss = -11149.824185681793
1
Iteration 14800: Loss = -11149.823803050227
Iteration 14900: Loss = -11149.826302265961
1
Iteration 15000: Loss = -11149.823818097282
Iteration 15100: Loss = -11149.851770863188
1
Iteration 15200: Loss = -11149.823796063596
Iteration 15300: Loss = -11149.823812240262
Iteration 15400: Loss = -11149.823815438867
Iteration 15500: Loss = -11149.823821207021
Iteration 15600: Loss = -11149.824214820266
1
Iteration 15700: Loss = -11149.823774337918
Iteration 15800: Loss = -11149.824449905818
1
Iteration 15900: Loss = -11149.82376639324
Iteration 16000: Loss = -11149.824155271963
1
Iteration 16100: Loss = -11149.823765730482
Iteration 16200: Loss = -11149.823896303615
1
Iteration 16300: Loss = -11149.824587155523
2
Iteration 16400: Loss = -11149.823840410845
Iteration 16500: Loss = -11149.82377300156
Iteration 16600: Loss = -11149.823808701402
Iteration 16700: Loss = -11149.82376820768
Iteration 16800: Loss = -11149.82404069041
1
Iteration 16900: Loss = -11149.823738608333
Iteration 17000: Loss = -11149.832245692704
1
Iteration 17100: Loss = -11149.823772891166
Iteration 17200: Loss = -11150.175170443614
1
Iteration 17300: Loss = -11149.82374410282
Iteration 17400: Loss = -11149.825461317258
1
Iteration 17500: Loss = -11149.823747768483
Iteration 17600: Loss = -11149.82388781284
1
Iteration 17700: Loss = -11149.823803755924
Iteration 17800: Loss = -11149.823869678266
Iteration 17900: Loss = -11149.823833107557
Iteration 18000: Loss = -11149.823731238464
Iteration 18100: Loss = -11149.824043133709
1
Iteration 18200: Loss = -11149.823718038962
Iteration 18300: Loss = -11149.823905165973
1
Iteration 18400: Loss = -11149.823785541896
Iteration 18500: Loss = -11149.823721292823
Iteration 18600: Loss = -11149.951289383707
1
Iteration 18700: Loss = -11149.823723254867
Iteration 18800: Loss = -11149.842256791379
1
Iteration 18900: Loss = -11149.823712481664
Iteration 19000: Loss = -11149.838695541948
1
Iteration 19100: Loss = -11149.823736412727
Iteration 19200: Loss = -11149.849208494246
1
Iteration 19300: Loss = -11149.82369991458
Iteration 19400: Loss = -11149.96651953681
1
Iteration 19500: Loss = -11149.823694964618
Iteration 19600: Loss = -11149.90177292055
1
Iteration 19700: Loss = -11149.823731942515
Iteration 19800: Loss = -11149.876704742184
1
Iteration 19900: Loss = -11149.823724175843
pi: tensor([[7.8350e-01, 2.1650e-01],
        [3.7213e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2172, 0.7828], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2426, 0.1980],
         [0.6611, 0.1582]],

        [[0.7120, 0.1825],
         [0.6786, 0.6145]],

        [[0.5192, 0.1970],
         [0.5925, 0.6314]],

        [[0.6477, 0.1615],
         [0.6659, 0.6024]],

        [[0.7036, 0.1878],
         [0.5661, 0.5137]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0045595124595072724
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0037746410354473374
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.02748058421094438
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01851330918808678
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.007431593898976406
Global Adjusted Rand Index: 0.011127707073219237
Average Adjusted Rand Index: 0.00901826676061059
11109.371720159359
[0.00010001874489448532, 0.011127707073219237] [-0.00027458864189853154, 0.00901826676061059] [11153.296146268769, 11149.828315114471]
-------------------------------------
This iteration is 84
True Objective function: Loss = -10929.823683882372
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22662.85903351053
Iteration 100: Loss = -11048.811990873373
Iteration 200: Loss = -11048.182299218284
Iteration 300: Loss = -11048.001557435266
Iteration 400: Loss = -11047.923827550278
Iteration 500: Loss = -11047.879435067827
Iteration 600: Loss = -11047.849340222761
Iteration 700: Loss = -11047.826669354208
Iteration 800: Loss = -11047.807973510706
Iteration 900: Loss = -11047.791092931686
Iteration 1000: Loss = -11047.773550789023
Iteration 1100: Loss = -11047.749817064887
Iteration 1200: Loss = -11047.693511293573
Iteration 1300: Loss = -11047.447106742335
Iteration 1400: Loss = -11047.229994340976
Iteration 1500: Loss = -11047.145039798172
Iteration 1600: Loss = -11047.09580319707
Iteration 1700: Loss = -11047.062286916187
Iteration 1800: Loss = -11047.03755388458
Iteration 1900: Loss = -11047.017887127122
Iteration 2000: Loss = -11047.001342743073
Iteration 2100: Loss = -11046.986374081946
Iteration 2200: Loss = -11046.97183266926
Iteration 2300: Loss = -11046.955985804198
Iteration 2400: Loss = -11046.935933287146
Iteration 2500: Loss = -11046.902669800527
Iteration 2600: Loss = -11046.830398534581
Iteration 2700: Loss = -11046.713053629095
Iteration 2800: Loss = -11046.608045370496
Iteration 2900: Loss = -11046.524292725155
Iteration 3000: Loss = -11046.453174833348
Iteration 3100: Loss = -11046.395147102736
Iteration 3200: Loss = -11046.351145191422
Iteration 3300: Loss = -11046.31831118372
Iteration 3400: Loss = -11046.29340196732
Iteration 3500: Loss = -11046.274547526495
Iteration 3600: Loss = -11046.259882264045
Iteration 3700: Loss = -11046.248200176591
Iteration 3800: Loss = -11046.23870860137
Iteration 3900: Loss = -11046.230926512253
Iteration 4000: Loss = -11046.224465860525
Iteration 4100: Loss = -11046.218877232955
Iteration 4200: Loss = -11046.214070277372
Iteration 4300: Loss = -11046.209865995943
Iteration 4400: Loss = -11046.206187206253
Iteration 4500: Loss = -11046.202954168217
Iteration 4600: Loss = -11046.200125456846
Iteration 4700: Loss = -11046.197572855779
Iteration 4800: Loss = -11046.195162455479
Iteration 4900: Loss = -11046.19301128104
Iteration 5000: Loss = -11046.190951491231
Iteration 5100: Loss = -11046.189079388336
Iteration 5200: Loss = -11046.187330504346
Iteration 5300: Loss = -11046.18575143797
Iteration 5400: Loss = -11046.184279022253
Iteration 5500: Loss = -11046.182949586218
Iteration 5600: Loss = -11046.181711860632
Iteration 5700: Loss = -11046.180556265243
Iteration 5800: Loss = -11046.179513228424
Iteration 5900: Loss = -11046.178554490734
Iteration 6000: Loss = -11046.17757868694
Iteration 6100: Loss = -11046.17670021419
Iteration 6200: Loss = -11046.17583638927
Iteration 6300: Loss = -11046.17505396524
Iteration 6400: Loss = -11046.17429929613
Iteration 6500: Loss = -11046.173576639676
Iteration 6600: Loss = -11046.172892405451
Iteration 6700: Loss = -11046.172285839106
Iteration 6800: Loss = -11046.171679615178
Iteration 6900: Loss = -11046.171086961576
Iteration 7000: Loss = -11046.170596053098
Iteration 7100: Loss = -11046.170044677347
Iteration 7200: Loss = -11046.169552225681
Iteration 7300: Loss = -11046.169205924087
Iteration 7400: Loss = -11046.168714586289
Iteration 7500: Loss = -11046.168329547045
Iteration 7600: Loss = -11046.167901160266
Iteration 7700: Loss = -11046.167496906184
Iteration 7800: Loss = -11046.167193470683
Iteration 7900: Loss = -11046.166840694563
Iteration 8000: Loss = -11046.166699684387
Iteration 8100: Loss = -11046.166284361358
Iteration 8200: Loss = -11046.16589275265
Iteration 8300: Loss = -11046.183435009427
1
Iteration 8400: Loss = -11046.165327518213
Iteration 8500: Loss = -11046.513191193524
1
Iteration 8600: Loss = -11046.164882534438
Iteration 8700: Loss = -11046.16468041328
Iteration 8800: Loss = -11046.172863430684
1
Iteration 8900: Loss = -11046.164282000613
Iteration 9000: Loss = -11046.164119673673
Iteration 9100: Loss = -11046.172336047664
1
Iteration 9200: Loss = -11046.163784056029
Iteration 9300: Loss = -11046.163636725916
Iteration 9400: Loss = -11046.178193916006
1
Iteration 9500: Loss = -11046.1633689097
Iteration 9600: Loss = -11046.163211081554
Iteration 9700: Loss = -11046.170008416231
1
Iteration 9800: Loss = -11046.16299367711
Iteration 9900: Loss = -11046.162873638104
Iteration 10000: Loss = -11046.246362688938
1
Iteration 10100: Loss = -11046.162661844217
Iteration 10200: Loss = -11046.16257423287
Iteration 10300: Loss = -11046.19013764066
1
Iteration 10400: Loss = -11046.162385846907
Iteration 10500: Loss = -11046.162234327689
Iteration 10600: Loss = -11046.162164913043
Iteration 10700: Loss = -11046.162327933027
1
Iteration 10800: Loss = -11046.162003628306
Iteration 10900: Loss = -11046.161922557088
Iteration 11000: Loss = -11046.161861368484
Iteration 11100: Loss = -11046.161973736807
1
Iteration 11200: Loss = -11046.161780142062
Iteration 11300: Loss = -11046.161742787675
Iteration 11400: Loss = -11046.162082157061
1
Iteration 11500: Loss = -11046.161651694583
Iteration 11600: Loss = -11046.161622954465
Iteration 11700: Loss = -11046.161845325923
1
Iteration 11800: Loss = -11046.161553815877
Iteration 11900: Loss = -11046.161501752462
Iteration 12000: Loss = -11046.161501575987
Iteration 12100: Loss = -11046.166837838962
1
Iteration 12200: Loss = -11046.161394655503
Iteration 12300: Loss = -11046.16138564336
Iteration 12400: Loss = -11046.206916470685
1
Iteration 12500: Loss = -11046.16135751608
Iteration 12600: Loss = -11046.161333861337
Iteration 12700: Loss = -11046.161252862514
Iteration 12800: Loss = -11046.161393081586
1
Iteration 12900: Loss = -11046.161274455037
Iteration 13000: Loss = -11046.161258152175
Iteration 13100: Loss = -11046.161928885256
1
Iteration 13200: Loss = -11046.161176434862
Iteration 13300: Loss = -11046.161152401844
Iteration 13400: Loss = -11046.303045624103
1
Iteration 13500: Loss = -11046.161110349834
Iteration 13600: Loss = -11046.161088710429
Iteration 13700: Loss = -11046.161033966862
Iteration 13800: Loss = -11046.161444794177
1
Iteration 13900: Loss = -11046.16098071471
Iteration 14000: Loss = -11046.160951216174
Iteration 14100: Loss = -11046.176089962639
1
Iteration 14200: Loss = -11046.160963794135
Iteration 14300: Loss = -11046.160927382442
Iteration 14400: Loss = -11046.160968860871
Iteration 14500: Loss = -11046.161217882442
1
Iteration 14600: Loss = -11046.16090165858
Iteration 14700: Loss = -11046.160902742758
Iteration 14800: Loss = -11046.162205370967
1
Iteration 14900: Loss = -11046.16090849039
Iteration 15000: Loss = -11046.160841264767
Iteration 15100: Loss = -11046.16121733535
1
Iteration 15200: Loss = -11046.161238164266
2
Iteration 15300: Loss = -11046.252498905124
3
Iteration 15400: Loss = -11046.161103376078
4
Iteration 15500: Loss = -11046.160809145036
Iteration 15600: Loss = -11046.1628240355
1
Iteration 15700: Loss = -11046.160784064756
Iteration 15800: Loss = -11046.162302984896
1
Iteration 15900: Loss = -11046.160823418833
Iteration 16000: Loss = -11046.160993106058
1
Iteration 16100: Loss = -11046.161987660285
2
Iteration 16200: Loss = -11046.160809609159
Iteration 16300: Loss = -11046.160927684694
1
Iteration 16400: Loss = -11046.160825418476
Iteration 16500: Loss = -11046.160804017332
Iteration 16600: Loss = -11046.16454451116
1
Iteration 16700: Loss = -11046.16083361357
Iteration 16800: Loss = -11046.160778820826
Iteration 16900: Loss = -11046.167522744345
1
Iteration 17000: Loss = -11046.16077186463
Iteration 17100: Loss = -11046.166132256834
1
Iteration 17200: Loss = -11046.160743455257
Iteration 17300: Loss = -11046.174050155654
1
Iteration 17400: Loss = -11046.160797880995
Iteration 17500: Loss = -11046.231710484182
1
Iteration 17600: Loss = -11046.160718597113
Iteration 17700: Loss = -11046.166382496922
1
Iteration 17800: Loss = -11046.160738958453
Iteration 17900: Loss = -11046.16264776725
1
Iteration 18000: Loss = -11046.162830338644
2
Iteration 18100: Loss = -11046.176625261804
3
Iteration 18200: Loss = -11046.161804202318
4
Iteration 18300: Loss = -11046.448387925662
5
Iteration 18400: Loss = -11046.160753507695
Iteration 18500: Loss = -11046.161150145728
1
Iteration 18600: Loss = -11046.16108575429
2
Iteration 18700: Loss = -11046.347900048266
3
Iteration 18800: Loss = -11046.160658866565
Iteration 18900: Loss = -11046.184642231223
1
Iteration 19000: Loss = -11046.160636884222
Iteration 19100: Loss = -11046.164926878118
1
Iteration 19200: Loss = -11046.160638017915
Iteration 19300: Loss = -11046.162485281075
1
Iteration 19400: Loss = -11046.160612898122
Iteration 19500: Loss = -11046.16919007564
1
Iteration 19600: Loss = -11046.160628099093
Iteration 19700: Loss = -11046.172350383698
1
Iteration 19800: Loss = -11046.16063286305
Iteration 19900: Loss = -11046.161105651492
1
pi: tensor([[9.9996e-01, 3.8615e-05],
        [4.2159e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0183, 0.9817], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1890, 0.2053],
         [0.5239, 0.1630]],

        [[0.6548, 0.1528],
         [0.6749, 0.6263]],

        [[0.6310, 0.2678],
         [0.5939, 0.7164]],

        [[0.6657, 0.1901],
         [0.5026, 0.5963]],

        [[0.5718, 0.1419],
         [0.6715, 0.5050]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.00035341041046094813
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
Global Adjusted Rand Index: 0.0001102299408497439
Average Adjusted Rand Index: -0.0019259905852283359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23569.595441091336
Iteration 100: Loss = -11049.559248703346
Iteration 200: Loss = -11048.417864815357
Iteration 300: Loss = -11047.982386219961
Iteration 400: Loss = -11047.753109473768
Iteration 500: Loss = -11047.580620231296
Iteration 600: Loss = -11047.446523176206
Iteration 700: Loss = -11047.34885975186
Iteration 800: Loss = -11047.282899092103
Iteration 900: Loss = -11047.23562608213
Iteration 1000: Loss = -11047.200699150426
Iteration 1100: Loss = -11047.173830400145
Iteration 1200: Loss = -11047.152583722524
Iteration 1300: Loss = -11047.13525480023
Iteration 1400: Loss = -11047.120925901498
Iteration 1500: Loss = -11047.108791957742
Iteration 1600: Loss = -11047.098263330168
Iteration 1700: Loss = -11047.088900178507
Iteration 1800: Loss = -11047.080291982731
Iteration 1900: Loss = -11047.072264150382
Iteration 2000: Loss = -11047.064515774382
Iteration 2100: Loss = -11047.056948662483
Iteration 2200: Loss = -11047.04935550783
Iteration 2300: Loss = -11047.041638134016
Iteration 2400: Loss = -11047.033486812757
Iteration 2500: Loss = -11047.024767721243
Iteration 2600: Loss = -11047.015173525197
Iteration 2700: Loss = -11047.004273695164
Iteration 2800: Loss = -11046.991343732629
Iteration 2900: Loss = -11046.975376935357
Iteration 3000: Loss = -11046.954248535207
Iteration 3100: Loss = -11046.922907697191
Iteration 3200: Loss = -11046.869115454178
Iteration 3300: Loss = -11046.784929931331
Iteration 3400: Loss = -11046.709046550815
Iteration 3500: Loss = -11046.642422604375
Iteration 3600: Loss = -11046.408012729276
Iteration 3700: Loss = -11046.157492753662
Iteration 3800: Loss = -11046.067157032476
Iteration 3900: Loss = -11046.022839803965
Iteration 4000: Loss = -11045.99639915311
Iteration 4100: Loss = -11045.979802710923
Iteration 4200: Loss = -11045.968431121779
Iteration 4300: Loss = -11045.960089444101
Iteration 4400: Loss = -11045.953700750777
Iteration 4500: Loss = -11045.948457418675
Iteration 4600: Loss = -11045.943840078435
Iteration 4700: Loss = -11045.939266513726
Iteration 4800: Loss = -11045.93552033784
Iteration 4900: Loss = -11045.93277225141
Iteration 5000: Loss = -11045.930470878891
Iteration 5100: Loss = -11045.928367281225
Iteration 5200: Loss = -11045.926387027373
Iteration 5300: Loss = -11045.924817593499
Iteration 5400: Loss = -11045.923247546361
Iteration 5500: Loss = -11045.92200803755
Iteration 5600: Loss = -11045.920851432713
Iteration 5700: Loss = -11045.919739621695
Iteration 5800: Loss = -11045.918662674427
Iteration 5900: Loss = -11045.91769575397
Iteration 6000: Loss = -11045.916916972197
Iteration 6100: Loss = -11045.916278078765
Iteration 6200: Loss = -11045.915661168146
Iteration 6300: Loss = -11045.915071482443
Iteration 6400: Loss = -11045.914586120582
Iteration 6500: Loss = -11045.914129751007
Iteration 6600: Loss = -11045.913687978107
Iteration 6700: Loss = -11045.913265177189
Iteration 6800: Loss = -11045.915644911553
1
Iteration 6900: Loss = -11045.912518947964
Iteration 7000: Loss = -11045.912148850233
Iteration 7100: Loss = -11045.911999461221
Iteration 7200: Loss = -11045.911494011918
Iteration 7300: Loss = -11045.911280260478
Iteration 7400: Loss = -11045.910989611084
Iteration 7500: Loss = -11045.91077297971
Iteration 7600: Loss = -11045.910594362851
Iteration 7700: Loss = -11045.910354702368
Iteration 7800: Loss = -11045.910186631729
Iteration 7900: Loss = -11045.910050435707
Iteration 8000: Loss = -11045.909896179155
Iteration 8100: Loss = -11045.910037076865
1
Iteration 8200: Loss = -11045.915275941952
2
Iteration 8300: Loss = -11045.911550584386
3
Iteration 8400: Loss = -11045.910520037123
4
Iteration 8500: Loss = -11045.909236625725
Iteration 8600: Loss = -11045.909107378911
Iteration 8700: Loss = -11045.908968384063
Iteration 8800: Loss = -11045.908887421152
Iteration 8900: Loss = -11045.92758923401
1
Iteration 9000: Loss = -11045.908672479087
Iteration 9100: Loss = -11045.908606007584
Iteration 9200: Loss = -11045.968581565692
1
Iteration 9300: Loss = -11045.908448719114
Iteration 9400: Loss = -11045.908361204763
Iteration 9500: Loss = -11045.910118486581
1
Iteration 9600: Loss = -11045.908276246999
Iteration 9700: Loss = -11045.908172230855
Iteration 9800: Loss = -11045.917072024355
1
Iteration 9900: Loss = -11045.908068985329
Iteration 10000: Loss = -11045.908015954834
Iteration 10100: Loss = -11045.908048533174
Iteration 10200: Loss = -11045.908023854441
Iteration 10300: Loss = -11045.907905665841
Iteration 10400: Loss = -11045.907881164345
Iteration 10500: Loss = -11045.90841580433
1
Iteration 10600: Loss = -11045.907767518693
Iteration 10700: Loss = -11045.907734373102
Iteration 10800: Loss = -11045.90868313559
1
Iteration 10900: Loss = -11045.907719740871
Iteration 11000: Loss = -11045.90766109308
Iteration 11100: Loss = -11045.908463859365
1
Iteration 11200: Loss = -11045.907604322756
Iteration 11300: Loss = -11045.907565451831
Iteration 11400: Loss = -11045.91017494347
1
Iteration 11500: Loss = -11045.907554449694
Iteration 11600: Loss = -11045.907534867238
Iteration 11700: Loss = -11045.907499320247
Iteration 11800: Loss = -11045.907519776405
Iteration 11900: Loss = -11045.907437177924
Iteration 12000: Loss = -11045.907434543984
Iteration 12100: Loss = -11045.91070173064
1
Iteration 12200: Loss = -11045.907404738517
Iteration 12300: Loss = -11045.907476323202
Iteration 12400: Loss = -11045.912607666685
1
Iteration 12500: Loss = -11045.907366501018
Iteration 12600: Loss = -11045.908203730478
1
Iteration 12700: Loss = -11045.907462069303
Iteration 12800: Loss = -11045.908451559068
1
Iteration 12900: Loss = -11045.907307590212
Iteration 13000: Loss = -11045.907426605525
1
Iteration 13100: Loss = -11045.907515268049
2
Iteration 13200: Loss = -11045.90730719235
Iteration 13300: Loss = -11045.91919769683
1
Iteration 13400: Loss = -11045.907279929139
Iteration 13500: Loss = -11045.907258054867
Iteration 13600: Loss = -11045.907411549608
1
Iteration 13700: Loss = -11045.907239164762
Iteration 13800: Loss = -11045.97243446319
1
Iteration 13900: Loss = -11045.907236484234
Iteration 14000: Loss = -11045.907953862803
1
Iteration 14100: Loss = -11045.907234162854
Iteration 14200: Loss = -11045.909201879596
1
Iteration 14300: Loss = -11045.907197051829
Iteration 14400: Loss = -11045.911775121938
1
Iteration 14500: Loss = -11045.907184263246
Iteration 14600: Loss = -11045.90862012564
1
Iteration 14700: Loss = -11045.907193521673
Iteration 14800: Loss = -11045.907990310941
1
Iteration 14900: Loss = -11045.909986898838
2
Iteration 15000: Loss = -11045.907200499405
Iteration 15100: Loss = -11045.907704958407
1
Iteration 15200: Loss = -11046.03802213385
2
Iteration 15300: Loss = -11045.907187132389
Iteration 15400: Loss = -11045.910280797283
1
Iteration 15500: Loss = -11045.958657823838
2
Iteration 15600: Loss = -11045.907187657505
Iteration 15700: Loss = -11045.907178551408
Iteration 15800: Loss = -11045.908811054205
1
Iteration 15900: Loss = -11045.907186578706
Iteration 16000: Loss = -11045.907437253178
1
Iteration 16100: Loss = -11045.907393061614
2
Iteration 16200: Loss = -11045.907960195018
3
Iteration 16300: Loss = -11045.907166126033
Iteration 16400: Loss = -11045.913893813115
1
Iteration 16500: Loss = -11045.907110509854
Iteration 16600: Loss = -11045.982572845978
1
Iteration 16700: Loss = -11045.907136445929
Iteration 16800: Loss = -11045.907159007083
Iteration 16900: Loss = -11045.907276349348
1
Iteration 17000: Loss = -11045.90713276328
Iteration 17100: Loss = -11045.907191162043
Iteration 17200: Loss = -11046.00475792935
1
Iteration 17300: Loss = -11045.907106061693
Iteration 17400: Loss = -11045.907268223642
1
Iteration 17500: Loss = -11045.90719229701
Iteration 17600: Loss = -11045.908925159461
1
Iteration 17700: Loss = -11045.90814833663
2
Iteration 17800: Loss = -11045.907164379476
Iteration 17900: Loss = -11045.909901655208
1
Iteration 18000: Loss = -11045.90939574346
2
Iteration 18100: Loss = -11045.907177549368
Iteration 18200: Loss = -11045.907148740895
Iteration 18300: Loss = -11045.907207487971
Iteration 18400: Loss = -11045.907937680906
1
Iteration 18500: Loss = -11045.91164540529
2
Iteration 18600: Loss = -11045.907118953895
Iteration 18700: Loss = -11045.907239424527
1
Iteration 18800: Loss = -11045.91094510092
2
Iteration 18900: Loss = -11045.907698300849
3
Iteration 19000: Loss = -11045.9071321169
Iteration 19100: Loss = -11045.912223816485
1
Iteration 19200: Loss = -11045.90955852645
2
Iteration 19300: Loss = -11045.910642350307
3
Iteration 19400: Loss = -11045.909276397973
4
Iteration 19500: Loss = -11045.907657709653
5
Iteration 19600: Loss = -11045.907087726177
Iteration 19700: Loss = -11045.911838063686
1
Iteration 19800: Loss = -11045.907353687655
2
Iteration 19900: Loss = -11045.907386173141
3
pi: tensor([[7.2722e-01, 2.7278e-01],
        [5.7246e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0405, 0.9595], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2610, 0.2065],
         [0.7063, 0.1619]],

        [[0.5792, 0.1701],
         [0.5681, 0.7147]],

        [[0.6569, 0.2533],
         [0.7056, 0.6754]],

        [[0.7289, 0.1894],
         [0.5792, 0.5603]],

        [[0.5989, 0.2367],
         [0.5067, 0.7035]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00035341041046094813
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01171303074670571
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: -0.000245181712665546
Average Adjusted Rand Index: 0.001790059945939022
10929.823683882372
[0.0001102299408497439, -0.000245181712665546] [-0.0019259905852283359, 0.001790059945939022] [11046.160631462972, 11045.90730986109]
-------------------------------------
This iteration is 85
True Objective function: Loss = -10798.593707704336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20973.309388428508
Iteration 100: Loss = -10925.407717655888
Iteration 200: Loss = -10919.126756531965
Iteration 300: Loss = -10918.63454499843
Iteration 400: Loss = -10918.406829406511
Iteration 500: Loss = -10918.268371544316
Iteration 600: Loss = -10918.171275706607
Iteration 700: Loss = -10918.082052373295
Iteration 800: Loss = -10917.88376334132
Iteration 900: Loss = -10916.589375520916
Iteration 1000: Loss = -10915.547119858966
Iteration 1100: Loss = -10915.459727919606
Iteration 1200: Loss = -10915.42497275566
Iteration 1300: Loss = -10915.394755585143
Iteration 1400: Loss = -10915.35114925391
Iteration 1500: Loss = -10915.169440883716
Iteration 1600: Loss = -10914.711925036141
Iteration 1700: Loss = -10914.546716608516
Iteration 1800: Loss = -10914.458696284944
Iteration 1900: Loss = -10914.41529577005
Iteration 2000: Loss = -10914.368545100102
Iteration 2100: Loss = -10914.330190964276
Iteration 2200: Loss = -10914.298593267964
Iteration 2300: Loss = -10914.272547555469
Iteration 2400: Loss = -10914.252580797393
Iteration 2500: Loss = -10914.237355773907
Iteration 2600: Loss = -10914.2249857644
Iteration 2700: Loss = -10914.21478086112
Iteration 2800: Loss = -10914.206318418535
Iteration 2900: Loss = -10914.199409599327
Iteration 3000: Loss = -10914.193894817085
Iteration 3100: Loss = -10914.189770152032
Iteration 3200: Loss = -10914.186843046271
Iteration 3300: Loss = -10914.184755936429
Iteration 3400: Loss = -10914.18327833249
Iteration 3500: Loss = -10914.182333048313
Iteration 3600: Loss = -10914.181673176601
Iteration 3700: Loss = -10914.18123011884
Iteration 3800: Loss = -10914.180973433475
Iteration 3900: Loss = -10914.180753572717
Iteration 4000: Loss = -10914.180685293339
Iteration 4100: Loss = -10914.180592513494
Iteration 4200: Loss = -10914.180525315629
Iteration 4300: Loss = -10914.180471675556
Iteration 4400: Loss = -10914.180394638448
Iteration 4500: Loss = -10914.18039231117
Iteration 4600: Loss = -10914.180317888247
Iteration 4700: Loss = -10914.180318611798
Iteration 4800: Loss = -10914.18075943621
1
Iteration 4900: Loss = -10914.180296338722
Iteration 5000: Loss = -10914.180438314874
1
Iteration 5100: Loss = -10914.180314225892
Iteration 5200: Loss = -10914.180387450273
Iteration 5300: Loss = -10914.180298415575
Iteration 5400: Loss = -10914.181112585678
1
Iteration 5500: Loss = -10914.180308711639
Iteration 5600: Loss = -10914.18032988625
Iteration 5700: Loss = -10914.181400445284
1
Iteration 5800: Loss = -10914.180310438367
Iteration 5900: Loss = -10914.180520942475
1
Iteration 6000: Loss = -10914.180421287894
2
Iteration 6100: Loss = -10914.18052677721
3
Iteration 6200: Loss = -10914.180319315017
Iteration 6300: Loss = -10914.180307122322
Iteration 6400: Loss = -10914.180297302199
Iteration 6500: Loss = -10914.180289969114
Iteration 6600: Loss = -10914.180332865404
Iteration 6700: Loss = -10914.181062456006
1
Iteration 6800: Loss = -10914.183665685201
2
Iteration 6900: Loss = -10914.180281018529
Iteration 7000: Loss = -10914.181419058465
1
Iteration 7100: Loss = -10914.180275539795
Iteration 7200: Loss = -10914.182168518046
1
Iteration 7300: Loss = -10914.18027729961
Iteration 7400: Loss = -10914.193035792823
1
Iteration 7500: Loss = -10914.180315714864
Iteration 7600: Loss = -10914.180316164668
Iteration 7700: Loss = -10914.186355715847
1
Iteration 7800: Loss = -10914.180304454896
Iteration 7900: Loss = -10914.180293758864
Iteration 8000: Loss = -10914.19991721333
1
Iteration 8100: Loss = -10914.180265263976
Iteration 8200: Loss = -10914.180292947101
Iteration 8300: Loss = -10914.259654369118
1
Iteration 8400: Loss = -10914.180262117545
Iteration 8500: Loss = -10914.180298621844
Iteration 8600: Loss = -10914.210059526482
1
Iteration 8700: Loss = -10914.180305556572
Iteration 8800: Loss = -10914.180240169566
Iteration 8900: Loss = -10914.187604865681
1
Iteration 9000: Loss = -10914.180322003042
Iteration 9100: Loss = -10914.18028581806
Iteration 9200: Loss = -10914.22295112137
1
Iteration 9300: Loss = -10914.180332245205
Iteration 9400: Loss = -10914.18032183719
Iteration 9500: Loss = -10914.26018859798
1
Iteration 9600: Loss = -10914.180262220736
Iteration 9700: Loss = -10914.180273382939
Iteration 9800: Loss = -10914.180287212572
Iteration 9900: Loss = -10914.18036994108
Iteration 10000: Loss = -10914.180287957652
Iteration 10100: Loss = -10914.180285233348
Iteration 10200: Loss = -10914.180448177232
1
Iteration 10300: Loss = -10914.180298265239
Iteration 10400: Loss = -10914.180385799033
Iteration 10500: Loss = -10914.180272338244
Iteration 10600: Loss = -10914.180305905937
Iteration 10700: Loss = -10914.633188970476
1
Iteration 10800: Loss = -10914.180335461939
Iteration 10900: Loss = -10914.18029796293
Iteration 11000: Loss = -10914.180328166625
Iteration 11100: Loss = -10914.180275712362
Iteration 11200: Loss = -10914.180308838964
Iteration 11300: Loss = -10914.180306398253
Iteration 11400: Loss = -10914.18045708158
1
Iteration 11500: Loss = -10914.180273829304
Iteration 11600: Loss = -10914.180398181266
1
Iteration 11700: Loss = -10914.180328364799
Iteration 11800: Loss = -10914.180295674338
Iteration 11900: Loss = -10914.317343378301
1
Iteration 12000: Loss = -10914.180264318462
Iteration 12100: Loss = -10914.180304042564
Iteration 12200: Loss = -10914.185814075985
1
Iteration 12300: Loss = -10914.18027490437
Iteration 12400: Loss = -10914.18040511251
1
Iteration 12500: Loss = -10914.180281700568
Iteration 12600: Loss = -10914.227051415279
1
Iteration 12700: Loss = -10914.180524704112
2
Iteration 12800: Loss = -10914.180436830378
3
Iteration 12900: Loss = -10914.182323103421
4
Iteration 13000: Loss = -10914.180380718672
Iteration 13100: Loss = -10914.187502671599
1
Iteration 13200: Loss = -10914.180311746313
Iteration 13300: Loss = -10914.236459371494
1
Iteration 13400: Loss = -10914.180282511636
Iteration 13500: Loss = -10914.208224528673
1
Iteration 13600: Loss = -10914.1982704429
2
Iteration 13700: Loss = -10914.180999073898
3
Iteration 13800: Loss = -10914.180384052335
4
Iteration 13900: Loss = -10914.252480271844
5
Iteration 14000: Loss = -10914.180320956677
Iteration 14100: Loss = -10914.185967532148
1
Iteration 14200: Loss = -10914.180733324607
2
Iteration 14300: Loss = -10914.18228530728
3
Iteration 14400: Loss = -10914.181208061129
4
Iteration 14500: Loss = -10914.180291781358
Iteration 14600: Loss = -10914.210011242136
1
Iteration 14700: Loss = -10914.180533033352
2
Iteration 14800: Loss = -10914.291245338234
3
Iteration 14900: Loss = -10914.18028997677
Iteration 15000: Loss = -10914.28152424117
1
Iteration 15100: Loss = -10914.180279400187
Iteration 15200: Loss = -10914.182003247031
1
Iteration 15300: Loss = -10914.180294694332
Iteration 15400: Loss = -10914.180363783786
Iteration 15500: Loss = -10914.181096494978
1
Iteration 15600: Loss = -10914.180461576356
Iteration 15700: Loss = -10914.1804941735
Iteration 15800: Loss = -10914.199956459977
1
Iteration 15900: Loss = -10914.180373420159
Iteration 16000: Loss = -10914.341846272098
1
Iteration 16100: Loss = -10914.180292972058
Iteration 16200: Loss = -10914.180283045393
Iteration 16300: Loss = -10914.180584768408
1
Iteration 16400: Loss = -10914.180288230476
Iteration 16500: Loss = -10914.18102417779
1
Iteration 16600: Loss = -10914.182398773486
2
Iteration 16700: Loss = -10914.180719795422
3
Iteration 16800: Loss = -10914.18064640586
4
Iteration 16900: Loss = -10914.180349349293
Iteration 17000: Loss = -10914.180336350346
Iteration 17100: Loss = -10914.180435115843
Iteration 17200: Loss = -10914.180558553502
1
Iteration 17300: Loss = -10914.180742241175
2
Iteration 17400: Loss = -10914.398072322841
3
Iteration 17500: Loss = -10914.180279892691
Iteration 17600: Loss = -10914.185739800585
1
Iteration 17700: Loss = -10914.180306210914
Iteration 17800: Loss = -10914.181014583623
1
Iteration 17900: Loss = -10914.180392732973
Iteration 18000: Loss = -10914.181100919244
1
Iteration 18100: Loss = -10914.180281720619
Iteration 18200: Loss = -10914.18466513861
1
Iteration 18300: Loss = -10914.181768616269
2
Iteration 18400: Loss = -10914.180472261865
3
Iteration 18500: Loss = -10914.180881135346
4
Iteration 18600: Loss = -10914.180400066394
5
Iteration 18700: Loss = -10914.18091038908
6
Iteration 18800: Loss = -10914.180421070085
7
Iteration 18900: Loss = -10914.213511088446
8
Iteration 19000: Loss = -10914.180285219381
Iteration 19100: Loss = -10914.187931591863
1
Iteration 19200: Loss = -10914.180321982632
Iteration 19300: Loss = -10914.187825655208
1
Iteration 19400: Loss = -10914.180312846607
Iteration 19500: Loss = -10914.180288484631
Iteration 19600: Loss = -10914.180417272179
1
Iteration 19700: Loss = -10914.208439012196
2
Iteration 19800: Loss = -10914.182583522104
3
Iteration 19900: Loss = -10914.181114357189
4
pi: tensor([[0.3741, 0.6259],
        [0.0147, 0.9853]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0497, 0.9503], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2535, 0.2244],
         [0.7111, 0.1576]],

        [[0.5482, 0.1985],
         [0.6870, 0.6766]],

        [[0.6962, 0.1630],
         [0.6691, 0.6695]],

        [[0.5067, 0.1809],
         [0.6328, 0.7132]],

        [[0.7183, 0.2856],
         [0.5712, 0.6675]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: -0.0004888125824373122
Average Adjusted Rand Index: -0.0008816361153023561
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24654.284163776847
Iteration 100: Loss = -10918.521676673929
Iteration 200: Loss = -10916.999648850526
Iteration 300: Loss = -10916.525520585988
Iteration 400: Loss = -10916.266359884217
Iteration 500: Loss = -10916.05824058901
Iteration 600: Loss = -10915.789920752164
Iteration 700: Loss = -10914.832551985332
Iteration 800: Loss = -10914.68869214584
Iteration 900: Loss = -10914.568873287995
Iteration 1000: Loss = -10914.493099646337
Iteration 1100: Loss = -10914.428507601173
Iteration 1200: Loss = -10914.372757162066
Iteration 1300: Loss = -10914.327772546008
Iteration 1400: Loss = -10914.291451755545
Iteration 1500: Loss = -10914.263128000952
Iteration 1600: Loss = -10914.241604204433
Iteration 1700: Loss = -10914.225103498131
Iteration 1800: Loss = -10914.212522753425
Iteration 1900: Loss = -10914.203448492059
Iteration 2000: Loss = -10914.197176359266
Iteration 2100: Loss = -10914.192928712675
Iteration 2200: Loss = -10914.189998483711
Iteration 2300: Loss = -10914.18794783351
Iteration 2400: Loss = -10914.186390091905
Iteration 2500: Loss = -10914.185238430608
Iteration 2600: Loss = -10914.184381235595
Iteration 2700: Loss = -10914.183711678807
Iteration 2800: Loss = -10914.183222950367
Iteration 2900: Loss = -10914.182805392431
Iteration 3000: Loss = -10914.18253020531
Iteration 3100: Loss = -10914.182247027677
Iteration 3200: Loss = -10914.182044620618
Iteration 3300: Loss = -10914.181895562635
Iteration 3400: Loss = -10914.18169906467
Iteration 3500: Loss = -10914.181517190105
Iteration 3600: Loss = -10914.181377950594
Iteration 3700: Loss = -10914.181223898438
Iteration 3800: Loss = -10914.181118699655
Iteration 3900: Loss = -10914.180981909496
Iteration 4000: Loss = -10914.180899470788
Iteration 4100: Loss = -10914.18081302899
Iteration 4200: Loss = -10914.180759823785
Iteration 4300: Loss = -10914.180682603655
Iteration 4400: Loss = -10914.180661952232
Iteration 4500: Loss = -10914.18063154154
Iteration 4600: Loss = -10914.180596801185
Iteration 4700: Loss = -10914.180576645644
Iteration 4800: Loss = -10914.180507671987
Iteration 4900: Loss = -10914.1805130166
Iteration 5000: Loss = -10914.18050291887
Iteration 5100: Loss = -10914.18044721679
Iteration 5200: Loss = -10914.180478252585
Iteration 5300: Loss = -10914.180450427639
Iteration 5400: Loss = -10914.180412095195
Iteration 5500: Loss = -10914.180443800207
Iteration 5600: Loss = -10914.18040716626
Iteration 5700: Loss = -10914.180421774157
Iteration 5800: Loss = -10914.180378210858
Iteration 5900: Loss = -10914.180381096778
Iteration 6000: Loss = -10914.180384110574
Iteration 6100: Loss = -10914.180375666469
Iteration 6200: Loss = -10914.180362779358
Iteration 6300: Loss = -10914.180362062538
Iteration 6400: Loss = -10914.180366695638
Iteration 6500: Loss = -10914.180319522105
Iteration 6600: Loss = -10914.180344301847
Iteration 6700: Loss = -10914.180340720543
Iteration 6800: Loss = -10914.180382599243
Iteration 6900: Loss = -10914.180323697608
Iteration 7000: Loss = -10914.182961960862
1
Iteration 7100: Loss = -10914.180333304097
Iteration 7200: Loss = -10914.182198945731
1
Iteration 7300: Loss = -10914.180296466167
Iteration 7400: Loss = -10914.18035258171
Iteration 7500: Loss = -10914.18033029551
Iteration 7600: Loss = -10914.180445154892
1
Iteration 7700: Loss = -10914.181293893067
2
Iteration 7800: Loss = -10914.18029263376
Iteration 7900: Loss = -10914.180343722395
Iteration 8000: Loss = -10914.184637393011
1
Iteration 8100: Loss = -10914.180297503424
Iteration 8200: Loss = -10914.180302410101
Iteration 8300: Loss = -10914.180304173178
Iteration 8400: Loss = -10914.18032534222
Iteration 8500: Loss = -10914.180386709302
Iteration 8600: Loss = -10914.200195133815
1
Iteration 8700: Loss = -10914.180286102086
Iteration 8800: Loss = -10914.181704567573
1
Iteration 8900: Loss = -10914.180317398395
Iteration 9000: Loss = -10914.180414156594
Iteration 9100: Loss = -10914.180301963013
Iteration 9200: Loss = -10914.180340477064
Iteration 9300: Loss = -10914.180292862518
Iteration 9400: Loss = -10914.242529216874
1
Iteration 9500: Loss = -10914.18033662342
Iteration 9600: Loss = -10914.180307174607
Iteration 9700: Loss = -10914.193198829606
1
Iteration 9800: Loss = -10914.180300915477
Iteration 9900: Loss = -10914.180298619161
Iteration 10000: Loss = -10914.181151088644
1
Iteration 10100: Loss = -10914.180312175813
Iteration 10200: Loss = -10914.180297158067
Iteration 10300: Loss = -10914.18025468797
Iteration 10400: Loss = -10914.180298829431
Iteration 10500: Loss = -10914.180276465539
Iteration 10600: Loss = -10914.190041828304
1
Iteration 10700: Loss = -10914.180292129913
Iteration 10800: Loss = -10914.180267592088
Iteration 10900: Loss = -10914.182300161974
1
Iteration 11000: Loss = -10914.180294390657
Iteration 11100: Loss = -10914.18028778787
Iteration 11200: Loss = -10914.181698333212
1
Iteration 11300: Loss = -10914.18026873498
Iteration 11400: Loss = -10914.180274428483
Iteration 11500: Loss = -10914.18035934738
Iteration 11600: Loss = -10914.180298338904
Iteration 11700: Loss = -10914.180287898593
Iteration 11800: Loss = -10914.180553264312
1
Iteration 11900: Loss = -10914.180295867376
Iteration 12000: Loss = -10914.211254175287
1
Iteration 12100: Loss = -10914.180347093257
Iteration 12200: Loss = -10914.18031168588
Iteration 12300: Loss = -10914.180310094229
Iteration 12400: Loss = -10914.180489455823
1
Iteration 12500: Loss = -10914.180301930024
Iteration 12600: Loss = -10914.181744559863
1
Iteration 12700: Loss = -10914.180357023452
Iteration 12800: Loss = -10914.185662134485
1
Iteration 12900: Loss = -10914.225160093223
2
Iteration 13000: Loss = -10914.180703594848
3
Iteration 13100: Loss = -10914.18068624982
4
Iteration 13200: Loss = -10914.185531435858
5
Iteration 13300: Loss = -10914.180284089045
Iteration 13400: Loss = -10914.19390726762
1
Iteration 13500: Loss = -10914.180309046935
Iteration 13600: Loss = -10914.18128760055
1
Iteration 13700: Loss = -10914.182472439965
2
Iteration 13800: Loss = -10914.180519937652
3
Iteration 13900: Loss = -10914.180631458701
4
Iteration 14000: Loss = -10914.186728781442
5
Iteration 14100: Loss = -10914.180574178723
6
Iteration 14200: Loss = -10914.218611957851
7
Iteration 14300: Loss = -10914.18029326195
Iteration 14400: Loss = -10914.191449424183
1
Iteration 14500: Loss = -10914.18038688191
Iteration 14600: Loss = -10914.180490171835
1
Iteration 14700: Loss = -10914.180348741731
Iteration 14800: Loss = -10914.182287380449
1
Iteration 14900: Loss = -10914.180583810363
2
Iteration 15000: Loss = -10914.180458388668
3
Iteration 15100: Loss = -10914.194954223576
4
Iteration 15200: Loss = -10914.180443973608
Iteration 15300: Loss = -10914.239194899494
1
Iteration 15400: Loss = -10914.180357986132
Iteration 15500: Loss = -10914.18237147429
1
Iteration 15600: Loss = -10914.180751959944
2
Iteration 15700: Loss = -10914.180367409223
Iteration 15800: Loss = -10914.180339860935
Iteration 15900: Loss = -10914.18030451178
Iteration 16000: Loss = -10914.182023321608
1
Iteration 16100: Loss = -10914.180381738011
Iteration 16200: Loss = -10914.328355556669
1
Iteration 16300: Loss = -10914.180827696855
2
Iteration 16400: Loss = -10914.180712940655
3
Iteration 16500: Loss = -10914.18032864021
Iteration 16600: Loss = -10914.180342066391
Iteration 16700: Loss = -10914.182443226455
1
Iteration 16800: Loss = -10914.18030170395
Iteration 16900: Loss = -10914.253220576453
1
Iteration 17000: Loss = -10914.180322703185
Iteration 17100: Loss = -10914.182559240422
1
Iteration 17200: Loss = -10914.180305747415
Iteration 17300: Loss = -10914.18723560409
1
Iteration 17400: Loss = -10914.180268282225
Iteration 17500: Loss = -10914.180293709853
Iteration 17600: Loss = -10914.1826267092
1
Iteration 17700: Loss = -10914.180293471409
Iteration 17800: Loss = -10914.246540956074
1
Iteration 17900: Loss = -10914.180442368217
2
Iteration 18000: Loss = -10914.1806604066
3
Iteration 18100: Loss = -10914.185416065611
4
Iteration 18200: Loss = -10914.180302074119
Iteration 18300: Loss = -10914.18037297405
Iteration 18400: Loss = -10914.190898056053
1
Iteration 18500: Loss = -10914.251738482311
2
Iteration 18600: Loss = -10914.318448937269
3
Iteration 18700: Loss = -10914.180787536521
4
Iteration 18800: Loss = -10914.180289250216
Iteration 18900: Loss = -10914.181041201364
1
Iteration 19000: Loss = -10914.18034615589
Iteration 19100: Loss = -10914.181043772687
1
Iteration 19200: Loss = -10914.301608701637
2
Iteration 19300: Loss = -10914.18029154013
Iteration 19400: Loss = -10914.180515374199
1
Iteration 19500: Loss = -10914.18036258011
Iteration 19600: Loss = -10914.180444950758
Iteration 19700: Loss = -10914.19506322794
1
Iteration 19800: Loss = -10914.180544076318
Iteration 19900: Loss = -10914.42880456123
1
pi: tensor([[0.3736, 0.6264],
        [0.0147, 0.9853]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0497, 0.9503], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2536, 0.2244],
         [0.6818, 0.1576]],

        [[0.7224, 0.1988],
         [0.6345, 0.6292]],

        [[0.6316, 0.1630],
         [0.5400, 0.5250]],

        [[0.5542, 0.1808],
         [0.5132, 0.6275]],

        [[0.7193, 0.2856],
         [0.5591, 0.6222]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: -0.0004888125824373122
Average Adjusted Rand Index: -0.0008816361153023561
10798.593707704336
[-0.0004888125824373122, -0.0004888125824373122] [-0.0008816361153023561, -0.0008816361153023561] [10914.180478585413, 10914.180300216607]
-------------------------------------
This iteration is 86
True Objective function: Loss = -10939.152826492535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24377.04303478835
Iteration 100: Loss = -11015.306870211441
Iteration 200: Loss = -11013.558170693363
Iteration 300: Loss = -11012.288647539342
Iteration 400: Loss = -11011.358199074823
Iteration 500: Loss = -11010.63790629899
Iteration 600: Loss = -11009.999795318236
Iteration 700: Loss = -11009.50549626647
Iteration 800: Loss = -11009.06659089551
Iteration 900: Loss = -11008.678156505039
Iteration 1000: Loss = -11008.341647914005
Iteration 1100: Loss = -11008.082162973193
Iteration 1200: Loss = -11007.91754702083
Iteration 1300: Loss = -11007.832734880865
Iteration 1400: Loss = -11007.79237721307
Iteration 1500: Loss = -11007.771684986403
Iteration 1600: Loss = -11007.760288322546
Iteration 1700: Loss = -11007.753830819685
Iteration 1800: Loss = -11007.749939678994
Iteration 1900: Loss = -11007.747340592481
Iteration 2000: Loss = -11007.74537583395
Iteration 2100: Loss = -11007.743636765812
Iteration 2200: Loss = -11007.742029386362
Iteration 2300: Loss = -11007.74056704533
Iteration 2400: Loss = -11007.739157312062
Iteration 2500: Loss = -11007.737834138172
Iteration 2600: Loss = -11007.736311309358
Iteration 2700: Loss = -11007.734562047774
Iteration 2800: Loss = -11007.732076400096
Iteration 2900: Loss = -11007.727899080573
Iteration 3000: Loss = -11007.719515770712
Iteration 3100: Loss = -11007.697821578216
Iteration 3200: Loss = -11007.613901185257
Iteration 3300: Loss = -11006.880064919254
Iteration 3400: Loss = -10955.335635247944
Iteration 3500: Loss = -10891.123731142674
Iteration 3600: Loss = -10890.967026119723
Iteration 3700: Loss = -10890.888779615181
Iteration 3800: Loss = -10890.858519258074
Iteration 3900: Loss = -10890.840043962951
Iteration 4000: Loss = -10890.832119634808
Iteration 4100: Loss = -10890.825568224365
Iteration 4200: Loss = -10890.819736789697
Iteration 4300: Loss = -10890.814853736574
Iteration 4400: Loss = -10890.81305929252
Iteration 4500: Loss = -10890.810954261293
Iteration 4600: Loss = -10890.809514597477
Iteration 4700: Loss = -10890.807811532282
Iteration 4800: Loss = -10890.80551269799
Iteration 4900: Loss = -10890.804752086951
Iteration 5000: Loss = -10890.817667628093
1
Iteration 5100: Loss = -10890.803478363036
Iteration 5200: Loss = -10890.80784995043
1
Iteration 5300: Loss = -10890.803167446062
Iteration 5400: Loss = -10890.800622217339
Iteration 5500: Loss = -10890.798370287617
Iteration 5600: Loss = -10890.798028088735
Iteration 5700: Loss = -10890.797820940497
Iteration 5800: Loss = -10890.797473882427
Iteration 5900: Loss = -10890.797284350692
Iteration 6000: Loss = -10890.79763623449
1
Iteration 6100: Loss = -10890.79715031904
Iteration 6200: Loss = -10890.796505096008
Iteration 6300: Loss = -10890.796308050392
Iteration 6400: Loss = -10890.797099891506
1
Iteration 6500: Loss = -10890.809678303118
2
Iteration 6600: Loss = -10890.798949711698
3
Iteration 6700: Loss = -10890.800698526951
4
Iteration 6800: Loss = -10890.797613038256
5
Iteration 6900: Loss = -10890.797385999034
6
Iteration 7000: Loss = -10890.794348099262
Iteration 7100: Loss = -10890.79388558493
Iteration 7200: Loss = -10890.793976458184
Iteration 7300: Loss = -10890.793721777172
Iteration 7400: Loss = -10890.793720594063
Iteration 7500: Loss = -10890.793560072168
Iteration 7600: Loss = -10890.793524352783
Iteration 7700: Loss = -10890.793384602082
Iteration 7800: Loss = -10890.793533645823
1
Iteration 7900: Loss = -10890.80407244017
2
Iteration 8000: Loss = -10890.79298926751
Iteration 8100: Loss = -10890.792986541852
Iteration 8200: Loss = -10890.792869912231
Iteration 8300: Loss = -10890.792765579285
Iteration 8400: Loss = -10890.792732785188
Iteration 8500: Loss = -10890.794863127385
1
Iteration 8600: Loss = -10890.792679761893
Iteration 8700: Loss = -10890.792654496656
Iteration 8800: Loss = -10890.792647125596
Iteration 8900: Loss = -10890.792459790278
Iteration 9000: Loss = -10890.792215772866
Iteration 9100: Loss = -10890.7924877632
1
Iteration 9200: Loss = -10890.792072279879
Iteration 9300: Loss = -10890.792044886342
Iteration 9400: Loss = -10890.7921559163
1
Iteration 9500: Loss = -10890.792282880888
2
Iteration 9600: Loss = -10890.80813309204
3
Iteration 9700: Loss = -10890.80881925971
4
Iteration 9800: Loss = -10890.803452221971
5
Iteration 9900: Loss = -10890.797916511205
6
Iteration 10000: Loss = -10890.795577319092
7
Iteration 10100: Loss = -10890.798765481544
8
Iteration 10200: Loss = -10890.887308142743
9
Iteration 10300: Loss = -10890.791918955758
Iteration 10400: Loss = -10890.792262337036
1
Iteration 10500: Loss = -10890.793395121316
2
Iteration 10600: Loss = -10890.80052159706
3
Iteration 10700: Loss = -10890.792626390017
4
Iteration 10800: Loss = -10890.8794360444
5
Iteration 10900: Loss = -10890.791704022748
Iteration 11000: Loss = -10890.794778571239
1
Iteration 11100: Loss = -10890.791624354946
Iteration 11200: Loss = -10890.792201176908
1
Iteration 11300: Loss = -10890.791652864631
Iteration 11400: Loss = -10890.85807214515
1
Iteration 11500: Loss = -10890.791669984066
Iteration 11600: Loss = -10890.79161968224
Iteration 11700: Loss = -10890.792569772724
1
Iteration 11800: Loss = -10890.791633384924
Iteration 11900: Loss = -10890.791650577172
Iteration 12000: Loss = -10890.797354667806
1
Iteration 12100: Loss = -10890.79161599783
Iteration 12200: Loss = -10890.791614835807
Iteration 12300: Loss = -10890.79171624398
1
Iteration 12400: Loss = -10890.791641293621
Iteration 12500: Loss = -10890.792160810079
1
Iteration 12600: Loss = -10891.068388009528
2
Iteration 12700: Loss = -10890.791621917133
Iteration 12800: Loss = -10890.795522987664
1
Iteration 12900: Loss = -10890.791882863447
2
Iteration 13000: Loss = -10890.791835430715
3
Iteration 13100: Loss = -10890.793244352137
4
Iteration 13200: Loss = -10890.801819594779
5
Iteration 13300: Loss = -10890.793889411287
6
Iteration 13400: Loss = -10890.797223069556
7
Iteration 13500: Loss = -10890.79166045269
Iteration 13600: Loss = -10890.811072234983
1
Iteration 13700: Loss = -10890.79158416562
Iteration 13800: Loss = -10890.793222651168
1
Iteration 13900: Loss = -10890.807270145126
2
Iteration 14000: Loss = -10890.792613672904
3
Iteration 14100: Loss = -10890.906206460975
4
Iteration 14200: Loss = -10890.791623173747
Iteration 14300: Loss = -10890.79217370287
1
Iteration 14400: Loss = -10890.791801141513
2
Iteration 14500: Loss = -10890.792522682834
3
Iteration 14600: Loss = -10890.802364048237
4
Iteration 14700: Loss = -10890.794163349523
5
Iteration 14800: Loss = -10890.791729887631
6
Iteration 14900: Loss = -10890.793998874142
7
Iteration 15000: Loss = -10890.807400209662
8
Iteration 15100: Loss = -10890.791618800255
Iteration 15200: Loss = -10890.81410343505
1
Iteration 15300: Loss = -10890.791597477015
Iteration 15400: Loss = -10890.837385660041
1
Iteration 15500: Loss = -10890.791589085986
Iteration 15600: Loss = -10890.949647711615
1
Iteration 15700: Loss = -10890.791625832366
Iteration 15800: Loss = -10890.791693484307
Iteration 15900: Loss = -10890.791654474913
Iteration 16000: Loss = -10890.804824730489
1
Iteration 16100: Loss = -10890.791630705493
Iteration 16200: Loss = -10890.791841578513
1
Iteration 16300: Loss = -10890.79160944143
Iteration 16400: Loss = -10890.792606218034
1
Iteration 16500: Loss = -10890.791649753482
Iteration 16600: Loss = -10890.793206025655
1
Iteration 16700: Loss = -10890.791852423037
2
Iteration 16800: Loss = -10890.791659685372
Iteration 16900: Loss = -10890.803844416287
1
Iteration 17000: Loss = -10890.792295353654
2
Iteration 17100: Loss = -10890.791641946817
Iteration 17200: Loss = -10890.79282346077
1
Iteration 17300: Loss = -10890.791642858403
Iteration 17400: Loss = -10890.7916069067
Iteration 17500: Loss = -10890.79180269295
1
Iteration 17600: Loss = -10890.791803183085
2
Iteration 17700: Loss = -10890.792302466933
3
Iteration 17800: Loss = -10890.81787785728
4
Iteration 17900: Loss = -10890.791593822973
Iteration 18000: Loss = -10890.849911926527
1
Iteration 18100: Loss = -10890.791583153634
Iteration 18200: Loss = -10890.813599689667
1
Iteration 18300: Loss = -10890.791587691014
Iteration 18400: Loss = -10890.813004635269
1
Iteration 18500: Loss = -10890.79160696907
Iteration 18600: Loss = -10890.791623096984
Iteration 18700: Loss = -10890.833658164398
1
Iteration 18800: Loss = -10890.7915920301
Iteration 18900: Loss = -10890.792766410765
1
Iteration 19000: Loss = -10890.962568237199
2
Iteration 19100: Loss = -10890.791620358403
Iteration 19200: Loss = -10890.822330880912
1
Iteration 19300: Loss = -10890.79158794297
Iteration 19400: Loss = -10890.799639202814
1
Iteration 19500: Loss = -10890.79234608302
2
Iteration 19600: Loss = -10890.794412831347
3
Iteration 19700: Loss = -10890.797729164666
4
Iteration 19800: Loss = -10890.80680506355
5
Iteration 19900: Loss = -10890.792085522437
6
pi: tensor([[0.7783, 0.2217],
        [0.2971, 0.7029]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4768, 0.5232], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2457, 0.1002],
         [0.5783, 0.1936]],

        [[0.5422, 0.1048],
         [0.6078, 0.5113]],

        [[0.5448, 0.0937],
         [0.6691, 0.6528]],

        [[0.5505, 0.1006],
         [0.5831, 0.6821]],

        [[0.6635, 0.1071],
         [0.5000, 0.7083]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721181988081385
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8447743642510657
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369480537608971
Global Adjusted Rand Index: 0.8168452110093819
Average Adjusted Rand Index: 0.8181591770759349
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20868.70037504019
Iteration 100: Loss = -11013.846292264294
Iteration 200: Loss = -11013.440661157501
Iteration 300: Loss = -11012.681532514815
Iteration 400: Loss = -11009.940139408474
Iteration 500: Loss = -11008.779822843278
Iteration 600: Loss = -11008.46217977431
Iteration 700: Loss = -11008.27143613064
Iteration 800: Loss = -11008.144892208229
Iteration 900: Loss = -11008.045853231291
Iteration 1000: Loss = -11007.92204449433
Iteration 1100: Loss = -11007.803293320409
Iteration 1200: Loss = -11007.764248341744
Iteration 1300: Loss = -11007.750512097347
Iteration 1400: Loss = -11007.743282766924
Iteration 1500: Loss = -11007.73832714161
Iteration 1600: Loss = -11007.734341056976
Iteration 1700: Loss = -11007.73083733203
Iteration 1800: Loss = -11007.727663347992
Iteration 1900: Loss = -11007.724700949653
Iteration 2000: Loss = -11007.72191965045
Iteration 2100: Loss = -11007.71924552794
Iteration 2200: Loss = -11007.716713035354
Iteration 2300: Loss = -11007.71428558732
Iteration 2400: Loss = -11007.711961608198
Iteration 2500: Loss = -11007.709620108679
Iteration 2600: Loss = -11007.70748367845
Iteration 2700: Loss = -11007.72010336125
1
Iteration 2800: Loss = -11007.70336350317
Iteration 2900: Loss = -11007.701402918547
Iteration 3000: Loss = -11007.699895961247
Iteration 3100: Loss = -11007.697750750978
Iteration 3200: Loss = -11007.696105673529
Iteration 3300: Loss = -11007.694497303948
Iteration 3400: Loss = -11007.692972920491
Iteration 3500: Loss = -11007.695723721761
1
Iteration 3600: Loss = -11007.690153060736
Iteration 3700: Loss = -11007.68885980165
Iteration 3800: Loss = -11007.68840461132
Iteration 3900: Loss = -11007.686472529676
Iteration 4000: Loss = -11007.68540863499
Iteration 4100: Loss = -11007.684347716515
Iteration 4200: Loss = -11007.683359918296
Iteration 4300: Loss = -11007.682784198607
Iteration 4400: Loss = -11007.681609881336
Iteration 4500: Loss = -11007.686604743642
1
Iteration 4600: Loss = -11007.680015195305
Iteration 4700: Loss = -11007.679319836661
Iteration 4800: Loss = -11007.678828367052
Iteration 4900: Loss = -11007.677981977346
Iteration 5000: Loss = -11007.6773950789
Iteration 5100: Loss = -11007.676860808495
Iteration 5200: Loss = -11007.676326904684
Iteration 5300: Loss = -11007.6758386462
Iteration 5400: Loss = -11007.675344670675
Iteration 5500: Loss = -11007.674902179675
Iteration 5600: Loss = -11007.675306346495
1
Iteration 5700: Loss = -11007.6741252274
Iteration 5800: Loss = -11007.673835870202
Iteration 5900: Loss = -11007.673414323563
Iteration 6000: Loss = -11007.673087201614
Iteration 6100: Loss = -11007.67280046946
Iteration 6200: Loss = -11007.672539615258
Iteration 6300: Loss = -11007.672424980256
Iteration 6400: Loss = -11007.67205017226
Iteration 6500: Loss = -11007.671858379283
Iteration 6600: Loss = -11007.671563364896
Iteration 6700: Loss = -11007.671358781572
Iteration 6800: Loss = -11007.672755841793
1
Iteration 6900: Loss = -11007.670932659397
Iteration 7000: Loss = -11007.698722473822
1
Iteration 7100: Loss = -11007.670643160633
Iteration 7200: Loss = -11007.670475868405
Iteration 7300: Loss = -11007.670327081683
Iteration 7400: Loss = -11007.67020460436
Iteration 7500: Loss = -11007.670065225264
Iteration 7600: Loss = -11007.70120852687
1
Iteration 7700: Loss = -11007.669885097468
Iteration 7800: Loss = -11007.684761663468
1
Iteration 7900: Loss = -11007.669663555587
Iteration 8000: Loss = -11007.669574067238
Iteration 8100: Loss = -11007.669605157354
Iteration 8200: Loss = -11007.6693824896
Iteration 8300: Loss = -11007.670645206328
1
Iteration 8400: Loss = -11007.669231822314
Iteration 8500: Loss = -11007.669143525476
Iteration 8600: Loss = -11007.703490171149
1
Iteration 8700: Loss = -11007.669018305456
Iteration 8800: Loss = -11007.668937671564
Iteration 8900: Loss = -11007.678792921555
1
Iteration 9000: Loss = -11007.668858910574
Iteration 9100: Loss = -11007.668806954693
Iteration 9200: Loss = -11007.675754436294
1
Iteration 9300: Loss = -11007.66870943714
Iteration 9400: Loss = -11007.668649204033
Iteration 9500: Loss = -11007.77497986959
1
Iteration 9600: Loss = -11007.668557943334
Iteration 9700: Loss = -11007.677243456086
1
Iteration 9800: Loss = -11007.669930147918
2
Iteration 9900: Loss = -11007.668593500417
Iteration 10000: Loss = -11007.668902858606
1
Iteration 10100: Loss = -11007.69167663059
2
Iteration 10200: Loss = -11007.670287581464
3
Iteration 10300: Loss = -11007.672713632544
4
Iteration 10400: Loss = -11007.689643289148
5
Iteration 10500: Loss = -11007.671067773666
6
Iteration 10600: Loss = -11007.687421752506
7
Iteration 10700: Loss = -11007.681235831807
8
Iteration 10800: Loss = -11007.670492161044
9
Iteration 10900: Loss = -11007.670201132285
10
Iteration 11000: Loss = -11007.71451179647
11
Iteration 11100: Loss = -11007.795488369993
12
Iteration 11200: Loss = -11007.706389390005
13
Iteration 11300: Loss = -11007.671731775052
14
Iteration 11400: Loss = -11007.676877767608
15
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[1.1957e-04, 9.9988e-01],
        [1.3293e-01, 8.6707e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0043, 0.9957], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2627, 0.1775],
         [0.5615, 0.1554]],

        [[0.5297, 0.2129],
         [0.6628, 0.6352]],

        [[0.6204, 0.1672],
         [0.6074, 0.5373]],

        [[0.6681, 0.1909],
         [0.6630, 0.5951]],

        [[0.7299, 0.1927],
         [0.6103, 0.5826]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.022723709951211306
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.010213452814961178
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0038714720373787324
Average Adjusted Rand Index: -0.006744355580888177
10939.152826492535
[0.8168452110093819, -0.0038714720373787324] [0.8181591770759349, -0.006744355580888177] [10890.79245186902, 11007.676877767608]
-------------------------------------
This iteration is 87
True Objective function: Loss = -10773.778154873846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24344.10276865782
Iteration 100: Loss = -10825.322794877404
Iteration 200: Loss = -10823.366940801276
Iteration 300: Loss = -10822.741410564253
Iteration 400: Loss = -10822.423766704242
Iteration 500: Loss = -10822.218354275687
Iteration 600: Loss = -10822.081929024098
Iteration 700: Loss = -10821.991249874449
Iteration 800: Loss = -10821.929884616342
Iteration 900: Loss = -10821.886847678761
Iteration 1000: Loss = -10821.855264683782
Iteration 1100: Loss = -10821.83089906256
Iteration 1200: Loss = -10821.810981116787
Iteration 1300: Loss = -10821.793404987913
Iteration 1400: Loss = -10821.77655619033
Iteration 1500: Loss = -10821.758415913648
Iteration 1600: Loss = -10821.736118841767
Iteration 1700: Loss = -10821.70400642849
Iteration 1800: Loss = -10821.648568231807
Iteration 1900: Loss = -10821.534445693324
Iteration 2000: Loss = -10821.294590515223
Iteration 2100: Loss = -10820.919074907955
Iteration 2200: Loss = -10820.375957542434
Iteration 2300: Loss = -10819.416896987104
Iteration 2400: Loss = -10818.921193870987
Iteration 2500: Loss = -10817.748268033673
Iteration 2600: Loss = -10817.552340863884
Iteration 2700: Loss = -10817.545709601956
Iteration 2800: Loss = -10817.541546242366
Iteration 2900: Loss = -10817.539748680323
Iteration 3000: Loss = -10817.538214967473
Iteration 3100: Loss = -10817.536936584334
Iteration 3200: Loss = -10817.535783234147
Iteration 3300: Loss = -10817.534833725658
Iteration 3400: Loss = -10817.533974400281
Iteration 3500: Loss = -10817.557182862625
1
Iteration 3600: Loss = -10817.532530959037
Iteration 3700: Loss = -10817.531890576302
Iteration 3800: Loss = -10817.53139255752
Iteration 3900: Loss = -10817.530852947659
Iteration 4000: Loss = -10817.531451210278
1
Iteration 4100: Loss = -10817.530003589898
Iteration 4200: Loss = -10817.52960891096
Iteration 4300: Loss = -10817.536006996068
1
Iteration 4400: Loss = -10817.528950195489
Iteration 4500: Loss = -10817.52864644098
Iteration 4600: Loss = -10817.528420585795
Iteration 4700: Loss = -10817.528170325697
Iteration 4800: Loss = -10817.528833965149
1
Iteration 4900: Loss = -10817.52771694549
Iteration 5000: Loss = -10817.527528385297
Iteration 5100: Loss = -10817.527679520139
1
Iteration 5200: Loss = -10817.527162928007
Iteration 5300: Loss = -10817.532642449834
1
Iteration 5400: Loss = -10817.526866372724
Iteration 5500: Loss = -10817.526743777024
Iteration 5600: Loss = -10817.527897709711
1
Iteration 5700: Loss = -10817.526500581409
Iteration 5800: Loss = -10817.526388368547
Iteration 5900: Loss = -10817.526477352627
Iteration 6000: Loss = -10817.526169762486
Iteration 6100: Loss = -10817.526213844014
Iteration 6200: Loss = -10817.526012579165
Iteration 6300: Loss = -10817.525938920562
Iteration 6400: Loss = -10817.528368428684
1
Iteration 6500: Loss = -10817.525782133353
Iteration 6600: Loss = -10817.525854309688
Iteration 6700: Loss = -10817.525668930855
Iteration 6800: Loss = -10817.525594614423
Iteration 6900: Loss = -10817.525811222316
1
Iteration 7000: Loss = -10817.525506095384
Iteration 7100: Loss = -10817.56537322208
1
Iteration 7200: Loss = -10817.52541534268
Iteration 7300: Loss = -10817.525363693463
Iteration 7400: Loss = -10817.525577334476
1
Iteration 7500: Loss = -10817.525261848225
Iteration 7600: Loss = -10817.531275566716
1
Iteration 7700: Loss = -10817.52522537827
Iteration 7800: Loss = -10817.525167404774
Iteration 7900: Loss = -10817.525147656686
Iteration 8000: Loss = -10817.525133253534
Iteration 8100: Loss = -10817.525088248187
Iteration 8200: Loss = -10817.525068209341
Iteration 8300: Loss = -10817.52503050547
Iteration 8400: Loss = -10817.526432139823
1
Iteration 8500: Loss = -10817.525011945103
Iteration 8600: Loss = -10817.615842702495
1
Iteration 8700: Loss = -10817.526051245246
2
Iteration 8800: Loss = -10817.525112095802
3
Iteration 8900: Loss = -10817.525009992043
Iteration 9000: Loss = -10817.5256920509
1
Iteration 9100: Loss = -10817.524873121365
Iteration 9200: Loss = -10817.5249009935
Iteration 9300: Loss = -10817.524831805977
Iteration 9400: Loss = -10817.526231590242
1
Iteration 9500: Loss = -10817.524834243914
Iteration 9600: Loss = -10817.525274733896
1
Iteration 9700: Loss = -10817.781345179868
2
Iteration 9800: Loss = -10817.525866563803
3
Iteration 9900: Loss = -10817.545460391937
4
Iteration 10000: Loss = -10817.525145650829
5
Iteration 10100: Loss = -10817.587323408226
6
Iteration 10200: Loss = -10817.540705231415
7
Iteration 10300: Loss = -10817.528381897253
8
Iteration 10400: Loss = -10817.524738063023
Iteration 10500: Loss = -10817.596591695705
1
Iteration 10600: Loss = -10817.524720955531
Iteration 10700: Loss = -10817.526551824027
1
Iteration 10800: Loss = -10817.52472403126
Iteration 10900: Loss = -10817.527881895918
1
Iteration 11000: Loss = -10817.524699297399
Iteration 11100: Loss = -10817.531186069515
1
Iteration 11200: Loss = -10817.524691380862
Iteration 11300: Loss = -10817.5491929397
1
Iteration 11400: Loss = -10817.525724732412
2
Iteration 11500: Loss = -10817.525737832839
3
Iteration 11600: Loss = -10817.525071313334
4
Iteration 11700: Loss = -10817.543540107376
5
Iteration 11800: Loss = -10817.525189779783
6
Iteration 11900: Loss = -10817.527162881013
7
Iteration 12000: Loss = -10817.790438887609
8
Iteration 12100: Loss = -10817.52465371594
Iteration 12200: Loss = -10817.5250522057
1
Iteration 12300: Loss = -10817.553908226193
2
Iteration 12400: Loss = -10817.52495693311
3
Iteration 12500: Loss = -10817.525262060879
4
Iteration 12600: Loss = -10817.674624460627
5
Iteration 12700: Loss = -10817.526330156972
6
Iteration 12800: Loss = -10817.525966582554
7
Iteration 12900: Loss = -10817.5248083083
8
Iteration 13000: Loss = -10817.545022373637
9
Iteration 13100: Loss = -10817.528083968764
10
Iteration 13200: Loss = -10817.524669197463
Iteration 13300: Loss = -10817.54443235262
1
Iteration 13400: Loss = -10817.630894622675
2
Iteration 13500: Loss = -10817.527423687856
3
Iteration 13600: Loss = -10817.524638497296
Iteration 13700: Loss = -10817.524971858524
1
Iteration 13800: Loss = -10817.524651429232
Iteration 13900: Loss = -10817.559317424257
1
Iteration 14000: Loss = -10817.525401472672
2
Iteration 14100: Loss = -10817.53407595769
3
Iteration 14200: Loss = -10817.625139607211
4
Iteration 14300: Loss = -10817.52511015077
5
Iteration 14400: Loss = -10817.525959353803
6
Iteration 14500: Loss = -10817.528814986645
7
Iteration 14600: Loss = -10817.548065066687
8
Iteration 14700: Loss = -10817.52460075847
Iteration 14800: Loss = -10817.53277680599
1
Iteration 14900: Loss = -10817.561961157324
2
Iteration 15000: Loss = -10817.524682442687
Iteration 15100: Loss = -10817.524629427473
Iteration 15200: Loss = -10817.669633107163
1
Iteration 15300: Loss = -10817.525685924209
2
Iteration 15400: Loss = -10817.524740210481
3
Iteration 15500: Loss = -10817.52859257164
4
Iteration 15600: Loss = -10817.529943705254
5
Iteration 15700: Loss = -10817.535148882665
6
Iteration 15800: Loss = -10817.526137200213
7
Iteration 15900: Loss = -10817.525385332328
8
Iteration 16000: Loss = -10817.524672653211
Iteration 16100: Loss = -10817.524586303729
Iteration 16200: Loss = -10817.525679543427
1
Iteration 16300: Loss = -10817.555917411442
2
Iteration 16400: Loss = -10817.524735695211
3
Iteration 16500: Loss = -10817.527210033308
4
Iteration 16600: Loss = -10817.524633485897
Iteration 16700: Loss = -10817.525106620069
1
Iteration 16800: Loss = -10817.569505704281
2
Iteration 16900: Loss = -10817.53997115524
3
Iteration 17000: Loss = -10817.527737046612
4
Iteration 17100: Loss = -10817.52689283324
5
Iteration 17200: Loss = -10817.52536017508
6
Iteration 17300: Loss = -10817.526066275628
7
Iteration 17400: Loss = -10817.524651632786
Iteration 17500: Loss = -10817.561093180095
1
Iteration 17600: Loss = -10817.52457520443
Iteration 17700: Loss = -10817.524702534942
1
Iteration 17800: Loss = -10817.726040672542
2
Iteration 17900: Loss = -10817.52457675958
Iteration 18000: Loss = -10817.526672540995
1
Iteration 18100: Loss = -10817.524610351793
Iteration 18200: Loss = -10817.524679441753
Iteration 18300: Loss = -10817.525048465306
1
Iteration 18400: Loss = -10817.526343571777
2
Iteration 18500: Loss = -10817.563056653375
3
Iteration 18600: Loss = -10817.52459792204
Iteration 18700: Loss = -10817.52594765671
1
Iteration 18800: Loss = -10817.524615544233
Iteration 18900: Loss = -10817.525285538384
1
Iteration 19000: Loss = -10817.525787427012
2
Iteration 19100: Loss = -10817.53074982427
3
Iteration 19200: Loss = -10817.540425114665
4
Iteration 19300: Loss = -10817.528137562327
5
Iteration 19400: Loss = -10817.576557912822
6
Iteration 19500: Loss = -10817.531450252964
7
Iteration 19600: Loss = -10817.554516966376
8
Iteration 19700: Loss = -10817.524649353254
Iteration 19800: Loss = -10817.542467811749
1
Iteration 19900: Loss = -10817.52631113849
2
pi: tensor([[6.2741e-07, 1.0000e+00],
        [1.5306e-01, 8.4694e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0098, 0.9902], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1871, 0.3089],
         [0.6089, 0.1593]],

        [[0.5032, 0.1864],
         [0.5828, 0.5357]],

        [[0.6826, 0.1453],
         [0.5320, 0.6798]],

        [[0.7241, 0.1708],
         [0.5512, 0.6696]],

        [[0.6606, 0.0904],
         [0.6114, 0.6656]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.009175716042558325
Global Adjusted Rand Index: -0.0053912133004892305
Average Adjusted Rand Index: -0.0007444288880169483
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22704.792960387404
Iteration 100: Loss = -10824.584847420885
Iteration 200: Loss = -10823.945315845978
Iteration 300: Loss = -10823.744645624798
Iteration 400: Loss = -10823.619248330691
Iteration 500: Loss = -10823.518752902282
Iteration 600: Loss = -10823.424407367744
Iteration 700: Loss = -10823.322236797523
Iteration 800: Loss = -10823.195543976366
Iteration 900: Loss = -10823.020371667872
Iteration 1000: Loss = -10822.771038142093
Iteration 1100: Loss = -10822.457943868083
Iteration 1200: Loss = -10822.106730569594
Iteration 1300: Loss = -10821.68280504955
Iteration 1400: Loss = -10821.143260947738
Iteration 1500: Loss = -10820.522184760839
Iteration 1600: Loss = -10820.007094752142
Iteration 1700: Loss = -10819.726341156691
Iteration 1800: Loss = -10819.596702307126
Iteration 1900: Loss = -10819.543007632976
Iteration 2000: Loss = -10819.515617008914
Iteration 2100: Loss = -10819.495959867312
Iteration 2200: Loss = -10819.47697819223
Iteration 2300: Loss = -10819.451724501718
Iteration 2400: Loss = -10819.373319213168
Iteration 2500: Loss = -10819.243037954215
Iteration 2600: Loss = -10819.216468246384
Iteration 2700: Loss = -10819.206710103193
Iteration 2800: Loss = -10819.200410391744
Iteration 2900: Loss = -10819.195909665621
Iteration 3000: Loss = -10819.190616165914
Iteration 3100: Loss = -10819.186486871446
Iteration 3200: Loss = -10819.182815708898
Iteration 3300: Loss = -10819.179359051972
Iteration 3400: Loss = -10819.176313328077
Iteration 3500: Loss = -10819.173583412161
Iteration 3600: Loss = -10819.171124094439
Iteration 3700: Loss = -10819.169025200204
Iteration 3800: Loss = -10819.167143833338
Iteration 3900: Loss = -10819.165514087028
Iteration 4000: Loss = -10819.164131177085
Iteration 4100: Loss = -10819.16295934852
Iteration 4200: Loss = -10819.16194621993
Iteration 4300: Loss = -10819.161131714684
Iteration 4400: Loss = -10819.160335097682
Iteration 4500: Loss = -10819.159639983003
Iteration 4600: Loss = -10819.184299806067
1
Iteration 4700: Loss = -10819.15857189421
Iteration 4800: Loss = -10819.158118947105
Iteration 4900: Loss = -10819.157758183746
Iteration 5000: Loss = -10819.15728044129
Iteration 5100: Loss = -10819.156877645124
Iteration 5200: Loss = -10819.156510202014
Iteration 5300: Loss = -10819.15613590357
Iteration 5400: Loss = -10819.156112837025
Iteration 5500: Loss = -10819.15546136663
Iteration 5600: Loss = -10819.155170614662
Iteration 5700: Loss = -10819.157678380197
1
Iteration 5800: Loss = -10819.15453545843
Iteration 5900: Loss = -10819.154177523054
Iteration 6000: Loss = -10819.1538837023
Iteration 6100: Loss = -10819.153570503655
Iteration 6200: Loss = -10819.153314160802
Iteration 6300: Loss = -10819.153026811873
Iteration 6400: Loss = -10819.152766074933
Iteration 6500: Loss = -10819.155274846746
1
Iteration 6600: Loss = -10819.1522161272
Iteration 6700: Loss = -10819.152010837972
Iteration 6800: Loss = -10819.15176533516
Iteration 6900: Loss = -10819.151493905814
Iteration 7000: Loss = -10819.15158861957
Iteration 7100: Loss = -10819.151084373187
Iteration 7200: Loss = -10819.15091170184
Iteration 7300: Loss = -10819.15561924303
1
Iteration 7400: Loss = -10819.150532303553
Iteration 7500: Loss = -10819.15034284139
Iteration 7600: Loss = -10819.150240099605
Iteration 7700: Loss = -10819.14997913659
Iteration 7800: Loss = -10819.165552690338
1
Iteration 7900: Loss = -10819.149979616082
Iteration 8000: Loss = -10819.149527591753
Iteration 8100: Loss = -10819.15015965909
1
Iteration 8200: Loss = -10819.14931230096
Iteration 8300: Loss = -10819.149134283916
Iteration 8400: Loss = -10819.149806842857
1
Iteration 8500: Loss = -10819.149182333385
Iteration 8600: Loss = -10819.148965253362
Iteration 8700: Loss = -10819.170890562726
1
Iteration 8800: Loss = -10819.148524532286
Iteration 8900: Loss = -10819.148610845234
Iteration 9000: Loss = -10819.149338092617
1
Iteration 9100: Loss = -10819.148801889813
2
Iteration 9200: Loss = -10819.157467094969
3
Iteration 9300: Loss = -10819.14893457298
4
Iteration 9400: Loss = -10819.148242598036
Iteration 9500: Loss = -10819.147897736879
Iteration 9600: Loss = -10819.180734737258
1
Iteration 9700: Loss = -10819.149473316793
2
Iteration 9800: Loss = -10819.147905241392
Iteration 9900: Loss = -10819.147694095136
Iteration 10000: Loss = -10819.147853923387
1
Iteration 10100: Loss = -10819.147696970187
Iteration 10200: Loss = -10819.166772038965
1
Iteration 10300: Loss = -10819.147945792782
2
Iteration 10400: Loss = -10819.149462022864
3
Iteration 10500: Loss = -10819.227866885369
4
Iteration 10600: Loss = -10819.14730209753
Iteration 10700: Loss = -10819.148203159248
1
Iteration 10800: Loss = -10819.147244163021
Iteration 10900: Loss = -10819.147163112666
Iteration 11000: Loss = -10819.167160760078
1
Iteration 11100: Loss = -10819.15950135554
2
Iteration 11200: Loss = -10819.148644201952
3
Iteration 11300: Loss = -10819.14784588117
4
Iteration 11400: Loss = -10819.150975610439
5
Iteration 11500: Loss = -10819.148556612205
6
Iteration 11600: Loss = -10819.147165953962
Iteration 11700: Loss = -10819.151671190992
1
Iteration 11800: Loss = -10819.160462680145
2
Iteration 11900: Loss = -10819.147596615776
3
Iteration 12000: Loss = -10819.147220530955
Iteration 12100: Loss = -10819.147368312842
1
Iteration 12200: Loss = -10819.150082221631
2
Iteration 12300: Loss = -10819.146816387838
Iteration 12400: Loss = -10819.15825929207
1
Iteration 12500: Loss = -10819.147020858856
2
Iteration 12600: Loss = -10819.147467099923
3
Iteration 12700: Loss = -10819.234371554683
4
Iteration 12800: Loss = -10819.146973261664
5
Iteration 12900: Loss = -10819.14841889506
6
Iteration 13000: Loss = -10819.146807929184
Iteration 13100: Loss = -10819.161359014162
1
Iteration 13200: Loss = -10819.1466294105
Iteration 13300: Loss = -10819.147123928227
1
Iteration 13400: Loss = -10819.152605491403
2
Iteration 13500: Loss = -10819.14671630613
Iteration 13600: Loss = -10819.149094711294
1
Iteration 13700: Loss = -10819.371575266256
2
Iteration 13800: Loss = -10819.147103248992
3
Iteration 13900: Loss = -10819.15369466086
4
Iteration 14000: Loss = -10819.146749865678
Iteration 14100: Loss = -10819.14674592897
Iteration 14200: Loss = -10819.234115456424
1
Iteration 14300: Loss = -10819.146566007468
Iteration 14400: Loss = -10819.149470005676
1
Iteration 14500: Loss = -10819.146514453798
Iteration 14600: Loss = -10819.148175345525
1
Iteration 14700: Loss = -10819.146605589747
Iteration 14800: Loss = -10819.242279082247
1
Iteration 14900: Loss = -10819.203568637575
2
Iteration 15000: Loss = -10819.1491289804
3
Iteration 15100: Loss = -10819.149394786991
4
Iteration 15200: Loss = -10819.146496040481
Iteration 15300: Loss = -10819.14654179647
Iteration 15400: Loss = -10819.146509765684
Iteration 15500: Loss = -10819.14685403796
1
Iteration 15600: Loss = -10819.155004383754
2
Iteration 15700: Loss = -10819.14804429344
3
Iteration 15800: Loss = -10819.380504073572
4
Iteration 15900: Loss = -10819.148102325667
5
Iteration 16000: Loss = -10819.147941850044
6
Iteration 16100: Loss = -10819.160955785812
7
Iteration 16200: Loss = -10819.146509741107
Iteration 16300: Loss = -10819.15825524793
1
Iteration 16400: Loss = -10819.146508884149
Iteration 16500: Loss = -10819.146747933004
1
Iteration 16600: Loss = -10819.146582888066
Iteration 16700: Loss = -10819.147735013572
1
Iteration 16800: Loss = -10819.154756066255
2
Iteration 16900: Loss = -10819.148329037911
3
Iteration 17000: Loss = -10819.160784323281
4
Iteration 17100: Loss = -10819.15979701071
5
Iteration 17200: Loss = -10819.148551389544
6
Iteration 17300: Loss = -10819.146517506564
Iteration 17400: Loss = -10819.151739976956
1
Iteration 17500: Loss = -10819.146611726615
Iteration 17600: Loss = -10819.146490358147
Iteration 17700: Loss = -10819.148936612575
1
Iteration 17800: Loss = -10819.146416871394
Iteration 17900: Loss = -10819.14645336121
Iteration 18000: Loss = -10819.146783300785
1
Iteration 18100: Loss = -10819.146764177134
2
Iteration 18200: Loss = -10819.146693731502
3
Iteration 18300: Loss = -10819.146452185849
Iteration 18400: Loss = -10819.17352838127
1
Iteration 18500: Loss = -10819.146422350099
Iteration 18600: Loss = -10819.157080763423
1
Iteration 18700: Loss = -10819.146840065812
2
Iteration 18800: Loss = -10819.146531858833
3
Iteration 18900: Loss = -10819.156086604842
4
Iteration 19000: Loss = -10819.146535340305
5
Iteration 19100: Loss = -10819.241273606705
6
Iteration 19200: Loss = -10819.147606102963
7
Iteration 19300: Loss = -10819.146677200317
8
Iteration 19400: Loss = -10819.152552394762
9
Iteration 19500: Loss = -10819.14762302017
10
Iteration 19600: Loss = -10819.146417538292
Iteration 19700: Loss = -10819.1467676287
1
Iteration 19800: Loss = -10819.15504885512
2
Iteration 19900: Loss = -10819.151144605497
3
pi: tensor([[8.4045e-01, 1.5955e-01],
        [9.9998e-01, 2.3930e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9603, 0.0397], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1613, 0.1389],
         [0.6010, 0.1755]],

        [[0.5280, 0.1834],
         [0.7147, 0.5372]],

        [[0.6299, 0.1402],
         [0.7120, 0.6292]],

        [[0.5540, 0.1680],
         [0.6052, 0.6896]],

        [[0.6360, 0.0918],
         [0.7103, 0.5675]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.009175716042558325
Global Adjusted Rand Index: -0.006144687388939376
Average Adjusted Rand Index: -0.001835143208511665
10773.778154873846
[-0.0053912133004892305, -0.006144687388939376] [-0.0007444288880169483, -0.001835143208511665] [10817.526052200532, 10819.151421300505]
-------------------------------------
This iteration is 88
True Objective function: Loss = -10908.428237641367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22931.065743033112
Iteration 100: Loss = -11076.00603365889
Iteration 200: Loss = -11075.34789384036
Iteration 300: Loss = -11075.020623876211
Iteration 400: Loss = -11072.381159194634
Iteration 500: Loss = -11063.296382897208
Iteration 600: Loss = -11012.946082268827
Iteration 700: Loss = -10932.988689406478
Iteration 800: Loss = -10918.912592269873
Iteration 900: Loss = -10914.71003972133
Iteration 1000: Loss = -10912.343311941788
Iteration 1100: Loss = -10912.147880200999
Iteration 1200: Loss = -10912.080224636038
Iteration 1300: Loss = -10912.042242048434
Iteration 1400: Loss = -10912.007936663475
Iteration 1500: Loss = -10911.968761393375
Iteration 1600: Loss = -10911.920816228654
Iteration 1700: Loss = -10911.880885977034
Iteration 1800: Loss = -10911.828598983366
Iteration 1900: Loss = -10911.71453793599
Iteration 2000: Loss = -10911.501691853526
Iteration 2100: Loss = -10911.143474768118
Iteration 2200: Loss = -10905.159445484975
Iteration 2300: Loss = -10891.269734270518
Iteration 2400: Loss = -10890.672548799728
Iteration 2500: Loss = -10888.038324069403
Iteration 2600: Loss = -10887.69289650002
Iteration 2700: Loss = -10887.662923415612
Iteration 2800: Loss = -10887.648491028494
Iteration 2900: Loss = -10887.64304178839
Iteration 3000: Loss = -10887.641443180994
Iteration 3100: Loss = -10887.636262799424
Iteration 3200: Loss = -10887.634402419952
Iteration 3300: Loss = -10887.649683909232
1
Iteration 3400: Loss = -10887.63144616786
Iteration 3500: Loss = -10887.629849696272
Iteration 3600: Loss = -10887.625876489217
Iteration 3700: Loss = -10887.620545641046
Iteration 3800: Loss = -10887.617495476778
Iteration 3900: Loss = -10887.619803357147
1
Iteration 4000: Loss = -10887.610425161502
Iteration 4100: Loss = -10887.619922517852
1
Iteration 4200: Loss = -10887.609518455383
Iteration 4300: Loss = -10887.611001762887
1
Iteration 4400: Loss = -10887.608837781763
Iteration 4500: Loss = -10887.60856925734
Iteration 4600: Loss = -10887.608276680628
Iteration 4700: Loss = -10887.608497258534
1
Iteration 4800: Loss = -10887.619263494784
2
Iteration 4900: Loss = -10887.607748445731
Iteration 5000: Loss = -10887.607701644334
Iteration 5100: Loss = -10887.606998508209
Iteration 5200: Loss = -10887.609571035311
1
Iteration 5300: Loss = -10887.609194237455
2
Iteration 5400: Loss = -10887.606415256605
Iteration 5500: Loss = -10887.606268785927
Iteration 5600: Loss = -10887.606854439373
1
Iteration 5700: Loss = -10887.607900883695
2
Iteration 5800: Loss = -10887.605854196627
Iteration 5900: Loss = -10887.605671346384
Iteration 6000: Loss = -10887.605350950631
Iteration 6100: Loss = -10887.604698377858
Iteration 6200: Loss = -10887.630736086383
1
Iteration 6300: Loss = -10887.604855627746
2
Iteration 6400: Loss = -10887.604193344898
Iteration 6500: Loss = -10887.602986914238
Iteration 6600: Loss = -10887.602668854992
Iteration 6700: Loss = -10887.603062111744
1
Iteration 6800: Loss = -10887.606964599716
2
Iteration 6900: Loss = -10887.602811168135
3
Iteration 7000: Loss = -10887.600034863877
Iteration 7100: Loss = -10887.601349482498
1
Iteration 7200: Loss = -10887.613960937746
2
Iteration 7300: Loss = -10887.59949171826
Iteration 7400: Loss = -10887.59904437711
Iteration 7500: Loss = -10887.600407405691
1
Iteration 7600: Loss = -10887.598568131667
Iteration 7700: Loss = -10887.595071017924
Iteration 7800: Loss = -10887.594957350724
Iteration 7900: Loss = -10887.595007496267
Iteration 8000: Loss = -10887.594915560892
Iteration 8100: Loss = -10887.595313582857
1
Iteration 8200: Loss = -10887.594833153882
Iteration 8300: Loss = -10887.599511483126
1
Iteration 8400: Loss = -10887.594801908335
Iteration 8500: Loss = -10887.594810495422
Iteration 8600: Loss = -10887.59482348675
Iteration 8700: Loss = -10887.59474730166
Iteration 8800: Loss = -10887.594732178259
Iteration 8900: Loss = -10887.594797010106
Iteration 9000: Loss = -10887.594705652216
Iteration 9100: Loss = -10887.594686792307
Iteration 9200: Loss = -10887.59493844279
1
Iteration 9300: Loss = -10887.594697909339
Iteration 9400: Loss = -10887.594656836136
Iteration 9500: Loss = -10887.595772346025
1
Iteration 9600: Loss = -10887.59462706933
Iteration 9700: Loss = -10887.595351204098
1
Iteration 9800: Loss = -10887.595907512268
2
Iteration 9900: Loss = -10887.59475992065
3
Iteration 10000: Loss = -10887.594747768417
4
Iteration 10100: Loss = -10887.737422642516
5
Iteration 10200: Loss = -10887.594587717147
Iteration 10300: Loss = -10887.595928351637
1
Iteration 10400: Loss = -10887.594510709196
Iteration 10500: Loss = -10887.59549798126
1
Iteration 10600: Loss = -10887.594478415973
Iteration 10700: Loss = -10887.689810538803
1
Iteration 10800: Loss = -10887.59594392789
2
Iteration 10900: Loss = -10887.596260211922
3
Iteration 11000: Loss = -10887.594041905098
Iteration 11100: Loss = -10887.594474793406
1
Iteration 11200: Loss = -10887.637989299978
2
Iteration 11300: Loss = -10887.5938981049
Iteration 11400: Loss = -10887.5973990515
1
Iteration 11500: Loss = -10887.593868143244
Iteration 11600: Loss = -10887.596451023983
1
Iteration 11700: Loss = -10887.59391289302
Iteration 11800: Loss = -10887.593875620856
Iteration 11900: Loss = -10887.601875372922
1
Iteration 12000: Loss = -10887.59381636195
Iteration 12100: Loss = -10887.595464037586
1
Iteration 12200: Loss = -10887.593579739942
Iteration 12300: Loss = -10887.593996959416
1
Iteration 12400: Loss = -10887.593532071565
Iteration 12500: Loss = -10887.599985343862
1
Iteration 12600: Loss = -10887.593538780497
Iteration 12700: Loss = -10887.593489612551
Iteration 12800: Loss = -10887.593377753321
Iteration 12900: Loss = -10887.593899358491
1
Iteration 13000: Loss = -10887.593378150703
Iteration 13100: Loss = -10887.600382136266
1
Iteration 13200: Loss = -10887.593424817083
Iteration 13300: Loss = -10887.593382073594
Iteration 13400: Loss = -10887.623149678298
1
Iteration 13500: Loss = -10887.59337358561
Iteration 13600: Loss = -10887.997170162178
1
Iteration 13700: Loss = -10887.59435917961
2
Iteration 13800: Loss = -10887.599559362601
3
Iteration 13900: Loss = -10887.593439177152
Iteration 14000: Loss = -10887.640063994422
1
Iteration 14100: Loss = -10887.593584817656
2
Iteration 14200: Loss = -10887.598329952476
3
Iteration 14300: Loss = -10887.593392031882
Iteration 14400: Loss = -10887.593969592272
1
Iteration 14500: Loss = -10887.59503287833
2
Iteration 14600: Loss = -10887.597405284903
3
Iteration 14700: Loss = -10887.59760719924
4
Iteration 14800: Loss = -10887.59336758459
Iteration 14900: Loss = -10887.593604960979
1
Iteration 15000: Loss = -10887.594166256637
2
Iteration 15100: Loss = -10887.606181401261
3
Iteration 15200: Loss = -10887.593403161341
Iteration 15300: Loss = -10887.758177719097
1
Iteration 15400: Loss = -10887.593385381206
Iteration 15500: Loss = -10887.593449763503
Iteration 15600: Loss = -10887.59360060853
1
Iteration 15700: Loss = -10887.59336727352
Iteration 15800: Loss = -10887.594087943115
1
Iteration 15900: Loss = -10887.59338128768
Iteration 16000: Loss = -10887.596629876447
1
Iteration 16100: Loss = -10887.593364052582
Iteration 16200: Loss = -10887.760603388308
1
Iteration 16300: Loss = -10887.59340020393
Iteration 16400: Loss = -10887.593394752084
Iteration 16500: Loss = -10887.593655255727
1
Iteration 16600: Loss = -10887.593388337988
Iteration 16700: Loss = -10887.596435144726
1
Iteration 16800: Loss = -10887.593831056585
2
Iteration 16900: Loss = -10887.59376438866
3
Iteration 17000: Loss = -10887.593389790081
Iteration 17100: Loss = -10887.596086668067
1
Iteration 17200: Loss = -10887.618490493338
2
Iteration 17300: Loss = -10887.594032113364
3
Iteration 17400: Loss = -10887.593369218243
Iteration 17500: Loss = -10887.593498528357
1
Iteration 17600: Loss = -10887.59340443948
Iteration 17700: Loss = -10887.675728944118
1
Iteration 17800: Loss = -10887.595176990753
2
Iteration 17900: Loss = -10887.596241926602
3
Iteration 18000: Loss = -10887.614385612223
4
Iteration 18100: Loss = -10887.595655472445
5
Iteration 18200: Loss = -10887.593375127602
Iteration 18300: Loss = -10887.595489464331
1
Iteration 18400: Loss = -10887.593367593287
Iteration 18500: Loss = -10887.5945215831
1
Iteration 18600: Loss = -10887.593326291033
Iteration 18700: Loss = -10887.619898212759
1
Iteration 18800: Loss = -10887.593337968807
Iteration 18900: Loss = -10887.593354153565
Iteration 19000: Loss = -10887.595713115723
1
Iteration 19100: Loss = -10887.593883568547
2
Iteration 19200: Loss = -10887.59885351199
3
Iteration 19300: Loss = -10887.593337532628
Iteration 19400: Loss = -10887.593477860584
1
Iteration 19500: Loss = -10887.600115506353
2
Iteration 19600: Loss = -10887.593334949794
Iteration 19700: Loss = -10887.649359729336
1
Iteration 19800: Loss = -10887.593317650784
Iteration 19900: Loss = -10887.59358793305
1
pi: tensor([[0.6320, 0.3680],
        [0.4784, 0.5216]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3647, 0.6353], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2574, 0.0902],
         [0.5900, 0.2087]],

        [[0.7228, 0.0957],
         [0.5441, 0.6483]],

        [[0.6287, 0.0901],
         [0.6802, 0.6378]],

        [[0.5981, 0.0916],
         [0.5691, 0.7224]],

        [[0.7253, 0.0933],
         [0.5907, 0.6507]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 88
Adjusted Rand Index: 0.5734405020005993
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448573745636614
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080863220989386
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.34916598795706805
Average Adjusted Rand Index: 0.8059209857228289
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21314.71623654573
Iteration 100: Loss = -11076.495061706812
Iteration 200: Loss = -11075.316999179333
Iteration 300: Loss = -11074.301320378454
Iteration 400: Loss = -11071.971468688345
Iteration 500: Loss = -11069.73821396023
Iteration 600: Loss = -11055.640350611187
Iteration 700: Loss = -11008.908929077394
Iteration 800: Loss = -10949.620444021617
Iteration 900: Loss = -10929.222121699833
Iteration 1000: Loss = -10923.360630479398
Iteration 1100: Loss = -10923.14730846511
Iteration 1200: Loss = -10922.841325169062
Iteration 1300: Loss = -10919.937119633792
Iteration 1400: Loss = -10919.808339292566
Iteration 1500: Loss = -10919.700526945313
Iteration 1600: Loss = -10919.651441453736
Iteration 1700: Loss = -10919.627587494462
Iteration 1800: Loss = -10919.604743633432
Iteration 1900: Loss = -10918.809727082935
Iteration 2000: Loss = -10914.830804244959
Iteration 2100: Loss = -10914.574936828447
Iteration 2200: Loss = -10912.285277502842
Iteration 2300: Loss = -10912.272388716983
Iteration 2400: Loss = -10912.255062385177
Iteration 2500: Loss = -10912.016708250274
Iteration 2600: Loss = -10912.010856413819
Iteration 2700: Loss = -10912.005732312027
Iteration 2800: Loss = -10912.001248479715
Iteration 2900: Loss = -10911.99747652692
Iteration 3000: Loss = -10911.993803748572
Iteration 3100: Loss = -10911.988683290472
Iteration 3200: Loss = -10911.848560822998
Iteration 3300: Loss = -10911.832610965586
Iteration 3400: Loss = -10911.825764660245
Iteration 3500: Loss = -10911.825510454233
Iteration 3600: Loss = -10911.819261683353
Iteration 3700: Loss = -10911.815908178607
Iteration 3800: Loss = -10911.814188409071
Iteration 3900: Loss = -10911.816757169903
1
Iteration 4000: Loss = -10911.798134722194
Iteration 4100: Loss = -10911.78226819587
Iteration 4200: Loss = -10911.757741127123
Iteration 4300: Loss = -10911.703011876716
Iteration 4400: Loss = -10911.610280267381
Iteration 4500: Loss = -10911.551928838924
Iteration 4600: Loss = -10911.526943118788
Iteration 4700: Loss = -10911.51355754837
Iteration 4800: Loss = -10911.51192420873
Iteration 4900: Loss = -10911.406287546686
Iteration 5000: Loss = -10911.161912496047
Iteration 5100: Loss = -10911.12311599054
Iteration 5200: Loss = -10911.121004295303
Iteration 5300: Loss = -10911.131730922909
1
Iteration 5400: Loss = -10911.085819073034
Iteration 5500: Loss = -10911.084154812886
Iteration 5600: Loss = -10911.051036706354
Iteration 5700: Loss = -10890.968925823807
Iteration 5800: Loss = -10887.832096168757
Iteration 5900: Loss = -10887.753829449035
Iteration 6000: Loss = -10887.755852469807
1
Iteration 6100: Loss = -10887.736187706967
Iteration 6200: Loss = -10887.734985127476
Iteration 6300: Loss = -10887.741955960242
1
Iteration 6400: Loss = -10887.733104252156
Iteration 6500: Loss = -10887.727643464365
Iteration 6600: Loss = -10887.630620691261
Iteration 6700: Loss = -10887.625695974286
Iteration 6800: Loss = -10887.617111137191
Iteration 6900: Loss = -10887.610584763139
Iteration 7000: Loss = -10887.610317826473
Iteration 7100: Loss = -10887.611279405148
1
Iteration 7200: Loss = -10887.609920780978
Iteration 7300: Loss = -10887.609649600376
Iteration 7400: Loss = -10887.609313685183
Iteration 7500: Loss = -10887.608439195235
Iteration 7600: Loss = -10887.613007269292
1
Iteration 7700: Loss = -10887.594152604608
Iteration 7800: Loss = -10887.594103262343
Iteration 7900: Loss = -10887.595066037718
1
Iteration 8000: Loss = -10887.594041189084
Iteration 8100: Loss = -10887.593953682564
Iteration 8200: Loss = -10887.59389212116
Iteration 8300: Loss = -10887.593951519397
Iteration 8400: Loss = -10887.593766520988
Iteration 8500: Loss = -10887.593714670227
Iteration 8600: Loss = -10887.595729908307
1
Iteration 8700: Loss = -10887.593682632343
Iteration 8800: Loss = -10887.593668446349
Iteration 8900: Loss = -10887.593903673349
1
Iteration 9000: Loss = -10887.593554920642
Iteration 9100: Loss = -10887.593550354473
Iteration 9200: Loss = -10887.593572545195
Iteration 9300: Loss = -10887.593531940498
Iteration 9400: Loss = -10887.593493725464
Iteration 9500: Loss = -10887.594151990115
1
Iteration 9600: Loss = -10887.593462206189
Iteration 9700: Loss = -10887.593470264075
Iteration 9800: Loss = -10887.593611209233
1
Iteration 9900: Loss = -10887.59340325191
Iteration 10000: Loss = -10887.701796418933
1
Iteration 10100: Loss = -10887.593331980292
Iteration 10200: Loss = -10887.59338656358
Iteration 10300: Loss = -10887.595090102175
1
Iteration 10400: Loss = -10887.593327994042
Iteration 10500: Loss = -10887.59333053456
Iteration 10600: Loss = -10887.59774537417
1
Iteration 10700: Loss = -10887.593302891552
Iteration 10800: Loss = -10887.594855417274
1
Iteration 10900: Loss = -10887.593251773522
Iteration 11000: Loss = -10887.685183418625
1
Iteration 11100: Loss = -10887.593170772965
Iteration 11200: Loss = -10887.605412818733
1
Iteration 11300: Loss = -10887.599843000538
2
Iteration 11400: Loss = -10887.594752509445
3
Iteration 11500: Loss = -10887.610048825096
4
Iteration 11600: Loss = -10887.597352910998
5
Iteration 11700: Loss = -10887.593183568888
Iteration 11800: Loss = -10887.596907711499
1
Iteration 11900: Loss = -10887.593153324995
Iteration 12000: Loss = -10887.593130216226
Iteration 12100: Loss = -10887.59331679828
1
Iteration 12200: Loss = -10887.593099104035
Iteration 12300: Loss = -10887.77507694252
1
Iteration 12400: Loss = -10887.593116082047
Iteration 12500: Loss = -10887.593092245625
Iteration 12600: Loss = -10887.593098119305
Iteration 12700: Loss = -10887.593112630393
Iteration 12800: Loss = -10887.593078205053
Iteration 12900: Loss = -10887.593087628778
Iteration 13000: Loss = -10887.593535611275
1
Iteration 13100: Loss = -10887.593038701381
Iteration 13200: Loss = -10887.593049317813
Iteration 13300: Loss = -10887.609529173225
1
Iteration 13400: Loss = -10887.593065223893
Iteration 13500: Loss = -10887.595164057013
1
Iteration 13600: Loss = -10887.594195648502
2
Iteration 13700: Loss = -10887.597659713514
3
Iteration 13800: Loss = -10887.59541996689
4
Iteration 13900: Loss = -10887.593180822776
5
Iteration 14000: Loss = -10887.60242059046
6
Iteration 14100: Loss = -10887.596489518119
7
Iteration 14200: Loss = -10887.593092434696
Iteration 14300: Loss = -10887.599310590158
1
Iteration 14400: Loss = -10887.5930595361
Iteration 14500: Loss = -10887.59308297953
Iteration 14600: Loss = -10887.593121529055
Iteration 14700: Loss = -10887.59311082239
Iteration 14800: Loss = -10887.593018808091
Iteration 14900: Loss = -10887.594734211973
1
Iteration 15000: Loss = -10887.592997800422
Iteration 15100: Loss = -10887.61458507584
1
Iteration 15200: Loss = -10887.593029694208
Iteration 15300: Loss = -10887.593020750794
Iteration 15400: Loss = -10887.634934249207
1
Iteration 15500: Loss = -10887.593027488043
Iteration 15600: Loss = -10887.593003751717
Iteration 15700: Loss = -10887.594949518523
1
Iteration 15800: Loss = -10887.593015440505
Iteration 15900: Loss = -10887.593608582532
1
Iteration 16000: Loss = -10887.593030939523
Iteration 16100: Loss = -10887.595430909607
1
Iteration 16200: Loss = -10887.593055299303
Iteration 16300: Loss = -10887.597510332103
1
Iteration 16400: Loss = -10887.592760857578
Iteration 16500: Loss = -10887.593968488645
1
Iteration 16600: Loss = -10887.594138277049
2
Iteration 16700: Loss = -10887.5927508862
Iteration 16800: Loss = -10887.59403619615
1
Iteration 16900: Loss = -10887.603978930962
2
Iteration 17000: Loss = -10887.59271616774
Iteration 17100: Loss = -10887.656812757918
1
Iteration 17200: Loss = -10887.59272671804
Iteration 17300: Loss = -10887.83893323778
1
Iteration 17400: Loss = -10887.592682410783
Iteration 17500: Loss = -10887.608311663553
1
Iteration 17600: Loss = -10887.597100858618
2
Iteration 17700: Loss = -10887.593914655425
3
Iteration 17800: Loss = -10887.595663285558
4
Iteration 17900: Loss = -10887.592979938907
5
Iteration 18000: Loss = -10887.593612982231
6
Iteration 18100: Loss = -10887.592691498523
Iteration 18200: Loss = -10887.592779861683
Iteration 18300: Loss = -10887.59275310146
Iteration 18400: Loss = -10887.592743036183
Iteration 18500: Loss = -10887.592703299217
Iteration 18600: Loss = -10887.59275814893
Iteration 18700: Loss = -10887.592725398057
Iteration 18800: Loss = -10887.595146689002
1
Iteration 18900: Loss = -10887.592918591346
2
Iteration 19000: Loss = -10887.594757438072
3
Iteration 19100: Loss = -10887.59267530518
Iteration 19200: Loss = -10887.596524013657
1
Iteration 19300: Loss = -10887.592943439502
2
Iteration 19400: Loss = -10887.59290652238
3
Iteration 19500: Loss = -10887.602940394814
4
Iteration 19600: Loss = -10887.714503807834
5
Iteration 19700: Loss = -10887.592687022252
Iteration 19800: Loss = -10887.592717809015
Iteration 19900: Loss = -10887.592793436339
pi: tensor([[0.5220, 0.4780],
        [0.3684, 0.6316]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6357, 0.3643], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2085, 0.0901],
         [0.6457, 0.2576]],

        [[0.6526, 0.0958],
         [0.5726, 0.6716]],

        [[0.5354, 0.0903],
         [0.5303, 0.5539]],

        [[0.5958, 0.0917],
         [0.6318, 0.7096]],

        [[0.7009, 0.0933],
         [0.6791, 0.5856]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 12
Adjusted Rand Index: 0.5734405020005993
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448573745636614
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080863220989386
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
Global Adjusted Rand Index: 0.34916598795706805
Average Adjusted Rand Index: 0.8059209857228289
10908.428237641367
[0.34916598795706805, 0.34916598795706805] [0.8059209857228289, 0.8059209857228289] [10887.594673876418, 10887.59272170276]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11100.705766105853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23352.90364565801
Iteration 100: Loss = -11246.729000661293
Iteration 200: Loss = -11096.977525459724
Iteration 300: Loss = -11061.139443450997
Iteration 400: Loss = -11060.674881821476
Iteration 500: Loss = -11060.416186931183
Iteration 600: Loss = -11060.284266192899
Iteration 700: Loss = -11060.208259545541
Iteration 800: Loss = -11059.660277898045
Iteration 900: Loss = -11059.647426338704
Iteration 1000: Loss = -11059.63855032335
Iteration 1100: Loss = -11059.632738203232
Iteration 1200: Loss = -11059.30255094959
Iteration 1300: Loss = -11059.258003148178
Iteration 1400: Loss = -11059.254111805756
Iteration 1500: Loss = -11059.244289837914
Iteration 1600: Loss = -11059.231548720913
Iteration 1700: Loss = -11059.225956140783
Iteration 1800: Loss = -11059.224472208172
Iteration 1900: Loss = -11059.227206888147
1
Iteration 2000: Loss = -11059.221955862598
Iteration 2100: Loss = -11059.22069683642
Iteration 2200: Loss = -11059.219168444803
Iteration 2300: Loss = -11059.216762811067
Iteration 2400: Loss = -11059.211634662495
Iteration 2500: Loss = -11059.209174181118
Iteration 2600: Loss = -11059.208359224967
Iteration 2700: Loss = -11059.207881932665
Iteration 2800: Loss = -11059.207423827456
Iteration 2900: Loss = -11059.206953268551
Iteration 3000: Loss = -11059.206470827132
Iteration 3100: Loss = -11059.20597386394
Iteration 3200: Loss = -11059.206165993464
1
Iteration 3300: Loss = -11059.204045831926
Iteration 3400: Loss = -11059.203574798345
Iteration 3500: Loss = -11059.20318447588
Iteration 3600: Loss = -11059.202425881967
Iteration 3700: Loss = -11059.202517664882
Iteration 3800: Loss = -11059.202721731257
1
Iteration 3900: Loss = -11059.200785868916
Iteration 4000: Loss = -11059.199984447536
Iteration 4100: Loss = -11059.202363323893
1
Iteration 4200: Loss = -11059.200315793641
2
Iteration 4300: Loss = -11059.186932960623
Iteration 4400: Loss = -11059.173931321078
Iteration 4500: Loss = -11059.174352433285
1
Iteration 4600: Loss = -11059.173252621607
Iteration 4700: Loss = -11059.173156513352
Iteration 4800: Loss = -11059.172328706252
Iteration 4900: Loss = -11059.171800242035
Iteration 5000: Loss = -11059.172209362729
1
Iteration 5100: Loss = -11059.171983598668
2
Iteration 5200: Loss = -11059.171637353047
Iteration 5300: Loss = -11059.171546013184
Iteration 5400: Loss = -11059.171263791744
Iteration 5500: Loss = -11059.170977996793
Iteration 5600: Loss = -11059.170727702642
Iteration 5700: Loss = -11059.177130681648
1
Iteration 5800: Loss = -11059.16949920191
Iteration 5900: Loss = -11059.169552559364
Iteration 6000: Loss = -11059.169505802069
Iteration 6100: Loss = -11059.169520372043
Iteration 6200: Loss = -11059.171939658814
1
Iteration 6300: Loss = -11059.182251300335
2
Iteration 6400: Loss = -11059.169830233526
3
Iteration 6500: Loss = -11059.171718968717
4
Iteration 6600: Loss = -11059.170747209051
5
Iteration 6700: Loss = -11059.17257832315
6
Iteration 6800: Loss = -11059.169648368978
7
Iteration 6900: Loss = -11059.170594033052
8
Iteration 7000: Loss = -11059.172611709993
9
Iteration 7100: Loss = -11059.170325461097
10
Iteration 7200: Loss = -11059.175654099907
11
Iteration 7300: Loss = -11059.174117201886
12
Iteration 7400: Loss = -11059.168877017772
Iteration 7500: Loss = -11059.168918715126
Iteration 7600: Loss = -11059.169289959753
1
Iteration 7700: Loss = -11059.16882794195
Iteration 7800: Loss = -11059.168899386297
Iteration 7900: Loss = -11059.170485021905
1
Iteration 8000: Loss = -11059.165441649291
Iteration 8100: Loss = -11059.16604101619
1
Iteration 8200: Loss = -11059.165446099083
Iteration 8300: Loss = -11059.164839817951
Iteration 8400: Loss = -11059.164817613399
Iteration 8500: Loss = -11059.1647879468
Iteration 8600: Loss = -11059.164805911658
Iteration 8700: Loss = -11059.174388641033
1
Iteration 8800: Loss = -11059.16471723983
Iteration 8900: Loss = -11059.164772571621
Iteration 9000: Loss = -11059.16508994766
1
Iteration 9100: Loss = -11059.164668817835
Iteration 9200: Loss = -11059.211246508205
1
Iteration 9300: Loss = -11059.163722307694
Iteration 9400: Loss = -11059.396817911509
1
Iteration 9500: Loss = -11059.163691584052
Iteration 9600: Loss = -11059.163713517662
Iteration 9700: Loss = -11059.16375737803
Iteration 9800: Loss = -11059.163700790292
Iteration 9900: Loss = -11059.17883134251
1
Iteration 10000: Loss = -11059.163674926604
Iteration 10100: Loss = -11059.163674409305
Iteration 10200: Loss = -11059.164392148912
1
Iteration 10300: Loss = -11059.163675017004
Iteration 10400: Loss = -11059.164272392005
1
Iteration 10500: Loss = -11059.163682312601
Iteration 10600: Loss = -11059.163629461242
Iteration 10700: Loss = -11059.163603292842
Iteration 10800: Loss = -11059.163741960952
1
Iteration 10900: Loss = -11059.163591655632
Iteration 11000: Loss = -11059.163621586602
Iteration 11100: Loss = -11059.167474442791
1
Iteration 11200: Loss = -11059.163600201895
Iteration 11300: Loss = -11059.181735575301
1
Iteration 11400: Loss = -11059.163631310597
Iteration 11500: Loss = -11059.163611867523
Iteration 11600: Loss = -11059.163916623622
1
Iteration 11700: Loss = -11059.163561811036
Iteration 11800: Loss = -11059.258612830195
1
Iteration 11900: Loss = -11059.163530162892
Iteration 12000: Loss = -11059.163554332048
Iteration 12100: Loss = -11059.1693268965
1
Iteration 12200: Loss = -11059.163510919074
Iteration 12300: Loss = -11059.163443972606
Iteration 12400: Loss = -11059.163733347275
1
Iteration 12500: Loss = -11059.163465558267
Iteration 12600: Loss = -11059.194999461775
1
Iteration 12700: Loss = -11059.163442309957
Iteration 12800: Loss = -11059.163564038641
1
Iteration 12900: Loss = -11059.16345433993
Iteration 13000: Loss = -11059.163347348709
Iteration 13100: Loss = -11059.163756226711
1
Iteration 13200: Loss = -11059.163183226761
Iteration 13300: Loss = -11059.378575641736
1
Iteration 13400: Loss = -11059.163199281467
Iteration 13500: Loss = -11059.163170423466
Iteration 13600: Loss = -11059.163160669132
Iteration 13700: Loss = -11059.165010824565
1
Iteration 13800: Loss = -11059.163151718914
Iteration 13900: Loss = -11059.369502016933
1
Iteration 14000: Loss = -11059.162605530262
Iteration 14100: Loss = -11059.162628565706
Iteration 14200: Loss = -11059.191850452113
1
Iteration 14300: Loss = -11059.162616416581
Iteration 14400: Loss = -11059.16262653122
Iteration 14500: Loss = -11059.164941760573
1
Iteration 14600: Loss = -11059.162621279815
Iteration 14700: Loss = -11059.162649156957
Iteration 14800: Loss = -11059.184644292694
1
Iteration 14900: Loss = -11059.162632208896
Iteration 15000: Loss = -11059.162616371881
Iteration 15100: Loss = -11059.179010731563
1
Iteration 15200: Loss = -11059.162668648407
Iteration 15300: Loss = -11059.162635274392
Iteration 15400: Loss = -11059.163177362017
1
Iteration 15500: Loss = -11059.162686949372
Iteration 15600: Loss = -11059.162640876653
Iteration 15700: Loss = -11059.162643899914
Iteration 15800: Loss = -11059.163443753117
1
Iteration 15900: Loss = -11059.162629607921
Iteration 16000: Loss = -11059.162620146755
Iteration 16100: Loss = -11059.163918719736
1
Iteration 16200: Loss = -11059.162636443534
Iteration 16300: Loss = -11059.162645923796
Iteration 16400: Loss = -11059.165236500388
1
Iteration 16500: Loss = -11059.162655383301
Iteration 16600: Loss = -11059.16263492574
Iteration 16700: Loss = -11059.176574785346
1
Iteration 16800: Loss = -11059.162693395145
Iteration 16900: Loss = -11059.162624420149
Iteration 17000: Loss = -11059.174780755602
1
Iteration 17100: Loss = -11059.162625928198
Iteration 17200: Loss = -11059.16264104781
Iteration 17300: Loss = -11059.162881582919
1
Iteration 17400: Loss = -11059.162644277556
Iteration 17500: Loss = -11059.2649264474
1
Iteration 17600: Loss = -11059.162686804151
Iteration 17700: Loss = -11059.162645439734
Iteration 17800: Loss = -11059.211208620267
1
Iteration 17900: Loss = -11059.16267998066
Iteration 18000: Loss = -11059.162629855067
Iteration 18100: Loss = -11059.165653720573
1
Iteration 18200: Loss = -11059.162649878992
Iteration 18300: Loss = -11059.162641249708
Iteration 18400: Loss = -11059.166911596169
1
Iteration 18500: Loss = -11059.162637727663
Iteration 18600: Loss = -11059.16261674127
Iteration 18700: Loss = -11059.25952951285
1
Iteration 18800: Loss = -11059.162631656725
Iteration 18900: Loss = -11059.162614597619
Iteration 19000: Loss = -11059.385702659958
1
Iteration 19100: Loss = -11059.162646112854
Iteration 19200: Loss = -11059.162622386752
Iteration 19300: Loss = -11059.211158965853
1
Iteration 19400: Loss = -11059.16266760185
Iteration 19500: Loss = -11059.162640342069
Iteration 19600: Loss = -11059.169956338928
1
Iteration 19700: Loss = -11059.162638154858
Iteration 19800: Loss = -11059.162628740723
Iteration 19900: Loss = -11059.322434343057
1
pi: tensor([[0.7055, 0.2945],
        [0.2126, 0.7874]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5152, 0.4848], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2045, 0.0904],
         [0.5244, 0.2588]],

        [[0.7120, 0.1024],
         [0.6335, 0.6983]],

        [[0.6825, 0.1043],
         [0.5739, 0.6680]],

        [[0.7191, 0.0963],
         [0.5351, 0.6242]],

        [[0.7237, 0.1073],
         [0.5413, 0.5787]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369964883895512
Global Adjusted Rand Index: 0.8758343553667446
Average Adjusted Rand Index: 0.876677202673745
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21590.77868449572
Iteration 100: Loss = -11252.319822104213
Iteration 200: Loss = -11250.138931661568
Iteration 300: Loss = -11247.297327215705
Iteration 400: Loss = -11237.791020784238
Iteration 500: Loss = -11136.587154502517
Iteration 600: Loss = -11121.035161501459
Iteration 700: Loss = -11117.515092728338
Iteration 800: Loss = -11117.219436141195
Iteration 900: Loss = -11117.03931532998
Iteration 1000: Loss = -11116.01102163685
Iteration 1100: Loss = -11114.198340698942
Iteration 1200: Loss = -11114.105809209646
Iteration 1300: Loss = -11114.065732758774
Iteration 1400: Loss = -11114.037765719168
Iteration 1500: Loss = -11114.01877139544
Iteration 1600: Loss = -11114.006659597695
Iteration 1700: Loss = -11113.998044831686
Iteration 1800: Loss = -11113.991137222056
Iteration 1900: Loss = -11113.985157361523
Iteration 2000: Loss = -11113.979010802706
Iteration 2100: Loss = -11113.976688751143
Iteration 2200: Loss = -11113.971374719065
Iteration 2300: Loss = -11113.965517397664
Iteration 2400: Loss = -11113.984399142775
1
Iteration 2500: Loss = -11113.955115220717
Iteration 2600: Loss = -11113.953431234379
Iteration 2700: Loss = -11113.985090652084
1
Iteration 2800: Loss = -11113.951072734575
Iteration 2900: Loss = -11113.950157002771
Iteration 3000: Loss = -11113.950493483879
1
Iteration 3100: Loss = -11113.948567897205
Iteration 3200: Loss = -11113.950589080798
1
Iteration 3300: Loss = -11113.947989461813
Iteration 3400: Loss = -11113.947368902387
Iteration 3500: Loss = -11113.961614059055
1
Iteration 3600: Loss = -11113.94360077758
Iteration 3700: Loss = -11113.935967540154
Iteration 3800: Loss = -11113.940868796839
1
Iteration 3900: Loss = -11113.934349458194
Iteration 4000: Loss = -11113.945707902018
1
Iteration 4100: Loss = -11113.933481456063
Iteration 4200: Loss = -11113.932914668048
Iteration 4300: Loss = -11113.954719862353
1
Iteration 4400: Loss = -11113.930841419031
Iteration 4500: Loss = -11113.930370655236
Iteration 4600: Loss = -11113.930084584126
Iteration 4700: Loss = -11113.929072721636
Iteration 4800: Loss = -11113.92874135754
Iteration 4900: Loss = -11113.929136934481
1
Iteration 5000: Loss = -11113.928353297
Iteration 5100: Loss = -11113.928238923869
Iteration 5200: Loss = -11113.928184093544
Iteration 5300: Loss = -11113.927981586767
Iteration 5400: Loss = -11113.927809146744
Iteration 5500: Loss = -11113.927747065456
Iteration 5600: Loss = -11113.927628397656
Iteration 5700: Loss = -11113.927517221771
Iteration 5800: Loss = -11113.927436800423
Iteration 5900: Loss = -11113.927346367813
Iteration 6000: Loss = -11113.944231486515
1
Iteration 6100: Loss = -11113.926667280237
Iteration 6200: Loss = -11113.926049946433
Iteration 6300: Loss = -11113.927727443168
1
Iteration 6400: Loss = -11113.925797232167
Iteration 6500: Loss = -11113.925731813088
Iteration 6600: Loss = -11113.926078163106
1
Iteration 6700: Loss = -11113.925592120288
Iteration 6800: Loss = -11113.930748479019
1
Iteration 6900: Loss = -11113.925264787926
Iteration 7000: Loss = -11113.92508316719
Iteration 7100: Loss = -11113.925076969132
Iteration 7200: Loss = -11113.924931754276
Iteration 7300: Loss = -11113.924877067127
Iteration 7400: Loss = -11113.924841161675
Iteration 7500: Loss = -11113.92485773979
Iteration 7600: Loss = -11113.926287485065
1
Iteration 7700: Loss = -11113.924810286562
Iteration 7800: Loss = -11113.924783703511
Iteration 7900: Loss = -11113.925052469653
1
Iteration 8000: Loss = -11113.926811003397
2
Iteration 8100: Loss = -11113.998228887525
3
Iteration 8200: Loss = -11113.924699055047
Iteration 8300: Loss = -11113.924696771295
Iteration 8400: Loss = -11113.924738494716
Iteration 8500: Loss = -11113.92467717548
Iteration 8600: Loss = -11113.924770808062
Iteration 8700: Loss = -11113.924684775386
Iteration 8800: Loss = -11113.924593136915
Iteration 8900: Loss = -11113.925095700713
1
Iteration 9000: Loss = -11113.924579019988
Iteration 9100: Loss = -11113.932501260198
1
Iteration 9200: Loss = -11113.923614633884
Iteration 9300: Loss = -11113.923598776619
Iteration 9400: Loss = -11113.92358340049
Iteration 9500: Loss = -11113.923669735546
Iteration 9600: Loss = -11113.92336192456
Iteration 9700: Loss = -11113.926894498642
1
Iteration 9800: Loss = -11113.922404794752
Iteration 9900: Loss = -11113.922360885841
Iteration 10000: Loss = -11113.928458479779
1
Iteration 10100: Loss = -11113.922389459276
Iteration 10200: Loss = -11113.922352687243
Iteration 10300: Loss = -11113.923654663395
1
Iteration 10400: Loss = -11113.922349556384
Iteration 10500: Loss = -11113.922356691182
Iteration 10600: Loss = -11113.945803265557
1
Iteration 10700: Loss = -11113.922341840524
Iteration 10800: Loss = -11113.92233339358
Iteration 10900: Loss = -11113.922403569384
Iteration 11000: Loss = -11113.922373195523
Iteration 11100: Loss = -11113.926694611546
1
Iteration 11200: Loss = -11113.9244617034
2
Iteration 11300: Loss = -11113.922333049624
Iteration 11400: Loss = -11113.92676529092
1
Iteration 11500: Loss = -11113.922303380534
Iteration 11600: Loss = -11113.922308875406
Iteration 11700: Loss = -11113.923227652198
1
Iteration 11800: Loss = -11113.922270200632
Iteration 11900: Loss = -11114.481401667457
1
Iteration 12000: Loss = -11113.922302346245
Iteration 12100: Loss = -11113.922301739232
Iteration 12200: Loss = -11114.02904263387
1
Iteration 12300: Loss = -11113.92232692653
Iteration 12400: Loss = -11113.92231081252
Iteration 12500: Loss = -11114.002952277811
1
Iteration 12600: Loss = -11113.922378346808
Iteration 12700: Loss = -11113.922288994407
Iteration 12800: Loss = -11114.42756365623
1
Iteration 12900: Loss = -11113.92231970259
Iteration 13000: Loss = -11113.922329959636
Iteration 13100: Loss = -11113.922527894449
1
Iteration 13200: Loss = -11113.922341247817
Iteration 13300: Loss = -11113.922304394244
Iteration 13400: Loss = -11113.942977501847
1
Iteration 13500: Loss = -11113.922320882733
Iteration 13600: Loss = -11114.16926776327
1
Iteration 13700: Loss = -11113.92232096561
Iteration 13800: Loss = -11113.922286032921
Iteration 13900: Loss = -11113.922372212888
Iteration 14000: Loss = -11113.922313645562
Iteration 14100: Loss = -11113.93942523475
1
Iteration 14200: Loss = -11113.922323311781
Iteration 14300: Loss = -11113.922276155608
Iteration 14400: Loss = -11113.92257051535
1
Iteration 14500: Loss = -11113.922299357344
Iteration 14600: Loss = -11113.922325130232
Iteration 14700: Loss = -11113.92237003232
Iteration 14800: Loss = -11113.922295844615
Iteration 14900: Loss = -11114.041821516057
1
Iteration 15000: Loss = -11113.92228279398
Iteration 15100: Loss = -11113.922279288085
Iteration 15200: Loss = -11113.942119736457
1
Iteration 15300: Loss = -11113.922304378102
Iteration 15400: Loss = -11113.922295812568
Iteration 15500: Loss = -11113.931221441631
1
Iteration 15600: Loss = -11113.922313915296
Iteration 15700: Loss = -11113.922290150831
Iteration 15800: Loss = -11113.92238329408
Iteration 15900: Loss = -11113.922254941283
Iteration 16000: Loss = -11113.949624330135
1
Iteration 16100: Loss = -11113.922215606202
Iteration 16200: Loss = -11113.938567569996
1
Iteration 16300: Loss = -11113.922241390595
Iteration 16400: Loss = -11113.978012253532
1
Iteration 16500: Loss = -11113.92220756016
Iteration 16600: Loss = -11113.92224036188
Iteration 16700: Loss = -11113.922425659217
1
Iteration 16800: Loss = -11113.922233769812
Iteration 16900: Loss = -11113.923919291718
1
Iteration 17000: Loss = -11113.92221917701
Iteration 17100: Loss = -11114.27929491841
1
Iteration 17200: Loss = -11113.922276884932
Iteration 17300: Loss = -11113.922225243505
Iteration 17400: Loss = -11113.922842705777
1
Iteration 17500: Loss = -11113.922247861741
Iteration 17600: Loss = -11114.004565982528
1
Iteration 17700: Loss = -11113.922213443204
Iteration 17800: Loss = -11113.922245038097
Iteration 17900: Loss = -11113.922498476297
1
Iteration 18000: Loss = -11113.922248747436
Iteration 18100: Loss = -11113.99268365421
1
Iteration 18200: Loss = -11113.92223341716
Iteration 18300: Loss = -11113.922257655591
Iteration 18400: Loss = -11113.92268223401
1
Iteration 18500: Loss = -11113.922214506914
Iteration 18600: Loss = -11114.08154546062
1
Iteration 18700: Loss = -11113.922227975894
Iteration 18800: Loss = -11113.922234717324
Iteration 18900: Loss = -11114.022150678162
1
Iteration 19000: Loss = -11113.922248458257
Iteration 19100: Loss = -11113.9222273322
Iteration 19200: Loss = -11113.927096330792
1
Iteration 19300: Loss = -11113.922240345193
Iteration 19400: Loss = -11113.922224808224
Iteration 19500: Loss = -11113.922457235694
1
Iteration 19600: Loss = -11113.922203729311
Iteration 19700: Loss = -11114.359679423347
1
Iteration 19800: Loss = -11113.922248751573
Iteration 19900: Loss = -11113.92221836258
pi: tensor([[0.7612, 0.2388],
        [0.3872, 0.6128]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0479, 0.9521], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2683, 0.0915],
         [0.6839, 0.1794]],

        [[0.6734, 0.1058],
         [0.5727, 0.7124]],

        [[0.6969, 0.1063],
         [0.7077, 0.5542]],

        [[0.6505, 0.0972],
         [0.6093, 0.5135]],

        [[0.7180, 0.1079],
         [0.5203, 0.6277]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691414645181857
Global Adjusted Rand Index: 0.5466494532692191
Average Adjusted Rand Index: 0.6719854152323791
11100.705766105853
[0.8758343553667446, 0.5466494532692191] [0.876677202673745, 0.6719854152323791] [11059.162631784213, 11113.924105912822]
-------------------------------------
This iteration is 90
True Objective function: Loss = -10978.744787558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23620.723914826012
Iteration 100: Loss = -11078.335333967136
Iteration 200: Loss = -11075.75716911819
Iteration 300: Loss = -11073.704871790249
Iteration 400: Loss = -11072.79147766201
Iteration 500: Loss = -11072.187558323381
Iteration 600: Loss = -11071.77165482013
Iteration 700: Loss = -11071.45079897861
Iteration 800: Loss = -11071.188385827803
Iteration 900: Loss = -11070.950322454622
Iteration 1000: Loss = -11070.71590262868
Iteration 1100: Loss = -11070.475809635147
Iteration 1200: Loss = -11070.21927567322
Iteration 1300: Loss = -11069.904810235681
Iteration 1400: Loss = -11069.36291805883
Iteration 1500: Loss = -11068.41008275457
Iteration 1600: Loss = -11067.001426323919
Iteration 1700: Loss = -11066.495422980788
Iteration 1800: Loss = -11066.30356818536
Iteration 1900: Loss = -11066.227994001192
Iteration 2000: Loss = -11066.18454079055
Iteration 2100: Loss = -11066.163425777813
Iteration 2200: Loss = -11066.148898034384
Iteration 2300: Loss = -11066.136761111351
Iteration 2400: Loss = -11066.128649180977
Iteration 2500: Loss = -11066.123900071361
Iteration 2600: Loss = -11066.120377790086
Iteration 2700: Loss = -11066.117574965456
Iteration 2800: Loss = -11066.115200231872
Iteration 2900: Loss = -11066.113238610458
Iteration 3000: Loss = -11066.111479634745
Iteration 3100: Loss = -11066.109947830164
Iteration 3200: Loss = -11066.108535035044
Iteration 3300: Loss = -11066.107168674824
Iteration 3400: Loss = -11066.105908382815
Iteration 3500: Loss = -11066.10468294215
Iteration 3600: Loss = -11066.103510715642
Iteration 3700: Loss = -11066.102358738211
Iteration 3800: Loss = -11066.101224672593
Iteration 3900: Loss = -11066.100147555268
Iteration 4000: Loss = -11066.099102652408
Iteration 4100: Loss = -11066.098079832853
Iteration 4200: Loss = -11066.097058359199
Iteration 4300: Loss = -11066.096077210468
Iteration 4400: Loss = -11066.095116860048
Iteration 4500: Loss = -11066.094197683766
Iteration 4600: Loss = -11066.093285917195
Iteration 4700: Loss = -11066.092409484902
Iteration 4800: Loss = -11066.091703746288
Iteration 4900: Loss = -11066.090714342294
Iteration 5000: Loss = -11066.089969341303
Iteration 5100: Loss = -11066.08920153725
Iteration 5200: Loss = -11066.08843788201
Iteration 5300: Loss = -11066.087726944392
Iteration 5400: Loss = -11066.087106679955
Iteration 5500: Loss = -11066.086395419345
Iteration 5600: Loss = -11066.085751603581
Iteration 5700: Loss = -11066.086996410944
1
Iteration 5800: Loss = -11066.084586491515
Iteration 5900: Loss = -11066.084015574617
Iteration 6000: Loss = -11066.090649614383
1
Iteration 6100: Loss = -11066.083032554734
Iteration 6200: Loss = -11066.082512870353
Iteration 6300: Loss = -11066.084077085014
1
Iteration 6400: Loss = -11066.081588414918
Iteration 6500: Loss = -11066.081209676919
Iteration 6600: Loss = -11066.081019873202
Iteration 6700: Loss = -11066.080435963226
Iteration 6800: Loss = -11066.080089794044
Iteration 6900: Loss = -11066.079736508844
Iteration 7000: Loss = -11066.079451590915
Iteration 7100: Loss = -11066.079417728184
Iteration 7200: Loss = -11066.078819864011
Iteration 7300: Loss = -11066.078551926243
Iteration 7400: Loss = -11066.0785192189
Iteration 7500: Loss = -11066.078013234388
Iteration 7600: Loss = -11066.079835456292
1
Iteration 7700: Loss = -11066.077525611247
Iteration 7800: Loss = -11066.07730082669
Iteration 7900: Loss = -11066.077403217942
1
Iteration 8000: Loss = -11066.081385535654
2
Iteration 8100: Loss = -11066.07641372882
Iteration 8200: Loss = -11066.08126974873
1
Iteration 8300: Loss = -11066.07538379148
Iteration 8400: Loss = -11066.075538901101
1
Iteration 8500: Loss = -11066.0716144904
Iteration 8600: Loss = -11066.064729358724
Iteration 8700: Loss = -11066.060895408127
Iteration 8800: Loss = -11066.0586726369
Iteration 8900: Loss = -11066.058183334482
Iteration 9000: Loss = -11066.126072625597
1
Iteration 9100: Loss = -11066.05769628483
Iteration 9200: Loss = -11066.057517422209
Iteration 9300: Loss = -11066.07271104507
1
Iteration 9400: Loss = -11066.057273431043
Iteration 9500: Loss = -11066.05711950475
Iteration 9600: Loss = -11066.082109251953
1
Iteration 9700: Loss = -11066.056929132772
Iteration 9800: Loss = -11066.056853495731
Iteration 9900: Loss = -11066.083526095268
1
Iteration 10000: Loss = -11066.05669613193
Iteration 10100: Loss = -11066.056620593705
Iteration 10200: Loss = -11066.05690696616
1
Iteration 10300: Loss = -11066.0564949726
Iteration 10400: Loss = -11066.056422831627
Iteration 10500: Loss = -11066.090738892794
1
Iteration 10600: Loss = -11066.056303014353
Iteration 10700: Loss = -11066.056281318772
Iteration 10800: Loss = -11066.056732731984
1
Iteration 10900: Loss = -11066.056170135418
Iteration 11000: Loss = -11066.056147923822
Iteration 11100: Loss = -11066.058479672338
1
Iteration 11200: Loss = -11066.05606463196
Iteration 11300: Loss = -11066.05604897492
Iteration 11400: Loss = -11066.0560441421
Iteration 11500: Loss = -11066.05624133275
1
Iteration 11600: Loss = -11066.0559846422
Iteration 11700: Loss = -11066.087593362443
1
Iteration 11800: Loss = -11066.176250722663
2
Iteration 11900: Loss = -11066.056659297701
3
Iteration 12000: Loss = -11066.055842933149
Iteration 12100: Loss = -11066.058668246573
1
Iteration 12200: Loss = -11066.055846464977
Iteration 12300: Loss = -11066.058005389643
1
Iteration 12400: Loss = -11066.055817346476
Iteration 12500: Loss = -11066.055885118909
Iteration 12600: Loss = -11066.078022332707
1
Iteration 12700: Loss = -11066.055715653938
Iteration 12800: Loss = -11066.056349971646
1
Iteration 12900: Loss = -11066.0557955534
Iteration 13000: Loss = -11066.056014620719
1
Iteration 13100: Loss = -11066.064099310775
2
Iteration 13200: Loss = -11066.055735156979
Iteration 13300: Loss = -11066.162637975427
1
Iteration 13400: Loss = -11066.055648141912
Iteration 13500: Loss = -11066.056230484459
1
Iteration 13600: Loss = -11066.056156156828
2
Iteration 13700: Loss = -11066.055701964206
Iteration 13800: Loss = -11066.057342351516
1
Iteration 13900: Loss = -11066.1011279788
2
Iteration 14000: Loss = -11066.05961861363
3
Iteration 14100: Loss = -11066.145542968648
4
Iteration 14200: Loss = -11066.055580243494
Iteration 14300: Loss = -11066.077402127763
1
Iteration 14400: Loss = -11066.071036342768
2
Iteration 14500: Loss = -11066.073107255354
3
Iteration 14600: Loss = -11066.055569839753
Iteration 14700: Loss = -11066.059003967355
1
Iteration 14800: Loss = -11066.056854643028
2
Iteration 14900: Loss = -11066.056524037916
3
Iteration 15000: Loss = -11066.099158079584
4
Iteration 15100: Loss = -11066.055567989473
Iteration 15200: Loss = -11066.055641683715
Iteration 15300: Loss = -11066.108551495914
1
Iteration 15400: Loss = -11066.0791324052
2
Iteration 15500: Loss = -11066.055536196422
Iteration 15600: Loss = -11066.05555800826
Iteration 15700: Loss = -11066.056392362401
1
Iteration 15800: Loss = -11066.056609481591
2
Iteration 15900: Loss = -11066.083227645257
3
Iteration 16000: Loss = -11066.055478968841
Iteration 16100: Loss = -11066.076788959424
1
Iteration 16200: Loss = -11066.055500483086
Iteration 16300: Loss = -11066.05558173078
Iteration 16400: Loss = -11066.055678880604
Iteration 16500: Loss = -11066.174686422548
1
Iteration 16600: Loss = -11066.060390081928
2
Iteration 16700: Loss = -11066.06487441247
3
Iteration 16800: Loss = -11066.057739180364
4
Iteration 16900: Loss = -11066.090996182402
5
Iteration 17000: Loss = -11066.076068891054
6
Iteration 17100: Loss = -11066.074134703429
7
Iteration 17200: Loss = -11066.117201436045
8
Iteration 17300: Loss = -11066.056819503152
9
Iteration 17400: Loss = -11066.07149192497
10
Iteration 17500: Loss = -11066.056256328673
11
Iteration 17600: Loss = -11066.060763632662
12
Iteration 17700: Loss = -11066.060359842815
13
Iteration 17800: Loss = -11066.056050285311
14
Iteration 17900: Loss = -11066.055484888144
Iteration 18000: Loss = -11066.073033590814
1
Iteration 18100: Loss = -11066.055489848022
Iteration 18200: Loss = -11066.055676086105
1
Iteration 18300: Loss = -11066.05594147728
2
Iteration 18400: Loss = -11066.056617119635
3
Iteration 18500: Loss = -11066.055513600011
Iteration 18600: Loss = -11066.056935956367
1
Iteration 18700: Loss = -11066.093406950073
2
Iteration 18800: Loss = -11066.05553913271
Iteration 18900: Loss = -11066.05554351796
Iteration 19000: Loss = -11066.058492680912
1
Iteration 19100: Loss = -11066.056486565793
2
Iteration 19200: Loss = -11066.132791217355
3
Iteration 19300: Loss = -11066.055486226795
Iteration 19400: Loss = -11066.0561895258
1
Iteration 19500: Loss = -11066.055475256284
Iteration 19600: Loss = -11066.065176781694
1
Iteration 19700: Loss = -11066.055488739481
Iteration 19800: Loss = -11066.05598529768
1
Iteration 19900: Loss = -11066.080132501866
2
pi: tensor([[9.2872e-01, 7.1277e-02],
        [1.0000e+00, 3.7190e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9941, 0.0059], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1629, 0.1246],
         [0.5570, 0.2757]],

        [[0.5665, 0.2547],
         [0.5914, 0.6177]],

        [[0.6950, 0.1862],
         [0.6700, 0.6914]],

        [[0.6996, 0.2010],
         [0.7282, 0.6189]],

        [[0.6050, 0.0795],
         [0.6106, 0.6831]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.019488297385266844
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.020655664058706252
Global Adjusted Rand Index: -4.806950955719799e-05
Average Adjusted Rand Index: -0.00802879228879462
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22493.478972798395
Iteration 100: Loss = -11076.586344688365
Iteration 200: Loss = -11075.797551933423
Iteration 300: Loss = -11075.591330298701
Iteration 400: Loss = -11075.470505646284
Iteration 500: Loss = -11075.289133938248
Iteration 600: Loss = -11072.525596524363
Iteration 700: Loss = -11071.037647866522
Iteration 800: Loss = -11070.312930524438
Iteration 900: Loss = -11069.677185016508
Iteration 1000: Loss = -11068.690357543117
Iteration 1100: Loss = -11067.018397187196
Iteration 1200: Loss = -11066.530368177811
Iteration 1300: Loss = -11066.353015687491
Iteration 1400: Loss = -11066.26747197741
Iteration 1500: Loss = -11066.216883148594
Iteration 1600: Loss = -11066.183248711495
Iteration 1700: Loss = -11066.162557727217
Iteration 1800: Loss = -11066.15253661712
Iteration 1900: Loss = -11066.145945616321
Iteration 2000: Loss = -11066.141407057708
Iteration 2100: Loss = -11066.138079554054
Iteration 2200: Loss = -11066.135446719134
Iteration 2300: Loss = -11066.133309342664
Iteration 2400: Loss = -11066.131550653505
Iteration 2500: Loss = -11066.130028548923
Iteration 2600: Loss = -11066.128666207356
Iteration 2700: Loss = -11066.127405712517
Iteration 2800: Loss = -11066.126189490577
Iteration 2900: Loss = -11066.125008041408
Iteration 3000: Loss = -11066.123813604374
Iteration 3100: Loss = -11066.12269713449
Iteration 3200: Loss = -11066.121492180122
Iteration 3300: Loss = -11066.120290763225
Iteration 3400: Loss = -11066.119027177408
Iteration 3500: Loss = -11066.117754604518
Iteration 3600: Loss = -11066.11640643082
Iteration 3700: Loss = -11066.11506013662
Iteration 3800: Loss = -11066.1136637167
Iteration 3900: Loss = -11066.112418460207
Iteration 4000: Loss = -11066.110808144273
Iteration 4100: Loss = -11066.109288663114
Iteration 4200: Loss = -11066.107992837653
Iteration 4300: Loss = -11066.106310706826
Iteration 4400: Loss = -11066.104832472904
Iteration 4500: Loss = -11066.104006090836
Iteration 4600: Loss = -11066.101890698754
Iteration 4700: Loss = -11066.10048782111
Iteration 4800: Loss = -11066.099113428076
Iteration 4900: Loss = -11066.097731632268
Iteration 5000: Loss = -11066.096434244235
Iteration 5100: Loss = -11066.095172637108
Iteration 5200: Loss = -11066.093968849476
Iteration 5300: Loss = -11066.092974506604
Iteration 5400: Loss = -11066.091720038052
Iteration 5500: Loss = -11066.09068719872
Iteration 5600: Loss = -11066.08983856744
Iteration 5700: Loss = -11066.088746582873
Iteration 5800: Loss = -11066.087845177402
Iteration 5900: Loss = -11066.08710603861
Iteration 6000: Loss = -11066.086230308327
Iteration 6100: Loss = -11066.08547404322
Iteration 6200: Loss = -11066.084796546325
Iteration 6300: Loss = -11066.084127764972
Iteration 6400: Loss = -11066.083888730722
Iteration 6500: Loss = -11066.08293407685
Iteration 6600: Loss = -11066.082473225706
Iteration 6700: Loss = -11066.081884942425
Iteration 6800: Loss = -11066.081377431103
Iteration 6900: Loss = -11066.081196555442
Iteration 7000: Loss = -11066.080480184513
Iteration 7100: Loss = -11066.080294410021
Iteration 7200: Loss = -11066.079761673782
Iteration 7300: Loss = -11066.079418648522
Iteration 7400: Loss = -11066.079135632668
Iteration 7500: Loss = -11066.078773889107
Iteration 7600: Loss = -11066.123282431709
1
Iteration 7700: Loss = -11066.078175099206
Iteration 7800: Loss = -11066.093039742229
1
Iteration 7900: Loss = -11066.077634484736
Iteration 8000: Loss = -11066.096480295146
1
Iteration 8100: Loss = -11066.077097522417
Iteration 8200: Loss = -11066.07686801377
Iteration 8300: Loss = -11066.076771531893
Iteration 8400: Loss = -11066.076314184547
Iteration 8500: Loss = -11066.075944720975
Iteration 8600: Loss = -11066.07579449968
Iteration 8700: Loss = -11066.07475330867
Iteration 8800: Loss = -11066.073402881315
Iteration 8900: Loss = -11066.07226524342
Iteration 9000: Loss = -11066.062909503153
Iteration 9100: Loss = -11066.058617743529
Iteration 9200: Loss = -11066.26185515244
1
Iteration 9300: Loss = -11066.057589616486
Iteration 9400: Loss = -11066.05742950813
Iteration 9500: Loss = -11066.52090014378
1
Iteration 9600: Loss = -11066.05708951647
Iteration 9700: Loss = -11066.057005333932
Iteration 9800: Loss = -11066.056888602236
Iteration 9900: Loss = -11066.05691363326
Iteration 10000: Loss = -11066.056712327449
Iteration 10100: Loss = -11066.056648551536
Iteration 10200: Loss = -11066.056824284107
1
Iteration 10300: Loss = -11066.056505747305
Iteration 10400: Loss = -11066.056454607458
Iteration 10500: Loss = -11066.058635073185
1
Iteration 10600: Loss = -11066.056342448264
Iteration 10700: Loss = -11066.056278061138
Iteration 10800: Loss = -11066.551591748701
1
Iteration 10900: Loss = -11066.056224135717
Iteration 11000: Loss = -11066.056141602592
Iteration 11100: Loss = -11066.05661858456
1
Iteration 11200: Loss = -11066.056052444632
Iteration 11300: Loss = -11066.056009923594
Iteration 11400: Loss = -11066.079784604657
1
Iteration 11500: Loss = -11066.055988351049
Iteration 11600: Loss = -11066.097416735907
1
Iteration 11700: Loss = -11066.055932986525
Iteration 11800: Loss = -11066.056276824476
1
Iteration 11900: Loss = -11066.055952001876
Iteration 12000: Loss = -11066.056598261217
1
Iteration 12100: Loss = -11066.055821559443
Iteration 12200: Loss = -11066.061563677833
1
Iteration 12300: Loss = -11066.055823688299
Iteration 12400: Loss = -11066.056839794965
1
Iteration 12500: Loss = -11066.055749775189
Iteration 12600: Loss = -11066.057547463079
1
Iteration 12700: Loss = -11066.06941110357
2
Iteration 12800: Loss = -11066.066044192408
3
Iteration 12900: Loss = -11066.055736200322
Iteration 13000: Loss = -11066.055882852806
1
Iteration 13100: Loss = -11066.058941333104
2
Iteration 13200: Loss = -11066.05757269746
3
Iteration 13300: Loss = -11066.078820354198
4
Iteration 13400: Loss = -11066.064032633763
5
Iteration 13500: Loss = -11066.056204319733
6
Iteration 13600: Loss = -11066.082482912927
7
Iteration 13700: Loss = -11066.055699904635
Iteration 13800: Loss = -11066.057045802414
1
Iteration 13900: Loss = -11066.05784485425
2
Iteration 14000: Loss = -11066.055700909266
Iteration 14100: Loss = -11066.055977712113
1
Iteration 14200: Loss = -11066.0586391287
2
Iteration 14300: Loss = -11066.055820699981
3
Iteration 14400: Loss = -11066.05571779941
Iteration 14500: Loss = -11066.055745238342
Iteration 14600: Loss = -11066.057030338392
1
Iteration 14700: Loss = -11066.055539676137
Iteration 14800: Loss = -11066.059228517217
1
Iteration 14900: Loss = -11066.056102186345
2
Iteration 15000: Loss = -11066.059648908542
3
Iteration 15100: Loss = -11066.063637996323
4
Iteration 15200: Loss = -11066.055542760803
Iteration 15300: Loss = -11066.056941770477
1
Iteration 15400: Loss = -11066.082101465481
2
Iteration 15500: Loss = -11066.055541508673
Iteration 15600: Loss = -11066.0772191719
1
Iteration 15700: Loss = -11066.055544146551
Iteration 15800: Loss = -11066.057449632868
1
Iteration 15900: Loss = -11066.055558054564
Iteration 16000: Loss = -11066.055536528016
Iteration 16100: Loss = -11066.056086684823
1
Iteration 16200: Loss = -11066.055562541596
Iteration 16300: Loss = -11066.08582161048
1
Iteration 16400: Loss = -11066.055749553798
2
Iteration 16500: Loss = -11066.055534780091
Iteration 16600: Loss = -11066.058848410961
1
Iteration 16700: Loss = -11066.056347024198
2
Iteration 16800: Loss = -11066.055497772508
Iteration 16900: Loss = -11066.056516496828
1
Iteration 17000: Loss = -11066.055539019399
Iteration 17100: Loss = -11066.055573558024
Iteration 17200: Loss = -11066.065995084438
1
Iteration 17300: Loss = -11066.078337886496
2
Iteration 17400: Loss = -11066.074915476636
3
Iteration 17500: Loss = -11066.055488147798
Iteration 17600: Loss = -11066.088909778635
1
Iteration 17700: Loss = -11066.05549269799
Iteration 17800: Loss = -11066.065902516237
1
Iteration 17900: Loss = -11066.080251681446
2
Iteration 18000: Loss = -11066.05937619477
3
Iteration 18100: Loss = -11066.05808452881
4
Iteration 18200: Loss = -11066.055475510477
Iteration 18300: Loss = -11066.06559952994
1
Iteration 18400: Loss = -11066.055499369219
Iteration 18500: Loss = -11066.064882182216
1
Iteration 18600: Loss = -11066.05694505578
2
Iteration 18700: Loss = -11066.056543592213
3
Iteration 18800: Loss = -11066.055692011269
4
Iteration 18900: Loss = -11066.055600585722
5
Iteration 19000: Loss = -11066.055697905898
6
Iteration 19100: Loss = -11066.055601281045
7
Iteration 19200: Loss = -11066.056562426818
8
Iteration 19300: Loss = -11066.05637609439
9
Iteration 19400: Loss = -11066.058266715347
10
Iteration 19500: Loss = -11066.062092946506
11
Iteration 19600: Loss = -11066.05903328748
12
Iteration 19700: Loss = -11066.056969216912
13
Iteration 19800: Loss = -11066.059409977872
14
Iteration 19900: Loss = -11066.059095464061
15
Stopping early at iteration 19900 due to no improvement.
pi: tensor([[3.8547e-06, 1.0000e+00],
        [7.1105e-02, 9.2890e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0059, 0.9941], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2756, 0.1243],
         [0.5142, 0.1632]],

        [[0.5854, 0.2551],
         [0.7035, 0.6111]],

        [[0.6396, 0.1860],
         [0.6262, 0.7111]],

        [[0.5127, 0.2008],
         [0.5064, 0.5636]],

        [[0.7030, 0.0795],
         [0.7298, 0.7093]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.019488297385266844
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.020655664058706252
Global Adjusted Rand Index: -4.806950955719799e-05
Average Adjusted Rand Index: -0.00802879228879462
10978.744787558191
[-4.806950955719799e-05, -4.806950955719799e-05] [-0.00802879228879462, -0.00802879228879462] [11066.062556701652, 11066.059095464061]
-------------------------------------
This iteration is 91
True Objective function: Loss = -10876.85827404651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21617.599947630333
Iteration 100: Loss = -10956.380480116819
Iteration 200: Loss = -10955.297749785477
Iteration 300: Loss = -10954.887630093048
Iteration 400: Loss = -10954.646517003706
Iteration 500: Loss = -10954.20079000449
Iteration 600: Loss = -10953.698003766367
Iteration 700: Loss = -10953.407234638413
Iteration 800: Loss = -10953.126717292163
Iteration 900: Loss = -10952.849725992219
Iteration 1000: Loss = -10952.684367859127
Iteration 1100: Loss = -10952.553455283412
Iteration 1200: Loss = -10952.396017851628
Iteration 1300: Loss = -10952.128362233172
Iteration 1400: Loss = -10952.047761855882
Iteration 1500: Loss = -10951.998813333328
Iteration 1600: Loss = -10951.963600278177
Iteration 1700: Loss = -10951.937174923914
Iteration 1800: Loss = -10951.916916794564
Iteration 1900: Loss = -10951.9008554576
Iteration 2000: Loss = -10951.887544215666
Iteration 2100: Loss = -10951.87609499938
Iteration 2200: Loss = -10951.866061077277
Iteration 2300: Loss = -10951.856937447637
Iteration 2400: Loss = -10951.848596237414
Iteration 2500: Loss = -10951.840972016123
Iteration 2600: Loss = -10951.833905139158
Iteration 2700: Loss = -10951.827463494938
Iteration 2800: Loss = -10951.82157226858
Iteration 2900: Loss = -10951.816320620865
Iteration 3000: Loss = -10951.811599092496
Iteration 3100: Loss = -10951.807459691463
Iteration 3200: Loss = -10951.803764601975
Iteration 3300: Loss = -10951.800607867368
Iteration 3400: Loss = -10951.797847406073
Iteration 3500: Loss = -10951.795415726367
Iteration 3600: Loss = -10951.793374457135
Iteration 3700: Loss = -10951.791533383925
Iteration 3800: Loss = -10951.789923558857
Iteration 3900: Loss = -10951.788549532514
Iteration 4000: Loss = -10951.787300273678
Iteration 4100: Loss = -10951.786206283416
Iteration 4200: Loss = -10951.78520029301
Iteration 4300: Loss = -10951.784338965712
Iteration 4400: Loss = -10951.7835314053
Iteration 4500: Loss = -10951.782811993757
Iteration 4600: Loss = -10951.78212016091
Iteration 4700: Loss = -10951.781519156968
Iteration 4800: Loss = -10951.780919953435
Iteration 4900: Loss = -10951.780417706217
Iteration 5000: Loss = -10951.779955605723
Iteration 5100: Loss = -10951.779505576955
Iteration 5200: Loss = -10951.779084047765
Iteration 5300: Loss = -10951.778702459656
Iteration 5400: Loss = -10951.778304753294
Iteration 5500: Loss = -10951.777980277633
Iteration 5600: Loss = -10951.777632327306
Iteration 5700: Loss = -10951.77736911877
Iteration 5800: Loss = -10951.777078247556
Iteration 5900: Loss = -10951.776853087047
Iteration 6000: Loss = -10951.77658459898
Iteration 6100: Loss = -10951.776320464169
Iteration 6200: Loss = -10951.776121350807
Iteration 6300: Loss = -10951.775882632366
Iteration 6400: Loss = -10951.7764246002
1
Iteration 6500: Loss = -10951.775493894253
Iteration 6600: Loss = -10951.775370763466
Iteration 6700: Loss = -10951.775164797991
Iteration 6800: Loss = -10951.774985894914
Iteration 6900: Loss = -10951.774951084024
Iteration 7000: Loss = -10951.774695065633
Iteration 7100: Loss = -10951.776765022507
1
Iteration 7200: Loss = -10951.774415925907
Iteration 7300: Loss = -10951.774294074787
Iteration 7400: Loss = -10951.774189523869
Iteration 7500: Loss = -10951.774033352189
Iteration 7600: Loss = -10951.773957526862
Iteration 7700: Loss = -10951.773833711775
Iteration 7800: Loss = -10951.773842091257
Iteration 7900: Loss = -10951.77799311069
1
Iteration 8000: Loss = -10951.775479855814
2
Iteration 8100: Loss = -10951.77348157375
Iteration 8200: Loss = -10951.799627322242
1
Iteration 8300: Loss = -10951.773333418609
Iteration 8400: Loss = -10951.776260858205
1
Iteration 8500: Loss = -10951.773179443207
Iteration 8600: Loss = -10951.773104166678
Iteration 8700: Loss = -10951.775979743188
1
Iteration 8800: Loss = -10951.773072451764
Iteration 8900: Loss = -10951.772928719174
Iteration 9000: Loss = -10951.772878261734
Iteration 9100: Loss = -10951.774019933968
1
Iteration 9200: Loss = -10951.772785202587
Iteration 9300: Loss = -10951.772715097733
Iteration 9400: Loss = -10951.774682028754
1
Iteration 9500: Loss = -10951.772607305094
Iteration 9600: Loss = -10951.77259643287
Iteration 9700: Loss = -10951.776421598923
1
Iteration 9800: Loss = -10951.77249504138
Iteration 9900: Loss = -10951.772462136301
Iteration 10000: Loss = -10951.772458029573
Iteration 10100: Loss = -10951.772434417513
Iteration 10200: Loss = -10951.772338084553
Iteration 10300: Loss = -10951.834854813509
1
Iteration 10400: Loss = -10951.772343905377
Iteration 10500: Loss = -10951.772284383038
Iteration 10600: Loss = -10951.772253063367
Iteration 10700: Loss = -10951.772245181288
Iteration 10800: Loss = -10951.772238350279
Iteration 10900: Loss = -10951.772183668707
Iteration 11000: Loss = -10951.772316417424
1
Iteration 11100: Loss = -10951.772175252454
Iteration 11200: Loss = -10951.772141600593
Iteration 11300: Loss = -10951.773265985074
1
Iteration 11400: Loss = -10951.77211721646
Iteration 11500: Loss = -10951.7721048574
Iteration 11600: Loss = -10951.773496560265
1
Iteration 11700: Loss = -10951.772090288785
Iteration 11800: Loss = -10951.771989051136
Iteration 11900: Loss = -10951.862522533187
1
Iteration 12000: Loss = -10951.77202925105
Iteration 12100: Loss = -10951.772028628342
Iteration 12200: Loss = -10952.266982861405
1
Iteration 12300: Loss = -10951.77200467964
Iteration 12400: Loss = -10951.771982387243
Iteration 12500: Loss = -10951.771961499966
Iteration 12600: Loss = -10951.78248944682
1
Iteration 12700: Loss = -10951.771928522596
Iteration 12800: Loss = -10951.77191373252
Iteration 12900: Loss = -10951.780768066634
1
Iteration 13000: Loss = -10951.771889584432
Iteration 13100: Loss = -10951.771895998432
Iteration 13200: Loss = -10951.773998555325
1
Iteration 13300: Loss = -10951.771884374546
Iteration 13400: Loss = -10951.77185722419
Iteration 13500: Loss = -10951.774156446894
1
Iteration 13600: Loss = -10951.772085508792
2
Iteration 13700: Loss = -10951.771904612973
Iteration 13800: Loss = -10951.773019598813
1
Iteration 13900: Loss = -10951.771840287985
Iteration 14000: Loss = -10951.772194632642
1
Iteration 14100: Loss = -10951.776610432013
2
Iteration 14200: Loss = -10951.771858027507
Iteration 14300: Loss = -10951.771824134465
Iteration 14400: Loss = -10951.77233627631
1
Iteration 14500: Loss = -10951.771837962733
Iteration 14600: Loss = -10951.772494594055
1
Iteration 14700: Loss = -10951.773598119767
2
Iteration 14800: Loss = -10951.771842440206
Iteration 14900: Loss = -10951.774807731133
1
Iteration 15000: Loss = -10951.77181596464
Iteration 15100: Loss = -10951.77376572336
1
Iteration 15200: Loss = -10951.772020023438
2
Iteration 15300: Loss = -10951.771805778151
Iteration 15400: Loss = -10951.772054729447
1
Iteration 15500: Loss = -10951.77196985696
2
Iteration 15600: Loss = -10951.771850948971
Iteration 15700: Loss = -10951.771779685138
Iteration 15800: Loss = -10951.771841739595
Iteration 15900: Loss = -10951.775586843412
1
Iteration 16000: Loss = -10951.771857743035
Iteration 16100: Loss = -10951.784653652734
1
Iteration 16200: Loss = -10951.771776098112
Iteration 16300: Loss = -10951.774015439985
1
Iteration 16400: Loss = -10951.771764162884
Iteration 16500: Loss = -10951.772764036745
1
Iteration 16600: Loss = -10951.771761288237
Iteration 16700: Loss = -10951.771796087052
Iteration 16800: Loss = -10951.771788790926
Iteration 16900: Loss = -10951.771788752232
Iteration 17000: Loss = -10951.82484175454
1
Iteration 17100: Loss = -10951.771769733019
Iteration 17200: Loss = -10951.771789268434
Iteration 17300: Loss = -10951.839517578443
1
Iteration 17400: Loss = -10951.771757196753
Iteration 17500: Loss = -10951.771792547317
Iteration 17600: Loss = -10951.79539855154
1
Iteration 17700: Loss = -10951.771770042695
Iteration 17800: Loss = -10952.091667005703
1
Iteration 17900: Loss = -10951.771762964581
Iteration 18000: Loss = -10951.771780879622
Iteration 18100: Loss = -10951.777578368708
1
Iteration 18200: Loss = -10951.77179375076
Iteration 18300: Loss = -10951.771812294912
Iteration 18400: Loss = -10951.771925256618
1
Iteration 18500: Loss = -10951.771759470033
Iteration 18600: Loss = -10951.778390754878
1
Iteration 18700: Loss = -10951.77179101764
Iteration 18800: Loss = -10951.833297185462
1
Iteration 18900: Loss = -10951.771794762939
Iteration 19000: Loss = -10951.77604787631
1
Iteration 19100: Loss = -10951.771739296279
Iteration 19200: Loss = -10951.771830866517
Iteration 19300: Loss = -10951.771801525767
Iteration 19400: Loss = -10951.771737949008
Iteration 19500: Loss = -10951.777063227566
1
Iteration 19600: Loss = -10951.771739824264
Iteration 19700: Loss = -10951.870469464631
1
Iteration 19800: Loss = -10951.771747879333
Iteration 19900: Loss = -10951.771786342071
pi: tensor([[9.9036e-07, 1.0000e+00],
        [2.0090e-02, 9.7991e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0103, 0.9897], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3378, 0.2660],
         [0.6481, 0.1595]],

        [[0.7001, 0.2731],
         [0.6241, 0.5255]],

        [[0.5562, 0.2317],
         [0.5809, 0.6913]],

        [[0.5952, 0.1458],
         [0.5509, 0.5511]],

        [[0.5763, 0.2067],
         [0.5357, 0.5132]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0014126242444810855
Average Adjusted Rand Index: 6.405021536884923e-06
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21130.07200558905
Iteration 100: Loss = -10955.017556456643
Iteration 200: Loss = -10954.672502780022
Iteration 300: Loss = -10954.561851927569
Iteration 400: Loss = -10954.49497210246
Iteration 500: Loss = -10954.427953405939
Iteration 600: Loss = -10954.338393222357
Iteration 700: Loss = -10954.200526669772
Iteration 800: Loss = -10953.879658192873
Iteration 900: Loss = -10952.696693088556
Iteration 1000: Loss = -10952.321401682819
Iteration 1100: Loss = -10952.186835703258
Iteration 1200: Loss = -10952.115541758358
Iteration 1300: Loss = -10952.071819771427
Iteration 1400: Loss = -10952.041405038466
Iteration 1500: Loss = -10952.018031389038
Iteration 1600: Loss = -10951.998829751998
Iteration 1700: Loss = -10951.982424491482
Iteration 1800: Loss = -10951.968019736769
Iteration 1900: Loss = -10951.955049860428
Iteration 2000: Loss = -10951.94324501798
Iteration 2100: Loss = -10951.932333232706
Iteration 2200: Loss = -10951.922207772064
Iteration 2300: Loss = -10951.912685756397
Iteration 2400: Loss = -10951.903605888941
Iteration 2500: Loss = -10951.894934553588
Iteration 2600: Loss = -10951.886660967584
Iteration 2700: Loss = -10951.878640418548
Iteration 2800: Loss = -10951.870931510255
Iteration 2900: Loss = -10951.863337762945
Iteration 3000: Loss = -10951.855923909585
Iteration 3100: Loss = -10951.848702215148
Iteration 3200: Loss = -10951.841650234232
Iteration 3300: Loss = -10951.834828158608
Iteration 3400: Loss = -10951.828236328296
Iteration 3500: Loss = -10951.821939897132
Iteration 3600: Loss = -10951.816083883714
Iteration 3700: Loss = -10951.810657536656
Iteration 3800: Loss = -10951.805832634893
Iteration 3900: Loss = -10951.801548365134
Iteration 4000: Loss = -10951.79777113377
Iteration 4100: Loss = -10951.794568440231
Iteration 4200: Loss = -10951.791872821543
Iteration 4300: Loss = -10951.789609685176
Iteration 4400: Loss = -10951.78768953066
Iteration 4500: Loss = -10951.78613186482
Iteration 4600: Loss = -10951.784736179989
Iteration 4700: Loss = -10951.783601565598
Iteration 4800: Loss = -10951.7826317014
Iteration 4900: Loss = -10951.781798265018
Iteration 5000: Loss = -10951.781034906027
Iteration 5100: Loss = -10951.78039669942
Iteration 5200: Loss = -10951.77983395953
Iteration 5300: Loss = -10951.779328741251
Iteration 5400: Loss = -10951.778850690464
Iteration 5500: Loss = -10951.778423985952
Iteration 5600: Loss = -10951.778064848051
Iteration 5700: Loss = -10951.777710975688
Iteration 5800: Loss = -10951.77738833185
Iteration 5900: Loss = -10951.777076635784
Iteration 6000: Loss = -10951.776834170041
Iteration 6100: Loss = -10951.776639844084
Iteration 6200: Loss = -10951.776301826882
Iteration 6300: Loss = -10951.776089633166
Iteration 6400: Loss = -10951.775956020916
Iteration 6500: Loss = -10951.775660559417
Iteration 6600: Loss = -10951.775702376179
Iteration 6700: Loss = -10951.775292921728
Iteration 6800: Loss = -10951.77516118738
Iteration 6900: Loss = -10951.774970344353
Iteration 7000: Loss = -10951.774853826611
Iteration 7100: Loss = -10951.774718021268
Iteration 7200: Loss = -10951.775046744748
1
Iteration 7300: Loss = -10951.774712159744
Iteration 7400: Loss = -10951.799116549213
1
Iteration 7500: Loss = -10951.774214337478
Iteration 7600: Loss = -10951.774090272404
Iteration 7700: Loss = -10951.773961472532
Iteration 7800: Loss = -10951.774016121786
Iteration 7900: Loss = -10951.773782137794
Iteration 8000: Loss = -10951.774253489702
1
Iteration 8100: Loss = -10951.773592374118
Iteration 8200: Loss = -10951.785783581972
1
Iteration 8300: Loss = -10951.77346114929
Iteration 8400: Loss = -10951.77336945555
Iteration 8500: Loss = -10951.805802723999
1
Iteration 8600: Loss = -10951.773200110392
Iteration 8700: Loss = -10951.773183355583
Iteration 8800: Loss = -10951.876037930217
1
Iteration 8900: Loss = -10951.773064126144
Iteration 9000: Loss = -10951.772978739065
Iteration 9100: Loss = -10951.885250898853
1
Iteration 9200: Loss = -10951.772887496609
Iteration 9300: Loss = -10951.77281002978
Iteration 9400: Loss = -10951.969267527393
1
Iteration 9500: Loss = -10951.772757912868
Iteration 9600: Loss = -10951.772686524822
Iteration 9700: Loss = -10951.936381295342
1
Iteration 9800: Loss = -10951.772605256916
Iteration 9900: Loss = -10951.772570428426
Iteration 10000: Loss = -10951.772535470904
Iteration 10100: Loss = -10951.773632291122
1
Iteration 10200: Loss = -10951.77246141738
Iteration 10300: Loss = -10951.772409034766
Iteration 10400: Loss = -10951.883601528587
1
Iteration 10500: Loss = -10951.77238120973
Iteration 10600: Loss = -10951.772337928873
Iteration 10700: Loss = -10951.815279984246
1
Iteration 10800: Loss = -10951.772259983327
Iteration 10900: Loss = -10951.772275180942
Iteration 11000: Loss = -10951.772676629262
1
Iteration 11100: Loss = -10951.77218525274
Iteration 11200: Loss = -10951.77219339321
Iteration 11300: Loss = -10951.772175917395
Iteration 11400: Loss = -10951.772246476465
Iteration 11500: Loss = -10951.772125547346
Iteration 11600: Loss = -10951.772081202183
Iteration 11700: Loss = -10951.772169790154
Iteration 11800: Loss = -10951.772117526205
Iteration 11900: Loss = -10951.772063285063
Iteration 12000: Loss = -10951.774762800636
1
Iteration 12100: Loss = -10951.77204009602
Iteration 12200: Loss = -10951.771999268913
Iteration 12300: Loss = -10951.858839145043
1
Iteration 12400: Loss = -10951.772025855269
Iteration 12500: Loss = -10951.771975930706
Iteration 12600: Loss = -10951.834961923892
1
Iteration 12700: Loss = -10951.771962267332
Iteration 12800: Loss = -10951.771981183205
Iteration 12900: Loss = -10951.847600690819
1
Iteration 13000: Loss = -10951.771944416472
Iteration 13100: Loss = -10951.771946932591
Iteration 13200: Loss = -10951.781418230288
1
Iteration 13300: Loss = -10951.771900504544
Iteration 13400: Loss = -10951.915808984333
1
Iteration 13500: Loss = -10951.771920185964
Iteration 13600: Loss = -10951.771889367681
Iteration 13700: Loss = -10951.771965788295
Iteration 13800: Loss = -10951.77187069717
Iteration 13900: Loss = -10951.86323683183
1
Iteration 14000: Loss = -10951.771837705042
Iteration 14100: Loss = -10951.77183141779
Iteration 14200: Loss = -10951.773169765263
1
Iteration 14300: Loss = -10951.771849654662
Iteration 14400: Loss = -10951.771808627196
Iteration 14500: Loss = -10951.772155742805
1
Iteration 14600: Loss = -10951.812337717636
2
Iteration 14700: Loss = -10951.771826478176
Iteration 14800: Loss = -10951.785172670334
1
Iteration 14900: Loss = -10951.771858089895
Iteration 15000: Loss = -10952.018475656492
1
Iteration 15100: Loss = -10951.771849183828
Iteration 15200: Loss = -10951.775250120743
1
Iteration 15300: Loss = -10951.771840417332
Iteration 15400: Loss = -10951.772813252463
1
Iteration 15500: Loss = -10951.77182345514
Iteration 15600: Loss = -10951.780476838961
1
Iteration 15700: Loss = -10951.771841670512
Iteration 15800: Loss = -10951.77179987227
Iteration 15900: Loss = -10951.782505860847
1
Iteration 16000: Loss = -10951.771782507665
Iteration 16100: Loss = -10951.771798242411
Iteration 16200: Loss = -10951.771894958474
Iteration 16300: Loss = -10951.77311913191
1
Iteration 16400: Loss = -10951.77179587292
Iteration 16500: Loss = -10951.771781225896
Iteration 16600: Loss = -10951.772137950093
1
Iteration 16700: Loss = -10951.771803791782
Iteration 16800: Loss = -10951.772429683502
1
Iteration 16900: Loss = -10951.814709816925
2
Iteration 17000: Loss = -10951.771785554609
Iteration 17100: Loss = -10951.803297676948
1
Iteration 17200: Loss = -10951.771749779915
Iteration 17300: Loss = -10951.771783874829
Iteration 17400: Loss = -10951.774109832777
1
Iteration 17500: Loss = -10951.771808689835
Iteration 17600: Loss = -10951.771904699895
Iteration 17700: Loss = -10951.771835998525
Iteration 17800: Loss = -10951.771765134825
Iteration 17900: Loss = -10951.771859227094
Iteration 18000: Loss = -10951.77178668197
Iteration 18100: Loss = -10951.77177399296
Iteration 18200: Loss = -10951.98043795688
1
Iteration 18300: Loss = -10951.771770534688
Iteration 18400: Loss = -10951.77174631846
Iteration 18500: Loss = -10951.776746340642
1
Iteration 18600: Loss = -10951.771737178857
Iteration 18700: Loss = -10951.802913441383
1
Iteration 18800: Loss = -10951.771790671406
Iteration 18900: Loss = -10951.7717510454
Iteration 19000: Loss = -10951.771873250516
1
Iteration 19100: Loss = -10951.771782471527
Iteration 19200: Loss = -10951.776128419562
1
Iteration 19300: Loss = -10951.771769288222
Iteration 19400: Loss = -10951.771764557245
Iteration 19500: Loss = -10951.772002797545
1
Iteration 19600: Loss = -10951.771777954733
Iteration 19700: Loss = -10951.777346593173
1
Iteration 19800: Loss = -10951.842795573335
2
Iteration 19900: Loss = -10951.805473444683
3
pi: tensor([[9.8011e-01, 1.9885e-02],
        [1.0000e+00, 1.0225e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9897, 0.0103], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1589, 0.2657],
         [0.5398, 0.3395]],

        [[0.6491, 0.2733],
         [0.6915, 0.5940]],

        [[0.6677, 0.2323],
         [0.7160, 0.6490]],

        [[0.6897, 0.1460],
         [0.7014, 0.5680]],

        [[0.5554, 0.2090],
         [0.6639, 0.6667]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0014126242444810855
Average Adjusted Rand Index: 6.405021536884923e-06
10876.85827404651
[0.0014126242444810855, 0.0014126242444810855] [6.405021536884923e-06, 6.405021536884923e-06] [10951.771907905922, 10951.853856770062]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11115.534970287514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20218.76888816899
Iteration 100: Loss = -11203.130901951745
Iteration 200: Loss = -11202.820679281504
Iteration 300: Loss = -11202.687790265387
Iteration 400: Loss = -11202.544329110644
Iteration 500: Loss = -11202.298869248407
Iteration 600: Loss = -11201.90691772211
Iteration 700: Loss = -11201.428689735021
Iteration 800: Loss = -11201.08356934443
Iteration 900: Loss = -11200.767669413055
Iteration 1000: Loss = -11200.480131040265
Iteration 1100: Loss = -11200.302178076745
Iteration 1200: Loss = -11200.23671008107
Iteration 1300: Loss = -11200.215415663719
Iteration 1400: Loss = -11200.207776517604
Iteration 1500: Loss = -11200.202411653416
Iteration 1600: Loss = -11200.198447164494
Iteration 1700: Loss = -11200.194528703
Iteration 1800: Loss = -11200.190417312342
Iteration 1900: Loss = -11200.185930047335
Iteration 2000: Loss = -11200.181088538833
Iteration 2100: Loss = -11200.175610630618
Iteration 2200: Loss = -11200.169599944824
Iteration 2300: Loss = -11200.162705242932
Iteration 2400: Loss = -11200.155068194828
Iteration 2500: Loss = -11200.146427721429
Iteration 2600: Loss = -11200.136916248035
Iteration 2700: Loss = -11200.1271220304
Iteration 2800: Loss = -11200.117412695365
Iteration 2900: Loss = -11200.1080931416
Iteration 3000: Loss = -11200.097986611376
Iteration 3100: Loss = -11200.084014748452
Iteration 3200: Loss = -11200.038682258517
Iteration 3300: Loss = -11199.565398612986
Iteration 3400: Loss = -11071.47591751066
Iteration 3500: Loss = -11068.032566444024
Iteration 3600: Loss = -11067.912012874358
Iteration 3700: Loss = -11067.8631763002
Iteration 3800: Loss = -11067.826842118666
Iteration 3900: Loss = -11067.804656372215
Iteration 4000: Loss = -11067.780442982896
Iteration 4100: Loss = -11067.765479476608
Iteration 4200: Loss = -11067.715521847196
Iteration 4300: Loss = -11067.699802070038
Iteration 4400: Loss = -11067.527854364234
Iteration 4500: Loss = -11067.515233759608
Iteration 4600: Loss = -11067.505032573794
Iteration 4700: Loss = -11067.489780944476
Iteration 4800: Loss = -11067.483109432565
Iteration 4900: Loss = -11067.480370279394
Iteration 5000: Loss = -11067.479489978276
Iteration 5100: Loss = -11067.477892742065
Iteration 5200: Loss = -11067.439481500174
Iteration 5300: Loss = -11067.416406142607
Iteration 5400: Loss = -11067.414352722213
Iteration 5500: Loss = -11067.413245971578
Iteration 5600: Loss = -11067.413782830703
1
Iteration 5700: Loss = -11067.409314813665
Iteration 5800: Loss = -11067.408931122942
Iteration 5900: Loss = -11067.406995536909
Iteration 6000: Loss = -11067.38196156277
Iteration 6100: Loss = -11067.381193147723
Iteration 6200: Loss = -11067.378817512747
Iteration 6300: Loss = -11067.375621722449
Iteration 6400: Loss = -11067.374682584272
Iteration 6500: Loss = -11067.360471076967
Iteration 6600: Loss = -11067.360082870095
Iteration 6700: Loss = -11067.358399720644
Iteration 6800: Loss = -11067.338995191822
Iteration 6900: Loss = -11067.33510561942
Iteration 7000: Loss = -11067.334609976948
Iteration 7100: Loss = -11067.334801116072
1
Iteration 7200: Loss = -11067.334406385484
Iteration 7300: Loss = -11067.334418315437
Iteration 7400: Loss = -11067.334081424886
Iteration 7500: Loss = -11067.356899032497
1
Iteration 7600: Loss = -11067.33246783472
Iteration 7700: Loss = -11067.328269022657
Iteration 7800: Loss = -11067.328177768874
Iteration 7900: Loss = -11067.32813061327
Iteration 8000: Loss = -11067.352390922355
1
Iteration 8100: Loss = -11067.327745419885
Iteration 8200: Loss = -11067.326759409458
Iteration 8300: Loss = -11067.326978856214
1
Iteration 8400: Loss = -11067.326413184714
Iteration 8500: Loss = -11067.329921224478
1
Iteration 8600: Loss = -11067.321612042173
Iteration 8700: Loss = -11067.31718767734
Iteration 8800: Loss = -11067.331919134202
1
Iteration 8900: Loss = -11067.316132792152
Iteration 9000: Loss = -11067.31838180035
1
Iteration 9100: Loss = -11067.320356422586
2
Iteration 9200: Loss = -11067.316059365176
Iteration 9300: Loss = -11067.31612124796
Iteration 9400: Loss = -11067.346231607637
1
Iteration 9500: Loss = -11067.33058264526
2
Iteration 9600: Loss = -11067.317027400579
3
Iteration 9700: Loss = -11067.316014536047
Iteration 9800: Loss = -11067.315328214765
Iteration 9900: Loss = -11067.3154050031
Iteration 10000: Loss = -11067.318979132111
1
Iteration 10100: Loss = -11067.559240445866
2
Iteration 10200: Loss = -11067.313711931307
Iteration 10300: Loss = -11067.33316423537
1
Iteration 10400: Loss = -11067.313171524997
Iteration 10500: Loss = -11067.38473909225
1
Iteration 10600: Loss = -11067.312969983186
Iteration 10700: Loss = -11067.312555115377
Iteration 10800: Loss = -11067.312772280631
1
Iteration 10900: Loss = -11067.312566680752
Iteration 11000: Loss = -11067.315032126533
1
Iteration 11100: Loss = -11067.312556869629
Iteration 11200: Loss = -11067.312826121542
1
Iteration 11300: Loss = -11067.31265850432
2
Iteration 11400: Loss = -11067.346245623456
3
Iteration 11500: Loss = -11067.312436488053
Iteration 11600: Loss = -11067.31553877846
1
Iteration 11700: Loss = -11067.31374669013
2
Iteration 11800: Loss = -11067.323000130753
3
Iteration 11900: Loss = -11067.325261843145
4
Iteration 12000: Loss = -11067.312447717693
Iteration 12100: Loss = -11067.314473536539
1
Iteration 12200: Loss = -11067.317604007822
2
Iteration 12300: Loss = -11067.313865441012
3
Iteration 12400: Loss = -11067.313167008771
4
Iteration 12500: Loss = -11067.312348574877
Iteration 12600: Loss = -11067.350029376723
1
Iteration 12700: Loss = -11067.31236552195
Iteration 12800: Loss = -11067.512308054158
1
Iteration 12900: Loss = -11067.3125568847
2
Iteration 13000: Loss = -11067.312897098185
3
Iteration 13100: Loss = -11067.312279160693
Iteration 13200: Loss = -11067.313670122052
1
Iteration 13300: Loss = -11067.312126474228
Iteration 13400: Loss = -11067.312559360384
1
Iteration 13500: Loss = -11067.316538262894
2
Iteration 13600: Loss = -11067.378129557641
3
Iteration 13700: Loss = -11067.311898170377
Iteration 13800: Loss = -11067.31227433426
1
Iteration 13900: Loss = -11067.311861270875
Iteration 14000: Loss = -11067.312273680565
1
Iteration 14100: Loss = -11067.339550453966
2
Iteration 14200: Loss = -11067.311698574305
Iteration 14300: Loss = -11067.3166114691
1
Iteration 14400: Loss = -11067.319544376054
2
Iteration 14500: Loss = -11067.311511311733
Iteration 14600: Loss = -11067.311699627317
1
Iteration 14700: Loss = -11067.313744507283
2
Iteration 14800: Loss = -11067.486239942007
3
Iteration 14900: Loss = -11067.311514470228
Iteration 15000: Loss = -11067.312256697807
1
Iteration 15100: Loss = -11067.311466430992
Iteration 15200: Loss = -11067.311512298884
Iteration 15300: Loss = -11067.316978259882
1
Iteration 15400: Loss = -11067.322353817162
2
Iteration 15500: Loss = -11067.34945920622
3
Iteration 15600: Loss = -11067.311475176157
Iteration 15700: Loss = -11067.312825573157
1
Iteration 15800: Loss = -11067.311584994035
2
Iteration 15900: Loss = -11067.311496078848
Iteration 16000: Loss = -11067.311753365013
1
Iteration 16100: Loss = -11067.31118633143
Iteration 16200: Loss = -11067.411749900017
1
Iteration 16300: Loss = -11067.311260920638
Iteration 16400: Loss = -11067.311720145755
1
Iteration 16500: Loss = -11067.31137843813
2
Iteration 16600: Loss = -11067.319498501814
3
Iteration 16700: Loss = -11067.313244174922
4
Iteration 16800: Loss = -11067.311340623932
Iteration 16900: Loss = -11067.311218372066
Iteration 17000: Loss = -11067.311904970305
1
Iteration 17100: Loss = -11067.31555025139
2
Iteration 17200: Loss = -11067.313049814933
3
Iteration 17300: Loss = -11067.311084647949
Iteration 17400: Loss = -11067.318413749337
1
Iteration 17500: Loss = -11067.316671477402
2
Iteration 17600: Loss = -11067.330964814631
3
Iteration 17700: Loss = -11067.311103212252
Iteration 17800: Loss = -11067.315256611864
1
Iteration 17900: Loss = -11067.316892278961
2
Iteration 18000: Loss = -11067.311212437413
3
Iteration 18100: Loss = -11067.3120986608
4
Iteration 18200: Loss = -11067.416546311117
5
Iteration 18300: Loss = -11067.31115349295
Iteration 18400: Loss = -11067.34865775069
1
Iteration 18500: Loss = -11067.311122270052
Iteration 18600: Loss = -11067.321787583773
1
Iteration 18700: Loss = -11067.311131800858
Iteration 18800: Loss = -11067.33207485927
1
Iteration 18900: Loss = -11067.311116742942
Iteration 19000: Loss = -11067.31126110793
1
Iteration 19100: Loss = -11067.31167222333
2
Iteration 19200: Loss = -11067.32416909906
3
Iteration 19300: Loss = -11067.311112869787
Iteration 19400: Loss = -11067.311628489828
1
Iteration 19500: Loss = -11067.311180335073
Iteration 19600: Loss = -11067.32546905235
1
Iteration 19700: Loss = -11067.318565442542
2
Iteration 19800: Loss = -11067.337498179437
3
Iteration 19900: Loss = -11067.311116460545
pi: tensor([[0.7757, 0.2243],
        [0.2207, 0.7793]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5755, 0.4245], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2486, 0.1065],
         [0.5205, 0.2087]],

        [[0.6270, 0.1125],
         [0.6081, 0.6551]],

        [[0.5769, 0.0984],
         [0.7068, 0.6300]],

        [[0.6409, 0.1123],
         [0.6517, 0.5366]],

        [[0.6074, 0.0953],
         [0.5097, 0.5376]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6690576291398103
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369294996039799
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
Global Adjusted Rand Index: 0.7952549698026875
Average Adjusted Rand Index: 0.7970709541862698
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22515.403117276397
Iteration 100: Loss = -11203.746023683712
Iteration 200: Loss = -11203.009611072013
Iteration 300: Loss = -11202.786961624151
Iteration 400: Loss = -11202.669247705857
Iteration 500: Loss = -11202.587149702069
Iteration 600: Loss = -11202.515940724687
Iteration 700: Loss = -11202.440235371956
Iteration 800: Loss = -11202.348985034609
Iteration 900: Loss = -11202.22395424393
Iteration 1000: Loss = -11202.00592010872
Iteration 1100: Loss = -11201.431993540486
Iteration 1200: Loss = -11200.350698166132
Iteration 1300: Loss = -11199.774170124201
Iteration 1400: Loss = -11199.479694303818
Iteration 1500: Loss = -11199.237546798098
Iteration 1600: Loss = -11198.80546442265
Iteration 1700: Loss = -11170.791784987725
Iteration 1800: Loss = -11083.248622891855
Iteration 1900: Loss = -11069.888505620085
Iteration 2000: Loss = -11069.431089411315
Iteration 2100: Loss = -11068.74266858628
Iteration 2200: Loss = -11068.656854104243
Iteration 2300: Loss = -11068.47867559814
Iteration 2400: Loss = -11068.442436866137
Iteration 2500: Loss = -11068.4261424454
Iteration 2600: Loss = -11068.41040005072
Iteration 2700: Loss = -11068.364064541074
Iteration 2800: Loss = -11068.309540371256
Iteration 2900: Loss = -11068.304248577144
Iteration 3000: Loss = -11068.29954253471
Iteration 3100: Loss = -11068.295897394064
Iteration 3200: Loss = -11068.293429904823
Iteration 3300: Loss = -11068.296115459141
1
Iteration 3400: Loss = -11068.28671901775
Iteration 3500: Loss = -11068.28537901949
Iteration 3600: Loss = -11068.284383762491
Iteration 3700: Loss = -11068.282995388367
Iteration 3800: Loss = -11068.281792685008
Iteration 3900: Loss = -11068.281172790259
Iteration 4000: Loss = -11068.279490816296
Iteration 4100: Loss = -11068.278729454536
Iteration 4200: Loss = -11068.278008558527
Iteration 4300: Loss = -11068.27800874443
Iteration 4400: Loss = -11068.274252634177
Iteration 4500: Loss = -11068.26994884075
Iteration 4600: Loss = -11068.269343854889
Iteration 4700: Loss = -11068.272774683
1
Iteration 4800: Loss = -11068.268331940952
Iteration 4900: Loss = -11068.26914080136
1
Iteration 5000: Loss = -11068.267712149549
Iteration 5100: Loss = -11068.267184900644
Iteration 5200: Loss = -11068.266813544944
Iteration 5300: Loss = -11068.266333894291
Iteration 5400: Loss = -11068.263840886291
Iteration 5500: Loss = -11068.261442705658
Iteration 5600: Loss = -11068.259573357667
Iteration 5700: Loss = -11068.255923144186
Iteration 5800: Loss = -11068.255597376461
Iteration 5900: Loss = -11068.249781982846
Iteration 6000: Loss = -11068.199416746678
Iteration 6100: Loss = -11068.199811374743
1
Iteration 6200: Loss = -11068.19863987322
Iteration 6300: Loss = -11068.198445169592
Iteration 6400: Loss = -11068.198373265233
Iteration 6500: Loss = -11068.200050171794
1
Iteration 6600: Loss = -11068.19819567241
Iteration 6700: Loss = -11068.209373223255
1
Iteration 6800: Loss = -11068.19804407644
Iteration 6900: Loss = -11068.19875389665
1
Iteration 7000: Loss = -11068.197877581697
Iteration 7100: Loss = -11068.198172188237
1
Iteration 7200: Loss = -11068.201062040795
2
Iteration 7300: Loss = -11068.193171472964
Iteration 7400: Loss = -11068.192634563471
Iteration 7500: Loss = -11068.19265236746
Iteration 7600: Loss = -11068.306034608746
1
Iteration 7700: Loss = -11068.134451073103
Iteration 7800: Loss = -11068.130715483763
Iteration 7900: Loss = -11068.144921313984
1
Iteration 8000: Loss = -11068.033464071255
Iteration 8100: Loss = -11068.033210791462
Iteration 8200: Loss = -11068.047220105233
1
Iteration 8300: Loss = -11068.015306423056
Iteration 8400: Loss = -11068.03176086182
1
Iteration 8500: Loss = -11067.968531328352
Iteration 8600: Loss = -11067.968286889529
Iteration 8700: Loss = -11067.967617643728
Iteration 8800: Loss = -11067.841600383714
Iteration 8900: Loss = -11067.83841423363
Iteration 9000: Loss = -11067.832993505634
Iteration 9100: Loss = -11067.83158407179
Iteration 9200: Loss = -11067.854288838456
1
Iteration 9300: Loss = -11067.845688804235
2
Iteration 9400: Loss = -11067.820219711464
Iteration 9500: Loss = -11067.821516965267
1
Iteration 9600: Loss = -11067.82024178644
Iteration 9700: Loss = -11067.820068379742
Iteration 9800: Loss = -11067.772727063315
Iteration 9900: Loss = -11067.759251689793
Iteration 10000: Loss = -11067.757821932295
Iteration 10100: Loss = -11067.760577950108
1
Iteration 10200: Loss = -11067.778444515352
2
Iteration 10300: Loss = -11067.75451596104
Iteration 10400: Loss = -11067.75589689841
1
Iteration 10500: Loss = -11067.75382267612
Iteration 10600: Loss = -11067.80261893767
1
Iteration 10700: Loss = -11067.733215418004
Iteration 10800: Loss = -11067.73340589257
1
Iteration 10900: Loss = -11067.733175549209
Iteration 11000: Loss = -11067.733174522635
Iteration 11100: Loss = -11067.732989372365
Iteration 11200: Loss = -11067.734240664895
1
Iteration 11300: Loss = -11067.732480882867
Iteration 11400: Loss = -11067.733140381108
1
Iteration 11500: Loss = -11067.764650618261
2
Iteration 11600: Loss = -11067.741300465326
3
Iteration 11700: Loss = -11067.73086103366
Iteration 11800: Loss = -11067.73077043159
Iteration 11900: Loss = -11067.729594803
Iteration 12000: Loss = -11067.729727359745
1
Iteration 12100: Loss = -11067.819899946024
2
Iteration 12200: Loss = -11067.72953160709
Iteration 12300: Loss = -11067.732330172717
1
Iteration 12400: Loss = -11067.725503517884
Iteration 12500: Loss = -11067.726457404258
1
Iteration 12600: Loss = -11067.741338331429
2
Iteration 12700: Loss = -11067.724242152999
Iteration 12800: Loss = -11067.7237687069
Iteration 12900: Loss = -11067.72566100154
1
Iteration 13000: Loss = -11067.783209694315
2
Iteration 13100: Loss = -11067.715006547463
Iteration 13200: Loss = -11067.942770366393
1
Iteration 13300: Loss = -11067.712942652406
Iteration 13400: Loss = -11067.71353626824
1
Iteration 13500: Loss = -11067.81181793719
2
Iteration 13600: Loss = -11067.712817969963
Iteration 13700: Loss = -11067.770219241458
1
Iteration 13800: Loss = -11067.712679598239
Iteration 13900: Loss = -11067.710651773483
Iteration 14000: Loss = -11067.707738799869
Iteration 14100: Loss = -11067.729001672807
1
Iteration 14200: Loss = -11067.73695899674
2
Iteration 14300: Loss = -11067.696811893644
Iteration 14400: Loss = -11067.694927984656
Iteration 14500: Loss = -11067.69616830793
1
Iteration 14600: Loss = -11067.701263753455
2
Iteration 14700: Loss = -11067.707843935403
3
Iteration 14800: Loss = -11067.69514487652
4
Iteration 14900: Loss = -11067.697152486526
5
Iteration 15000: Loss = -11067.737419737394
6
Iteration 15100: Loss = -11067.694719906778
Iteration 15200: Loss = -11067.695100524139
1
Iteration 15300: Loss = -11067.694397291492
Iteration 15400: Loss = -11067.694516125743
1
Iteration 15500: Loss = -11067.943153282136
2
Iteration 15600: Loss = -11067.692858243276
Iteration 15700: Loss = -11067.807683184668
1
Iteration 15800: Loss = -11067.692806524603
Iteration 15900: Loss = -11067.703361088981
1
Iteration 16000: Loss = -11067.692776645983
Iteration 16100: Loss = -11067.720619198022
1
Iteration 16200: Loss = -11067.692452734515
Iteration 16300: Loss = -11067.705822083768
1
Iteration 16400: Loss = -11067.69276252854
2
Iteration 16500: Loss = -11067.849023937606
3
Iteration 16600: Loss = -11067.692598092499
4
Iteration 16700: Loss = -11067.692519396183
Iteration 16800: Loss = -11067.694986562681
1
Iteration 16900: Loss = -11067.694614857986
2
Iteration 17000: Loss = -11067.692554785512
Iteration 17100: Loss = -11067.697550112085
1
Iteration 17200: Loss = -11067.687968252116
Iteration 17300: Loss = -11067.684518244405
Iteration 17400: Loss = -11067.682061381835
Iteration 17500: Loss = -11067.679958342467
Iteration 17600: Loss = -11067.679839506054
Iteration 17700: Loss = -11067.705306174199
1
Iteration 17800: Loss = -11067.679660014746
Iteration 17900: Loss = -11067.778165980337
1
Iteration 18000: Loss = -11067.717383127812
2
Iteration 18100: Loss = -11067.679129197459
Iteration 18200: Loss = -11067.677145780637
Iteration 18300: Loss = -11067.670236614285
Iteration 18400: Loss = -11067.5496121591
Iteration 18500: Loss = -11067.58869020418
1
Iteration 18600: Loss = -11067.527205255008
Iteration 18700: Loss = -11067.487527439303
Iteration 18800: Loss = -11067.635750050096
1
Iteration 18900: Loss = -11067.486951283638
Iteration 19000: Loss = -11067.48681778915
Iteration 19100: Loss = -11067.517375272912
1
Iteration 19200: Loss = -11067.667224097002
2
Iteration 19300: Loss = -11067.489532306648
3
Iteration 19400: Loss = -11067.48954430416
4
Iteration 19500: Loss = -11067.485125884641
Iteration 19600: Loss = -11067.485436734685
1
Iteration 19700: Loss = -11067.486351381676
2
Iteration 19800: Loss = -11067.577198414056
3
Iteration 19900: Loss = -11067.482127063708
pi: tensor([[0.7794, 0.2206],
        [0.2250, 0.7750]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4235, 0.5765], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2086, 0.1063],
         [0.7108, 0.2487]],

        [[0.6448, 0.1126],
         [0.6631, 0.6417]],

        [[0.5714, 0.0984],
         [0.6026, 0.7002]],

        [[0.5993, 0.1124],
         [0.7086, 0.7139]],

        [[0.5250, 0.0953],
         [0.6837, 0.6487]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 91
Adjusted Rand Index: 0.6690576291398103
time is 1
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369294996039799
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 3
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369552685595733
Global Adjusted Rand Index: 0.7952549698026875
Average Adjusted Rand Index: 0.7970709541862698
11115.534970287514
[0.7952549698026875, 0.7952549698026875] [0.7970709541862698, 0.7970709541862698] [11067.311619551585, 11067.484033892426]
-------------------------------------
This iteration is 93
True Objective function: Loss = -10963.56206775153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24371.273387568224
Iteration 100: Loss = -11075.369135233159
Iteration 200: Loss = -11073.428562666122
Iteration 300: Loss = -11070.88454790854
Iteration 400: Loss = -11066.919023739523
Iteration 500: Loss = -11062.552422823728
Iteration 600: Loss = -11045.814189701203
Iteration 700: Loss = -11024.420098368004
Iteration 800: Loss = -11021.145366085648
Iteration 900: Loss = -11020.394610304927
Iteration 1000: Loss = -11020.09785153421
Iteration 1100: Loss = -11019.97120576941
Iteration 1200: Loss = -11019.895451805085
Iteration 1300: Loss = -11019.8409734808
Iteration 1400: Loss = -11019.795945038533
Iteration 1500: Loss = -11019.75351340096
Iteration 1600: Loss = -11019.708001489344
Iteration 1700: Loss = -11019.654677542587
Iteration 1800: Loss = -11019.592018478812
Iteration 1900: Loss = -11019.526591546179
Iteration 2000: Loss = -11019.471614166285
Iteration 2100: Loss = -11019.433964246136
Iteration 2200: Loss = -11019.410607305836
Iteration 2300: Loss = -11019.396165768556
Iteration 2400: Loss = -11019.386844203796
Iteration 2500: Loss = -11019.380596394763
Iteration 2600: Loss = -11019.37621408306
Iteration 2700: Loss = -11019.373117716807
Iteration 2800: Loss = -11019.370816123403
Iteration 2900: Loss = -11019.369055881854
Iteration 3000: Loss = -11019.36760144028
Iteration 3100: Loss = -11019.366440472408
Iteration 3200: Loss = -11019.365367984576
Iteration 3300: Loss = -11019.36453564927
Iteration 3400: Loss = -11019.363750452707
Iteration 3500: Loss = -11019.363038510864
Iteration 3600: Loss = -11019.362472480165
Iteration 3700: Loss = -11019.3619285155
Iteration 3800: Loss = -11019.361466574815
Iteration 3900: Loss = -11019.36100107
Iteration 4000: Loss = -11019.360614500887
Iteration 4100: Loss = -11019.360268953256
Iteration 4200: Loss = -11019.359915748155
Iteration 4300: Loss = -11019.359609770847
Iteration 4400: Loss = -11019.359359034403
Iteration 4500: Loss = -11019.359153962108
Iteration 4600: Loss = -11019.358915909583
Iteration 4700: Loss = -11019.358721251181
Iteration 4800: Loss = -11019.358506466764
Iteration 4900: Loss = -11019.358337948122
Iteration 5000: Loss = -11019.358208155916
Iteration 5100: Loss = -11019.358048616246
Iteration 5200: Loss = -11019.357916613484
Iteration 5300: Loss = -11019.357791841825
Iteration 5400: Loss = -11019.357680057465
Iteration 5500: Loss = -11019.357598308838
Iteration 5600: Loss = -11019.358320047295
1
Iteration 5700: Loss = -11019.357422144783
Iteration 5800: Loss = -11019.357328652566
Iteration 5900: Loss = -11019.358085969625
1
Iteration 6000: Loss = -11019.35719229819
Iteration 6100: Loss = -11019.357142070492
Iteration 6200: Loss = -11019.357058224889
Iteration 6300: Loss = -11019.357588391205
1
Iteration 6400: Loss = -11019.357003195633
Iteration 6500: Loss = -11019.357339957443
1
Iteration 6600: Loss = -11019.356890880514
Iteration 6700: Loss = -11019.356883669807
Iteration 6800: Loss = -11019.357194415446
1
Iteration 6900: Loss = -11019.357545285558
2
Iteration 7000: Loss = -11019.356917119223
Iteration 7100: Loss = -11019.356788041787
Iteration 7200: Loss = -11019.35671464349
Iteration 7300: Loss = -11019.356907199372
1
Iteration 7400: Loss = -11019.357180231098
2
Iteration 7500: Loss = -11019.356624017162
Iteration 7600: Loss = -11019.356663523302
Iteration 7700: Loss = -11019.35844984624
1
Iteration 7800: Loss = -11019.38752904984
2
Iteration 7900: Loss = -11019.356571448974
Iteration 8000: Loss = -11019.359062385733
1
Iteration 8100: Loss = -11019.367118502005
2
Iteration 8200: Loss = -11019.356570726974
Iteration 8300: Loss = -11019.368883751493
1
Iteration 8400: Loss = -11019.356467748723
Iteration 8500: Loss = -11019.356498462244
Iteration 8600: Loss = -11019.357037251346
1
Iteration 8700: Loss = -11019.356815750534
2
Iteration 8800: Loss = -11019.358016695098
3
Iteration 8900: Loss = -11019.36167315717
4
Iteration 9000: Loss = -11019.356408516853
Iteration 9100: Loss = -11019.360072834712
1
Iteration 9200: Loss = -11019.356434471301
Iteration 9300: Loss = -11019.357197173655
1
Iteration 9400: Loss = -11019.356386034871
Iteration 9500: Loss = -11019.356429791236
Iteration 9600: Loss = -11019.356531100011
1
Iteration 9700: Loss = -11019.356381654177
Iteration 9800: Loss = -11019.576059995561
1
Iteration 9900: Loss = -11019.356413137562
Iteration 10000: Loss = -11019.356395137977
Iteration 10100: Loss = -11019.405398564657
1
Iteration 10200: Loss = -11019.356380308047
Iteration 10300: Loss = -11019.356371448177
Iteration 10400: Loss = -11019.383650986923
1
Iteration 10500: Loss = -11019.35641750133
Iteration 10600: Loss = -11019.35649443647
Iteration 10700: Loss = -11019.450278749786
1
Iteration 10800: Loss = -11019.356361783677
Iteration 10900: Loss = -11019.356853448026
1
Iteration 11000: Loss = -11019.356438663237
Iteration 11100: Loss = -11019.356351864088
Iteration 11200: Loss = -11019.36714907218
1
Iteration 11300: Loss = -11019.35636465537
Iteration 11400: Loss = -11019.356370703208
Iteration 11500: Loss = -11019.356470769055
1
Iteration 11600: Loss = -11019.532991922237
2
Iteration 11700: Loss = -11019.35638725595
Iteration 11800: Loss = -11019.41133795266
1
Iteration 11900: Loss = -11019.356393338516
Iteration 12000: Loss = -11019.356430178757
Iteration 12100: Loss = -11019.357792497634
1
Iteration 12200: Loss = -11019.378711946903
2
Iteration 12300: Loss = -11019.356309902314
Iteration 12400: Loss = -11019.386621790003
1
Iteration 12500: Loss = -11019.35802208808
2
Iteration 12600: Loss = -11019.356448380455
3
Iteration 12700: Loss = -11019.361868075359
4
Iteration 12800: Loss = -11019.370945416436
5
Iteration 12900: Loss = -11019.380673298336
6
Iteration 13000: Loss = -11019.35635721638
Iteration 13100: Loss = -11019.356581550861
1
Iteration 13200: Loss = -11019.372326024515
2
Iteration 13300: Loss = -11019.357774824237
3
Iteration 13400: Loss = -11019.356364573263
Iteration 13500: Loss = -11019.358059441449
1
Iteration 13600: Loss = -11019.356999310527
2
Iteration 13700: Loss = -11019.387151405796
3
Iteration 13800: Loss = -11019.356324028791
Iteration 13900: Loss = -11019.356471946516
1
Iteration 14000: Loss = -11019.356347043129
Iteration 14100: Loss = -11019.356428738198
Iteration 14200: Loss = -11019.385004074096
1
Iteration 14300: Loss = -11019.399432228469
2
Iteration 14400: Loss = -11019.356360938327
Iteration 14500: Loss = -11019.356429450296
Iteration 14600: Loss = -11019.359158453475
1
Iteration 14700: Loss = -11019.364216947311
2
Iteration 14800: Loss = -11019.508473035019
3
Iteration 14900: Loss = -11019.35644926723
Iteration 15000: Loss = -11019.356435892958
Iteration 15100: Loss = -11019.356930127899
1
Iteration 15200: Loss = -11019.36240817745
2
Iteration 15300: Loss = -11019.357340036591
3
Iteration 15400: Loss = -11019.356796977376
4
Iteration 15500: Loss = -11019.358445644737
5
Iteration 15600: Loss = -11019.36158192175
6
Iteration 15700: Loss = -11019.405897187824
7
Iteration 15800: Loss = -11019.363341289312
8
Iteration 15900: Loss = -11019.385249321856
9
Iteration 16000: Loss = -11019.363471053664
10
Iteration 16100: Loss = -11019.457137050444
11
Iteration 16200: Loss = -11019.356633640351
12
Iteration 16300: Loss = -11019.357301989574
13
Iteration 16400: Loss = -11019.35831646464
14
Iteration 16500: Loss = -11019.357409729588
15
Stopping early at iteration 16500 due to no improvement.
pi: tensor([[0.9769, 0.0231],
        [0.8877, 0.1123]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4013, 0.5987], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1601, 0.1014],
         [0.6379, 0.2800]],

        [[0.6989, 0.2096],
         [0.5432, 0.5595]],

        [[0.5231, 0.1204],
         [0.6623, 0.6935]],

        [[0.5487, 0.2042],
         [0.7090, 0.6382]],

        [[0.6464, 0.1737],
         [0.5359, 0.5134]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8077418405269161
time is 1
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.04902242729341788
Average Adjusted Rand Index: 0.1596089741659893
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23355.797787507287
Iteration 100: Loss = -11074.632848437072
Iteration 200: Loss = -11062.136252173545
Iteration 300: Loss = -11021.825740902112
Iteration 400: Loss = -11020.41773247699
Iteration 500: Loss = -11019.972349807213
Iteration 600: Loss = -11019.877693360118
Iteration 700: Loss = -11019.803632555497
Iteration 800: Loss = -11019.750713543259
Iteration 900: Loss = -11019.710385140765
Iteration 1000: Loss = -11019.664389197887
Iteration 1100: Loss = -11019.608867389617
Iteration 1200: Loss = -11016.766171459918
Iteration 1300: Loss = -10991.741597622815
Iteration 1400: Loss = -10939.258803109466
Iteration 1500: Loss = -10924.758121806039
Iteration 1600: Loss = -10923.708250123109
Iteration 1700: Loss = -10922.94669968108
Iteration 1800: Loss = -10922.897245652237
Iteration 1900: Loss = -10922.520525240856
Iteration 2000: Loss = -10922.50771843879
Iteration 2100: Loss = -10922.50075285365
Iteration 2200: Loss = -10922.493056958085
Iteration 2300: Loss = -10921.573772823976
Iteration 2400: Loss = -10921.558305493
Iteration 2500: Loss = -10921.496545400629
Iteration 2600: Loss = -10921.194272486096
Iteration 2700: Loss = -10921.013961558798
Iteration 2800: Loss = -10920.986810405464
Iteration 2900: Loss = -10920.983168181965
Iteration 3000: Loss = -10920.978785915575
Iteration 3100: Loss = -10920.976102050556
Iteration 3200: Loss = -10920.973477813348
Iteration 3300: Loss = -10914.153908992092
Iteration 3400: Loss = -10914.151862271308
Iteration 3500: Loss = -10914.14755543303
Iteration 3600: Loss = -10914.121872295813
Iteration 3700: Loss = -10914.12299299891
1
Iteration 3800: Loss = -10914.120298031201
Iteration 3900: Loss = -10914.11966929082
Iteration 4000: Loss = -10914.120881821866
1
Iteration 4100: Loss = -10914.05669511854
Iteration 4200: Loss = -10914.055166219701
Iteration 4300: Loss = -10914.051889337017
Iteration 4400: Loss = -10914.043534220567
Iteration 4500: Loss = -10913.276484092703
Iteration 4600: Loss = -10913.283689499991
1
Iteration 4700: Loss = -10913.27589108317
Iteration 4800: Loss = -10913.280192157279
1
Iteration 4900: Loss = -10913.274264509995
Iteration 5000: Loss = -10913.274088039316
Iteration 5100: Loss = -10913.273079069275
Iteration 5200: Loss = -10913.272717908434
Iteration 5300: Loss = -10913.272148015267
Iteration 5400: Loss = -10913.27676239698
1
Iteration 5500: Loss = -10913.271493345374
Iteration 5600: Loss = -10913.271308681387
Iteration 5700: Loss = -10913.271202106216
Iteration 5800: Loss = -10913.270871535422
Iteration 5900: Loss = -10913.272719080482
1
Iteration 6000: Loss = -10913.26265241663
Iteration 6100: Loss = -10913.262009923545
Iteration 6200: Loss = -10913.261693903045
Iteration 6300: Loss = -10913.261621964297
Iteration 6400: Loss = -10913.26285519879
1
Iteration 6500: Loss = -10913.263670550585
2
Iteration 6600: Loss = -10913.261228999148
Iteration 6700: Loss = -10913.2610537511
Iteration 6800: Loss = -10913.260952612913
Iteration 6900: Loss = -10913.26263468796
1
Iteration 7000: Loss = -10913.261087008479
2
Iteration 7100: Loss = -10913.261105751653
3
Iteration 7200: Loss = -10913.260261842383
Iteration 7300: Loss = -10913.233558392325
Iteration 7400: Loss = -10913.19319143394
Iteration 7500: Loss = -10913.192922883083
Iteration 7600: Loss = -10913.341447532037
1
Iteration 7700: Loss = -10913.192682730123
Iteration 7800: Loss = -10913.21239038061
1
Iteration 7900: Loss = -10913.192235573557
Iteration 8000: Loss = -10913.199605016724
1
Iteration 8100: Loss = -10913.192092190731
Iteration 8200: Loss = -10913.211737270401
1
Iteration 8300: Loss = -10913.191458670244
Iteration 8400: Loss = -10913.191779492314
1
Iteration 8500: Loss = -10913.191333921557
Iteration 8600: Loss = -10913.191363457812
Iteration 8700: Loss = -10913.219067662063
1
Iteration 8800: Loss = -10913.191330072023
Iteration 8900: Loss = -10913.191286775582
Iteration 9000: Loss = -10913.192249180735
1
Iteration 9100: Loss = -10913.191252275006
Iteration 9200: Loss = -10913.19091662509
Iteration 9300: Loss = -10913.19115173169
1
Iteration 9400: Loss = -10913.19051811984
Iteration 9500: Loss = -10913.190500316729
Iteration 9600: Loss = -10913.191100000236
1
Iteration 9700: Loss = -10913.190418672977
Iteration 9800: Loss = -10913.190879727603
1
Iteration 9900: Loss = -10913.190024733296
Iteration 10000: Loss = -10913.18921112324
Iteration 10100: Loss = -10913.205430139204
1
Iteration 10200: Loss = -10913.188685373938
Iteration 10300: Loss = -10913.2787392407
1
Iteration 10400: Loss = -10913.188292115461
Iteration 10500: Loss = -10913.188473132406
1
Iteration 10600: Loss = -10913.211914526066
2
Iteration 10700: Loss = -10913.188267258769
Iteration 10800: Loss = -10913.188654983176
1
Iteration 10900: Loss = -10913.185034128383
Iteration 11000: Loss = -10913.258551652507
1
Iteration 11100: Loss = -10913.185007413354
Iteration 11200: Loss = -10913.1849693528
Iteration 11300: Loss = -10913.185086107223
1
Iteration 11400: Loss = -10913.184973650687
Iteration 11500: Loss = -10913.230524885586
1
Iteration 11600: Loss = -10913.184194609343
Iteration 11700: Loss = -10913.18419763033
Iteration 11800: Loss = -10913.185649339865
1
Iteration 11900: Loss = -10913.200878566036
2
Iteration 12000: Loss = -10913.184207067787
Iteration 12100: Loss = -10913.3176909881
1
Iteration 12200: Loss = -10913.184183446416
Iteration 12300: Loss = -10913.210641338625
1
Iteration 12400: Loss = -10913.184206552953
Iteration 12500: Loss = -10913.184795912626
1
Iteration 12600: Loss = -10913.186145612484
2
Iteration 12700: Loss = -10913.1842064807
Iteration 12800: Loss = -10913.18432074444
1
Iteration 12900: Loss = -10913.212470593897
2
Iteration 13000: Loss = -10913.182413818731
Iteration 13100: Loss = -10913.17699824166
Iteration 13200: Loss = -10913.165879794407
Iteration 13300: Loss = -10913.188872005616
1
Iteration 13400: Loss = -10913.16588602641
Iteration 13500: Loss = -10913.165867361999
Iteration 13600: Loss = -10913.15758596252
Iteration 13700: Loss = -10913.157218052129
Iteration 13800: Loss = -10913.157246480469
Iteration 13900: Loss = -10913.157233816377
Iteration 14000: Loss = -10913.180526724187
1
Iteration 14100: Loss = -10913.160589074572
2
Iteration 14200: Loss = -10913.157623935183
3
Iteration 14300: Loss = -10913.206597643346
4
Iteration 14400: Loss = -10913.157172680967
Iteration 14500: Loss = -10913.157823959222
1
Iteration 14600: Loss = -10913.157165415534
Iteration 14700: Loss = -10913.157438648916
1
Iteration 14800: Loss = -10913.157299213044
2
Iteration 14900: Loss = -10913.157291008694
3
Iteration 15000: Loss = -10913.159156882748
4
Iteration 15100: Loss = -10913.33516475856
5
Iteration 15200: Loss = -10913.157160084691
Iteration 15300: Loss = -10913.157373594333
1
Iteration 15400: Loss = -10913.15681709999
Iteration 15500: Loss = -10913.156776534694
Iteration 15600: Loss = -10913.156653306936
Iteration 15700: Loss = -10913.157602345946
1
Iteration 15800: Loss = -10913.156671869834
Iteration 15900: Loss = -10913.156844274394
1
Iteration 16000: Loss = -10913.156691122935
Iteration 16100: Loss = -10913.156661608156
Iteration 16200: Loss = -10913.171044469775
1
Iteration 16300: Loss = -10913.153210866809
Iteration 16400: Loss = -10913.153265436684
Iteration 16500: Loss = -10913.142808626384
Iteration 16600: Loss = -10913.14183580026
Iteration 16700: Loss = -10913.142789938413
1
Iteration 16800: Loss = -10913.141707987235
Iteration 16900: Loss = -10913.141760842254
Iteration 17000: Loss = -10913.143596482578
1
Iteration 17100: Loss = -10913.141588207865
Iteration 17200: Loss = -10913.141872542328
1
Iteration 17300: Loss = -10913.141584863266
Iteration 17400: Loss = -10913.272092597963
1
Iteration 17500: Loss = -10913.141322046637
Iteration 17600: Loss = -10913.141315550347
Iteration 17700: Loss = -10913.144722230769
1
Iteration 17800: Loss = -10913.14131572131
Iteration 17900: Loss = -10913.14135597836
Iteration 18000: Loss = -10913.13952484214
Iteration 18100: Loss = -10913.139410593229
Iteration 18200: Loss = -10913.140618735813
1
Iteration 18300: Loss = -10913.13929039325
Iteration 18400: Loss = -10913.139763579315
1
Iteration 18500: Loss = -10913.155208354725
2
Iteration 18600: Loss = -10913.140597223079
3
Iteration 18700: Loss = -10913.174938144539
4
Iteration 18800: Loss = -10913.306069691398
5
Iteration 18900: Loss = -10913.139320049104
Iteration 19000: Loss = -10913.139457284857
1
Iteration 19100: Loss = -10913.368151970277
2
Iteration 19200: Loss = -10913.139249482616
Iteration 19300: Loss = -10913.16402919343
1
Iteration 19400: Loss = -10913.13925189557
Iteration 19500: Loss = -10913.157777939374
1
Iteration 19600: Loss = -10913.139232240415
Iteration 19700: Loss = -10913.13923733266
Iteration 19800: Loss = -10913.189770641993
1
Iteration 19900: Loss = -10913.205905920615
2
pi: tensor([[0.7226, 0.2774],
        [0.2471, 0.7529]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6266, 0.3734], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2457, 0.0967],
         [0.5015, 0.2125]],

        [[0.5250, 0.1050],
         [0.7263, 0.5182]],

        [[0.6934, 0.0865],
         [0.7078, 0.6695]],

        [[0.6019, 0.0992],
         [0.5215, 0.6542]],

        [[0.6285, 0.0978],
         [0.6226, 0.6713]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 1
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721016799725718
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.604435145582853
Global Adjusted Rand Index: 0.8241120474144994
Average Adjusted Rand Index: 0.8282452056821483
10963.56206775153
[0.04902242729341788, 0.8241120474144994] [0.1596089741659893, 0.8282452056821483] [11019.357409729588, 10913.157579905079]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11086.185360912514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21468.22920604488
Iteration 100: Loss = -11209.997751518045
Iteration 200: Loss = -11209.580427964
Iteration 300: Loss = -11209.422198110058
Iteration 400: Loss = -11209.322040102887
Iteration 500: Loss = -11209.247469247477
Iteration 600: Loss = -11209.184902928297
Iteration 700: Loss = -11209.126872040251
Iteration 800: Loss = -11209.06717685589
Iteration 900: Loss = -11209.000999157934
Iteration 1000: Loss = -11208.929047957969
Iteration 1100: Loss = -11208.860668501919
Iteration 1200: Loss = -11208.803335955827
Iteration 1300: Loss = -11208.75721574412
Iteration 1400: Loss = -11208.71946778252
Iteration 1500: Loss = -11208.686962405432
Iteration 1600: Loss = -11208.657502752465
Iteration 1700: Loss = -11208.63052052136
Iteration 1800: Loss = -11208.605938903092
Iteration 1900: Loss = -11208.584055781766
Iteration 2000: Loss = -11208.56474357267
Iteration 2100: Loss = -11208.547853379465
Iteration 2200: Loss = -11208.532879454628
Iteration 2300: Loss = -11208.51942112683
Iteration 2400: Loss = -11208.507162989663
Iteration 2500: Loss = -11208.495449963124
Iteration 2600: Loss = -11208.484098384373
Iteration 2700: Loss = -11208.472658634651
Iteration 2800: Loss = -11208.460678872443
Iteration 2900: Loss = -11208.447840026725
Iteration 3000: Loss = -11208.433804231203
Iteration 3100: Loss = -11208.418898702661
Iteration 3200: Loss = -11208.403685885312
Iteration 3300: Loss = -11208.38886285947
Iteration 3400: Loss = -11208.375139299049
Iteration 3500: Loss = -11208.362780834554
Iteration 3600: Loss = -11208.35196381355
Iteration 3700: Loss = -11208.342639387645
Iteration 3800: Loss = -11208.334723353459
Iteration 3900: Loss = -11208.327895337747
Iteration 4000: Loss = -11208.320722383372
Iteration 4100: Loss = -11208.307082430129
Iteration 4200: Loss = -11208.20893802784
Iteration 4300: Loss = -11207.17705194322
Iteration 4400: Loss = -11205.899966058487
Iteration 4500: Loss = -11205.238935820813
Iteration 4600: Loss = -11204.953213724308
Iteration 4700: Loss = -11204.247017028563
Iteration 4800: Loss = -11203.67023258533
Iteration 4900: Loss = -11203.425929684628
Iteration 5000: Loss = -11203.313831823863
Iteration 5100: Loss = -11203.256560392416
Iteration 5200: Loss = -11203.22393398279
Iteration 5300: Loss = -11203.203402495668
Iteration 5400: Loss = -11203.189348596512
Iteration 5500: Loss = -11203.17915980098
Iteration 5600: Loss = -11203.171390132042
Iteration 5700: Loss = -11203.165320278578
Iteration 5800: Loss = -11203.160418605412
Iteration 5900: Loss = -11203.156387563511
Iteration 6000: Loss = -11203.152999901276
Iteration 6100: Loss = -11203.15010870017
Iteration 6200: Loss = -11203.14768097765
Iteration 6300: Loss = -11203.145531548333
Iteration 6400: Loss = -11203.14366167017
Iteration 6500: Loss = -11203.142021766582
Iteration 6600: Loss = -11203.140594415267
Iteration 6700: Loss = -11203.139319235059
Iteration 6800: Loss = -11203.138119524321
Iteration 6900: Loss = -11203.137107397817
Iteration 7000: Loss = -11203.136198104678
Iteration 7100: Loss = -11203.135309257408
Iteration 7200: Loss = -11203.134524584611
Iteration 7300: Loss = -11203.133845369632
Iteration 7400: Loss = -11203.133339356698
Iteration 7500: Loss = -11203.132613032429
Iteration 7600: Loss = -11203.132050847711
Iteration 7700: Loss = -11203.459106259144
1
Iteration 7800: Loss = -11203.131137441216
Iteration 7900: Loss = -11203.130663955075
Iteration 8000: Loss = -11203.130275691079
Iteration 8100: Loss = -11203.150016405347
1
Iteration 8200: Loss = -11203.1295829459
Iteration 8300: Loss = -11203.12925691808
Iteration 8400: Loss = -11203.128984681263
Iteration 8500: Loss = -11203.128758442446
Iteration 8600: Loss = -11203.1284335441
Iteration 8700: Loss = -11203.128168853482
Iteration 8800: Loss = -11203.128030396765
Iteration 8900: Loss = -11203.127833911634
Iteration 9000: Loss = -11203.12755021766
Iteration 9100: Loss = -11203.127350291415
Iteration 9200: Loss = -11203.260696559844
1
Iteration 9300: Loss = -11203.127015682514
Iteration 9400: Loss = -11203.12683600132
Iteration 9500: Loss = -11203.126717621652
Iteration 9600: Loss = -11203.126888682902
1
Iteration 9700: Loss = -11203.126437568826
Iteration 9800: Loss = -11203.126358982847
Iteration 9900: Loss = -11203.126249240688
Iteration 10000: Loss = -11203.126173616969
Iteration 10100: Loss = -11203.125989400347
Iteration 10200: Loss = -11203.125925356213
Iteration 10300: Loss = -11203.126124653729
1
Iteration 10400: Loss = -11203.125726160622
Iteration 10500: Loss = -11203.125750013476
Iteration 10600: Loss = -11203.125583054118
Iteration 10700: Loss = -11203.125486582265
Iteration 10800: Loss = -11203.131230789384
1
Iteration 10900: Loss = -11203.125385857587
Iteration 11000: Loss = -11203.125332054515
Iteration 11100: Loss = -11203.125243425928
Iteration 11200: Loss = -11203.125394920637
1
Iteration 11300: Loss = -11203.125135309734
Iteration 11400: Loss = -11203.125079467156
Iteration 11500: Loss = -11203.132372021935
1
Iteration 11600: Loss = -11203.124992341933
Iteration 11700: Loss = -11203.124984039188
Iteration 11800: Loss = -11203.160438807417
1
Iteration 11900: Loss = -11203.124952664366
Iteration 12000: Loss = -11203.124832749181
Iteration 12100: Loss = -11203.155155611652
1
Iteration 12200: Loss = -11203.124725751493
Iteration 12300: Loss = -11203.124771991863
Iteration 12400: Loss = -11203.124725579497
Iteration 12500: Loss = -11203.124685021352
Iteration 12600: Loss = -11203.129238269012
1
Iteration 12700: Loss = -11203.124662813616
Iteration 12800: Loss = -11203.124697095218
Iteration 12900: Loss = -11203.124600789328
Iteration 13000: Loss = -11203.125238662971
1
Iteration 13100: Loss = -11203.124846306433
2
Iteration 13200: Loss = -11203.124522370643
Iteration 13300: Loss = -11203.12575680646
1
Iteration 13400: Loss = -11203.125488121908
2
Iteration 13500: Loss = -11203.128837518292
3
Iteration 13600: Loss = -11203.124761219126
4
Iteration 13700: Loss = -11203.124475935818
Iteration 13800: Loss = -11203.125111039522
1
Iteration 13900: Loss = -11203.124520271647
Iteration 14000: Loss = -11203.124482157977
Iteration 14100: Loss = -11203.124516049795
Iteration 14200: Loss = -11203.124360673964
Iteration 14300: Loss = -11203.198972996717
1
Iteration 14400: Loss = -11203.128059951297
2
Iteration 14500: Loss = -11203.40213956483
3
Iteration 14600: Loss = -11203.127179725667
4
Iteration 14700: Loss = -11203.126181992937
5
Iteration 14800: Loss = -11203.126612707965
6
Iteration 14900: Loss = -11203.12500410102
7
Iteration 15000: Loss = -11203.137815479575
8
Iteration 15100: Loss = -11203.124404820197
Iteration 15200: Loss = -11203.298580332535
1
Iteration 15300: Loss = -11203.126041285826
2
Iteration 15400: Loss = -11203.125254695882
3
Iteration 15500: Loss = -11203.126356814215
4
Iteration 15600: Loss = -11203.124391257494
Iteration 15700: Loss = -11203.126610261701
1
Iteration 15800: Loss = -11203.12652584165
2
Iteration 15900: Loss = -11203.464331830057
3
Iteration 16000: Loss = -11203.124283535195
Iteration 16100: Loss = -11203.12427134538
Iteration 16200: Loss = -11203.125963115886
1
Iteration 16300: Loss = -11203.124276921455
Iteration 16400: Loss = -11203.184554516982
1
Iteration 16500: Loss = -11203.124235192829
Iteration 16600: Loss = -11203.125604896364
1
Iteration 16700: Loss = -11203.124306533457
Iteration 16800: Loss = -11203.1254742243
1
Iteration 16900: Loss = -11203.12529537984
2
Iteration 17000: Loss = -11203.12469110846
3
Iteration 17100: Loss = -11203.12428762903
Iteration 17200: Loss = -11203.125063705507
1
Iteration 17300: Loss = -11203.124639925512
2
Iteration 17400: Loss = -11203.181238246481
3
Iteration 17500: Loss = -11203.124274102849
Iteration 17600: Loss = -11203.124718163257
1
Iteration 17700: Loss = -11203.124904610064
2
Iteration 17800: Loss = -11203.12504763573
3
Iteration 17900: Loss = -11203.124749411034
4
Iteration 18000: Loss = -11203.12489323983
5
Iteration 18100: Loss = -11203.124864190762
6
Iteration 18200: Loss = -11203.129047382785
7
Iteration 18300: Loss = -11203.124981679974
8
Iteration 18400: Loss = -11203.124443632894
9
Iteration 18500: Loss = -11203.12572100297
10
Iteration 18600: Loss = -11203.124979491944
11
Iteration 18700: Loss = -11203.132961820113
12
Iteration 18800: Loss = -11203.124397940046
13
Iteration 18900: Loss = -11203.124864098287
14
Iteration 19000: Loss = -11203.127310555989
15
Stopping early at iteration 19000 due to no improvement.
pi: tensor([[1.0000e+00, 9.7441e-09],
        [1.0363e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9801, 0.0199], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1659, 0.2365],
         [0.6803, 0.7989]],

        [[0.5943, 0.1837],
         [0.7202, 0.5523]],

        [[0.6270, 0.2654],
         [0.7189, 0.5239]],

        [[0.7282, 0.2092],
         [0.5422, 0.5881]],

        [[0.5812, 0.1976],
         [0.6856, 0.5076]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: -2.7862102935448927e-05
Average Adjusted Rand Index: 0.0025662693846000744
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22097.28877334364
Iteration 100: Loss = -11210.180057954438
Iteration 200: Loss = -11209.502158967973
Iteration 300: Loss = -11209.322166234533
Iteration 400: Loss = -11209.18996926232
Iteration 500: Loss = -11209.048451914683
Iteration 600: Loss = -11208.896890872205
Iteration 700: Loss = -11208.743004106065
Iteration 800: Loss = -11208.600717661946
Iteration 900: Loss = -11208.487339932672
Iteration 1000: Loss = -11208.400270854587
Iteration 1100: Loss = -11208.33518366046
Iteration 1200: Loss = -11208.287236755425
Iteration 1300: Loss = -11208.24457832901
Iteration 1400: Loss = -11208.192957694146
Iteration 1500: Loss = -11208.101237582267
Iteration 1600: Loss = -11207.919229921872
Iteration 1700: Loss = -11207.640947554077
Iteration 1800: Loss = -11206.894577522608
Iteration 1900: Loss = -11206.389476626211
Iteration 2000: Loss = -11205.904376130075
Iteration 2100: Loss = -11205.507913970121
Iteration 2200: Loss = -11205.268559286855
Iteration 2300: Loss = -11205.112710168789
Iteration 2400: Loss = -11205.000655065662
Iteration 2500: Loss = -11204.901299609142
Iteration 2600: Loss = -11204.508182686322
Iteration 2700: Loss = -11204.13970902996
Iteration 2800: Loss = -11203.881164393146
Iteration 2900: Loss = -11203.692627075821
Iteration 3000: Loss = -11203.555566804635
Iteration 3100: Loss = -11203.456205270419
Iteration 3200: Loss = -11203.383938363715
Iteration 3300: Loss = -11203.3311701338
Iteration 3400: Loss = -11203.292192550905
Iteration 3500: Loss = -11203.263178169585
Iteration 3600: Loss = -11203.241147004208
Iteration 3700: Loss = -11203.224243709756
Iteration 3800: Loss = -11203.210935589668
Iteration 3900: Loss = -11203.200311223081
Iteration 4000: Loss = -11203.191655400653
Iteration 4100: Loss = -11203.184513068558
Iteration 4200: Loss = -11203.17850200748
Iteration 4300: Loss = -11203.173320993981
Iteration 4400: Loss = -11203.16893015284
Iteration 4500: Loss = -11203.165029385491
Iteration 4600: Loss = -11203.161649573065
Iteration 4700: Loss = -11203.158641562997
Iteration 4800: Loss = -11203.155942930203
Iteration 4900: Loss = -11203.153547117721
Iteration 5000: Loss = -11203.15138624673
Iteration 5100: Loss = -11203.149391544242
Iteration 5200: Loss = -11203.147619201458
Iteration 5300: Loss = -11203.146017688161
Iteration 5400: Loss = -11203.144549996561
Iteration 5500: Loss = -11203.143153806934
Iteration 5600: Loss = -11203.141963952663
Iteration 5700: Loss = -11203.140798145376
Iteration 5800: Loss = -11203.139747549527
Iteration 5900: Loss = -11203.138738871648
Iteration 6000: Loss = -11203.13787864408
Iteration 6100: Loss = -11203.137071360294
Iteration 6200: Loss = -11203.136269375607
Iteration 6300: Loss = -11203.135530159556
Iteration 6400: Loss = -11203.13490472366
Iteration 6500: Loss = -11203.134255210067
Iteration 6600: Loss = -11203.133659265306
Iteration 6700: Loss = -11203.133131884522
Iteration 6800: Loss = -11203.1326153204
Iteration 6900: Loss = -11203.132151182474
Iteration 7000: Loss = -11203.13167302097
Iteration 7100: Loss = -11203.131277616429
Iteration 7200: Loss = -11203.13090137824
Iteration 7300: Loss = -11203.130554843097
Iteration 7400: Loss = -11203.13021312614
Iteration 7500: Loss = -11203.129846105438
Iteration 7600: Loss = -11203.129549770141
Iteration 7700: Loss = -11203.42984161894
1
Iteration 7800: Loss = -11203.128992418202
Iteration 7900: Loss = -11203.128725651972
Iteration 8000: Loss = -11203.128490803623
Iteration 8100: Loss = -11203.146579671313
1
Iteration 8200: Loss = -11203.12803794417
Iteration 8300: Loss = -11203.127858701433
Iteration 8400: Loss = -11203.127668543151
Iteration 8500: Loss = -11203.133978165231
1
Iteration 8600: Loss = -11203.127303941168
Iteration 8700: Loss = -11203.127132633192
Iteration 8800: Loss = -11203.12700121659
Iteration 8900: Loss = -11203.179744373989
1
Iteration 9000: Loss = -11203.126733521602
Iteration 9100: Loss = -11203.126597081366
Iteration 9200: Loss = -11203.126475048046
Iteration 9300: Loss = -11203.133601400406
1
Iteration 9400: Loss = -11203.126252217207
Iteration 9500: Loss = -11203.126178221208
Iteration 9600: Loss = -11203.126028350762
Iteration 9700: Loss = -11203.12594703301
Iteration 9800: Loss = -11203.125818792198
Iteration 9900: Loss = -11203.12580006206
Iteration 10000: Loss = -11203.125686211006
Iteration 10100: Loss = -11203.125671340269
Iteration 10200: Loss = -11203.12556198042
Iteration 10300: Loss = -11203.12570712917
1
Iteration 10400: Loss = -11203.12537915304
Iteration 10500: Loss = -11203.125335696972
Iteration 10600: Loss = -11203.127813354447
1
Iteration 10700: Loss = -11203.125251796879
Iteration 10800: Loss = -11203.125213794432
Iteration 10900: Loss = -11203.15349664977
1
Iteration 11000: Loss = -11203.125131522991
Iteration 11100: Loss = -11203.125049178776
Iteration 11200: Loss = -11203.136829175095
1
Iteration 11300: Loss = -11203.1249606821
Iteration 11400: Loss = -11203.124942223902
Iteration 11500: Loss = -11203.126169326637
1
Iteration 11600: Loss = -11203.124900353885
Iteration 11700: Loss = -11203.124827282494
Iteration 11800: Loss = -11203.124781768383
Iteration 11900: Loss = -11203.134607025302
1
Iteration 12000: Loss = -11203.12474624445
Iteration 12100: Loss = -11203.124732481005
Iteration 12200: Loss = -11203.153179339683
1
Iteration 12300: Loss = -11203.124707549918
Iteration 12400: Loss = -11203.1246353511
Iteration 12500: Loss = -11203.133260556411
1
Iteration 12600: Loss = -11203.124762188882
2
Iteration 12700: Loss = -11203.449816762322
3
Iteration 12800: Loss = -11203.124539011456
Iteration 12900: Loss = -11203.124555204924
Iteration 13000: Loss = -11203.1255382813
1
Iteration 13100: Loss = -11203.124482486259
Iteration 13200: Loss = -11203.12445330842
Iteration 13300: Loss = -11203.341297435973
1
Iteration 13400: Loss = -11203.125541319047
2
Iteration 13500: Loss = -11203.124419807025
Iteration 13600: Loss = -11203.128999749646
1
Iteration 13700: Loss = -11203.126405147395
2
Iteration 13800: Loss = -11203.124745538918
3
Iteration 13900: Loss = -11203.124393389448
Iteration 14000: Loss = -11203.12446654845
Iteration 14100: Loss = -11203.12448207854
Iteration 14200: Loss = -11203.124372356184
Iteration 14300: Loss = -11203.128046631258
1
Iteration 14400: Loss = -11203.12436239002
Iteration 14500: Loss = -11203.12507565265
1
Iteration 14600: Loss = -11203.125488870088
2
Iteration 14700: Loss = -11203.125221489989
3
Iteration 14800: Loss = -11203.377075042668
4
Iteration 14900: Loss = -11203.12464932292
5
Iteration 15000: Loss = -11203.379871122368
6
Iteration 15100: Loss = -11203.124528259405
7
Iteration 15200: Loss = -11203.435004194907
8
Iteration 15300: Loss = -11203.12530213159
9
Iteration 15400: Loss = -11203.1247511357
10
Iteration 15500: Loss = -11203.127636401512
11
Iteration 15600: Loss = -11203.1247772484
12
Iteration 15700: Loss = -11203.124491161261
13
Iteration 15800: Loss = -11203.124331348941
Iteration 15900: Loss = -11203.133315347797
1
Iteration 16000: Loss = -11203.12478937868
2
Iteration 16100: Loss = -11203.125080379667
3
Iteration 16200: Loss = -11203.125496262894
4
Iteration 16300: Loss = -11203.12435949516
Iteration 16400: Loss = -11203.124705841085
1
Iteration 16500: Loss = -11203.125579841868
2
Iteration 16600: Loss = -11203.128536174443
3
Iteration 16700: Loss = -11203.127251681373
4
Iteration 16800: Loss = -11203.395719072027
5
Iteration 16900: Loss = -11203.124543669272
6
Iteration 17000: Loss = -11203.124711819342
7
Iteration 17100: Loss = -11203.124475232551
8
Iteration 17200: Loss = -11203.125415348864
9
Iteration 17300: Loss = -11203.162563961632
10
Iteration 17400: Loss = -11203.125475808469
11
Iteration 17500: Loss = -11203.124662526972
12
Iteration 17600: Loss = -11203.124376818156
Iteration 17700: Loss = -11203.124340266279
Iteration 17800: Loss = -11203.128433950811
1
Iteration 17900: Loss = -11203.126074616132
2
Iteration 18000: Loss = -11203.124844856025
3
Iteration 18100: Loss = -11203.125454998917
4
Iteration 18200: Loss = -11203.125662520271
5
Iteration 18300: Loss = -11203.124859313335
6
Iteration 18400: Loss = -11203.125020322537
7
Iteration 18500: Loss = -11203.128287579806
8
Iteration 18600: Loss = -11203.124632443612
9
Iteration 18700: Loss = -11203.126629262231
10
Iteration 18800: Loss = -11203.12508444666
11
Iteration 18900: Loss = -11203.127683438603
12
Iteration 19000: Loss = -11203.12429958417
Iteration 19100: Loss = -11203.126370621909
1
Iteration 19200: Loss = -11203.126719859629
2
Iteration 19300: Loss = -11203.125437871555
3
Iteration 19400: Loss = -11203.125843483233
4
Iteration 19500: Loss = -11203.124365893565
Iteration 19600: Loss = -11203.124519260964
1
Iteration 19700: Loss = -11203.124655024572
2
Iteration 19800: Loss = -11203.131223265525
3
Iteration 19900: Loss = -11203.124766418778
4
pi: tensor([[1.0000e+00, 4.3068e-09],
        [5.0347e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9800, 0.0200], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1655, 0.2346],
         [0.5101, 0.7989]],

        [[0.6450, 0.1838],
         [0.6597, 0.5903]],

        [[0.7034, 0.2654],
         [0.5880, 0.5800]],

        [[0.5928, 0.2091],
         [0.5904, 0.5022]],

        [[0.5230, 0.1991],
         [0.5659, 0.5057]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: -2.7862102935448927e-05
Average Adjusted Rand Index: 0.0025662693846000744
11086.185360912514
[-2.7862102935448927e-05, -2.7862102935448927e-05] [0.0025662693846000744, 0.0025662693846000744] [11203.127310555989, 11203.13965285491]
-------------------------------------
This iteration is 95
True Objective function: Loss = -10854.539688909359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22126.938892645878
Iteration 100: Loss = -10826.70198290044
Iteration 200: Loss = -10807.005510597393
Iteration 300: Loss = -10806.796637120833
Iteration 400: Loss = -10806.720704134339
Iteration 500: Loss = -10806.68740318347
Iteration 600: Loss = -10806.67075842965
Iteration 700: Loss = -10806.660479502758
Iteration 800: Loss = -10806.653342917414
Iteration 900: Loss = -10806.647995489237
Iteration 1000: Loss = -10806.644515464697
Iteration 1100: Loss = -10806.641977143592
Iteration 1200: Loss = -10806.640057746114
Iteration 1300: Loss = -10806.638521421457
Iteration 1400: Loss = -10806.6372882992
Iteration 1500: Loss = -10806.636310527334
Iteration 1600: Loss = -10806.63551446067
Iteration 1700: Loss = -10806.63486937626
Iteration 1800: Loss = -10806.651009799418
1
Iteration 1900: Loss = -10806.633843180183
Iteration 2000: Loss = -10806.633479168559
Iteration 2100: Loss = -10806.644683302304
1
Iteration 2200: Loss = -10806.632831416731
Iteration 2300: Loss = -10806.632559556165
Iteration 2400: Loss = -10806.632375638297
Iteration 2500: Loss = -10806.632156401367
Iteration 2600: Loss = -10806.632383842618
1
Iteration 2700: Loss = -10806.631836074803
Iteration 2800: Loss = -10806.631705708156
Iteration 2900: Loss = -10806.631579001163
Iteration 3000: Loss = -10806.63145470846
Iteration 3100: Loss = -10806.631551999037
Iteration 3200: Loss = -10806.631263246138
Iteration 3300: Loss = -10806.63793009901
1
Iteration 3400: Loss = -10806.631124020972
Iteration 3500: Loss = -10806.631065001777
Iteration 3600: Loss = -10806.632042471376
1
Iteration 3700: Loss = -10806.630902570108
Iteration 3800: Loss = -10806.63088920322
Iteration 3900: Loss = -10806.631064524881
1
Iteration 4000: Loss = -10806.630775447893
Iteration 4100: Loss = -10806.63073922759
Iteration 4200: Loss = -10806.630707331009
Iteration 4300: Loss = -10806.630777830429
Iteration 4400: Loss = -10806.630655216855
Iteration 4500: Loss = -10806.630621117534
Iteration 4600: Loss = -10806.630686053104
Iteration 4700: Loss = -10806.63055164469
Iteration 4800: Loss = -10806.630592788892
Iteration 4900: Loss = -10806.630493940758
Iteration 5000: Loss = -10806.636317806562
1
Iteration 5100: Loss = -10806.630485800819
Iteration 5200: Loss = -10806.630516268477
Iteration 5300: Loss = -10806.630467123845
Iteration 5400: Loss = -10806.630454476131
Iteration 5500: Loss = -10806.63045118538
Iteration 5600: Loss = -10806.630381303168
Iteration 5700: Loss = -10806.632707750654
1
Iteration 5800: Loss = -10806.631940501702
2
Iteration 5900: Loss = -10806.630322143494
Iteration 6000: Loss = -10806.630553281186
1
Iteration 6100: Loss = -10806.63030356051
Iteration 6200: Loss = -10806.630319541915
Iteration 6300: Loss = -10806.639969113548
1
Iteration 6400: Loss = -10806.6303445876
Iteration 6500: Loss = -10806.630323300918
Iteration 6600: Loss = -10806.630545647817
1
Iteration 6700: Loss = -10806.63190135857
2
Iteration 6800: Loss = -10806.630375624916
Iteration 6900: Loss = -10806.630378278223
Iteration 7000: Loss = -10806.634194651795
1
Iteration 7100: Loss = -10806.63660904254
2
Iteration 7200: Loss = -10806.630415492038
Iteration 7300: Loss = -10806.630606857594
1
Iteration 7400: Loss = -10806.65775472999
2
Iteration 7500: Loss = -10806.630251716939
Iteration 7600: Loss = -10806.630653708613
1
Iteration 7700: Loss = -10806.630208207509
Iteration 7800: Loss = -10806.647900189902
1
Iteration 7900: Loss = -10806.635261524965
2
Iteration 8000: Loss = -10806.630744468712
3
Iteration 8100: Loss = -10806.63025696963
Iteration 8200: Loss = -10806.630268511994
Iteration 8300: Loss = -10806.630367550002
Iteration 8400: Loss = -10806.630255750648
Iteration 8500: Loss = -10806.630300030885
Iteration 8600: Loss = -10806.638854139499
1
Iteration 8700: Loss = -10806.630645296924
2
Iteration 8800: Loss = -10806.655782871978
3
Iteration 8900: Loss = -10806.635570386938
4
Iteration 9000: Loss = -10806.642582378823
5
Iteration 9100: Loss = -10806.638940790892
6
Iteration 9200: Loss = -10806.630227658756
Iteration 9300: Loss = -10806.630298843844
Iteration 9400: Loss = -10806.646620959444
1
Iteration 9500: Loss = -10806.630199180849
Iteration 9600: Loss = -10806.630654829816
1
Iteration 9700: Loss = -10806.630411428245
2
Iteration 9800: Loss = -10806.63028989566
Iteration 9900: Loss = -10806.630505654724
1
Iteration 10000: Loss = -10806.641009221335
2
Iteration 10100: Loss = -10806.632506829572
3
Iteration 10200: Loss = -10806.630207080543
Iteration 10300: Loss = -10806.752305404729
1
Iteration 10400: Loss = -10806.630208568007
Iteration 10500: Loss = -10806.645848066517
1
Iteration 10600: Loss = -10806.638875038343
2
Iteration 10700: Loss = -10806.632525638815
3
Iteration 10800: Loss = -10806.68877871609
4
Iteration 10900: Loss = -10806.630387562029
5
Iteration 11000: Loss = -10806.6306757258
6
Iteration 11100: Loss = -10806.63031056843
7
Iteration 11200: Loss = -10806.630551196133
8
Iteration 11300: Loss = -10806.633561071832
9
Iteration 11400: Loss = -10806.675224649307
10
Iteration 11500: Loss = -10806.630230070976
Iteration 11600: Loss = -10806.63028322045
Iteration 11700: Loss = -10806.630450770519
1
Iteration 11800: Loss = -10806.639819796872
2
Iteration 11900: Loss = -10806.643620512274
3
Iteration 12000: Loss = -10806.634362338926
4
Iteration 12100: Loss = -10806.630219894625
Iteration 12200: Loss = -10806.630212141165
Iteration 12300: Loss = -10806.756008815399
1
Iteration 12400: Loss = -10806.630204892277
Iteration 12500: Loss = -10806.630272158842
Iteration 12600: Loss = -10806.694401007759
1
Iteration 12700: Loss = -10806.63915293279
2
Iteration 12800: Loss = -10806.63018107122
Iteration 12900: Loss = -10806.630349190538
1
Iteration 13000: Loss = -10806.633922083536
2
Iteration 13100: Loss = -10806.630215480942
Iteration 13200: Loss = -10806.630230389683
Iteration 13300: Loss = -10806.630186380391
Iteration 13400: Loss = -10806.630203225825
Iteration 13500: Loss = -10806.630358000135
1
Iteration 13600: Loss = -10806.630316213697
2
Iteration 13700: Loss = -10806.636193112123
3
Iteration 13800: Loss = -10806.650806239017
4
Iteration 13900: Loss = -10806.63680433938
5
Iteration 14000: Loss = -10806.632225609961
6
Iteration 14100: Loss = -10806.644259328401
7
Iteration 14200: Loss = -10806.64379944318
8
Iteration 14300: Loss = -10806.640093800563
9
Iteration 14400: Loss = -10806.63608407129
10
Iteration 14500: Loss = -10806.638686994756
11
Iteration 14600: Loss = -10806.643422191732
12
Iteration 14700: Loss = -10806.630215777644
Iteration 14800: Loss = -10806.630528339718
1
Iteration 14900: Loss = -10806.71799590578
2
Iteration 15000: Loss = -10806.63019670149
Iteration 15100: Loss = -10806.657988730025
1
Iteration 15200: Loss = -10806.647448596299
2
Iteration 15300: Loss = -10806.6438686539
3
Iteration 15400: Loss = -10806.640532366384
4
Iteration 15500: Loss = -10806.630709262005
5
Iteration 15600: Loss = -10806.63049561343
6
Iteration 15700: Loss = -10806.630854420804
7
Iteration 15800: Loss = -10806.729144736697
8
Iteration 15900: Loss = -10806.630369169043
9
Iteration 16000: Loss = -10806.639916021975
10
Iteration 16100: Loss = -10806.631279053498
11
Iteration 16200: Loss = -10806.630867299067
12
Iteration 16300: Loss = -10806.63032948199
13
Iteration 16400: Loss = -10806.63724279101
14
Iteration 16500: Loss = -10806.630189510632
Iteration 16600: Loss = -10806.630469302041
1
Iteration 16700: Loss = -10806.633065924196
2
Iteration 16800: Loss = -10806.739118273887
3
Iteration 16900: Loss = -10806.635131188948
4
Iteration 17000: Loss = -10806.630209013612
Iteration 17100: Loss = -10806.631016711448
1
Iteration 17200: Loss = -10806.66078265784
2
Iteration 17300: Loss = -10806.63159908483
3
Iteration 17400: Loss = -10806.632591010402
4
Iteration 17500: Loss = -10806.691491483658
5
Iteration 17600: Loss = -10806.63021411764
Iteration 17700: Loss = -10806.630355184645
1
Iteration 17800: Loss = -10806.648134132405
2
Iteration 17900: Loss = -10806.641780739821
3
Iteration 18000: Loss = -10806.63039357754
4
Iteration 18100: Loss = -10806.630559410009
5
Iteration 18200: Loss = -10806.63022939405
Iteration 18300: Loss = -10806.630413741424
1
Iteration 18400: Loss = -10806.633448090788
2
Iteration 18500: Loss = -10806.64579319308
3
Iteration 18600: Loss = -10806.630206933725
Iteration 18700: Loss = -10806.630524385582
1
Iteration 18800: Loss = -10806.630447602112
2
Iteration 18900: Loss = -10806.633764978045
3
Iteration 19000: Loss = -10806.630940975214
4
Iteration 19100: Loss = -10806.630573516642
5
Iteration 19200: Loss = -10806.635195923545
6
Iteration 19300: Loss = -10806.63020723692
Iteration 19400: Loss = -10806.63032528317
1
Iteration 19500: Loss = -10806.66242516986
2
Iteration 19600: Loss = -10806.634540472569
3
Iteration 19700: Loss = -10806.67370264609
4
Iteration 19800: Loss = -10806.74543548868
5
Iteration 19900: Loss = -10806.661619283726
6
pi: tensor([[0.7435, 0.2565],
        [0.2149, 0.7851]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4544, 0.5456], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2054, 0.1016],
         [0.5501, 0.2420]],

        [[0.6728, 0.0987],
         [0.7107, 0.6361]],

        [[0.6870, 0.0953],
         [0.6576, 0.6999]],

        [[0.5956, 0.0951],
         [0.6163, 0.7091]],

        [[0.6541, 0.0947],
         [0.5206, 0.5473]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369525201115061
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.8909169496927284
Average Adjusted Rand Index: 0.8918733156803912
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23130.983572718505
Iteration 100: Loss = -10957.702661354013
Iteration 200: Loss = -10956.57159147854
Iteration 300: Loss = -10956.07226803401
Iteration 400: Loss = -10955.664980498928
Iteration 500: Loss = -10955.136531613027
Iteration 600: Loss = -10954.405276399042
Iteration 700: Loss = -10953.535283690795
Iteration 800: Loss = -10953.021682871522
Iteration 900: Loss = -10952.306371433702
Iteration 1000: Loss = -10951.657111296012
Iteration 1100: Loss = -10951.051054813526
Iteration 1200: Loss = -10950.328542105734
Iteration 1300: Loss = -10950.039745360398
Iteration 1400: Loss = -10949.773363051163
Iteration 1500: Loss = -10949.6341174244
Iteration 1600: Loss = -10949.535802061375
Iteration 1700: Loss = -10949.394653841298
Iteration 1800: Loss = -10949.28408750642
Iteration 1900: Loss = -10949.183033571975
Iteration 2000: Loss = -10949.08471650884
Iteration 2100: Loss = -10948.914886965798
Iteration 2200: Loss = -10948.764235735736
Iteration 2300: Loss = -10948.705718248593
Iteration 2400: Loss = -10948.385330338066
Iteration 2500: Loss = -10948.217765438067
Iteration 2600: Loss = -10948.142038625036
Iteration 2700: Loss = -10948.023526590852
Iteration 2800: Loss = -10947.928902708307
Iteration 2900: Loss = -10947.80781039136
Iteration 3000: Loss = -10947.66343178926
Iteration 3100: Loss = -10947.319757137786
Iteration 3200: Loss = -10940.474008694211
Iteration 3300: Loss = -10917.64798902925
Iteration 3400: Loss = -10869.700769082914
Iteration 3500: Loss = -10852.970504678888
Iteration 3600: Loss = -10833.056916292016
Iteration 3700: Loss = -10828.29623841117
Iteration 3800: Loss = -10821.728893071373
Iteration 3900: Loss = -10821.543092963506
Iteration 4000: Loss = -10817.856911725507
Iteration 4100: Loss = -10817.366984621152
Iteration 4200: Loss = -10817.346461317551
Iteration 4300: Loss = -10817.249997348112
Iteration 4400: Loss = -10817.23802742494
Iteration 4500: Loss = -10817.171485169081
Iteration 4600: Loss = -10817.161783825077
Iteration 4700: Loss = -10817.148378784408
Iteration 4800: Loss = -10817.12721381552
Iteration 4900: Loss = -10817.121469432675
Iteration 5000: Loss = -10817.118198050606
Iteration 5100: Loss = -10817.115571328262
Iteration 5200: Loss = -10817.102286194608
Iteration 5300: Loss = -10813.197247517091
Iteration 5400: Loss = -10813.176255704111
Iteration 5500: Loss = -10813.166489265179
Iteration 5600: Loss = -10813.164445909528
Iteration 5700: Loss = -10813.161696378735
Iteration 5800: Loss = -10813.15430331538
Iteration 5900: Loss = -10813.074220598972
Iteration 6000: Loss = -10813.06828262085
Iteration 6100: Loss = -10813.06004537787
Iteration 6200: Loss = -10813.05743723133
Iteration 6300: Loss = -10813.033147950424
Iteration 6400: Loss = -10813.031262696028
Iteration 6500: Loss = -10813.030192069875
Iteration 6600: Loss = -10813.029091535951
Iteration 6700: Loss = -10813.028470889876
Iteration 6800: Loss = -10813.03260096681
1
Iteration 6900: Loss = -10813.02588215095
Iteration 7000: Loss = -10812.956984691282
Iteration 7100: Loss = -10812.956185003148
Iteration 7200: Loss = -10810.972705278284
Iteration 7300: Loss = -10810.837234857328
Iteration 7400: Loss = -10810.836365224655
Iteration 7500: Loss = -10810.835705438803
Iteration 7600: Loss = -10810.814088645859
Iteration 7700: Loss = -10810.797500631648
Iteration 7800: Loss = -10810.828846721348
1
Iteration 7900: Loss = -10810.792270625276
Iteration 8000: Loss = -10810.72859240091
Iteration 8100: Loss = -10810.712354501688
Iteration 8200: Loss = -10810.704143262421
Iteration 8300: Loss = -10810.702424528901
Iteration 8400: Loss = -10810.701528293264
Iteration 8500: Loss = -10810.720839272246
1
Iteration 8600: Loss = -10810.69485667787
Iteration 8700: Loss = -10810.694477281993
Iteration 8800: Loss = -10810.685873356535
Iteration 8900: Loss = -10810.676477502539
Iteration 9000: Loss = -10810.676043532312
Iteration 9100: Loss = -10810.675354952578
Iteration 9200: Loss = -10810.674863020431
Iteration 9300: Loss = -10810.676898227677
1
Iteration 9400: Loss = -10810.674025339216
Iteration 9500: Loss = -10810.689132455953
1
Iteration 9600: Loss = -10810.665670298948
Iteration 9700: Loss = -10810.6568805881
Iteration 9800: Loss = -10810.664224238584
1
Iteration 9900: Loss = -10810.628303612924
Iteration 10000: Loss = -10810.628009433784
Iteration 10100: Loss = -10810.65471887113
1
Iteration 10200: Loss = -10810.630061764698
2
Iteration 10300: Loss = -10810.620436846666
Iteration 10400: Loss = -10810.558890818695
Iteration 10500: Loss = -10810.556457520916
Iteration 10600: Loss = -10810.555138458603
Iteration 10700: Loss = -10810.558958556903
1
Iteration 10800: Loss = -10810.579545898094
2
Iteration 10900: Loss = -10810.553516223403
Iteration 11000: Loss = -10810.553852722878
1
Iteration 11100: Loss = -10810.546421286164
Iteration 11200: Loss = -10810.538460057014
Iteration 11300: Loss = -10810.536398020266
Iteration 11400: Loss = -10810.560157819744
1
Iteration 11500: Loss = -10810.53631534298
Iteration 11600: Loss = -10810.536494615706
1
Iteration 11700: Loss = -10810.594851972683
2
Iteration 11800: Loss = -10810.536123508175
Iteration 11900: Loss = -10810.532561233813
Iteration 12000: Loss = -10810.593501701587
1
Iteration 12100: Loss = -10810.538893100445
2
Iteration 12200: Loss = -10810.529486756863
Iteration 12300: Loss = -10810.553920933531
1
Iteration 12400: Loss = -10807.38089501653
Iteration 12500: Loss = -10807.36505500175
Iteration 12600: Loss = -10807.364483270203
Iteration 12700: Loss = -10807.380466366678
1
Iteration 12800: Loss = -10807.406738878206
2
Iteration 12900: Loss = -10807.36464576536
3
Iteration 13000: Loss = -10807.364725420353
4
Iteration 13100: Loss = -10807.38541579821
5
Iteration 13200: Loss = -10807.423319104786
6
Iteration 13300: Loss = -10807.35914115292
Iteration 13400: Loss = -10807.358584936592
Iteration 13500: Loss = -10807.549397696468
1
Iteration 13600: Loss = -10807.31641868935
Iteration 13700: Loss = -10807.274541681483
Iteration 13800: Loss = -10807.282239865928
1
Iteration 13900: Loss = -10807.265793089999
Iteration 14000: Loss = -10807.254420534066
Iteration 14100: Loss = -10807.307469397594
1
Iteration 14200: Loss = -10807.28497092996
2
Iteration 14300: Loss = -10807.262773632494
3
Iteration 14400: Loss = -10807.262075028786
4
Iteration 14500: Loss = -10807.325072162379
5
Iteration 14600: Loss = -10807.255074910818
6
Iteration 14700: Loss = -10807.255988136427
7
Iteration 14800: Loss = -10807.262316885512
8
Iteration 14900: Loss = -10807.316788532835
9
Iteration 15000: Loss = -10807.268390928399
10
Iteration 15100: Loss = -10807.253354025146
Iteration 15200: Loss = -10807.253441388395
Iteration 15300: Loss = -10807.472902818452
1
Iteration 15400: Loss = -10807.250702683807
Iteration 15500: Loss = -10807.279510816978
1
Iteration 15600: Loss = -10807.101052439639
Iteration 15700: Loss = -10807.075300344228
Iteration 15800: Loss = -10807.06815363191
Iteration 15900: Loss = -10807.067524834152
Iteration 16000: Loss = -10807.067373029331
Iteration 16100: Loss = -10807.071523126926
1
Iteration 16200: Loss = -10807.069196664475
2
Iteration 16300: Loss = -10807.069221658603
3
Iteration 16400: Loss = -10807.067323968198
Iteration 16500: Loss = -10807.06955051948
1
Iteration 16600: Loss = -10807.0672451795
Iteration 16700: Loss = -10807.06731813839
Iteration 16800: Loss = -10807.097342475887
1
Iteration 16900: Loss = -10807.085803547057
2
Iteration 17000: Loss = -10807.065787402353
Iteration 17100: Loss = -10807.138679156187
1
Iteration 17200: Loss = -10807.065421125775
Iteration 17300: Loss = -10807.065109328254
Iteration 17400: Loss = -10807.06635189454
1
Iteration 17500: Loss = -10807.102726572573
2
Iteration 17600: Loss = -10807.064648917914
Iteration 17700: Loss = -10807.06555928848
1
Iteration 17800: Loss = -10807.068188136236
2
Iteration 17900: Loss = -10807.065187485892
3
Iteration 18000: Loss = -10807.106184771763
4
Iteration 18100: Loss = -10807.064589138257
Iteration 18200: Loss = -10807.064744017072
1
Iteration 18300: Loss = -10807.07250895133
2
Iteration 18400: Loss = -10807.065386449507
3
Iteration 18500: Loss = -10807.0664239933
4
Iteration 18600: Loss = -10807.123930643915
5
Iteration 18700: Loss = -10807.064416744764
Iteration 18800: Loss = -10807.17774725547
1
Iteration 18900: Loss = -10807.073088837751
2
Iteration 19000: Loss = -10807.302124122218
3
Iteration 19100: Loss = -10807.065389574771
4
Iteration 19200: Loss = -10807.065911134096
5
Iteration 19300: Loss = -10807.187457262424
6
Iteration 19400: Loss = -10807.064620547199
7
Iteration 19500: Loss = -10807.064839556122
8
Iteration 19600: Loss = -10807.067325095437
9
Iteration 19700: Loss = -10807.17102985578
10
Iteration 19800: Loss = -10807.066987882414
11
Iteration 19900: Loss = -10807.057747146111
pi: tensor([[0.7474, 0.2526],
        [0.2139, 0.7861]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4541, 0.5459], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2047, 0.1018],
         [0.6110, 0.2425]],

        [[0.5279, 0.0990],
         [0.7102, 0.6900]],

        [[0.5387, 0.0960],
         [0.6832, 0.7232]],

        [[0.7169, 0.0954],
         [0.5802, 0.5880]],

        [[0.5347, 0.0952],
         [0.5334, 0.5595]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 93
Adjusted Rand Index: 0.7369525201115061
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.8909169496927284
Average Adjusted Rand Index: 0.8918733156803912
10854.539688909359
[0.8909169496927284, 0.8909169496927284] [0.8918733156803912, 0.8918733156803912] [10806.658502107126, 10807.058810567149]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11113.180190766367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22242.598275312517
Iteration 100: Loss = -11259.25872498335
Iteration 200: Loss = -11257.882260558987
Iteration 300: Loss = -11257.149993163675
Iteration 400: Loss = -11256.4967928552
Iteration 500: Loss = -11255.638086127474
Iteration 600: Loss = -11254.915211240941
Iteration 700: Loss = -11254.553933996453
Iteration 800: Loss = -11254.370641737241
Iteration 900: Loss = -11254.274758242049
Iteration 1000: Loss = -11254.219379584272
Iteration 1100: Loss = -11254.183148363558
Iteration 1200: Loss = -11254.15649291105
Iteration 1300: Loss = -11254.13442123889
Iteration 1400: Loss = -11254.113023350206
Iteration 1500: Loss = -11254.086670213766
Iteration 1600: Loss = -11254.039687134215
Iteration 1700: Loss = -11253.87154664605
Iteration 1800: Loss = -11252.964927438115
Iteration 1900: Loss = -11252.416737281748
Iteration 2000: Loss = -11251.800675413662
Iteration 2100: Loss = -11217.014886662642
Iteration 2200: Loss = -11120.001724896509
Iteration 2300: Loss = -11110.670912741332
Iteration 2400: Loss = -11110.377201141308
Iteration 2500: Loss = -11110.189759157809
Iteration 2600: Loss = -11107.082116041938
Iteration 2700: Loss = -11106.75145076932
Iteration 2800: Loss = -11106.723288433734
Iteration 2900: Loss = -11106.70230317173
Iteration 3000: Loss = -11106.682696790494
Iteration 3100: Loss = -11106.657349314723
Iteration 3200: Loss = -11106.646097754165
Iteration 3300: Loss = -11104.915593470567
Iteration 3400: Loss = -11102.658715269172
Iteration 3500: Loss = -11102.426943192184
Iteration 3600: Loss = -11101.755871987229
Iteration 3700: Loss = -11101.737403399682
Iteration 3800: Loss = -11101.637191565196
Iteration 3900: Loss = -11097.643815165282
Iteration 4000: Loss = -11095.8353977406
Iteration 4100: Loss = -11095.054222964674
Iteration 4200: Loss = -11094.974518922183
Iteration 4300: Loss = -11094.9675880467
Iteration 4400: Loss = -11094.962957687268
Iteration 4500: Loss = -11094.959857337357
Iteration 4600: Loss = -11094.954109170065
Iteration 4700: Loss = -11094.950732670162
Iteration 4800: Loss = -11094.943663157594
Iteration 4900: Loss = -11094.238561890745
Iteration 5000: Loss = -11094.225396299853
Iteration 5100: Loss = -11094.220242177742
Iteration 5200: Loss = -11094.189653401107
Iteration 5300: Loss = -11094.189008935371
Iteration 5400: Loss = -11094.186368112922
Iteration 5500: Loss = -11094.181899073938
Iteration 5600: Loss = -11094.146786999367
Iteration 5700: Loss = -11094.146422755497
Iteration 5800: Loss = -11094.147041308164
1
Iteration 5900: Loss = -11094.14749958816
2
Iteration 6000: Loss = -11094.144877781335
Iteration 6100: Loss = -11094.144141871477
Iteration 6200: Loss = -11094.14449230552
1
Iteration 6300: Loss = -11094.141181624647
Iteration 6400: Loss = -11094.140004112354
Iteration 6500: Loss = -11094.141283527782
1
Iteration 6600: Loss = -11094.124639623198
Iteration 6700: Loss = -11094.122097496902
Iteration 6800: Loss = -11094.117336067093
Iteration 6900: Loss = -11094.117084820924
Iteration 7000: Loss = -11094.11639688213
Iteration 7100: Loss = -11094.101641291832
Iteration 7200: Loss = -11094.100962204759
Iteration 7300: Loss = -11094.100540456382
Iteration 7400: Loss = -11094.098252461949
Iteration 7500: Loss = -11094.098017063912
Iteration 7600: Loss = -11094.097265425287
Iteration 7700: Loss = -11094.097277087318
Iteration 7800: Loss = -11094.097218532854
Iteration 7900: Loss = -11094.096942869275
Iteration 8000: Loss = -11094.135570509929
1
Iteration 8100: Loss = -11094.096734805942
Iteration 8200: Loss = -11094.096622664023
Iteration 8300: Loss = -11094.117113863414
1
Iteration 8400: Loss = -11094.09636596192
Iteration 8500: Loss = -11094.130806917114
1
Iteration 8600: Loss = -11094.09623879651
Iteration 8700: Loss = -11094.096203202174
Iteration 8800: Loss = -11094.096155163736
Iteration 8900: Loss = -11094.096106829793
Iteration 9000: Loss = -11094.09746283204
1
Iteration 9100: Loss = -11094.098273321002
2
Iteration 9200: Loss = -11094.096938580496
3
Iteration 9300: Loss = -11094.131012314963
4
Iteration 9400: Loss = -11094.116546784619
5
Iteration 9500: Loss = -11094.093310497947
Iteration 9600: Loss = -11094.08483994482
Iteration 9700: Loss = -11094.107809680614
1
Iteration 9800: Loss = -11094.08277742308
Iteration 9900: Loss = -11094.082930033272
1
Iteration 10000: Loss = -11094.140525716155
2
Iteration 10100: Loss = -11094.086155282075
3
Iteration 10200: Loss = -11094.090901609818
4
Iteration 10300: Loss = -11094.086709339217
5
Iteration 10400: Loss = -11094.221010431496
6
Iteration 10500: Loss = -11094.082152265735
Iteration 10600: Loss = -11094.081474928296
Iteration 10700: Loss = -11094.084500231505
1
Iteration 10800: Loss = -11094.105754652162
2
Iteration 10900: Loss = -11094.081290803508
Iteration 11000: Loss = -11094.081440604925
1
Iteration 11100: Loss = -11094.086093873811
2
Iteration 11200: Loss = -11094.08242276284
3
Iteration 11300: Loss = -11094.084056324165
4
Iteration 11400: Loss = -11094.095001577027
5
Iteration 11500: Loss = -11094.0813482374
Iteration 11600: Loss = -11094.083205226378
1
Iteration 11700: Loss = -11094.08572495723
2
Iteration 11800: Loss = -11094.085117607632
3
Iteration 11900: Loss = -11094.110192981683
4
Iteration 12000: Loss = -11094.081185794359
Iteration 12100: Loss = -11094.081865493341
1
Iteration 12200: Loss = -11094.08624731503
2
Iteration 12300: Loss = -11094.09419826385
3
Iteration 12400: Loss = -11094.084340900437
4
Iteration 12500: Loss = -11094.08456123465
5
Iteration 12600: Loss = -11094.08168450211
6
Iteration 12700: Loss = -11094.081661817823
7
Iteration 12800: Loss = -11094.081520675973
8
Iteration 12900: Loss = -11094.081469435703
9
Iteration 13000: Loss = -11094.081453955132
10
Iteration 13100: Loss = -11094.09111780942
11
Iteration 13200: Loss = -11094.146855577494
12
Iteration 13300: Loss = -11094.100567758638
13
Iteration 13400: Loss = -11094.084638335444
14
Iteration 13500: Loss = -11094.082919951301
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7419, 0.2581],
        [0.3744, 0.6256]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1712, 0.8288], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2666, 0.1001],
         [0.6805, 0.2012]],

        [[0.5942, 0.1041],
         [0.6021, 0.5375]],

        [[0.6839, 0.0974],
         [0.6361, 0.7203]],

        [[0.6170, 0.0921],
         [0.6478, 0.6442]],

        [[0.7281, 0.1002],
         [0.5841, 0.7280]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 74
Adjusted Rand Index: 0.21400363955978657
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721141809334062
time is 2
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721016799725718
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9206925302859384
Global Adjusted Rand Index: 0.4083716387263021
Average Adjusted Rand Index: 0.7199440223119569
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24083.98004582358
Iteration 100: Loss = -11260.19023902306
Iteration 200: Loss = -11259.016378638995
Iteration 300: Loss = -11258.396889803633
Iteration 400: Loss = -11257.345265335147
Iteration 500: Loss = -11256.72955376059
Iteration 600: Loss = -11255.714717594705
Iteration 700: Loss = -11254.804652678788
Iteration 800: Loss = -11254.366191486248
Iteration 900: Loss = -11253.92143985507
Iteration 1000: Loss = -11253.075185637394
Iteration 1100: Loss = -11252.60307893253
Iteration 1200: Loss = -11245.350998367083
Iteration 1300: Loss = -11124.733644966753
Iteration 1400: Loss = -11107.619121770807
Iteration 1500: Loss = -11107.05928594626
Iteration 1600: Loss = -11106.486811354416
Iteration 1700: Loss = -11106.414082490635
Iteration 1800: Loss = -11106.357227910546
Iteration 1900: Loss = -11106.30012160873
Iteration 2000: Loss = -11106.260163640723
Iteration 2100: Loss = -11106.116439177991
Iteration 2200: Loss = -11101.267918074624
Iteration 2300: Loss = -11099.295436176173
Iteration 2400: Loss = -11096.694202233337
Iteration 2500: Loss = -11094.883799690164
Iteration 2600: Loss = -11094.545169536908
Iteration 2700: Loss = -11094.520539283083
Iteration 2800: Loss = -11094.512479952713
Iteration 2900: Loss = -11094.497035073562
Iteration 3000: Loss = -11094.489520539079
Iteration 3100: Loss = -11094.403537396873
Iteration 3200: Loss = -11094.38692399791
Iteration 3300: Loss = -11094.384898611072
Iteration 3400: Loss = -11094.385292032717
1
Iteration 3500: Loss = -11094.381679830209
Iteration 3600: Loss = -11094.379504408644
Iteration 3700: Loss = -11094.375640127448
Iteration 3800: Loss = -11094.35940105587
Iteration 3900: Loss = -11094.355297006474
Iteration 4000: Loss = -11094.352369637376
Iteration 4100: Loss = -11094.341940599985
Iteration 4200: Loss = -11094.166486705346
Iteration 4300: Loss = -11094.091661989218
Iteration 4400: Loss = -11094.090145007302
Iteration 4500: Loss = -11094.089427725476
Iteration 4600: Loss = -11094.088963979542
Iteration 4700: Loss = -11094.090702854908
1
Iteration 4800: Loss = -11094.0882713226
Iteration 4900: Loss = -11094.088029610637
Iteration 5000: Loss = -11094.104205198788
1
Iteration 5100: Loss = -11094.08748543272
Iteration 5200: Loss = -11094.08956669362
1
Iteration 5300: Loss = -11094.088432101782
2
Iteration 5400: Loss = -11094.086667364709
Iteration 5500: Loss = -11094.087762129144
1
Iteration 5600: Loss = -11094.08599403928
Iteration 5700: Loss = -11094.085695714202
Iteration 5800: Loss = -11094.096126415267
1
Iteration 5900: Loss = -11094.08385468957
Iteration 6000: Loss = -11094.08362341371
Iteration 6100: Loss = -11094.082971739988
Iteration 6200: Loss = -11094.083296548059
1
Iteration 6300: Loss = -11094.088492675544
2
Iteration 6400: Loss = -11094.082213500114
Iteration 6500: Loss = -11094.082114832272
Iteration 6600: Loss = -11094.08321315795
1
Iteration 6700: Loss = -11094.081981498777
Iteration 6800: Loss = -11094.082747363405
1
Iteration 6900: Loss = -11094.081841812413
Iteration 7000: Loss = -11094.081814757594
Iteration 7100: Loss = -11094.084819186584
1
Iteration 7200: Loss = -11094.085214877685
2
Iteration 7300: Loss = -11094.096157710965
3
Iteration 7400: Loss = -11094.081572704494
Iteration 7500: Loss = -11094.08169642741
1
Iteration 7600: Loss = -11094.101173883077
2
Iteration 7700: Loss = -11094.084663048525
3
Iteration 7800: Loss = -11094.09845115598
4
Iteration 7900: Loss = -11094.081325361334
Iteration 8000: Loss = -11094.081406683
Iteration 8100: Loss = -11094.082575225759
1
Iteration 8200: Loss = -11094.08125156277
Iteration 8300: Loss = -11094.081375525375
1
Iteration 8400: Loss = -11094.081413590098
2
Iteration 8500: Loss = -11094.081453851188
3
Iteration 8600: Loss = -11094.081177703016
Iteration 8700: Loss = -11094.083289356064
1
Iteration 8800: Loss = -11094.081085064776
Iteration 8900: Loss = -11094.081601351669
1
Iteration 9000: Loss = -11094.190426038758
2
Iteration 9100: Loss = -11094.081030085052
Iteration 9200: Loss = -11094.086890374392
1
Iteration 9300: Loss = -11094.097941534103
2
Iteration 9400: Loss = -11094.079839208996
Iteration 9500: Loss = -11094.083250397714
1
Iteration 9600: Loss = -11094.079638460384
Iteration 9700: Loss = -11094.093740314329
1
Iteration 9800: Loss = -11094.12284180767
2
Iteration 9900: Loss = -11094.081837557522
3
Iteration 10000: Loss = -11094.08677435196
4
Iteration 10100: Loss = -11094.091501206067
5
Iteration 10200: Loss = -11094.090971727312
6
Iteration 10300: Loss = -11094.08437787901
7
Iteration 10400: Loss = -11094.082048994189
8
Iteration 10500: Loss = -11094.134107509813
9
Iteration 10600: Loss = -11094.079578188517
Iteration 10700: Loss = -11094.079694993208
1
Iteration 10800: Loss = -11094.269903912307
2
Iteration 10900: Loss = -11094.079550968121
Iteration 11000: Loss = -11094.103644070488
1
Iteration 11100: Loss = -11094.078505903384
Iteration 11200: Loss = -11094.07878103519
1
Iteration 11300: Loss = -11094.091253787768
2
Iteration 11400: Loss = -11094.07800660065
Iteration 11500: Loss = -11094.081191242187
1
Iteration 11600: Loss = -11094.097963848431
2
Iteration 11700: Loss = -11094.07878065866
3
Iteration 11800: Loss = -11094.083270330982
4
Iteration 11900: Loss = -11094.077932502025
Iteration 12000: Loss = -11094.077369670938
Iteration 12100: Loss = -11094.077894435244
1
Iteration 12200: Loss = -11094.07939638499
2
Iteration 12300: Loss = -11094.077705233898
3
Iteration 12400: Loss = -11094.086896528252
4
Iteration 12500: Loss = -11094.24325069948
5
Iteration 12600: Loss = -11094.0799199609
6
Iteration 12700: Loss = -11094.077624866455
7
Iteration 12800: Loss = -11094.077401438275
Iteration 12900: Loss = -11094.078126299213
1
Iteration 13000: Loss = -11094.098960203326
2
Iteration 13100: Loss = -11094.0772193641
Iteration 13200: Loss = -11094.081105740297
1
Iteration 13300: Loss = -11094.078732418106
2
Iteration 13400: Loss = -11094.077932054937
3
Iteration 13500: Loss = -11094.081300350077
4
Iteration 13600: Loss = -11094.078315874618
5
Iteration 13700: Loss = -11094.1039083201
6
Iteration 13800: Loss = -11094.078715829939
7
Iteration 13900: Loss = -11094.07767103636
8
Iteration 14000: Loss = -11094.07946104484
9
Iteration 14100: Loss = -11094.079716195823
10
Iteration 14200: Loss = -11094.10857792214
11
Iteration 14300: Loss = -11094.113194356212
12
Iteration 14400: Loss = -11094.077152748083
Iteration 14500: Loss = -11094.078242529868
1
Iteration 14600: Loss = -11094.141765351686
2
Iteration 14700: Loss = -11094.077562632898
3
Iteration 14800: Loss = -11094.07821377373
4
Iteration 14900: Loss = -11094.21007424033
5
Iteration 15000: Loss = -11094.08559469825
6
Iteration 15100: Loss = -11094.077207794
Iteration 15200: Loss = -11094.077444773278
1
Iteration 15300: Loss = -11094.083353693195
2
Iteration 15400: Loss = -11094.07981612701
3
Iteration 15500: Loss = -11094.078090657984
4
Iteration 15600: Loss = -11094.083864525692
5
Iteration 15700: Loss = -11094.078055914302
6
Iteration 15800: Loss = -11094.077208014387
Iteration 15900: Loss = -11094.078041194363
1
Iteration 16000: Loss = -11094.322090683752
2
Iteration 16100: Loss = -11094.07715662841
Iteration 16200: Loss = -11094.08067700972
1
Iteration 16300: Loss = -11094.094504030081
2
Iteration 16400: Loss = -11094.08906235148
3
Iteration 16500: Loss = -11094.077208632696
Iteration 16600: Loss = -11094.078941764412
1
Iteration 16700: Loss = -11094.077957651989
2
Iteration 16800: Loss = -11094.077286101096
Iteration 16900: Loss = -11094.079934364801
1
Iteration 17000: Loss = -11094.08107443215
2
Iteration 17100: Loss = -11094.077206986703
Iteration 17200: Loss = -11094.081883119541
1
Iteration 17300: Loss = -11094.077663968914
2
Iteration 17400: Loss = -11094.077288555678
Iteration 17500: Loss = -11094.077227272635
Iteration 17600: Loss = -11094.0771503172
Iteration 17700: Loss = -11094.091148134676
1
Iteration 17800: Loss = -11094.122039116268
2
Iteration 17900: Loss = -11094.077136308626
Iteration 18000: Loss = -11094.077418569532
1
Iteration 18100: Loss = -11094.078354082068
2
Iteration 18200: Loss = -11094.077217249453
Iteration 18300: Loss = -11094.079281678069
1
Iteration 18400: Loss = -11094.07820927934
2
Iteration 18500: Loss = -11094.081378589724
3
Iteration 18600: Loss = -11094.083412362623
4
Iteration 18700: Loss = -11094.078840210745
5
Iteration 18800: Loss = -11094.077088703876
Iteration 18900: Loss = -11094.07793622407
1
Iteration 19000: Loss = -11094.11695909781
2
Iteration 19100: Loss = -11094.077133487688
Iteration 19200: Loss = -11094.112604144902
1
Iteration 19300: Loss = -11094.077289440183
2
Iteration 19400: Loss = -11094.113798326018
3
Iteration 19500: Loss = -11094.079043798913
4
Iteration 19600: Loss = -11094.086316595158
5
Iteration 19700: Loss = -11094.080056688468
6
Iteration 19800: Loss = -11094.084163167
7
Iteration 19900: Loss = -11094.080632713829
8
pi: tensor([[0.6264, 0.3736],
        [0.2587, 0.7413]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8288, 0.1712], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2011, 0.1001],
         [0.6992, 0.2666]],

        [[0.5083, 0.1042],
         [0.5688, 0.6499]],

        [[0.7176, 0.0975],
         [0.7126, 0.5673]],

        [[0.5273, 0.0920],
         [0.5538, 0.6167]],

        [[0.6967, 0.1000],
         [0.7235, 0.5809]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 26
Adjusted Rand Index: 0.21400363955978657
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 2
tensor([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721016799725718
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9206925302859384
Global Adjusted Rand Index: 0.4083716387263021
Average Adjusted Rand Index: 0.7199440223119569
11113.180190766367
[0.4083716387263021, 0.4083716387263021] [0.7199440223119569, 0.7199440223119569] [11094.082919951301, 11094.077932481909]
-------------------------------------
This iteration is 97
True Objective function: Loss = -10949.636779073497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22031.2976986296
Iteration 100: Loss = -11020.176378886466
Iteration 200: Loss = -11018.91311115287
Iteration 300: Loss = -11018.559865686968
Iteration 400: Loss = -11018.421339015214
Iteration 500: Loss = -11018.335613013625
Iteration 600: Loss = -11018.260567156492
Iteration 700: Loss = -11018.17831193977
Iteration 800: Loss = -11018.084613187117
Iteration 900: Loss = -11017.99503627204
Iteration 1000: Loss = -11017.921934065758
Iteration 1100: Loss = -11017.856509287472
Iteration 1200: Loss = -11017.774022638887
Iteration 1300: Loss = -11017.668217532228
Iteration 1400: Loss = -11017.611306845361
Iteration 1500: Loss = -11017.563673063405
Iteration 1600: Loss = -11017.514595590232
Iteration 1700: Loss = -11017.461822462981
Iteration 1800: Loss = -11017.404277138165
Iteration 1900: Loss = -11017.341509995105
Iteration 2000: Loss = -11017.272925592182
Iteration 2100: Loss = -11017.19842497475
Iteration 2200: Loss = -11017.119155117049
Iteration 2300: Loss = -11017.041943434788
Iteration 2400: Loss = -11016.97635214439
Iteration 2500: Loss = -11016.920714594533
Iteration 2600: Loss = -11016.862897392608
Iteration 2700: Loss = -11016.805175063722
Iteration 2800: Loss = -11016.743875036003
Iteration 2900: Loss = -11016.674616401733
Iteration 3000: Loss = -11016.58345661754
Iteration 3100: Loss = -11016.424206591495
Iteration 3200: Loss = -11016.114203293415
Iteration 3300: Loss = -11015.65877305046
Iteration 3400: Loss = -11015.063303223136
Iteration 3500: Loss = -10902.6745216852
Iteration 3600: Loss = -10902.314712948753
Iteration 3700: Loss = -10902.237801168503
Iteration 3800: Loss = -10902.161082763152
Iteration 3900: Loss = -10902.142409738057
Iteration 4000: Loss = -10902.122714549505
Iteration 4100: Loss = -10902.110706564155
Iteration 4200: Loss = -10902.096629891614
Iteration 4300: Loss = -10902.076068743032
Iteration 4400: Loss = -10902.069575980679
Iteration 4500: Loss = -10902.065874181691
Iteration 4600: Loss = -10902.061920144992
Iteration 4700: Loss = -10902.052623668043
Iteration 4800: Loss = -10902.044518289822
Iteration 4900: Loss = -10902.03367727588
Iteration 5000: Loss = -10902.029267139547
Iteration 5100: Loss = -10902.027852270832
Iteration 5200: Loss = -10902.029995770488
1
Iteration 5300: Loss = -10902.026806191841
Iteration 5400: Loss = -10902.027331816478
1
Iteration 5500: Loss = -10902.029175817997
2
Iteration 5600: Loss = -10902.024953976657
Iteration 5700: Loss = -10902.024089122593
Iteration 5800: Loss = -10902.025461619794
1
Iteration 5900: Loss = -10902.02273865842
Iteration 6000: Loss = -10902.02224980579
Iteration 6100: Loss = -10902.021357107757
Iteration 6200: Loss = -10902.021334462457
Iteration 6300: Loss = -10902.018368100864
Iteration 6400: Loss = -10902.03073154549
1
Iteration 6500: Loss = -10902.017736247051
Iteration 6600: Loss = -10902.017542781441
Iteration 6700: Loss = -10902.01738598403
Iteration 6800: Loss = -10902.01815418356
1
Iteration 6900: Loss = -10902.022990199672
2
Iteration 7000: Loss = -10902.017451087928
Iteration 7100: Loss = -10902.016811676822
Iteration 7200: Loss = -10902.016225668607
Iteration 7300: Loss = -10902.024302263371
1
Iteration 7400: Loss = -10902.041561209913
2
Iteration 7500: Loss = -10902.014781828802
Iteration 7600: Loss = -10902.014584017423
Iteration 7700: Loss = -10902.024047805739
1
Iteration 7800: Loss = -10902.014335575957
Iteration 7900: Loss = -10902.01726387614
1
Iteration 8000: Loss = -10902.014102005858
Iteration 8100: Loss = -10902.050034536767
1
Iteration 8200: Loss = -10902.013778385744
Iteration 8300: Loss = -10902.013679199032
Iteration 8400: Loss = -10902.013635235608
Iteration 8500: Loss = -10902.01355785753
Iteration 8600: Loss = -10902.029180495794
1
Iteration 8700: Loss = -10902.013407882132
Iteration 8800: Loss = -10902.01326485475
Iteration 8900: Loss = -10902.020017515868
1
Iteration 9000: Loss = -10902.013155070938
Iteration 9100: Loss = -10902.013099570519
Iteration 9200: Loss = -10902.019201070234
1
Iteration 9300: Loss = -10902.013010062492
Iteration 9400: Loss = -10902.012973925892
Iteration 9500: Loss = -10902.112364225684
1
Iteration 9600: Loss = -10902.012847948163
Iteration 9700: Loss = -10902.048715473255
1
Iteration 9800: Loss = -10902.012696382433
Iteration 9900: Loss = -10902.012605243712
Iteration 10000: Loss = -10902.012550258632
Iteration 10100: Loss = -10902.01221811086
Iteration 10200: Loss = -10902.012001138488
Iteration 10300: Loss = -10902.011912000087
Iteration 10400: Loss = -10902.014214908806
1
Iteration 10500: Loss = -10902.012014766806
2
Iteration 10600: Loss = -10902.081159343103
3
Iteration 10700: Loss = -10902.01185684849
Iteration 10800: Loss = -10902.014138555134
1
Iteration 10900: Loss = -10902.011820064676
Iteration 11000: Loss = -10902.012404418927
1
Iteration 11100: Loss = -10902.012589430355
2
Iteration 11200: Loss = -10902.011875686374
Iteration 11300: Loss = -10902.012131046546
1
Iteration 11400: Loss = -10902.017192541865
2
Iteration 11500: Loss = -10902.011918724858
Iteration 11600: Loss = -10902.01882198589
1
Iteration 11700: Loss = -10902.01587135214
2
Iteration 11800: Loss = -10902.01242939523
3
Iteration 11900: Loss = -10902.011824847992
Iteration 12000: Loss = -10902.020359806542
1
Iteration 12100: Loss = -10902.01180685515
Iteration 12200: Loss = -10902.012978928105
1
Iteration 12300: Loss = -10902.011832763077
Iteration 12400: Loss = -10902.014978548446
1
Iteration 12500: Loss = -10902.011816861846
Iteration 12600: Loss = -10902.013950341634
1
Iteration 12700: Loss = -10902.015477373352
2
Iteration 12800: Loss = -10902.01251602597
3
Iteration 12900: Loss = -10902.018536348158
4
Iteration 13000: Loss = -10902.01230398587
5
Iteration 13100: Loss = -10902.01234243674
6
Iteration 13200: Loss = -10902.014369080045
7
Iteration 13300: Loss = -10902.015490738771
8
Iteration 13400: Loss = -10902.011854783752
Iteration 13500: Loss = -10902.012021103777
1
Iteration 13600: Loss = -10902.016866799553
2
Iteration 13700: Loss = -10902.014465514922
3
Iteration 13800: Loss = -10902.011987677284
4
Iteration 13900: Loss = -10902.02578216889
5
Iteration 14000: Loss = -10902.03172787196
6
Iteration 14100: Loss = -10902.016475224154
7
Iteration 14200: Loss = -10902.024891304309
8
Iteration 14300: Loss = -10902.012010806682
9
Iteration 14400: Loss = -10902.014259502757
10
Iteration 14500: Loss = -10902.044428913117
11
Iteration 14600: Loss = -10902.012764158575
12
Iteration 14700: Loss = -10902.01179008563
Iteration 14800: Loss = -10902.012379211976
1
Iteration 14900: Loss = -10902.170245150759
2
Iteration 15000: Loss = -10902.01178729304
Iteration 15100: Loss = -10902.013828475463
1
Iteration 15200: Loss = -10902.014329246122
2
Iteration 15300: Loss = -10902.030617553468
3
Iteration 15400: Loss = -10902.013915318103
4
Iteration 15500: Loss = -10902.020050963192
5
Iteration 15600: Loss = -10902.06877367336
6
Iteration 15700: Loss = -10902.01177832327
Iteration 15800: Loss = -10902.012121498596
1
Iteration 15900: Loss = -10902.011786208435
Iteration 16000: Loss = -10902.01183570313
Iteration 16100: Loss = -10902.011780968345
Iteration 16200: Loss = -10902.011877511219
Iteration 16300: Loss = -10902.01178693968
Iteration 16400: Loss = -10902.012199237937
1
Iteration 16500: Loss = -10902.011770930318
Iteration 16600: Loss = -10902.012015332875
1
Iteration 16700: Loss = -10902.01209826355
2
Iteration 16800: Loss = -10902.012456389893
3
Iteration 16900: Loss = -10902.014795479485
4
Iteration 17000: Loss = -10902.01588467723
5
Iteration 17100: Loss = -10902.051922369816
6
Iteration 17200: Loss = -10902.011810998469
Iteration 17300: Loss = -10902.025407314994
1
Iteration 17400: Loss = -10902.011783013808
Iteration 17500: Loss = -10902.075581062001
1
Iteration 17600: Loss = -10902.011774638195
Iteration 17700: Loss = -10902.011778213231
Iteration 17800: Loss = -10902.012441743873
1
Iteration 17900: Loss = -10902.011940918903
2
Iteration 18000: Loss = -10902.01406078856
3
Iteration 18100: Loss = -10902.012039270952
4
Iteration 18200: Loss = -10902.011777325677
Iteration 18300: Loss = -10902.039150001337
1
Iteration 18400: Loss = -10902.01176013459
Iteration 18500: Loss = -10902.14251697726
1
Iteration 18600: Loss = -10902.011750627928
Iteration 18700: Loss = -10902.012346968822
1
Iteration 18800: Loss = -10902.026114188378
2
Iteration 18900: Loss = -10902.01310561292
3
Iteration 19000: Loss = -10902.012063128874
4
Iteration 19100: Loss = -10902.012861709783
5
Iteration 19200: Loss = -10902.011889560381
6
Iteration 19300: Loss = -10902.012807654202
7
Iteration 19400: Loss = -10902.046223104073
8
Iteration 19500: Loss = -10902.173355671986
9
Iteration 19600: Loss = -10902.01181164086
Iteration 19700: Loss = -10902.011833833118
Iteration 19800: Loss = -10902.013099434416
1
Iteration 19900: Loss = -10902.045614339657
2
pi: tensor([[0.7750, 0.2250],
        [0.2278, 0.7722]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4700, 0.5300], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1990, 0.0993],
         [0.5214, 0.2457]],

        [[0.6942, 0.0985],
         [0.5654, 0.5470]],

        [[0.6355, 0.1024],
         [0.5680, 0.5819]],

        [[0.6562, 0.1068],
         [0.7258, 0.5624]],

        [[0.6856, 0.1062],
         [0.5984, 0.5241]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 92
Adjusted Rand Index: 0.7026374762971901
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6690831299615492
Global Adjusted Rand Index: 0.8168497905351527
Average Adjusted Rand Index: 0.8191502239247128
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20703.833762169877
Iteration 100: Loss = -11018.980515759453
Iteration 200: Loss = -11018.390517773185
Iteration 300: Loss = -11018.21137043674
Iteration 400: Loss = -11018.088844210184
Iteration 500: Loss = -11017.97148123622
Iteration 600: Loss = -11017.860487274302
Iteration 700: Loss = -11017.769255958148
Iteration 800: Loss = -11017.700385884564
Iteration 900: Loss = -11017.642763520038
Iteration 1000: Loss = -11017.58365372392
Iteration 1100: Loss = -11017.505561844737
Iteration 1200: Loss = -11017.412125888706
Iteration 1300: Loss = -11017.36217006001
Iteration 1400: Loss = -11017.323061271833
Iteration 1500: Loss = -11017.282905680027
Iteration 1600: Loss = -11017.237042848608
Iteration 1700: Loss = -11017.179474550909
Iteration 1800: Loss = -11017.10627534385
Iteration 1900: Loss = -11017.028737520443
Iteration 2000: Loss = -11016.965529140902
Iteration 2100: Loss = -11016.91161421348
Iteration 2200: Loss = -11016.858925015253
Iteration 2300: Loss = -11016.807235674522
Iteration 2400: Loss = -11016.75491507424
Iteration 2500: Loss = -11016.701426565382
Iteration 2600: Loss = -11016.637101975839
Iteration 2700: Loss = -11016.54001949806
Iteration 2800: Loss = -11016.359790483886
Iteration 2900: Loss = -11016.03495859841
Iteration 3000: Loss = -11015.607477470905
Iteration 3100: Loss = -11014.984675154577
Iteration 3200: Loss = -10902.703470134164
Iteration 3300: Loss = -10902.327790065696
Iteration 3400: Loss = -10902.239503017876
Iteration 3500: Loss = -10902.154797911406
Iteration 3600: Loss = -10902.138967227394
Iteration 3700: Loss = -10902.118827380764
Iteration 3800: Loss = -10902.10928860479
Iteration 3900: Loss = -10902.097109476988
Iteration 4000: Loss = -10902.079708717818
Iteration 4100: Loss = -10902.067729921195
Iteration 4200: Loss = -10902.060245391063
Iteration 4300: Loss = -10902.055467547258
Iteration 4400: Loss = -10902.038452183977
Iteration 4500: Loss = -10902.032360798043
Iteration 4600: Loss = -10902.048775806252
1
Iteration 4700: Loss = -10902.027259606919
Iteration 4800: Loss = -10902.026431001967
Iteration 4900: Loss = -10902.025738556948
Iteration 5000: Loss = -10902.02556326925
Iteration 5100: Loss = -10902.024720760855
Iteration 5200: Loss = -10902.024234007638
Iteration 5300: Loss = -10902.02379075797
Iteration 5400: Loss = -10902.023934443938
1
Iteration 5500: Loss = -10902.022816948
Iteration 5600: Loss = -10902.022307671496
Iteration 5700: Loss = -10902.037236477474
1
Iteration 5800: Loss = -10902.021308184369
Iteration 5900: Loss = -10902.021281214682
Iteration 6000: Loss = -10902.020283448397
Iteration 6100: Loss = -10902.022795924655
1
Iteration 6200: Loss = -10902.019939804046
Iteration 6300: Loss = -10902.02257993963
1
Iteration 6400: Loss = -10902.018168146456
Iteration 6500: Loss = -10902.017130055616
Iteration 6600: Loss = -10902.016714714111
Iteration 6700: Loss = -10902.026452368646
1
Iteration 6800: Loss = -10902.016608589485
Iteration 6900: Loss = -10902.016694218044
Iteration 7000: Loss = -10902.016218284534
Iteration 7100: Loss = -10902.016183513526
Iteration 7200: Loss = -10902.01604483964
Iteration 7300: Loss = -10902.016926691427
1
Iteration 7400: Loss = -10902.023904448552
2
Iteration 7500: Loss = -10902.01576509187
Iteration 7600: Loss = -10902.01572691679
Iteration 7700: Loss = -10902.015448029835
Iteration 7800: Loss = -10902.01529732205
Iteration 7900: Loss = -10902.015047215158
Iteration 8000: Loss = -10902.015478469666
1
Iteration 8100: Loss = -10902.013916288255
Iteration 8200: Loss = -10902.013701526495
Iteration 8300: Loss = -10902.013855853113
1
Iteration 8400: Loss = -10902.01336504005
Iteration 8500: Loss = -10902.013383613148
Iteration 8600: Loss = -10902.015860196208
1
Iteration 8700: Loss = -10902.013158473812
Iteration 8800: Loss = -10902.0152533569
1
Iteration 8900: Loss = -10902.013651811856
2
Iteration 9000: Loss = -10902.012936746798
Iteration 9100: Loss = -10902.042308849
1
Iteration 9200: Loss = -10902.01289915524
Iteration 9300: Loss = -10902.01388687062
1
Iteration 9400: Loss = -10902.012821271857
Iteration 9500: Loss = -10902.028114345067
1
Iteration 9600: Loss = -10902.012796106736
Iteration 9700: Loss = -10902.012792907728
Iteration 9800: Loss = -10902.014499649746
1
Iteration 9900: Loss = -10902.012782459604
Iteration 10000: Loss = -10902.012739699378
Iteration 10100: Loss = -10902.01270872757
Iteration 10200: Loss = -10902.01258890001
Iteration 10300: Loss = -10902.012696749114
1
Iteration 10400: Loss = -10902.012534182717
Iteration 10500: Loss = -10902.012476473032
Iteration 10600: Loss = -10902.102032739378
1
Iteration 10700: Loss = -10902.012440621644
Iteration 10800: Loss = -10902.012667260118
1
Iteration 10900: Loss = -10902.012550404568
2
Iteration 11000: Loss = -10902.013201180866
3
Iteration 11100: Loss = -10902.042749041948
4
Iteration 11200: Loss = -10902.011873235775
Iteration 11300: Loss = -10902.011945045564
Iteration 11400: Loss = -10902.0164837565
1
Iteration 11500: Loss = -10902.011809295413
Iteration 11600: Loss = -10902.012017912179
1
Iteration 11700: Loss = -10902.011818269546
Iteration 11800: Loss = -10902.012004852917
1
Iteration 11900: Loss = -10902.012223531714
2
Iteration 12000: Loss = -10902.011821707736
Iteration 12100: Loss = -10902.026888180488
1
Iteration 12200: Loss = -10902.011800406688
Iteration 12300: Loss = -10902.017043342315
1
Iteration 12400: Loss = -10902.011766081892
Iteration 12500: Loss = -10902.011766871527
Iteration 12600: Loss = -10902.011905586962
1
Iteration 12700: Loss = -10902.011805219085
Iteration 12800: Loss = -10902.081057514923
1
Iteration 12900: Loss = -10902.011754455028
Iteration 13000: Loss = -10902.023543688641
1
Iteration 13100: Loss = -10902.047589052148
2
Iteration 13200: Loss = -10902.022655747573
3
Iteration 13300: Loss = -10902.083219909857
4
Iteration 13400: Loss = -10902.052867441402
5
Iteration 13500: Loss = -10902.07873011241
6
Iteration 13600: Loss = -10902.027390452016
7
Iteration 13700: Loss = -10902.025257095289
8
Iteration 13800: Loss = -10902.013471921297
9
Iteration 13900: Loss = -10902.011794276521
Iteration 14000: Loss = -10902.013896860883
1
Iteration 14100: Loss = -10902.011787044055
Iteration 14200: Loss = -10902.012254131847
1
Iteration 14300: Loss = -10902.014257129355
2
Iteration 14400: Loss = -10902.073802588568
3
Iteration 14500: Loss = -10902.016621100753
4
Iteration 14600: Loss = -10902.043341588022
5
Iteration 14700: Loss = -10902.094165430599
6
Iteration 14800: Loss = -10902.022361606205
7
Iteration 14900: Loss = -10902.011796880635
Iteration 15000: Loss = -10902.011994675044
1
Iteration 15100: Loss = -10902.012800842984
2
Iteration 15200: Loss = -10902.035296486576
3
Iteration 15300: Loss = -10902.14010594419
4
Iteration 15400: Loss = -10902.012516505321
5
Iteration 15500: Loss = -10902.011809431173
Iteration 15600: Loss = -10902.01534606441
1
Iteration 15700: Loss = -10902.014242379673
2
Iteration 15800: Loss = -10902.013038940884
3
Iteration 15900: Loss = -10902.044488483745
4
Iteration 16000: Loss = -10902.022130017984
5
Iteration 16100: Loss = -10902.020024778905
6
Iteration 16200: Loss = -10902.012293472377
7
Iteration 16300: Loss = -10902.011864615459
Iteration 16400: Loss = -10902.012246768096
1
Iteration 16500: Loss = -10902.018019559366
2
Iteration 16600: Loss = -10902.113212778217
3
Iteration 16700: Loss = -10902.167645522259
4
Iteration 16800: Loss = -10902.011784882698
Iteration 16900: Loss = -10902.011939801047
1
Iteration 17000: Loss = -10902.014516476196
2
Iteration 17100: Loss = -10902.01212884567
3
Iteration 17200: Loss = -10902.011800114979
Iteration 17300: Loss = -10902.012540080477
1
Iteration 17400: Loss = -10902.056020396947
2
Iteration 17500: Loss = -10902.018286538216
3
Iteration 17600: Loss = -10902.018705121449
4
Iteration 17700: Loss = -10902.084017426172
5
Iteration 17800: Loss = -10902.029328646337
6
Iteration 17900: Loss = -10902.024613290961
7
Iteration 18000: Loss = -10902.034350859893
8
Iteration 18100: Loss = -10902.016112385838
9
Iteration 18200: Loss = -10902.032252129171
10
Iteration 18300: Loss = -10902.012010690669
11
Iteration 18400: Loss = -10902.01184274769
Iteration 18500: Loss = -10902.061417327275
1
Iteration 18600: Loss = -10902.011775705792
Iteration 18700: Loss = -10902.013139076727
1
Iteration 18800: Loss = -10902.013186877382
2
Iteration 18900: Loss = -10902.014929030753
3
Iteration 19000: Loss = -10902.076631372965
4
Iteration 19100: Loss = -10902.064290341146
5
Iteration 19200: Loss = -10902.011794968514
Iteration 19300: Loss = -10902.01189075176
Iteration 19400: Loss = -10902.086263032832
1
Iteration 19500: Loss = -10902.011765516014
Iteration 19600: Loss = -10902.053406178078
1
Iteration 19700: Loss = -10902.011863482401
Iteration 19800: Loss = -10902.011846118472
Iteration 19900: Loss = -10902.014570793166
1
pi: tensor([[0.7773, 0.2227],
        [0.2273, 0.7727]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4711, 0.5289], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1984, 0.0997],
         [0.5777, 0.2461]],

        [[0.6649, 0.0988],
         [0.7036, 0.6264]],

        [[0.5150, 0.1027],
         [0.6619, 0.7178]],

        [[0.5786, 0.1069],
         [0.5647, 0.5460]],

        [[0.6767, 0.1071],
         [0.5153, 0.5731]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 92
Adjusted Rand Index: 0.7026374762971901
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6690831299615492
Global Adjusted Rand Index: 0.8168497905351527
Average Adjusted Rand Index: 0.8191502239247128
10949.636779073497
[0.8168497905351527, 0.8168497905351527] [0.8191502239247128, 0.8191502239247128] [10902.039518729312, 10902.013264822801]
-------------------------------------
This iteration is 98
True Objective function: Loss = -10935.82964587653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23537.793574887615
Iteration 100: Loss = -11008.206507491324
Iteration 200: Loss = -11007.075433533839
Iteration 300: Loss = -11006.426363047374
Iteration 400: Loss = -11005.483973878288
Iteration 500: Loss = -11004.93974234278
Iteration 600: Loss = -11004.596751034522
Iteration 700: Loss = -11004.286536670057
Iteration 800: Loss = -11003.946865722784
Iteration 900: Loss = -11003.619505055938
Iteration 1000: Loss = -11003.264282614302
Iteration 1100: Loss = -11002.802238710725
Iteration 1200: Loss = -11001.993648628439
Iteration 1300: Loss = -10998.827211455164
Iteration 1400: Loss = -10995.78306608492
Iteration 1500: Loss = -10957.953115691791
Iteration 1600: Loss = -10939.8737550803
Iteration 1700: Loss = -10939.491160783631
Iteration 1800: Loss = -10939.2992116712
Iteration 1900: Loss = -10939.244752582168
Iteration 2000: Loss = -10939.203731844462
Iteration 2100: Loss = -10939.149140889815
Iteration 2200: Loss = -10939.124648389225
Iteration 2300: Loss = -10939.106102709591
Iteration 2400: Loss = -10939.086479680185
Iteration 2500: Loss = -10939.061910322855
Iteration 2600: Loss = -10939.033657127533
Iteration 2700: Loss = -10938.997798649816
Iteration 2800: Loss = -10938.86094140489
Iteration 2900: Loss = -10938.359348268805
Iteration 3000: Loss = -10937.097560286431
Iteration 3100: Loss = -10937.022881809286
Iteration 3200: Loss = -10936.975145721595
Iteration 3300: Loss = -10935.8057243619
Iteration 3400: Loss = -10933.119771667989
Iteration 3500: Loss = -10932.850869645581
Iteration 3600: Loss = -10932.800616067521
Iteration 3700: Loss = -10932.649667681995
Iteration 3800: Loss = -10932.108220495575
Iteration 3900: Loss = -10932.105048151865
Iteration 4000: Loss = -10932.102124627685
Iteration 4100: Loss = -10932.096866439319
Iteration 4200: Loss = -10932.05106889391
Iteration 4300: Loss = -10932.03489338926
Iteration 4400: Loss = -10931.828294807854
Iteration 4500: Loss = -10931.106467763133
Iteration 4600: Loss = -10930.851616649052
Iteration 4700: Loss = -10926.214947980978
Iteration 4800: Loss = -10926.174576064564
Iteration 4900: Loss = -10926.154450511925
Iteration 5000: Loss = -10926.147227486435
Iteration 5100: Loss = -10926.14663774976
Iteration 5200: Loss = -10926.146495022362
Iteration 5300: Loss = -10926.145886762035
Iteration 5400: Loss = -10926.145484268358
Iteration 5500: Loss = -10926.145806528863
1
Iteration 5600: Loss = -10926.145002410001
Iteration 5700: Loss = -10926.144807281698
Iteration 5800: Loss = -10926.144626458294
Iteration 5900: Loss = -10926.144681669104
Iteration 6000: Loss = -10926.14437762006
Iteration 6100: Loss = -10926.144253684955
Iteration 6200: Loss = -10926.144138213254
Iteration 6300: Loss = -10926.143899667362
Iteration 6400: Loss = -10926.144514883435
1
Iteration 6500: Loss = -10926.139447359148
Iteration 6600: Loss = -10926.139425565483
Iteration 6700: Loss = -10926.156975741611
1
Iteration 6800: Loss = -10926.139308004096
Iteration 6900: Loss = -10926.139283280861
Iteration 7000: Loss = -10926.142554538008
1
Iteration 7100: Loss = -10926.139083975188
Iteration 7200: Loss = -10926.13894343297
Iteration 7300: Loss = -10926.13891195332
Iteration 7400: Loss = -10926.157788906776
1
Iteration 7500: Loss = -10926.141370839732
2
Iteration 7600: Loss = -10926.141241292076
3
Iteration 7700: Loss = -10926.17665389516
4
Iteration 7800: Loss = -10926.138545134165
Iteration 7900: Loss = -10926.138536113182
Iteration 8000: Loss = -10926.138497798447
Iteration 8100: Loss = -10926.138480131445
Iteration 8200: Loss = -10926.140577307133
1
Iteration 8300: Loss = -10926.138473774174
Iteration 8400: Loss = -10926.1384299179
Iteration 8500: Loss = -10926.153332980839
1
Iteration 8600: Loss = -10926.13830690957
Iteration 8700: Loss = -10926.13826102666
Iteration 8800: Loss = -10926.13978888111
1
Iteration 8900: Loss = -10926.138180368116
Iteration 9000: Loss = -10926.138164605527
Iteration 9100: Loss = -10926.149557946841
1
Iteration 9200: Loss = -10926.138179678957
Iteration 9300: Loss = -10926.174996586422
1
Iteration 9400: Loss = -10926.15219954948
2
Iteration 9500: Loss = -10926.147998495226
3
Iteration 9600: Loss = -10926.14369673568
4
Iteration 9700: Loss = -10926.152802796078
5
Iteration 9800: Loss = -10926.235749294503
6
Iteration 9900: Loss = -10926.13805128901
Iteration 10000: Loss = -10926.138333967443
1
Iteration 10100: Loss = -10926.275704012816
2
Iteration 10200: Loss = -10926.137480266474
Iteration 10300: Loss = -10926.140841045488
1
Iteration 10400: Loss = -10926.133816201014
Iteration 10500: Loss = -10926.134720190299
1
Iteration 10600: Loss = -10926.134214084315
2
Iteration 10700: Loss = -10926.133814662244
Iteration 10800: Loss = -10926.186974756338
1
Iteration 10900: Loss = -10926.133769302707
Iteration 11000: Loss = -10926.15870266155
1
Iteration 11100: Loss = -10926.133765029037
Iteration 11200: Loss = -10926.141367152357
1
Iteration 11300: Loss = -10926.149473800013
2
Iteration 11400: Loss = -10926.134007003193
3
Iteration 11500: Loss = -10926.133780255304
Iteration 11600: Loss = -10926.154429276914
1
Iteration 11700: Loss = -10926.133784222066
Iteration 11800: Loss = -10926.135638760383
1
Iteration 11900: Loss = -10926.135082139206
2
Iteration 12000: Loss = -10926.135189328284
3
Iteration 12100: Loss = -10926.162157194673
4
Iteration 12200: Loss = -10926.13685360363
5
Iteration 12300: Loss = -10926.134912261121
6
Iteration 12400: Loss = -10926.134058790736
7
Iteration 12500: Loss = -10926.139685465943
8
Iteration 12600: Loss = -10926.137548684736
9
Iteration 12700: Loss = -10926.140245917484
10
Iteration 12800: Loss = -10926.141911291626
11
Iteration 12900: Loss = -10926.145083876412
12
Iteration 13000: Loss = -10926.133802739425
Iteration 13100: Loss = -10926.133560044775
Iteration 13200: Loss = -10926.25374305785
1
Iteration 13300: Loss = -10926.135767753443
2
Iteration 13400: Loss = -10926.166935325116
3
Iteration 13500: Loss = -10926.14526551474
4
Iteration 13600: Loss = -10926.284679672292
5
Iteration 13700: Loss = -10926.133748679702
6
Iteration 13800: Loss = -10926.13363467637
Iteration 13900: Loss = -10926.133784818645
1
Iteration 14000: Loss = -10926.133923874713
2
Iteration 14100: Loss = -10926.134924817236
3
Iteration 14200: Loss = -10926.185628068051
4
Iteration 14300: Loss = -10926.141052119541
5
Iteration 14400: Loss = -10926.137773037697
6
Iteration 14500: Loss = -10926.13669395039
7
Iteration 14600: Loss = -10926.134203841279
8
Iteration 14700: Loss = -10926.134064960022
9
Iteration 14800: Loss = -10926.134643944417
10
Iteration 14900: Loss = -10926.166449546452
11
Iteration 15000: Loss = -10926.133521138914
Iteration 15100: Loss = -10926.13692107313
1
Iteration 15200: Loss = -10926.155821145167
2
Iteration 15300: Loss = -10926.133649102469
3
Iteration 15400: Loss = -10926.133669831732
4
Iteration 15500: Loss = -10926.144034126572
5
Iteration 15600: Loss = -10926.133577100958
Iteration 15700: Loss = -10926.133696571807
1
Iteration 15800: Loss = -10926.15229106864
2
Iteration 15900: Loss = -10926.138854176957
3
Iteration 16000: Loss = -10926.136859477705
4
Iteration 16100: Loss = -10926.133822673837
5
Iteration 16200: Loss = -10926.162552553576
6
Iteration 16300: Loss = -10926.25732536098
7
Iteration 16400: Loss = -10926.135888563202
8
Iteration 16500: Loss = -10926.133499216758
Iteration 16600: Loss = -10926.133865204909
1
Iteration 16700: Loss = -10926.183156878591
2
Iteration 16800: Loss = -10926.133721542676
3
Iteration 16900: Loss = -10926.133525452948
Iteration 17000: Loss = -10926.181983405979
1
Iteration 17100: Loss = -10926.148622697654
2
Iteration 17200: Loss = -10926.133541521605
Iteration 17300: Loss = -10926.133649911824
1
Iteration 17400: Loss = -10926.13846031262
2
Iteration 17500: Loss = -10926.201214634826
3
Iteration 17600: Loss = -10926.134253191112
4
Iteration 17700: Loss = -10926.148487493963
5
Iteration 17800: Loss = -10926.136423282685
6
Iteration 17900: Loss = -10926.152578373616
7
Iteration 18000: Loss = -10926.210773701941
8
Iteration 18100: Loss = -10926.134708635578
9
Iteration 18200: Loss = -10926.14919971171
10
Iteration 18300: Loss = -10926.269300918328
11
Iteration 18400: Loss = -10926.135884030511
12
Iteration 18500: Loss = -10926.133972953854
13
Iteration 18600: Loss = -10926.134501660854
14
Iteration 18700: Loss = -10926.180908611046
15
Stopping early at iteration 18700 due to no improvement.
pi: tensor([[0.7425, 0.2575],
        [0.3392, 0.6608]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2895, 0.7105], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2413, 0.1002],
         [0.7273, 0.2015]],

        [[0.6047, 0.0947],
         [0.6055, 0.6818]],

        [[0.6060, 0.1104],
         [0.6971, 0.6378]],

        [[0.7057, 0.1020],
         [0.5558, 0.5932]],

        [[0.6179, 0.1029],
         [0.5880, 0.7251]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 67
Adjusted Rand Index: 0.10818078479145701
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080650161849864
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691181331636993
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.4397741542240412
Average Adjusted Rand Index: 0.6932324747105778
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22516.868435180862
Iteration 100: Loss = -11007.35330249709
Iteration 200: Loss = -11006.590643695308
Iteration 300: Loss = -11005.731700446251
Iteration 400: Loss = -11004.528541361391
Iteration 500: Loss = -11003.38865351054
Iteration 600: Loss = -11002.012580068938
Iteration 700: Loss = -10998.607244005623
Iteration 800: Loss = -10990.143429010312
Iteration 900: Loss = -10939.567240193579
Iteration 1000: Loss = -10939.182834823736
Iteration 1100: Loss = -10938.595513895798
Iteration 1200: Loss = -10932.463731461716
Iteration 1300: Loss = -10895.541620432788
Iteration 1400: Loss = -10891.523761874209
Iteration 1500: Loss = -10891.506411257116
Iteration 1600: Loss = -10891.497871833099
Iteration 1700: Loss = -10891.494246817667
Iteration 1800: Loss = -10891.491088634819
Iteration 1900: Loss = -10891.486580324265
Iteration 2000: Loss = -10891.478594513324
Iteration 2100: Loss = -10891.473276133122
Iteration 2200: Loss = -10891.472553524778
Iteration 2300: Loss = -10891.472046369005
Iteration 2400: Loss = -10891.471607671165
Iteration 2500: Loss = -10891.471275780257
Iteration 2600: Loss = -10891.470882648413
Iteration 2700: Loss = -10891.470616368495
Iteration 2800: Loss = -10891.470422028973
Iteration 2900: Loss = -10891.470190237209
Iteration 3000: Loss = -10891.470029438768
Iteration 3100: Loss = -10891.469883516718
Iteration 3200: Loss = -10891.469722935797
Iteration 3300: Loss = -10891.469869575034
1
Iteration 3400: Loss = -10891.476238918356
2
Iteration 3500: Loss = -10891.469439001401
Iteration 3600: Loss = -10891.469269353593
Iteration 3700: Loss = -10891.470265433003
1
Iteration 3800: Loss = -10891.469088857828
Iteration 3900: Loss = -10891.469248587957
1
Iteration 4000: Loss = -10891.468843300927
Iteration 4100: Loss = -10891.46976674831
1
Iteration 4200: Loss = -10891.468597642575
Iteration 4300: Loss = -10891.468620773863
Iteration 4400: Loss = -10891.473520632973
1
Iteration 4500: Loss = -10891.46946933277
2
Iteration 4600: Loss = -10891.470502209857
3
Iteration 4700: Loss = -10891.468239510481
Iteration 4800: Loss = -10891.46862955904
1
Iteration 4900: Loss = -10891.468304165288
Iteration 5000: Loss = -10891.468174553484
Iteration 5100: Loss = -10891.468114939487
Iteration 5200: Loss = -10891.469537117135
1
Iteration 5300: Loss = -10891.468151486022
Iteration 5400: Loss = -10891.469856725735
1
Iteration 5500: Loss = -10891.468005399372
Iteration 5600: Loss = -10891.470904669091
1
Iteration 5700: Loss = -10891.467754447516
Iteration 5800: Loss = -10891.46712088001
Iteration 5900: Loss = -10891.466385853773
Iteration 6000: Loss = -10891.466438944786
Iteration 6100: Loss = -10891.466623130242
1
Iteration 6200: Loss = -10891.466503526011
Iteration 6300: Loss = -10891.466425154083
Iteration 6400: Loss = -10891.466483205171
Iteration 6500: Loss = -10891.46661658856
1
Iteration 6600: Loss = -10891.469147542213
2
Iteration 6700: Loss = -10891.466270222088
Iteration 6800: Loss = -10891.466294340496
Iteration 6900: Loss = -10891.466253253579
Iteration 7000: Loss = -10891.469108775385
1
Iteration 7100: Loss = -10891.466631730684
2
Iteration 7200: Loss = -10891.466187775353
Iteration 7300: Loss = -10891.466411892327
1
Iteration 7400: Loss = -10891.467288310942
2
Iteration 7500: Loss = -10891.466792589772
3
Iteration 7600: Loss = -10891.46744440614
4
Iteration 7700: Loss = -10891.466171341279
Iteration 7800: Loss = -10891.466074974294
Iteration 7900: Loss = -10891.470007297092
1
Iteration 8000: Loss = -10891.467169663354
2
Iteration 8100: Loss = -10891.465856219838
Iteration 8200: Loss = -10891.487260336855
1
Iteration 8300: Loss = -10891.46580278717
Iteration 8400: Loss = -10891.490762445454
1
Iteration 8500: Loss = -10891.465814926627
Iteration 8600: Loss = -10891.465895367417
Iteration 8700: Loss = -10891.465815470716
Iteration 8800: Loss = -10891.465777732634
Iteration 8900: Loss = -10891.467845931405
1
Iteration 9000: Loss = -10891.465771695479
Iteration 9100: Loss = -10891.46579883856
Iteration 9200: Loss = -10891.466725654112
1
Iteration 9300: Loss = -10891.46574602662
Iteration 9400: Loss = -10891.465787433415
Iteration 9500: Loss = -10891.466071397024
1
Iteration 9600: Loss = -10891.467154979751
2
Iteration 9700: Loss = -10891.482752664164
3
Iteration 9800: Loss = -10891.46203740328
Iteration 9900: Loss = -10891.461040295155
Iteration 10000: Loss = -10891.462514902018
1
Iteration 10100: Loss = -10891.461904899095
2
Iteration 10200: Loss = -10891.460966797382
Iteration 10300: Loss = -10891.534019519713
1
Iteration 10400: Loss = -10891.460988555755
Iteration 10500: Loss = -10891.685066062772
1
Iteration 10600: Loss = -10891.460998828408
Iteration 10700: Loss = -10891.466139958757
1
Iteration 10800: Loss = -10891.5296264965
2
Iteration 10900: Loss = -10891.473017062986
3
Iteration 11000: Loss = -10891.461542060133
4
Iteration 11100: Loss = -10891.557343343331
5
Iteration 11200: Loss = -10891.490633836147
6
Iteration 11300: Loss = -10891.489527928843
7
Iteration 11400: Loss = -10891.460998369881
Iteration 11500: Loss = -10891.461738858638
1
Iteration 11600: Loss = -10891.47802620273
2
Iteration 11700: Loss = -10891.47290380562
3
Iteration 11800: Loss = -10891.461086407553
Iteration 11900: Loss = -10891.466403149803
1
Iteration 12000: Loss = -10891.460951884947
Iteration 12100: Loss = -10891.461392466445
1
Iteration 12200: Loss = -10891.460926676758
Iteration 12300: Loss = -10891.461251063272
1
Iteration 12400: Loss = -10891.501003253337
2
Iteration 12500: Loss = -10891.462755447175
3
Iteration 12600: Loss = -10891.461132724096
4
Iteration 12700: Loss = -10891.462202433719
5
Iteration 12800: Loss = -10891.472108102042
6
Iteration 12900: Loss = -10891.463107819409
7
Iteration 13000: Loss = -10891.499873634744
8
Iteration 13100: Loss = -10891.467118424454
9
Iteration 13200: Loss = -10891.461123945726
10
Iteration 13300: Loss = -10891.465764498895
11
Iteration 13400: Loss = -10891.46804163405
12
Iteration 13500: Loss = -10891.466564410628
13
Iteration 13600: Loss = -10891.461595074732
14
Iteration 13700: Loss = -10891.46199745336
15
Stopping early at iteration 13700 due to no improvement.
pi: tensor([[0.7628, 0.2372],
        [0.2736, 0.7264]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4925, 0.5075], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2424, 0.0951],
         [0.7206, 0.2046]],

        [[0.6757, 0.0947],
         [0.5024, 0.5471]],

        [[0.6306, 0.1109],
         [0.5993, 0.5556]],

        [[0.6364, 0.1021],
         [0.5182, 0.5096]],

        [[0.6273, 0.1029],
         [0.6371, 0.5671]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448326530612245
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691181331636993
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.8314071489807862
Average Adjusted Rand Index: 0.8327238130151358
10935.82964587653
[0.4397741542240412, 0.8314071489807862] [0.6932324747105778, 0.8327238130151358] [10926.180908611046, 10891.46199745336]
-------------------------------------
This iteration is 99
True Objective function: Loss = -10910.173539543353
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23107.119266525875
Iteration 100: Loss = -10962.540817815761
Iteration 200: Loss = -10961.589331452884
Iteration 300: Loss = -10961.348269799295
Iteration 400: Loss = -10961.240577147659
Iteration 500: Loss = -10961.183286159003
Iteration 600: Loss = -10961.149039758826
Iteration 700: Loss = -10961.126665501035
Iteration 800: Loss = -10961.111041953236
Iteration 900: Loss = -10961.09954898029
Iteration 1000: Loss = -10961.09077987357
Iteration 1100: Loss = -10961.083791384142
Iteration 1200: Loss = -10961.078035100789
Iteration 1300: Loss = -10961.073253568424
Iteration 1400: Loss = -10961.069064702984
Iteration 1500: Loss = -10961.065385095315
Iteration 1600: Loss = -10961.062055571183
Iteration 1700: Loss = -10961.058844317227
Iteration 1800: Loss = -10961.055709801998
Iteration 1900: Loss = -10961.052515398327
Iteration 2000: Loss = -10961.049262903243
Iteration 2100: Loss = -10961.045632797075
Iteration 2200: Loss = -10961.041610437855
Iteration 2300: Loss = -10961.036902242004
Iteration 2400: Loss = -10961.031158206806
Iteration 2500: Loss = -10961.024062159298
Iteration 2600: Loss = -10961.015127189692
Iteration 2700: Loss = -10961.004107500372
Iteration 2800: Loss = -10960.991131956374
Iteration 2900: Loss = -10960.976353607899
Iteration 3000: Loss = -10960.960243401252
Iteration 3100: Loss = -10960.943412590594
Iteration 3200: Loss = -10960.927498233477
Iteration 3300: Loss = -10960.914451337521
Iteration 3400: Loss = -10960.905402860628
Iteration 3500: Loss = -10960.899809053331
Iteration 3600: Loss = -10960.896313278681
Iteration 3700: Loss = -10960.89388802421
Iteration 3800: Loss = -10960.892014808867
Iteration 3900: Loss = -10960.89017509766
Iteration 4000: Loss = -10960.8882359669
Iteration 4100: Loss = -10960.885950566177
Iteration 4200: Loss = -10960.882803459956
Iteration 4300: Loss = -10960.877223993784
Iteration 4400: Loss = -10960.858975294133
Iteration 4500: Loss = -10960.800769285172
Iteration 4600: Loss = -10960.764487199984
Iteration 4700: Loss = -10960.711934041636
Iteration 4800: Loss = -10960.68649853854
Iteration 4900: Loss = -10960.663909814743
Iteration 5000: Loss = -10960.654329260227
Iteration 5100: Loss = -10960.648916201648
Iteration 5200: Loss = -10960.645404990957
Iteration 5300: Loss = -10960.64383697149
Iteration 5400: Loss = -10960.641334907721
Iteration 5500: Loss = -10960.640098119477
Iteration 5600: Loss = -10960.639062146183
Iteration 5700: Loss = -10960.638271013511
Iteration 5800: Loss = -10960.637638013966
Iteration 5900: Loss = -10960.637071517218
Iteration 6000: Loss = -10960.680172592514
1
Iteration 6100: Loss = -10960.636222914747
Iteration 6200: Loss = -10960.635898120016
Iteration 6300: Loss = -10960.635678146118
Iteration 6400: Loss = -10960.635299718528
Iteration 6500: Loss = -10960.685517698297
1
Iteration 6600: Loss = -10960.635816666987
2
Iteration 6700: Loss = -10960.634723016281
Iteration 6800: Loss = -10960.63453244387
Iteration 6900: Loss = -10960.634389565043
Iteration 7000: Loss = -10960.634327818934
Iteration 7100: Loss = -10960.634150302667
Iteration 7200: Loss = -10960.6340843373
Iteration 7300: Loss = -10960.634370217516
1
Iteration 7400: Loss = -10960.635677476786
2
Iteration 7500: Loss = -10960.633833679987
Iteration 7600: Loss = -10960.633732873735
Iteration 7700: Loss = -10960.633933879288
1
Iteration 7800: Loss = -10960.63359900586
Iteration 7900: Loss = -10960.63355007596
Iteration 8000: Loss = -10960.633527496298
Iteration 8100: Loss = -10960.633438851713
Iteration 8200: Loss = -10960.633479396469
Iteration 8300: Loss = -10960.633356708431
Iteration 8400: Loss = -10960.635678039429
1
Iteration 8500: Loss = -10960.633278348278
Iteration 8600: Loss = -10960.633383677967
1
Iteration 8700: Loss = -10960.640298701042
2
Iteration 8800: Loss = -10960.646523204216
3
Iteration 8900: Loss = -10960.633223385934
Iteration 9000: Loss = -10960.633574297686
1
Iteration 9100: Loss = -10960.65694412144
2
Iteration 9200: Loss = -10960.6367208371
3
Iteration 9300: Loss = -10960.633241917572
Iteration 9400: Loss = -10960.67258526853
1
Iteration 9500: Loss = -10960.63653537953
2
Iteration 9600: Loss = -10960.636234780417
3
Iteration 9700: Loss = -10960.633714361245
4
Iteration 9800: Loss = -10960.635277473226
5
Iteration 9900: Loss = -10960.638188316478
6
Iteration 10000: Loss = -10960.641265195281
7
Iteration 10100: Loss = -10960.661845063085
8
Iteration 10200: Loss = -10960.637822300114
9
Iteration 10300: Loss = -10960.66991595099
10
Iteration 10400: Loss = -10960.632989896207
Iteration 10500: Loss = -10960.633820537603
1
Iteration 10600: Loss = -10960.649354683292
2
Iteration 10700: Loss = -10960.636129123519
3
Iteration 10800: Loss = -10960.647645172918
4
Iteration 10900: Loss = -10960.650197060339
5
Iteration 11000: Loss = -10960.821875375674
6
Iteration 11100: Loss = -10960.633625872046
7
Iteration 11200: Loss = -10960.633570919414
8
Iteration 11300: Loss = -10960.639283712208
9
Iteration 11400: Loss = -10960.632947256036
Iteration 11500: Loss = -10960.660986069326
1
Iteration 11600: Loss = -10961.067444641703
2
Iteration 11700: Loss = -10960.636437076357
3
Iteration 11800: Loss = -10960.633399630235
4
Iteration 11900: Loss = -10960.633256055737
5
Iteration 12000: Loss = -10960.632941524105
Iteration 12100: Loss = -10960.759462940523
1
Iteration 12200: Loss = -10960.63329851049
2
Iteration 12300: Loss = -10960.724248476874
3
Iteration 12400: Loss = -10960.770578510925
4
Iteration 12500: Loss = -10960.646928518017
5
Iteration 12600: Loss = -10960.644007228106
6
Iteration 12700: Loss = -10960.661282941439
7
Iteration 12800: Loss = -10960.7596698786
8
Iteration 12900: Loss = -10960.69707055293
9
Iteration 13000: Loss = -10960.689400485422
10
Iteration 13100: Loss = -10960.80100190396
11
Iteration 13200: Loss = -10960.635068977112
12
Iteration 13300: Loss = -10960.638926662647
13
Iteration 13400: Loss = -10960.762552610258
14
Iteration 13500: Loss = -10960.632818105949
Iteration 13600: Loss = -10960.632945860929
1
Iteration 13700: Loss = -10960.687180503302
2
Iteration 13800: Loss = -10960.633237009857
3
Iteration 13900: Loss = -10960.742960218153
4
Iteration 14000: Loss = -10960.632917768846
Iteration 14100: Loss = -10960.63296274084
Iteration 14200: Loss = -10960.634749698584
1
Iteration 14300: Loss = -10960.635259221917
2
Iteration 14400: Loss = -10960.671348746344
3
Iteration 14500: Loss = -10960.633541446274
4
Iteration 14600: Loss = -10960.692768447567
5
Iteration 14700: Loss = -10960.6330691677
6
Iteration 14800: Loss = -10960.632987584035
Iteration 14900: Loss = -10960.644254341605
1
Iteration 15000: Loss = -10960.635501452463
2
Iteration 15100: Loss = -10960.637858090831
3
Iteration 15200: Loss = -10960.702094465454
4
Iteration 15300: Loss = -10960.63282142647
Iteration 15400: Loss = -10960.632918485377
Iteration 15500: Loss = -10960.638916400136
1
Iteration 15600: Loss = -10960.649868215572
2
Iteration 15700: Loss = -10960.636196684132
3
Iteration 15800: Loss = -10960.633322251286
4
Iteration 15900: Loss = -10960.633201315897
5
Iteration 16000: Loss = -10960.633983032074
6
Iteration 16100: Loss = -10960.652098749486
7
Iteration 16200: Loss = -10960.88185615974
8
Iteration 16300: Loss = -10960.655006079473
9
Iteration 16400: Loss = -10960.711917534596
10
Iteration 16500: Loss = -10960.6327852937
Iteration 16600: Loss = -10960.633046194049
1
Iteration 16700: Loss = -10960.632911259767
2
Iteration 16800: Loss = -10960.633015399339
3
Iteration 16900: Loss = -10960.638876488836
4
Iteration 17000: Loss = -10960.744875531995
5
Iteration 17100: Loss = -10960.633028326123
6
Iteration 17200: Loss = -10960.633203940019
7
Iteration 17300: Loss = -10960.65779676112
8
Iteration 17400: Loss = -10960.639397843623
9
Iteration 17500: Loss = -10960.63437098091
10
Iteration 17600: Loss = -10960.63287492208
Iteration 17700: Loss = -10960.634408511045
1
Iteration 17800: Loss = -10960.678302383085
2
Iteration 17900: Loss = -10960.63641810482
3
Iteration 18000: Loss = -10960.647469971096
4
Iteration 18100: Loss = -10960.632782931538
Iteration 18200: Loss = -10960.63308232177
1
Iteration 18300: Loss = -10960.646150438739
2
Iteration 18400: Loss = -10960.633806125732
3
Iteration 18500: Loss = -10960.63630782432
4
Iteration 18600: Loss = -10960.672050517995
5
Iteration 18700: Loss = -10960.632919172023
6
Iteration 18800: Loss = -10960.634036358542
7
Iteration 18900: Loss = -10960.709044168489
8
Iteration 19000: Loss = -10960.632766286355
Iteration 19100: Loss = -10960.632986651783
1
Iteration 19200: Loss = -10960.63490118523
2
Iteration 19300: Loss = -10960.640076109481
3
Iteration 19400: Loss = -10960.64782619492
4
Iteration 19500: Loss = -10960.632768403102
Iteration 19600: Loss = -10960.636242538305
1
Iteration 19700: Loss = -10960.632770874325
Iteration 19800: Loss = -10960.633599888137
1
Iteration 19900: Loss = -10960.633387615477
2
pi: tensor([[9.9999e-01, 9.4536e-06],
        [1.9019e-01, 8.0981e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0224, 0.9776], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1672, 0.1540],
         [0.6015, 0.1595]],

        [[0.6279, 0.1647],
         [0.6796, 0.5324]],

        [[0.5504, 0.1621],
         [0.6537, 0.5466]],

        [[0.5790, 0.1609],
         [0.6432, 0.6844]],

        [[0.7273, 0.1662],
         [0.6295, 0.7200]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 36
Adjusted Rand Index: 0.07051344743276283
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.007753184531427323
Average Adjusted Rand Index: 0.01379489359370401
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21915.649603040398
Iteration 100: Loss = -10962.974657456807
Iteration 200: Loss = -10961.805636481964
Iteration 300: Loss = -10961.394108577253
Iteration 400: Loss = -10961.267510412305
Iteration 500: Loss = -10961.210105638827
Iteration 600: Loss = -10961.17445794664
Iteration 700: Loss = -10961.148775856185
Iteration 800: Loss = -10961.128760829075
Iteration 900: Loss = -10961.11233151704
Iteration 1000: Loss = -10961.098514641892
Iteration 1100: Loss = -10961.086687951729
Iteration 1200: Loss = -10961.076415774989
Iteration 1300: Loss = -10961.067378548714
Iteration 1400: Loss = -10961.059268317302
Iteration 1500: Loss = -10961.051911813865
Iteration 1600: Loss = -10961.045171032196
Iteration 1700: Loss = -10961.03893077622
Iteration 1800: Loss = -10961.033040131237
Iteration 1900: Loss = -10961.027351925899
Iteration 2000: Loss = -10961.021894864836
Iteration 2100: Loss = -10961.016491937602
Iteration 2200: Loss = -10961.011112184931
Iteration 2300: Loss = -10961.005641791717
Iteration 2400: Loss = -10961.000065892602
Iteration 2500: Loss = -10960.994308124513
Iteration 2600: Loss = -10960.988312195319
Iteration 2700: Loss = -10960.982047551166
Iteration 2800: Loss = -10960.975397762862
Iteration 2900: Loss = -10960.968469554658
Iteration 3000: Loss = -10960.961278969113
Iteration 3100: Loss = -10960.953830706923
Iteration 3200: Loss = -10960.946360290514
Iteration 3300: Loss = -10960.939012722043
Iteration 3400: Loss = -10960.93204331803
Iteration 3500: Loss = -10960.925758820007
Iteration 3600: Loss = -10960.920172022828
Iteration 3700: Loss = -10960.915422469834
Iteration 3800: Loss = -10960.911467702412
Iteration 3900: Loss = -10960.908186965471
Iteration 4000: Loss = -10960.905446602186
Iteration 4100: Loss = -10960.903155872187
Iteration 4200: Loss = -10960.90129393417
Iteration 4300: Loss = -10960.899716071559
Iteration 4400: Loss = -10960.898363932096
Iteration 4500: Loss = -10960.897210205478
Iteration 4600: Loss = -10960.896181362217
Iteration 4700: Loss = -10960.895263129049
Iteration 4800: Loss = -10960.89443331435
Iteration 4900: Loss = -10960.893621938989
Iteration 5000: Loss = -10960.892820147583
Iteration 5100: Loss = -10960.892105605617
Iteration 5200: Loss = -10960.891269376563
Iteration 5300: Loss = -10960.890473279605
Iteration 5400: Loss = -10960.88953373838
Iteration 5500: Loss = -10960.88857961624
Iteration 5600: Loss = -10960.887411554553
Iteration 5700: Loss = -10960.886056148589
Iteration 5800: Loss = -10960.884180169678
Iteration 5900: Loss = -10960.881133272605
Iteration 6000: Loss = -10960.87429815857
Iteration 6100: Loss = -10960.85051387956
Iteration 6200: Loss = -10960.808962257715
Iteration 6300: Loss = -10960.779771148329
Iteration 6400: Loss = -10960.760132556208
Iteration 6500: Loss = -10960.738845570468
Iteration 6600: Loss = -10960.726165295879
Iteration 6700: Loss = -10960.738005118796
1
Iteration 6800: Loss = -10960.696960421528
Iteration 6900: Loss = -10960.686118298041
Iteration 7000: Loss = -10960.674367584734
Iteration 7100: Loss = -10960.666078230637
Iteration 7200: Loss = -10960.659936970555
Iteration 7300: Loss = -10960.654813029842
Iteration 7400: Loss = -10960.651264097516
Iteration 7500: Loss = -10960.648298918055
Iteration 7600: Loss = -10960.646092213876
Iteration 7700: Loss = -10960.64441934867
Iteration 7800: Loss = -10960.643180877529
Iteration 7900: Loss = -10960.68680323136
1
Iteration 8000: Loss = -10960.64064990044
Iteration 8100: Loss = -10960.639799537652
Iteration 8200: Loss = -10960.639349985535
Iteration 8300: Loss = -10960.638438401056
Iteration 8400: Loss = -10960.638354744327
Iteration 8500: Loss = -10960.637421181516
Iteration 8600: Loss = -10960.639240012768
1
Iteration 8700: Loss = -10960.63708882416
Iteration 8800: Loss = -10960.637847423071
1
Iteration 8900: Loss = -10960.636007405534
Iteration 9000: Loss = -10960.652621294017
1
Iteration 9100: Loss = -10960.635514320931
Iteration 9200: Loss = -10960.635644759112
1
Iteration 9300: Loss = -10960.694589878682
2
Iteration 9400: Loss = -10960.638813753203
3
Iteration 9500: Loss = -10960.643926179073
4
Iteration 9600: Loss = -10960.63756676757
5
Iteration 9700: Loss = -10960.63898758379
6
Iteration 9800: Loss = -10960.634964336514
Iteration 9900: Loss = -10960.668257841937
1
Iteration 10000: Loss = -10960.634542952443
Iteration 10100: Loss = -10960.656937570211
1
Iteration 10200: Loss = -10960.682435662071
2
Iteration 10300: Loss = -10960.639960469482
3
Iteration 10400: Loss = -10960.6445513016
4
Iteration 10500: Loss = -10960.634186103074
Iteration 10600: Loss = -10960.640537550678
1
Iteration 10700: Loss = -10960.643730642429
2
Iteration 10800: Loss = -10960.643076229295
3
Iteration 10900: Loss = -10960.633562266092
Iteration 11000: Loss = -10960.634737102635
1
Iteration 11100: Loss = -10960.643873547797
2
Iteration 11200: Loss = -10960.633709857295
3
Iteration 11300: Loss = -10960.633603993385
Iteration 11400: Loss = -10960.657251900615
1
Iteration 11500: Loss = -10960.669113421101
2
Iteration 11600: Loss = -10960.6357116139
3
Iteration 11700: Loss = -10960.638603704376
4
Iteration 11800: Loss = -10960.634487158813
5
Iteration 11900: Loss = -10960.637607224402
6
Iteration 12000: Loss = -10960.63340292961
Iteration 12100: Loss = -10960.633466797259
Iteration 12200: Loss = -10960.63350301705
Iteration 12300: Loss = -10960.635310123293
1
Iteration 12400: Loss = -10960.63380607999
2
Iteration 12500: Loss = -10960.637853030039
3
Iteration 12600: Loss = -10960.65174659093
4
Iteration 12700: Loss = -10960.634563957015
5
Iteration 12800: Loss = -10960.633859753008
6
Iteration 12900: Loss = -10960.63328932843
Iteration 13000: Loss = -10960.635996897043
1
Iteration 13100: Loss = -10960.63696909413
2
Iteration 13200: Loss = -10960.633017613596
Iteration 13300: Loss = -10960.639877688442
1
Iteration 13400: Loss = -10960.632971229214
Iteration 13500: Loss = -10960.633305859299
1
Iteration 13600: Loss = -10960.632984775437
Iteration 13700: Loss = -10960.633256452502
1
Iteration 13800: Loss = -10960.71714189286
2
Iteration 13900: Loss = -10960.632994446303
Iteration 14000: Loss = -10960.633289978174
1
Iteration 14100: Loss = -10960.636701634927
2
Iteration 14200: Loss = -10960.739442608019
3
Iteration 14300: Loss = -10960.695618897988
4
Iteration 14400: Loss = -10960.636967274086
5
Iteration 14500: Loss = -10960.632879023933
Iteration 14600: Loss = -10960.63439525232
1
Iteration 14700: Loss = -10960.646328573544
2
Iteration 14800: Loss = -10960.667062271034
3
Iteration 14900: Loss = -10960.633789675438
4
Iteration 15000: Loss = -10960.63426622525
5
Iteration 15100: Loss = -10960.633187741505
6
Iteration 15200: Loss = -10960.633144671696
7
Iteration 15300: Loss = -10960.632901326626
Iteration 15400: Loss = -10960.662029706262
1
Iteration 15500: Loss = -10960.660702143412
2
Iteration 15600: Loss = -10960.675541989684
3
Iteration 15700: Loss = -10960.635157655668
4
Iteration 15800: Loss = -10960.647980577807
5
Iteration 15900: Loss = -10960.647507283113
6
Iteration 16000: Loss = -10960.642445221785
7
Iteration 16100: Loss = -10960.636629874558
8
Iteration 16200: Loss = -10960.642736350701
9
Iteration 16300: Loss = -10960.642102648095
10
Iteration 16400: Loss = -10960.632915363425
Iteration 16500: Loss = -10960.635948484116
1
Iteration 16600: Loss = -10960.685659506527
2
Iteration 16700: Loss = -10960.71408313958
3
Iteration 16800: Loss = -10960.641853493025
4
Iteration 16900: Loss = -10960.66252655722
5
Iteration 17000: Loss = -10960.633090175701
6
Iteration 17100: Loss = -10960.632962381029
Iteration 17200: Loss = -10960.636456905542
1
Iteration 17300: Loss = -10960.68766729582
2
Iteration 17400: Loss = -10960.634896493988
3
Iteration 17500: Loss = -10960.668354300207
4
Iteration 17600: Loss = -10960.632832324438
Iteration 17700: Loss = -10960.767883387003
1
Iteration 17800: Loss = -10960.66166053396
2
Iteration 17900: Loss = -10960.63403308377
3
Iteration 18000: Loss = -10960.636477753626
4
Iteration 18100: Loss = -10960.93995616052
5
Iteration 18200: Loss = -10960.634462374348
6
Iteration 18300: Loss = -10960.634708833022
7
Iteration 18400: Loss = -10960.63476179032
8
Iteration 18500: Loss = -10960.653164004701
9
Iteration 18600: Loss = -10960.63556053169
10
Iteration 18700: Loss = -10960.633705813492
11
Iteration 18800: Loss = -10960.638854079925
12
Iteration 18900: Loss = -10960.632825608744
Iteration 19000: Loss = -10960.633958387165
1
Iteration 19100: Loss = -10960.633699189844
2
Iteration 19200: Loss = -10960.633817133414
3
Iteration 19300: Loss = -10960.643242700142
4
Iteration 19400: Loss = -10960.632903260323
Iteration 19500: Loss = -10960.6328149053
Iteration 19600: Loss = -10960.632865551246
Iteration 19700: Loss = -10960.6328096414
Iteration 19800: Loss = -10960.632884533765
Iteration 19900: Loss = -10960.632919142803
pi: tensor([[9.9996e-01, 4.2353e-05],
        [1.8702e-01, 8.1298e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0217, 0.9783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1674, 0.1538],
         [0.5223, 0.1595]],

        [[0.5707, 0.1649],
         [0.6720, 0.7212]],

        [[0.6947, 0.1621],
         [0.5855, 0.6295]],

        [[0.6312, 0.1609],
         [0.5958, 0.6087]],

        [[0.6448, 0.1662],
         [0.5761, 0.5905]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 38
Adjusted Rand Index: 0.05021663354725218
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002193311857154732
Global Adjusted Rand Index: 0.007726232796098067
Average Adjusted Rand Index: 0.00960466433801949
10910.173539543353
[0.007753184531427323, 0.007726232796098067] [0.01379489359370401, 0.00960466433801949] [10960.633104607217, 10960.633764859325]
