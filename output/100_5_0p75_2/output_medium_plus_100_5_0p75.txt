nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:101: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [15:04<24:52:34, 904.59s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [28:01<22:34:32, 829.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [40:21<21:15:12, 788.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [54:39<21:45:55, 816.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [1:10:01<22:32:17, 854.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [1:26:41<23:35:58, 903.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [1:41:26<23:11:34, 897.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [1:59:46<24:35:10, 962.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [2:12:55<22:57:12, 908.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [2:27:55<22:37:58, 905.31s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [2:41:33<21:43:19, 878.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [3:00:53<23:34:07, 964.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [3:20:16<24:45:42, 1024.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [3:33:40<22:53:06, 957.98s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [3:46:43<21:22:07, 905.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [4:07:05<23:20:44, 1000.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [4:19:36<21:20:30, 925.67s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [4:34:00<20:39:42, 907.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [4:56:05<23:13:54, 1032.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [5:09:10<21:17:27, 958.10s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [5:24:53<20:55:48, 953.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [5:44:34<22:08:20, 1021.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [6:06:05<23:35:04, 1102.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [6:22:16<22:26:44, 1063.22s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [6:42:40<23:09:05, 1111.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [7:01:11<22:50:35, 1111.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [7:18:56<22:15:11, 1097.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [7:36:19<21:37:08, 1080.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [7:54:28<21:22:15, 1083.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [8:15:19<22:02:44, 1133.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [8:33:17<21:24:32, 1116.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [8:55:03<22:10:17, 1173.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 33%|███▎      | 33/100 [9:07:47<19:33:13, 1050.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 34%|███▍      | 34/100 [9:23:10<18:33:47, 1012.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 35%|███▌      | 35/100 [9:40:21<18:22:42, 1017.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 36%|███▌      | 36/100 [9:55:43<17:35:09, 989.22s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 37%|███▋      | 37/100 [10:13:01<17:34:10, 1003.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 38%|███▊      | 38/100 [10:34:38<18:48:13, 1091.83s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 39%|███▉      | 39/100 [10:47:52<16:59:04, 1002.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 40%|████      | 40/100 [11:07:43<17:38:56, 1058.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 41%|████      | 41/100 [11:20:34<15:56:26, 972.65s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 42%|████▏     | 42/100 [11:38:02<16:02:07, 995.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 43%|████▎     | 43/100 [11:55:02<15:52:24, 1002.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 44%|████▍     | 44/100 [12:15:19<16:35:50, 1066.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 45%|████▌     | 45/100 [12:28:04<14:55:05, 976.46s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 46%|████▌     | 46/100 [12:41:03<13:45:28, 917.20s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-------------------------------------
This iteration is 0
True Objective function: Loss = -11927.873878522678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22920.389415499078
Iteration 100: Loss = -12798.783874808058
Iteration 200: Loss = -12309.036139937532
Iteration 300: Loss = -12125.625851771674
Iteration 400: Loss = -12076.274837371651
Iteration 500: Loss = -12021.334631568376
Iteration 600: Loss = -11995.348331775229
Iteration 700: Loss = -11979.37191069578
Iteration 800: Loss = -11943.103753879694
Iteration 900: Loss = -11935.004624269905
Iteration 1000: Loss = -11931.854757762938
Iteration 1100: Loss = -11929.389592154346
Iteration 1200: Loss = -11926.91300597035
Iteration 1300: Loss = -11926.862782526883
Iteration 1400: Loss = -11926.620112321732
Iteration 1500: Loss = -11914.39785634269
Iteration 1600: Loss = -11914.376406504785
Iteration 1700: Loss = -11914.359062408485
Iteration 1800: Loss = -11914.343881760564
Iteration 1900: Loss = -11914.324529129597
Iteration 2000: Loss = -11914.24262926371
Iteration 2100: Loss = -11914.233439494488
Iteration 2200: Loss = -11914.226031563985
Iteration 2300: Loss = -11914.219660081471
Iteration 2400: Loss = -11914.214117402367
Iteration 2500: Loss = -11914.20977248653
Iteration 2600: Loss = -11914.204855682203
Iteration 2700: Loss = -11914.200953096446
Iteration 2800: Loss = -11914.222816628464
1
Iteration 2900: Loss = -11914.194253072741
Iteration 3000: Loss = -11914.194545934748
1
Iteration 3100: Loss = -11914.188364827087
Iteration 3200: Loss = -11914.184904063271
Iteration 3300: Loss = -11914.154132849757
Iteration 3400: Loss = -11913.687881351188
Iteration 3500: Loss = -11913.691653073054
1
Iteration 3600: Loss = -11913.684619479774
Iteration 3700: Loss = -11913.682879717146
Iteration 3800: Loss = -11913.681454782542
Iteration 3900: Loss = -11913.680166474502
Iteration 4000: Loss = -11913.679066150558
Iteration 4100: Loss = -11913.67805168126
Iteration 4200: Loss = -11913.678605948662
1
Iteration 4300: Loss = -11913.675389370106
Iteration 4400: Loss = -11913.676090634857
1
Iteration 4500: Loss = -11913.673106817541
Iteration 4600: Loss = -11913.673012984824
Iteration 4700: Loss = -11913.671438259653
Iteration 4800: Loss = -11913.67218957368
1
Iteration 4900: Loss = -11913.67010784032
Iteration 5000: Loss = -11913.67019801159
Iteration 5100: Loss = -11913.669042353895
Iteration 5200: Loss = -11913.668535674233
Iteration 5300: Loss = -11913.66810045285
Iteration 5400: Loss = -11913.668198601834
Iteration 5500: Loss = -11913.667576045338
Iteration 5600: Loss = -11913.673720081748
1
Iteration 5700: Loss = -11913.667531605051
Iteration 5800: Loss = -11913.666107927213
Iteration 5900: Loss = -11913.67233623926
1
Iteration 6000: Loss = -11913.673910103058
2
Iteration 6100: Loss = -11913.66298615337
Iteration 6200: Loss = -11913.66236199428
Iteration 6300: Loss = -11913.664412895125
1
Iteration 6400: Loss = -11913.661817648579
Iteration 6500: Loss = -11913.663893638277
1
Iteration 6600: Loss = -11913.741981627249
2
Iteration 6700: Loss = -11913.661163189781
Iteration 6800: Loss = -11913.661107649606
Iteration 6900: Loss = -11913.688863062916
1
Iteration 7000: Loss = -11913.66063400996
Iteration 7100: Loss = -11913.683966559667
1
Iteration 7200: Loss = -11913.660332329697
Iteration 7300: Loss = -11913.66031591449
Iteration 7400: Loss = -11913.660166469232
Iteration 7500: Loss = -11913.663851249901
1
Iteration 7600: Loss = -11913.65989253955
Iteration 7700: Loss = -11913.660469862556
1
Iteration 7800: Loss = -11913.659779567137
Iteration 7900: Loss = -11913.661618075506
1
Iteration 8000: Loss = -11913.659532964475
Iteration 8100: Loss = -11913.71392069155
1
Iteration 8200: Loss = -11913.659913118081
2
Iteration 8300: Loss = -11913.73521611567
3
Iteration 8400: Loss = -11913.659120147666
Iteration 8500: Loss = -11913.659840778193
1
Iteration 8600: Loss = -11913.65934997283
2
Iteration 8700: Loss = -11913.65941485257
3
Iteration 8800: Loss = -11913.658854748583
Iteration 8900: Loss = -11913.658847768891
Iteration 9000: Loss = -11913.667553653402
1
Iteration 9100: Loss = -11913.659024771196
2
Iteration 9200: Loss = -11913.658657416578
Iteration 9300: Loss = -11913.659629390319
1
Iteration 9400: Loss = -11913.721852345254
2
Iteration 9500: Loss = -11913.658516721724
Iteration 9600: Loss = -11913.669722623168
1
Iteration 9700: Loss = -11913.658522099377
Iteration 9800: Loss = -11913.66230619642
1
Iteration 9900: Loss = -11913.658398026178
Iteration 10000: Loss = -11913.658583373264
1
Iteration 10100: Loss = -11913.720606176968
2
Iteration 10200: Loss = -11913.663846137715
3
Iteration 10300: Loss = -11913.670284735914
4
Iteration 10400: Loss = -11913.663153019677
5
Iteration 10500: Loss = -11913.667484721747
6
Iteration 10600: Loss = -11913.667769582265
7
Iteration 10700: Loss = -11913.666403773906
8
Iteration 10800: Loss = -11913.659571593571
9
Iteration 10900: Loss = -11913.68855500749
10
Iteration 11000: Loss = -11913.665982735492
11
Iteration 11100: Loss = -11913.786511436385
12
Iteration 11200: Loss = -11913.658523723974
13
Iteration 11300: Loss = -11913.658391333214
Iteration 11400: Loss = -11913.67770015196
1
Iteration 11500: Loss = -11913.681115866955
2
Iteration 11600: Loss = -11913.659821844198
3
Iteration 11700: Loss = -11913.693364423129
4
Iteration 11800: Loss = -11913.659740521045
5
Iteration 11900: Loss = -11913.659967864225
6
Iteration 12000: Loss = -11913.659157283095
7
Iteration 12100: Loss = -11913.704009579154
8
Iteration 12200: Loss = -11913.685572396054
9
Iteration 12300: Loss = -11913.666297547574
10
Iteration 12400: Loss = -11913.68191734343
11
Iteration 12500: Loss = -11913.674526057648
12
Iteration 12600: Loss = -11913.663546564207
13
Iteration 12700: Loss = -11913.667726967227
14
Iteration 12800: Loss = -11913.674430258918
15
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[0.7864, 0.2136],
        [0.2805, 0.7195]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5303, 0.4697], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4057, 0.1024],
         [0.5666, 0.2014]],

        [[0.5297, 0.1049],
         [0.7148, 0.6101]],

        [[0.7212, 0.1053],
         [0.6292, 0.6502]],

        [[0.6819, 0.0970],
         [0.5073, 0.7183]],

        [[0.7256, 0.1093],
         [0.5453, 0.7237]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9760945186375971
Average Adjusted Rand Index: 0.9759969586354561
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20559.12072551924
Iteration 100: Loss = -12834.604712217855
Iteration 200: Loss = -12579.59980627816
Iteration 300: Loss = -11985.831671037164
Iteration 400: Loss = -11918.97512955751
Iteration 500: Loss = -11917.732704320157
Iteration 600: Loss = -11917.242739245903
Iteration 700: Loss = -11916.980381292275
Iteration 800: Loss = -11916.822683681214
Iteration 900: Loss = -11916.716393324918
Iteration 1000: Loss = -11916.640232545904
Iteration 1100: Loss = -11916.583379385256
Iteration 1200: Loss = -11916.539568649507
Iteration 1300: Loss = -11916.504897082134
Iteration 1400: Loss = -11916.476840705545
Iteration 1500: Loss = -11916.453417195868
Iteration 1600: Loss = -11916.432037383755
Iteration 1700: Loss = -11914.476886748962
Iteration 1800: Loss = -11914.352471777107
Iteration 1900: Loss = -11914.334047570685
Iteration 2000: Loss = -11913.818581719659
Iteration 2100: Loss = -11913.8082184795
Iteration 2200: Loss = -11913.800436729323
Iteration 2300: Loss = -11913.793597484448
Iteration 2400: Loss = -11913.78760554611
Iteration 2500: Loss = -11913.78228620231
Iteration 2600: Loss = -11913.778151693787
Iteration 2700: Loss = -11913.773285855901
Iteration 2800: Loss = -11913.769427202316
Iteration 2900: Loss = -11913.769905615914
1
Iteration 3000: Loss = -11913.762870407374
Iteration 3100: Loss = -11913.760023664867
Iteration 3200: Loss = -11913.758643073668
Iteration 3300: Loss = -11913.755046244618
Iteration 3400: Loss = -11913.753905272084
Iteration 3500: Loss = -11913.751071959437
Iteration 3600: Loss = -11913.74908966578
Iteration 3700: Loss = -11913.752478189204
1
Iteration 3800: Loss = -11913.745915438212
Iteration 3900: Loss = -11913.744422041376
Iteration 4000: Loss = -11913.743278019847
Iteration 4100: Loss = -11913.741998464116
Iteration 4200: Loss = -11913.741437792269
Iteration 4300: Loss = -11913.736918006642
Iteration 4400: Loss = -11913.6701912218
Iteration 4500: Loss = -11913.668892616917
Iteration 4600: Loss = -11913.674242949579
1
Iteration 4700: Loss = -11913.667082369726
Iteration 4800: Loss = -11913.66635557214
Iteration 4900: Loss = -11913.665546087159
Iteration 5000: Loss = -11913.667161744332
1
Iteration 5100: Loss = -11913.664303621343
Iteration 5200: Loss = -11913.663654854836
Iteration 5300: Loss = -11913.665031090288
1
Iteration 5400: Loss = -11913.664271585825
2
Iteration 5500: Loss = -11913.662279864633
Iteration 5600: Loss = -11913.664073777209
1
Iteration 5700: Loss = -11913.662115819794
Iteration 5800: Loss = -11913.66122327359
Iteration 5900: Loss = -11913.660662396804
Iteration 6000: Loss = -11913.660275640106
Iteration 6100: Loss = -11913.663268477427
1
Iteration 6200: Loss = -11913.65965031849
Iteration 6300: Loss = -11913.659337477795
Iteration 6400: Loss = -11913.659117476609
Iteration 6500: Loss = -11913.665431022167
1
Iteration 6600: Loss = -11913.685879631217
2
Iteration 6700: Loss = -11913.660604708277
3
Iteration 6800: Loss = -11913.658378915517
Iteration 6900: Loss = -11913.65962787229
1
Iteration 7000: Loss = -11913.747409771744
2
Iteration 7100: Loss = -11913.657627629507
Iteration 7200: Loss = -11913.65784458836
1
Iteration 7300: Loss = -11913.657287519009
Iteration 7400: Loss = -11913.658757580939
1
Iteration 7500: Loss = -11913.660144456764
2
Iteration 7600: Loss = -11913.679237207076
3
Iteration 7700: Loss = -11913.65672281679
Iteration 7800: Loss = -11913.65662132572
Iteration 7900: Loss = -11913.656672320147
Iteration 8000: Loss = -11913.656419384228
Iteration 8100: Loss = -11913.656322140247
Iteration 8200: Loss = -11913.656357565347
Iteration 8300: Loss = -11913.65606752109
Iteration 8400: Loss = -11913.655742528961
Iteration 8500: Loss = -11913.65525671729
Iteration 8600: Loss = -11913.653844226268
Iteration 8700: Loss = -11913.653444302141
Iteration 8800: Loss = -11913.655146856767
1
Iteration 8900: Loss = -11913.6566351707
2
Iteration 9000: Loss = -11913.656162353644
3
Iteration 9100: Loss = -11913.653320065172
Iteration 9200: Loss = -11913.681942568619
1
Iteration 9300: Loss = -11913.655717779795
2
Iteration 9400: Loss = -11913.653307522038
Iteration 9500: Loss = -11913.659954922534
1
Iteration 9600: Loss = -11913.653976190817
2
Iteration 9700: Loss = -11913.653064315662
Iteration 9800: Loss = -11913.654529860823
1
Iteration 9900: Loss = -11913.654276366507
2
Iteration 10000: Loss = -11913.661322062042
3
Iteration 10100: Loss = -11913.7353179488
4
Iteration 10200: Loss = -11913.66376517382
5
Iteration 10300: Loss = -11913.652970735582
Iteration 10400: Loss = -11913.662307989453
1
Iteration 10500: Loss = -11913.666786247646
2
Iteration 10600: Loss = -11913.654357727717
3
Iteration 10700: Loss = -11913.659043743168
4
Iteration 10800: Loss = -11913.658022452919
5
Iteration 10900: Loss = -11913.653099501887
6
Iteration 11000: Loss = -11913.673113232579
7
Iteration 11100: Loss = -11913.661596871145
8
Iteration 11200: Loss = -11913.71967819777
9
Iteration 11300: Loss = -11913.65695843877
10
Iteration 11400: Loss = -11913.656449797963
11
Iteration 11500: Loss = -11913.654107410079
12
Iteration 11600: Loss = -11913.65300429481
Iteration 11700: Loss = -11913.654278674987
1
Iteration 11800: Loss = -11913.65310854649
2
Iteration 11900: Loss = -11913.65422751182
3
Iteration 12000: Loss = -11913.65446141948
4
Iteration 12100: Loss = -11913.658830428723
5
Iteration 12200: Loss = -11913.660132680823
6
Iteration 12300: Loss = -11913.658687219593
7
Iteration 12400: Loss = -11913.662818497356
8
Iteration 12500: Loss = -11913.658523825789
9
Iteration 12600: Loss = -11913.652938096267
Iteration 12700: Loss = -11913.653180202024
1
Iteration 12800: Loss = -11913.657299906135
2
Iteration 12900: Loss = -11913.656263867128
3
Iteration 13000: Loss = -11913.679049162438
4
Iteration 13100: Loss = -11913.654753676261
5
Iteration 13200: Loss = -11913.6620053191
6
Iteration 13300: Loss = -11913.660278086789
7
Iteration 13400: Loss = -11913.65472871179
8
Iteration 13500: Loss = -11913.653906704196
9
Iteration 13600: Loss = -11913.660232433174
10
Iteration 13700: Loss = -11913.659411275234
11
Iteration 13800: Loss = -11913.656527504534
12
Iteration 13900: Loss = -11913.656247178435
13
Iteration 14000: Loss = -11913.682431399506
14
Iteration 14100: Loss = -11913.65330066377
15
Stopping early at iteration 14100 due to no improvement.
pi: tensor([[0.7190, 0.2810],
        [0.2150, 0.7850]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4698, 0.5302], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2008, 0.1022],
         [0.6780, 0.4065]],

        [[0.6252, 0.1052],
         [0.6931, 0.5963]],

        [[0.5941, 0.1056],
         [0.5121, 0.6817]],

        [[0.5755, 0.0969],
         [0.6727, 0.6043]],

        [[0.5463, 0.1098],
         [0.5981, 0.7216]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9760945186375971
Average Adjusted Rand Index: 0.9759969586354561
11927.873878522678
[0.9760945186375971, 0.9760945186375971] [0.9759969586354561, 0.9759969586354561] [11913.674430258918, 11913.65330066377]
-------------------------------------
This iteration is 1
True Objective function: Loss = -11581.569170498846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20321.379159318687
Iteration 100: Loss = -12412.771980176727
Iteration 200: Loss = -11892.270362207511
Iteration 300: Loss = -11722.555683998235
Iteration 400: Loss = -11721.529986258976
Iteration 500: Loss = -11721.112088562957
Iteration 600: Loss = -11720.655913455657
Iteration 700: Loss = -11719.740428070398
Iteration 800: Loss = -11719.541496160251
Iteration 900: Loss = -11719.459320510574
Iteration 1000: Loss = -11719.402034170229
Iteration 1100: Loss = -11719.335433685692
Iteration 1200: Loss = -11719.28965065676
Iteration 1300: Loss = -11719.247886089983
Iteration 1400: Loss = -11719.224026999516
Iteration 1500: Loss = -11719.206895484345
Iteration 1600: Loss = -11719.194591680302
Iteration 1700: Loss = -11719.185498741994
Iteration 1800: Loss = -11719.178024125618
Iteration 1900: Loss = -11719.17169593323
Iteration 2000: Loss = -11719.166197439477
Iteration 2100: Loss = -11719.161451870805
Iteration 2200: Loss = -11719.15723376531
Iteration 2300: Loss = -11719.15359452163
Iteration 2400: Loss = -11719.150399066088
Iteration 2500: Loss = -11719.160951075808
1
Iteration 2600: Loss = -11719.145022136618
Iteration 2700: Loss = -11719.142790126782
Iteration 2800: Loss = -11719.140749799059
Iteration 2900: Loss = -11719.138946733063
Iteration 3000: Loss = -11719.137260394771
Iteration 3100: Loss = -11719.135808645973
Iteration 3200: Loss = -11719.13442495831
Iteration 3300: Loss = -11719.134108958351
Iteration 3400: Loss = -11719.133743949214
Iteration 3500: Loss = -11719.130995961077
Iteration 3600: Loss = -11719.130359804056
Iteration 3700: Loss = -11719.129218439746
Iteration 3800: Loss = -11719.13059214401
1
Iteration 3900: Loss = -11719.130328983467
2
Iteration 4000: Loss = -11719.127061180736
Iteration 4100: Loss = -11719.126307967103
Iteration 4200: Loss = -11719.125699723643
Iteration 4300: Loss = -11719.125165793032
Iteration 4400: Loss = -11719.124911686491
Iteration 4500: Loss = -11719.12555964107
1
Iteration 4600: Loss = -11719.123794521956
Iteration 4700: Loss = -11719.123483541809
Iteration 4800: Loss = -11719.12350350105
Iteration 4900: Loss = -11719.122624962969
Iteration 5000: Loss = -11719.122344351008
Iteration 5100: Loss = -11719.12195311664
Iteration 5200: Loss = -11719.121720511792
Iteration 5300: Loss = -11719.122195468144
1
Iteration 5400: Loss = -11719.121189270963
Iteration 5500: Loss = -11719.120953341086
Iteration 5600: Loss = -11719.123397545376
1
Iteration 5700: Loss = -11719.120519411603
Iteration 5800: Loss = -11719.121594603293
1
Iteration 5900: Loss = -11719.120113344232
Iteration 6000: Loss = -11719.12147077843
1
Iteration 6100: Loss = -11719.125124474886
2
Iteration 6200: Loss = -11719.119615034702
Iteration 6300: Loss = -11719.119463972753
Iteration 6400: Loss = -11719.135274732016
1
Iteration 6500: Loss = -11719.119151472583
Iteration 6600: Loss = -11719.119114932048
Iteration 6700: Loss = -11719.122937381368
1
Iteration 6800: Loss = -11719.119767216662
2
Iteration 6900: Loss = -11719.119041236367
Iteration 7000: Loss = -11719.121235596444
1
Iteration 7100: Loss = -11719.11829221609
Iteration 7200: Loss = -11719.118172079023
Iteration 7300: Loss = -11719.118376111206
1
Iteration 7400: Loss = -11719.117967940321
Iteration 7500: Loss = -11719.117894261039
Iteration 7600: Loss = -11719.189198338363
1
Iteration 7700: Loss = -11719.117758826515
Iteration 7800: Loss = -11719.11775972936
Iteration 7900: Loss = -11719.11762364247
Iteration 8000: Loss = -11719.117783831094
1
Iteration 8100: Loss = -11719.118012904608
2
Iteration 8200: Loss = -11719.119026130455
3
Iteration 8300: Loss = -11719.118863780292
4
Iteration 8400: Loss = -11719.118629781977
5
Iteration 8500: Loss = -11719.119326835957
6
Iteration 8600: Loss = -11719.11753846651
Iteration 8700: Loss = -11719.11756057877
Iteration 8800: Loss = -11719.122960878592
1
Iteration 8900: Loss = -11719.149013994485
2
Iteration 9000: Loss = -11719.1222155781
3
Iteration 9100: Loss = -11719.117714063641
4
Iteration 9200: Loss = -11719.117680010222
5
Iteration 9300: Loss = -11719.201367212403
6
Iteration 9400: Loss = -11719.122364930314
7
Iteration 9500: Loss = -11719.118473092956
8
Iteration 9600: Loss = -11719.120641346024
9
Iteration 9700: Loss = -11719.220498891416
10
Iteration 9800: Loss = -11719.152038172191
11
Iteration 9900: Loss = -11719.12098044489
12
Iteration 10000: Loss = -11719.155467430666
13
Iteration 10100: Loss = -11719.123020369165
14
Iteration 10200: Loss = -11719.120181220724
15
Stopping early at iteration 10200 due to no improvement.
pi: tensor([[0.6372, 0.3628],
        [0.2554, 0.7446]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9714, 0.0286], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.0956],
         [0.5701, 0.4060]],

        [[0.6373, 0.1069],
         [0.5400, 0.6332]],

        [[0.7118, 0.0916],
         [0.6645, 0.7276]],

        [[0.6011, 0.1078],
         [0.6865, 0.5479]],

        [[0.7162, 0.0939],
         [0.6630, 0.6799]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6521827097111664
Average Adjusted Rand Index: 0.7989761372023008
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21397.619586802564
Iteration 100: Loss = -12406.08479400827
Iteration 200: Loss = -12022.33021883152
Iteration 300: Loss = -11762.727558369057
Iteration 400: Loss = -11760.499192521738
Iteration 500: Loss = -11741.313184947234
Iteration 600: Loss = -11740.639462673727
Iteration 700: Loss = -11740.374487186667
Iteration 800: Loss = -11739.033114275479
Iteration 900: Loss = -11737.497769206657
Iteration 1000: Loss = -11722.19183552982
Iteration 1100: Loss = -11712.427326327226
Iteration 1200: Loss = -11673.474331027586
Iteration 1300: Loss = -11656.843844409246
Iteration 1400: Loss = -11656.6291621825
Iteration 1500: Loss = -11626.298174718184
Iteration 1600: Loss = -11614.781658409633
Iteration 1700: Loss = -11595.951788756844
Iteration 1800: Loss = -11595.907744382775
Iteration 1900: Loss = -11595.868384633784
Iteration 2000: Loss = -11583.644568980113
Iteration 2100: Loss = -11583.582986655038
Iteration 2200: Loss = -11576.014033058404
Iteration 2300: Loss = -11576.003113382394
Iteration 2400: Loss = -11575.998713205707
Iteration 2500: Loss = -11575.986350868276
Iteration 2600: Loss = -11575.978427925604
Iteration 2700: Loss = -11575.970778555788
Iteration 2800: Loss = -11575.949593527092
Iteration 2900: Loss = -11575.820670545423
Iteration 3000: Loss = -11575.817658724944
Iteration 3100: Loss = -11575.817129166953
Iteration 3200: Loss = -11575.811913162
Iteration 3300: Loss = -11575.809782394352
Iteration 3400: Loss = -11575.807776827074
Iteration 3500: Loss = -11575.80589051756
Iteration 3600: Loss = -11575.804090115756
Iteration 3700: Loss = -11575.802359942772
Iteration 3800: Loss = -11575.80004835225
Iteration 3900: Loss = -11575.788774288076
Iteration 4000: Loss = -11575.616254052407
Iteration 4100: Loss = -11575.616523894892
1
Iteration 4200: Loss = -11575.617539948784
2
Iteration 4300: Loss = -11575.614606001138
Iteration 4400: Loss = -11575.615045545568
1
Iteration 4500: Loss = -11575.61135146651
Iteration 4600: Loss = -11575.610457881543
Iteration 4700: Loss = -11575.60980560669
Iteration 4800: Loss = -11575.6092208865
Iteration 4900: Loss = -11575.608890416992
Iteration 5000: Loss = -11575.619933479586
1
Iteration 5100: Loss = -11575.607628787204
Iteration 5200: Loss = -11575.607205904214
Iteration 5300: Loss = -11575.612209515863
1
Iteration 5400: Loss = -11575.606401357261
Iteration 5500: Loss = -11575.606063166688
Iteration 5600: Loss = -11575.609339284027
1
Iteration 5700: Loss = -11575.605267551226
Iteration 5800: Loss = -11575.605408376805
1
Iteration 5900: Loss = -11575.604639604224
Iteration 6000: Loss = -11575.604936999332
1
Iteration 6100: Loss = -11575.606649242261
2
Iteration 6200: Loss = -11575.6039340678
Iteration 6300: Loss = -11575.603862171653
Iteration 6400: Loss = -11575.606924648857
1
Iteration 6500: Loss = -11575.602737947273
Iteration 6600: Loss = -11575.578458243173
Iteration 6700: Loss = -11575.578371819467
Iteration 6800: Loss = -11575.588189265945
1
Iteration 6900: Loss = -11575.581398376064
2
Iteration 7000: Loss = -11575.576262015291
Iteration 7100: Loss = -11575.576495674024
1
Iteration 7200: Loss = -11575.649261747401
2
Iteration 7300: Loss = -11575.575745495924
Iteration 7400: Loss = -11575.575171813836
Iteration 7500: Loss = -11575.576120501277
1
Iteration 7600: Loss = -11575.576586453897
2
Iteration 7700: Loss = -11575.577723448187
3
Iteration 7800: Loss = -11575.574812054763
Iteration 7900: Loss = -11575.574729497828
Iteration 8000: Loss = -11575.574423433043
Iteration 8100: Loss = -11575.71763134476
1
Iteration 8200: Loss = -11575.574223883317
Iteration 8300: Loss = -11575.597666467105
1
Iteration 8400: Loss = -11575.574102788707
Iteration 8500: Loss = -11575.662733961253
1
Iteration 8600: Loss = -11575.574194143563
Iteration 8700: Loss = -11575.575695451189
1
Iteration 8800: Loss = -11575.57400680428
Iteration 8900: Loss = -11575.574484668512
1
Iteration 9000: Loss = -11575.576986023927
2
Iteration 9100: Loss = -11575.578237716196
3
Iteration 9200: Loss = -11575.576326270373
4
Iteration 9300: Loss = -11575.576810939745
5
Iteration 9400: Loss = -11575.573981038056
Iteration 9500: Loss = -11575.5738070536
Iteration 9600: Loss = -11575.573891438016
Iteration 9700: Loss = -11575.68343097731
1
Iteration 9800: Loss = -11575.573601160217
Iteration 9900: Loss = -11575.575337630129
1
Iteration 10000: Loss = -11575.579195050856
2
Iteration 10100: Loss = -11575.676335147593
3
Iteration 10200: Loss = -11575.647625911126
4
Iteration 10300: Loss = -11575.577569896952
5
Iteration 10400: Loss = -11575.574154014437
6
Iteration 10500: Loss = -11575.693155790343
7
Iteration 10600: Loss = -11575.58739359306
8
Iteration 10700: Loss = -11575.580012104723
9
Iteration 10800: Loss = -11575.573504498629
Iteration 10900: Loss = -11575.57529888408
1
Iteration 11000: Loss = -11575.58066349528
2
Iteration 11100: Loss = -11575.593272318476
3
Iteration 11200: Loss = -11575.684545708189
4
Iteration 11300: Loss = -11575.588396130537
5
Iteration 11400: Loss = -11575.637144238808
6
Iteration 11500: Loss = -11575.613856788483
7
Iteration 11600: Loss = -11575.586134072095
8
Iteration 11700: Loss = -11575.636868565567
9
Iteration 11800: Loss = -11575.660855775837
10
Iteration 11900: Loss = -11575.589498732794
11
Iteration 12000: Loss = -11575.57636337536
12
Iteration 12100: Loss = -11575.573179728039
Iteration 12200: Loss = -11575.577888078818
1
Iteration 12300: Loss = -11575.688708633126
2
Iteration 12400: Loss = -11575.577145298634
3
Iteration 12500: Loss = -11575.575385254906
4
Iteration 12600: Loss = -11575.59107717081
5
Iteration 12700: Loss = -11575.579071384751
6
Iteration 12800: Loss = -11575.673665559207
7
Iteration 12900: Loss = -11575.582883750816
8
Iteration 13000: Loss = -11575.573355317996
9
Iteration 13100: Loss = -11575.576313512727
10
Iteration 13200: Loss = -11575.577326086772
11
Iteration 13300: Loss = -11575.578800501164
12
Iteration 13400: Loss = -11575.624707065412
13
Iteration 13500: Loss = -11575.573819337511
14
Iteration 13600: Loss = -11575.596695243214
15
Stopping early at iteration 13600 due to no improvement.
pi: tensor([[0.7724, 0.2276],
        [0.2537, 0.7463]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4483, 0.5517], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4036, 0.0992],
         [0.6079, 0.1978]],

        [[0.6860, 0.1064],
         [0.5290, 0.6932]],

        [[0.5818, 0.0928],
         [0.6322, 0.6266]],

        [[0.6113, 0.1079],
         [0.6194, 0.6022]],

        [[0.5006, 0.0940],
         [0.5117, 0.5314]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999740011123
Average Adjusted Rand Index: 0.992
11581.569170498846
[0.6521827097111664, 0.9919999740011123] [0.7989761372023008, 0.992] [11719.120181220724, 11575.596695243214]
-------------------------------------
This iteration is 2
True Objective function: Loss = -11605.545322522208
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21402.476046683147
Iteration 100: Loss = -12377.644675306701
Iteration 200: Loss = -12079.512036483617
Iteration 300: Loss = -11923.14126158423
Iteration 400: Loss = -11917.170866653807
Iteration 500: Loss = -11862.748284665342
Iteration 600: Loss = -11791.09812216234
Iteration 700: Loss = -11752.197383319046
Iteration 800: Loss = -11751.904118558828
Iteration 900: Loss = -11731.983098124905
Iteration 1000: Loss = -11731.861663852791
Iteration 1100: Loss = -11731.814706428813
Iteration 1200: Loss = -11731.784331252285
Iteration 1300: Loss = -11731.761884366564
Iteration 1400: Loss = -11731.738154484769
Iteration 1500: Loss = -11730.263873864436
Iteration 1600: Loss = -11730.21919164685
Iteration 1700: Loss = -11730.208776174642
Iteration 1800: Loss = -11730.204032451857
Iteration 1900: Loss = -11730.19411536711
Iteration 2000: Loss = -11730.189034621875
Iteration 2100: Loss = -11730.184416999116
Iteration 2200: Loss = -11730.182344646291
Iteration 2300: Loss = -11730.177164034001
Iteration 2400: Loss = -11730.174231124618
Iteration 2500: Loss = -11730.174442339303
1
Iteration 2600: Loss = -11730.173771379292
Iteration 2700: Loss = -11730.16721656326
Iteration 2800: Loss = -11730.16506596493
Iteration 2900: Loss = -11730.163507666444
Iteration 3000: Loss = -11730.164172842145
1
Iteration 3100: Loss = -11730.164387969007
2
Iteration 3200: Loss = -11730.156819853819
Iteration 3300: Loss = -11730.145176340468
Iteration 3400: Loss = -11730.143947814982
Iteration 3500: Loss = -11730.142863489065
Iteration 3600: Loss = -11730.141965328074
Iteration 3700: Loss = -11730.141321666862
Iteration 3800: Loss = -11730.141428203291
1
Iteration 3900: Loss = -11730.148796691461
2
Iteration 4000: Loss = -11730.139002126592
Iteration 4100: Loss = -11730.13998600412
1
Iteration 4200: Loss = -11730.13903720319
Iteration 4300: Loss = -11730.146351517133
1
Iteration 4400: Loss = -11730.137348681093
Iteration 4500: Loss = -11730.136526119573
Iteration 4600: Loss = -11730.135928663118
Iteration 4700: Loss = -11730.136377895064
1
Iteration 4800: Loss = -11730.135619843693
Iteration 4900: Loss = -11730.13790647091
1
Iteration 5000: Loss = -11730.135001994022
Iteration 5100: Loss = -11730.13428171626
Iteration 5200: Loss = -11730.134348409832
Iteration 5300: Loss = -11730.133648831721
Iteration 5400: Loss = -11730.133481991905
Iteration 5500: Loss = -11730.131431912905
Iteration 5600: Loss = -11730.13089245294
Iteration 5700: Loss = -11730.130551496537
Iteration 5800: Loss = -11730.13081321569
1
Iteration 5900: Loss = -11730.130862919865
2
Iteration 6000: Loss = -11730.130009924675
Iteration 6100: Loss = -11730.133226010563
1
Iteration 6200: Loss = -11730.13738164634
2
Iteration 6300: Loss = -11730.13007258611
Iteration 6400: Loss = -11730.128444046064
Iteration 6500: Loss = -11730.137565418394
1
Iteration 6600: Loss = -11730.118500306562
Iteration 6700: Loss = -11730.116852834068
Iteration 6800: Loss = -11730.127684522262
1
Iteration 6900: Loss = -11730.115822332365
Iteration 7000: Loss = -11730.116618361077
1
Iteration 7100: Loss = -11730.1156338714
Iteration 7200: Loss = -11730.115569126914
Iteration 7300: Loss = -11730.120486004633
1
Iteration 7400: Loss = -11730.115410833343
Iteration 7500: Loss = -11730.116505718128
1
Iteration 7600: Loss = -11730.129784379997
2
Iteration 7700: Loss = -11730.115677216156
3
Iteration 7800: Loss = -11730.115189209962
Iteration 7900: Loss = -11730.116003815645
1
Iteration 8000: Loss = -11730.115111858555
Iteration 8100: Loss = -11730.11507834248
Iteration 8200: Loss = -11730.126926126686
1
Iteration 8300: Loss = -11730.11492922401
Iteration 8400: Loss = -11730.165551058986
1
Iteration 8500: Loss = -11730.114939089372
Iteration 8600: Loss = -11730.11693164895
1
Iteration 8700: Loss = -11730.121664908165
2
Iteration 8800: Loss = -11730.114998101963
Iteration 8900: Loss = -11730.13086649089
1
Iteration 9000: Loss = -11730.117401146039
2
Iteration 9100: Loss = -11730.11452169029
Iteration 9200: Loss = -11730.115847642799
1
Iteration 9300: Loss = -11730.121956532172
2
Iteration 9400: Loss = -11730.11416374048
Iteration 9500: Loss = -11730.114712266131
1
Iteration 9600: Loss = -11730.123775468484
2
Iteration 9700: Loss = -11730.114312689802
3
Iteration 9800: Loss = -11730.116344052754
4
Iteration 9900: Loss = -11730.12672416063
5
Iteration 10000: Loss = -11730.239725646232
6
Iteration 10100: Loss = -11730.120650603945
7
Iteration 10200: Loss = -11730.116771276225
8
Iteration 10300: Loss = -11730.115441339958
9
Iteration 10400: Loss = -11730.145118057158
10
Iteration 10500: Loss = -11730.123626973269
11
Iteration 10600: Loss = -11730.117143831067
12
Iteration 10700: Loss = -11730.127229252388
13
Iteration 10800: Loss = -11730.119320696944
14
Iteration 10900: Loss = -11730.206570829301
15
Stopping early at iteration 10900 due to no improvement.
pi: tensor([[0.5937, 0.4063],
        [0.4816, 0.5184]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5380, 0.4620], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2144, 0.0939],
         [0.6809, 0.4017]],

        [[0.5926, 0.0959],
         [0.6360, 0.5568]],

        [[0.5029, 0.1133],
         [0.5919, 0.6094]],

        [[0.7104, 0.1035],
         [0.6071, 0.6133]],

        [[0.6345, 0.0996],
         [0.6729, 0.6251]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 26
Adjusted Rand Index: 0.2236482808306714
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4889831440460554
Average Adjusted Rand Index: 0.8367289978934241
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21392.083764776664
Iteration 100: Loss = -12390.756817654166
Iteration 200: Loss = -12087.429251902371
Iteration 300: Loss = -11836.539855975572
Iteration 400: Loss = -11785.156014725811
Iteration 500: Loss = -11779.501824312156
Iteration 600: Loss = -11779.317193464889
Iteration 700: Loss = -11779.215460200974
Iteration 800: Loss = -11779.145041313739
Iteration 900: Loss = -11779.077926779264
Iteration 1000: Loss = -11778.830169239858
Iteration 1100: Loss = -11775.196390607489
Iteration 1200: Loss = -11773.503828291126
Iteration 1300: Loss = -11773.478562870898
Iteration 1400: Loss = -11773.450326636294
Iteration 1500: Loss = -11772.435963787555
Iteration 1600: Loss = -11772.426472999485
Iteration 1700: Loss = -11772.418992033405
Iteration 1800: Loss = -11772.411962173011
Iteration 1900: Loss = -11771.854307325226
Iteration 2000: Loss = -11770.469586710185
Iteration 2100: Loss = -11770.412669817255
Iteration 2200: Loss = -11770.37857203062
Iteration 2300: Loss = -11770.36567004418
Iteration 2400: Loss = -11770.361721140906
Iteration 2500: Loss = -11770.359037999802
Iteration 2600: Loss = -11770.35609177235
Iteration 2700: Loss = -11768.113997417031
Iteration 2800: Loss = -11763.98601274833
Iteration 2900: Loss = -11762.81001728907
Iteration 3000: Loss = -11762.780611119817
Iteration 3100: Loss = -11762.773431236232
Iteration 3200: Loss = -11762.659267226987
Iteration 3300: Loss = -11762.654096215409
Iteration 3400: Loss = -11761.853135067575
Iteration 3500: Loss = -11761.850251408137
Iteration 3600: Loss = -11761.84912982356
Iteration 3700: Loss = -11761.848039212047
Iteration 3800: Loss = -11761.848158555875
1
Iteration 3900: Loss = -11761.845843020186
Iteration 4000: Loss = -11756.407962813808
Iteration 4100: Loss = -11756.285245560259
Iteration 4200: Loss = -11749.57840167312
Iteration 4300: Loss = -11749.10409604347
Iteration 4400: Loss = -11749.043990597456
Iteration 4500: Loss = -11749.042387682153
Iteration 4600: Loss = -11749.041819448263
Iteration 4700: Loss = -11749.041750963996
Iteration 4800: Loss = -11749.04748976479
1
Iteration 4900: Loss = -11749.040489910274
Iteration 5000: Loss = -11749.040401940178
Iteration 5100: Loss = -11749.041393073288
1
Iteration 5200: Loss = -11749.03964987137
Iteration 5300: Loss = -11749.039431751204
Iteration 5400: Loss = -11749.039418311588
Iteration 5500: Loss = -11749.03969491334
1
Iteration 5600: Loss = -11749.039241954595
Iteration 5700: Loss = -11749.03876949842
Iteration 5800: Loss = -11749.038819591526
Iteration 5900: Loss = -11749.038790648283
Iteration 6000: Loss = -11749.039493734856
1
Iteration 6100: Loss = -11749.024584197668
Iteration 6200: Loss = -11749.024451531996
Iteration 6300: Loss = -11749.024403489446
Iteration 6400: Loss = -11749.024505959685
1
Iteration 6500: Loss = -11749.025796412
2
Iteration 6600: Loss = -11749.026676252286
3
Iteration 6700: Loss = -11749.041227625461
4
Iteration 6800: Loss = -11749.023890970486
Iteration 6900: Loss = -11749.025131318529
1
Iteration 7000: Loss = -11749.026234151068
2
Iteration 7100: Loss = -11749.003025676928
Iteration 7200: Loss = -11748.992915496714
Iteration 7300: Loss = -11749.047890406486
1
Iteration 7400: Loss = -11748.9954187464
2
Iteration 7500: Loss = -11748.996121633982
3
Iteration 7600: Loss = -11749.00501058055
4
Iteration 7700: Loss = -11749.000498795349
5
Iteration 7800: Loss = -11748.992581362592
Iteration 7900: Loss = -11748.992617555996
Iteration 8000: Loss = -11748.992715974675
Iteration 8100: Loss = -11749.128593039775
1
Iteration 8200: Loss = -11748.989351437576
Iteration 8300: Loss = -11749.04201290467
1
Iteration 8400: Loss = -11748.988315380018
Iteration 8500: Loss = -11748.990528602935
1
Iteration 8600: Loss = -11748.987168153928
Iteration 8700: Loss = -11748.986627931661
Iteration 8800: Loss = -11748.986565834606
Iteration 8900: Loss = -11748.988322645817
1
Iteration 9000: Loss = -11748.989853091776
2
Iteration 9100: Loss = -11749.012214974782
3
Iteration 9200: Loss = -11748.993103194794
4
Iteration 9300: Loss = -11748.989207083247
5
Iteration 9400: Loss = -11749.065617065462
6
Iteration 9500: Loss = -11748.986728064461
7
Iteration 9600: Loss = -11748.97991963581
Iteration 9700: Loss = -11748.98167671093
1
Iteration 9800: Loss = -11748.97979873459
Iteration 9900: Loss = -11748.987346575725
1
Iteration 10000: Loss = -11748.978360430861
Iteration 10100: Loss = -11749.001021971879
1
Iteration 10200: Loss = -11748.978015315906
Iteration 10300: Loss = -11748.99576855293
1
Iteration 10400: Loss = -11748.978194959744
2
Iteration 10500: Loss = -11748.979190198395
3
Iteration 10600: Loss = -11748.979527441794
4
Iteration 10700: Loss = -11749.003299754426
5
Iteration 10800: Loss = -11748.9802489242
6
Iteration 10900: Loss = -11749.039861669575
7
Iteration 11000: Loss = -11749.04169307326
8
Iteration 11100: Loss = -11749.022166886194
9
Iteration 11200: Loss = -11748.987875643363
10
Iteration 11300: Loss = -11749.004997157748
11
Iteration 11400: Loss = -11749.044064703692
12
Iteration 11500: Loss = -11749.150257051811
13
Iteration 11600: Loss = -11748.987210427193
14
Iteration 11700: Loss = -11749.06131436496
15
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.6236, 0.3764],
        [0.3139, 0.6861]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7666, 0.2334], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2152, 0.0949],
         [0.5539, 0.3960]],

        [[0.5808, 0.0960],
         [0.5187, 0.7152]],

        [[0.5550, 0.1130],
         [0.7226, 0.5197]],

        [[0.6698, 0.1032],
         [0.5276, 0.5356]],

        [[0.7064, 0.0987],
         [0.6551, 0.7005]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 30
Adjusted Rand Index: 0.15232592957920138
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5116856969724113
Average Adjusted Rand Index: 0.8224645276431304
11605.545322522208
[0.4889831440460554, 0.5116856969724113] [0.8367289978934241, 0.8224645276431304] [11730.206570829301, 11749.06131436496]
-------------------------------------
This iteration is 3
True Objective function: Loss = -11518.691343430504
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21607.014388369502
Iteration 100: Loss = -11513.939412369367
Iteration 200: Loss = -11512.358855593917
Iteration 300: Loss = -11512.095691738396
Iteration 400: Loss = -11511.978908734416
Iteration 500: Loss = -11511.9153955274
Iteration 600: Loss = -11511.876585307158
Iteration 700: Loss = -11511.850993251555
Iteration 800: Loss = -11511.833079626984
Iteration 900: Loss = -11511.819993075638
Iteration 1000: Loss = -11511.810133244177
Iteration 1100: Loss = -11511.802475388828
Iteration 1200: Loss = -11511.796448338371
Iteration 1300: Loss = -11511.7916162333
Iteration 1400: Loss = -11511.787688203647
Iteration 1500: Loss = -11511.784492434843
Iteration 1600: Loss = -11511.781769555888
Iteration 1700: Loss = -11511.77946740258
Iteration 1800: Loss = -11511.777473666425
Iteration 1900: Loss = -11511.775792297036
Iteration 2000: Loss = -11511.774358533434
Iteration 2100: Loss = -11511.773158782578
Iteration 2200: Loss = -11511.771940679911
Iteration 2300: Loss = -11511.770917207721
Iteration 2400: Loss = -11511.78326780618
1
Iteration 2500: Loss = -11511.771906384445
2
Iteration 2600: Loss = -11511.769006099332
Iteration 2700: Loss = -11511.772988983452
1
Iteration 2800: Loss = -11511.767364411387
Iteration 2900: Loss = -11511.766907045077
Iteration 3000: Loss = -11511.767652575314
1
Iteration 3100: Loss = -11511.773746805553
2
Iteration 3200: Loss = -11511.765601829671
Iteration 3300: Loss = -11511.765756153782
1
Iteration 3400: Loss = -11511.776162870872
2
Iteration 3500: Loss = -11511.764630610845
Iteration 3600: Loss = -11511.767710621089
1
Iteration 3700: Loss = -11511.764095021144
Iteration 3800: Loss = -11511.765697662666
1
Iteration 3900: Loss = -11511.770021703522
2
Iteration 4000: Loss = -11511.763486548763
Iteration 4100: Loss = -11511.763711840935
1
Iteration 4200: Loss = -11511.763128559169
Iteration 4300: Loss = -11511.762986076768
Iteration 4400: Loss = -11511.762892494558
Iteration 4500: Loss = -11511.76271822534
Iteration 4600: Loss = -11511.764337734072
1
Iteration 4700: Loss = -11511.762484855904
Iteration 4800: Loss = -11511.762336818496
Iteration 4900: Loss = -11511.765286948454
1
Iteration 5000: Loss = -11511.762121807471
Iteration 5100: Loss = -11511.762914523237
1
Iteration 5200: Loss = -11511.764497698145
2
Iteration 5300: Loss = -11511.761869231763
Iteration 5400: Loss = -11511.765949693474
1
Iteration 5500: Loss = -11511.763360981915
2
Iteration 5600: Loss = -11511.767482215899
3
Iteration 5700: Loss = -11511.761640076607
Iteration 5800: Loss = -11511.762211881749
1
Iteration 5900: Loss = -11511.761550661802
Iteration 6000: Loss = -11511.762278829056
1
Iteration 6100: Loss = -11511.762149588178
2
Iteration 6200: Loss = -11511.761564628887
Iteration 6300: Loss = -11511.761409589226
Iteration 6400: Loss = -11511.76139564984
Iteration 6500: Loss = -11511.761321170852
Iteration 6600: Loss = -11511.761264625948
Iteration 6700: Loss = -11511.761317627075
Iteration 6800: Loss = -11511.761574190321
1
Iteration 6900: Loss = -11511.762219445862
2
Iteration 7000: Loss = -11511.76111114875
Iteration 7100: Loss = -11511.761723497044
1
Iteration 7200: Loss = -11511.761055064631
Iteration 7300: Loss = -11511.761014297414
Iteration 7400: Loss = -11511.760995193476
Iteration 7500: Loss = -11511.760991134119
Iteration 7600: Loss = -11511.76141343178
1
Iteration 7700: Loss = -11511.765136903732
2
Iteration 7800: Loss = -11511.763343573668
3
Iteration 7900: Loss = -11511.761374891654
4
Iteration 8000: Loss = -11511.760890638903
Iteration 8100: Loss = -11511.760970410342
Iteration 8200: Loss = -11511.762019687363
1
Iteration 8300: Loss = -11511.760819372055
Iteration 8400: Loss = -11511.760968579805
1
Iteration 8500: Loss = -11511.760819534895
Iteration 8600: Loss = -11511.762566055671
1
Iteration 8700: Loss = -11511.760833897451
Iteration 8800: Loss = -11511.760811211967
Iteration 8900: Loss = -11511.771474067646
1
Iteration 9000: Loss = -11511.760713186786
Iteration 9100: Loss = -11511.824531784583
1
Iteration 9200: Loss = -11511.760702695274
Iteration 9300: Loss = -11511.769978170898
1
Iteration 9400: Loss = -11511.761032810677
2
Iteration 9500: Loss = -11511.787901618243
3
Iteration 9600: Loss = -11511.761890408705
4
Iteration 9700: Loss = -11511.765902939625
5
Iteration 9800: Loss = -11511.76874215717
6
Iteration 9900: Loss = -11511.774912649887
7
Iteration 10000: Loss = -11511.760750212958
Iteration 10100: Loss = -11511.768397759566
1
Iteration 10200: Loss = -11511.761534809879
2
Iteration 10300: Loss = -11511.772155637103
3
Iteration 10400: Loss = -11511.760898607772
4
Iteration 10500: Loss = -11511.795168673569
5
Iteration 10600: Loss = -11511.760779019527
Iteration 10700: Loss = -11511.761052307435
1
Iteration 10800: Loss = -11511.772837923976
2
Iteration 10900: Loss = -11511.762652195
3
Iteration 11000: Loss = -11511.761861651146
4
Iteration 11100: Loss = -11511.773987231798
5
Iteration 11200: Loss = -11511.762287602458
6
Iteration 11300: Loss = -11511.761186491975
7
Iteration 11400: Loss = -11511.762298083628
8
Iteration 11500: Loss = -11511.766699060054
9
Iteration 11600: Loss = -11511.76495869262
10
Iteration 11700: Loss = -11511.761067961394
11
Iteration 11800: Loss = -11511.764311052804
12
Iteration 11900: Loss = -11511.760993701291
13
Iteration 12000: Loss = -11511.7608629378
Iteration 12100: Loss = -11511.765782191074
1
Iteration 12200: Loss = -11511.905954083883
2
Iteration 12300: Loss = -11511.770006736893
3
Iteration 12400: Loss = -11511.76415390971
4
Iteration 12500: Loss = -11511.847430961454
5
Iteration 12600: Loss = -11511.762769411365
6
Iteration 12700: Loss = -11511.762769459488
7
Iteration 12800: Loss = -11511.76292171021
8
Iteration 12900: Loss = -11511.764982952462
9
Iteration 13000: Loss = -11511.77952268936
10
Iteration 13100: Loss = -11511.898426316242
11
Iteration 13200: Loss = -11511.770076803883
12
Iteration 13300: Loss = -11511.769202488236
13
Iteration 13400: Loss = -11511.765434192655
14
Iteration 13500: Loss = -11511.785150877557
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7483, 0.2517],
        [0.1995, 0.8005]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5110, 0.4890], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3930, 0.0996],
         [0.6438, 0.2009]],

        [[0.7244, 0.1000],
         [0.7283, 0.6743]],

        [[0.5080, 0.1115],
         [0.5631, 0.6847]],

        [[0.6846, 0.1011],
         [0.5805, 0.5943]],

        [[0.6959, 0.0962],
         [0.7254, 0.5556]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21674.250942079023
Iteration 100: Loss = -12209.202379585966
Iteration 200: Loss = -11514.726025803444
Iteration 300: Loss = -11512.713889638395
Iteration 400: Loss = -11512.30317028151
Iteration 500: Loss = -11512.132174928212
Iteration 600: Loss = -11512.034218415436
Iteration 700: Loss = -11511.971524957406
Iteration 800: Loss = -11511.928823583074
Iteration 900: Loss = -11511.89832218333
Iteration 1000: Loss = -11511.875619337303
Iteration 1100: Loss = -11511.858226860331
Iteration 1200: Loss = -11511.844477015955
Iteration 1300: Loss = -11511.83346922297
Iteration 1400: Loss = -11511.824505377053
Iteration 1500: Loss = -11511.817089727374
Iteration 1600: Loss = -11511.810869257242
Iteration 1700: Loss = -11511.80555468138
Iteration 1800: Loss = -11511.80102412151
Iteration 1900: Loss = -11511.797094159918
Iteration 2000: Loss = -11511.793689512953
Iteration 2100: Loss = -11511.790683989917
Iteration 2200: Loss = -11511.78810268797
Iteration 2300: Loss = -11511.785803759674
Iteration 2400: Loss = -11511.78375598078
Iteration 2500: Loss = -11511.781909693464
Iteration 2600: Loss = -11511.780282395974
Iteration 2700: Loss = -11511.778782236554
Iteration 2800: Loss = -11511.77741894748
Iteration 2900: Loss = -11511.776212371331
Iteration 3000: Loss = -11511.775038757769
Iteration 3100: Loss = -11511.774040893242
Iteration 3200: Loss = -11511.773020325125
Iteration 3300: Loss = -11511.772123428223
Iteration 3400: Loss = -11511.772511520567
1
Iteration 3500: Loss = -11511.77068151615
Iteration 3600: Loss = -11511.770053930455
Iteration 3700: Loss = -11511.769480795732
Iteration 3800: Loss = -11511.774889553742
1
Iteration 3900: Loss = -11511.768356028942
Iteration 4000: Loss = -11511.773000930509
1
Iteration 4100: Loss = -11511.767474095766
Iteration 4200: Loss = -11511.776432043464
1
Iteration 4300: Loss = -11511.766741423738
Iteration 4400: Loss = -11511.768445387781
1
Iteration 4500: Loss = -11511.772458423315
2
Iteration 4600: Loss = -11511.766730622494
Iteration 4700: Loss = -11511.765525516881
Iteration 4800: Loss = -11511.766905666871
1
Iteration 4900: Loss = -11511.764939447517
Iteration 5000: Loss = -11511.76501970316
Iteration 5100: Loss = -11511.76504597737
Iteration 5200: Loss = -11511.770781618256
1
Iteration 5300: Loss = -11511.764296576452
Iteration 5400: Loss = -11511.769217283387
1
Iteration 5500: Loss = -11511.7639121081
Iteration 5600: Loss = -11511.76422788929
1
Iteration 5700: Loss = -11511.763683173502
Iteration 5800: Loss = -11511.763833098421
1
Iteration 5900: Loss = -11511.763385744765
Iteration 6000: Loss = -11511.763314138652
Iteration 6100: Loss = -11511.763093359641
Iteration 6200: Loss = -11511.76303241443
Iteration 6300: Loss = -11511.763123132978
Iteration 6400: Loss = -11511.762844303412
Iteration 6500: Loss = -11511.762731159444
Iteration 6600: Loss = -11511.773584196259
1
Iteration 6700: Loss = -11511.76269091715
Iteration 6800: Loss = -11511.765358568366
1
Iteration 6900: Loss = -11511.76281078019
2
Iteration 7000: Loss = -11511.762679964666
Iteration 7100: Loss = -11511.763718221086
1
Iteration 7200: Loss = -11511.763245354488
2
Iteration 7300: Loss = -11511.763897257031
3
Iteration 7400: Loss = -11511.762133002825
Iteration 7500: Loss = -11511.76227813781
1
Iteration 7600: Loss = -11511.76200596048
Iteration 7700: Loss = -11511.761900036929
Iteration 7800: Loss = -11511.765813756372
1
Iteration 7900: Loss = -11511.763848525708
2
Iteration 8000: Loss = -11511.76157854004
Iteration 8100: Loss = -11511.762584769207
1
Iteration 8200: Loss = -11511.762128425995
2
Iteration 8300: Loss = -11511.763566243455
3
Iteration 8400: Loss = -11511.770482350406
4
Iteration 8500: Loss = -11511.782144633973
5
Iteration 8600: Loss = -11511.76139480702
Iteration 8700: Loss = -11511.761615854872
1
Iteration 8800: Loss = -11511.761331411855
Iteration 8900: Loss = -11511.761444148784
1
Iteration 9000: Loss = -11511.761579224123
2
Iteration 9100: Loss = -11511.764039932332
3
Iteration 9200: Loss = -11511.761633950948
4
Iteration 9300: Loss = -11511.7619008372
5
Iteration 9400: Loss = -11511.762720445682
6
Iteration 9500: Loss = -11511.76398965606
7
Iteration 9600: Loss = -11511.773960016091
8
Iteration 9700: Loss = -11511.803557953479
9
Iteration 9800: Loss = -11511.761342733882
Iteration 9900: Loss = -11511.765910802147
1
Iteration 10000: Loss = -11511.772944719909
2
Iteration 10100: Loss = -11511.765558569285
3
Iteration 10200: Loss = -11511.762569751818
4
Iteration 10300: Loss = -11511.76253300346
5
Iteration 10400: Loss = -11511.767285824735
6
Iteration 10500: Loss = -11511.781009240001
7
Iteration 10600: Loss = -11511.762322556377
8
Iteration 10700: Loss = -11511.761296980361
Iteration 10800: Loss = -11511.762232060935
1
Iteration 10900: Loss = -11511.775145524185
2
Iteration 11000: Loss = -11511.913469598932
3
Iteration 11100: Loss = -11511.761113483784
Iteration 11200: Loss = -11511.764074682935
1
Iteration 11300: Loss = -11511.765670139346
2
Iteration 11400: Loss = -11511.763322096755
3
Iteration 11500: Loss = -11511.888011348554
4
Iteration 11600: Loss = -11511.762317073433
5
Iteration 11700: Loss = -11511.762307782803
6
Iteration 11800: Loss = -11511.782478510428
7
Iteration 11900: Loss = -11511.762072722633
8
Iteration 12000: Loss = -11511.775007474545
9
Iteration 12100: Loss = -11511.762841805183
10
Iteration 12200: Loss = -11511.761878924457
11
Iteration 12300: Loss = -11511.761677856619
12
Iteration 12400: Loss = -11511.767306807047
13
Iteration 12500: Loss = -11511.764491413174
14
Iteration 12600: Loss = -11511.79373796768
15
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.7472, 0.2528],
        [0.2009, 0.7991]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5098, 0.4902], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3934, 0.0996],
         [0.7125, 0.2016]],

        [[0.7307, 0.1004],
         [0.7073, 0.6683]],

        [[0.5856, 0.1116],
         [0.5728, 0.5634]],

        [[0.6961, 0.1010],
         [0.6991, 0.6845]],

        [[0.6499, 0.0961],
         [0.6626, 0.6432]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
11518.691343430504
[0.9919999997943784, 0.9919999997943784] [0.9919993417272899, 0.9919993417272899] [11511.785150877557, 11511.79373796768]
-------------------------------------
This iteration is 4
True Objective function: Loss = -11655.353329662514
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21607.088844459566
Iteration 100: Loss = -12402.62416927303
Iteration 200: Loss = -12389.15357612981
Iteration 300: Loss = -12073.990429666128
Iteration 400: Loss = -11797.495496579226
Iteration 500: Loss = -11780.725730268816
Iteration 600: Loss = -11776.153157364939
Iteration 700: Loss = -11775.866160315058
Iteration 800: Loss = -11775.69807361309
Iteration 900: Loss = -11775.587882823578
Iteration 1000: Loss = -11775.507964402032
Iteration 1100: Loss = -11775.4444655409
Iteration 1200: Loss = -11775.400093768989
Iteration 1300: Loss = -11775.351894844332
Iteration 1400: Loss = -11773.755133467954
Iteration 1500: Loss = -11773.730781837796
Iteration 1600: Loss = -11773.689138608748
Iteration 1700: Loss = -11773.376899036684
Iteration 1800: Loss = -11773.363677500347
Iteration 1900: Loss = -11773.352778459443
Iteration 2000: Loss = -11773.343509197557
Iteration 2100: Loss = -11773.335403186049
Iteration 2200: Loss = -11773.328378872679
Iteration 2300: Loss = -11773.32208086676
Iteration 2400: Loss = -11773.3164281523
Iteration 2500: Loss = -11773.311123933885
Iteration 2600: Loss = -11773.307010320974
Iteration 2700: Loss = -11773.29961251896
Iteration 2800: Loss = -11773.29550516904
Iteration 2900: Loss = -11773.29361854735
Iteration 3000: Loss = -11773.289366689638
Iteration 3100: Loss = -11773.286765673625
Iteration 3200: Loss = -11773.284566244614
Iteration 3300: Loss = -11773.282268474815
Iteration 3400: Loss = -11773.280322375951
Iteration 3500: Loss = -11773.280766357871
1
Iteration 3600: Loss = -11773.276921164233
Iteration 3700: Loss = -11773.275424450712
Iteration 3800: Loss = -11773.278497383435
1
Iteration 3900: Loss = -11773.272725954726
Iteration 4000: Loss = -11773.27282808957
1
Iteration 4100: Loss = -11773.27125713546
Iteration 4200: Loss = -11773.271454924972
1
Iteration 4300: Loss = -11773.26844465181
Iteration 4400: Loss = -11773.274218745968
1
Iteration 4500: Loss = -11773.266632541145
Iteration 4600: Loss = -11773.26605316764
Iteration 4700: Loss = -11773.265036894103
Iteration 4800: Loss = -11773.264262181336
Iteration 4900: Loss = -11773.263897453478
Iteration 5000: Loss = -11773.260099959258
Iteration 5100: Loss = -11773.257718790474
Iteration 5200: Loss = -11773.257408481219
Iteration 5300: Loss = -11773.256530664592
Iteration 5400: Loss = -11773.257419416612
1
Iteration 5500: Loss = -11773.25549577167
Iteration 5600: Loss = -11773.255520159562
Iteration 5700: Loss = -11773.254706104855
Iteration 5800: Loss = -11773.25436518063
Iteration 5900: Loss = -11773.255199582622
1
Iteration 6000: Loss = -11773.2564517059
2
Iteration 6100: Loss = -11773.25341657631
Iteration 6200: Loss = -11773.253180809665
Iteration 6300: Loss = -11773.25798161909
1
Iteration 6400: Loss = -11773.25319928432
Iteration 6500: Loss = -11773.253922845313
1
Iteration 6600: Loss = -11773.25869726457
2
Iteration 6700: Loss = -11773.252046869242
Iteration 6800: Loss = -11773.252799401946
1
Iteration 6900: Loss = -11773.257975814859
2
Iteration 7000: Loss = -11773.251453161705
Iteration 7100: Loss = -11773.251384459585
Iteration 7200: Loss = -11773.256505620808
1
Iteration 7300: Loss = -11773.251074548421
Iteration 7400: Loss = -11773.251139830618
Iteration 7500: Loss = -11773.25141347293
1
Iteration 7600: Loss = -11773.281737734691
2
Iteration 7700: Loss = -11773.250538964676
Iteration 7800: Loss = -11773.251603313409
1
Iteration 7900: Loss = -11773.250499298549
Iteration 8000: Loss = -11773.263305486062
1
Iteration 8100: Loss = -11773.250159763811
Iteration 8200: Loss = -11773.250089236077
Iteration 8300: Loss = -11773.332073853682
1
Iteration 8400: Loss = -11773.249955003927
Iteration 8500: Loss = -11773.249802322613
Iteration 8600: Loss = -11773.250307018692
1
Iteration 8700: Loss = -11773.249686168907
Iteration 8800: Loss = -11773.250710357042
1
Iteration 8900: Loss = -11773.255662102914
2
Iteration 9000: Loss = -11773.253340144163
3
Iteration 9100: Loss = -11773.252672201425
4
Iteration 9200: Loss = -11773.249824976072
5
Iteration 9300: Loss = -11773.249091920086
Iteration 9400: Loss = -11773.253056526746
1
Iteration 9500: Loss = -11773.248991504275
Iteration 9600: Loss = -11773.24918231422
1
Iteration 9700: Loss = -11773.248878205319
Iteration 9800: Loss = -11773.250457159533
1
Iteration 9900: Loss = -11773.248813338167
Iteration 10000: Loss = -11773.376538177168
1
Iteration 10100: Loss = -11773.248742317443
Iteration 10200: Loss = -11773.303693193173
1
Iteration 10300: Loss = -11773.254095220998
2
Iteration 10400: Loss = -11773.25109155673
3
Iteration 10500: Loss = -11773.249470521598
4
Iteration 10600: Loss = -11773.249114403072
5
Iteration 10700: Loss = -11773.24942812006
6
Iteration 10800: Loss = -11773.251733288125
7
Iteration 10900: Loss = -11773.259261229601
8
Iteration 11000: Loss = -11773.365486124614
9
Iteration 11100: Loss = -11773.31890023738
10
Iteration 11200: Loss = -11773.254766860091
11
Iteration 11300: Loss = -11773.249137591438
12
Iteration 11400: Loss = -11773.250857179308
13
Iteration 11500: Loss = -11773.24992461311
14
Iteration 11600: Loss = -11773.282527756102
15
Stopping early at iteration 11600 due to no improvement.
pi: tensor([[0.7040, 0.2960],
        [0.4031, 0.5969]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4941, 0.5059], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2210, 0.0999],
         [0.6145, 0.3866]],

        [[0.7240, 0.1134],
         [0.6411, 0.7063]],

        [[0.7235, 0.1007],
         [0.6488, 0.7108]],

        [[0.5964, 0.0969],
         [0.6946, 0.6367]],

        [[0.5186, 0.0899],
         [0.6119, 0.6958]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 25
Adjusted Rand Index: 0.24435159688200234
Global Adjusted Rand Index: 0.48338696149095417
Average Adjusted Rand Index: 0.8408701313095369
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22525.714121168945
Iteration 100: Loss = -12402.157785808316
Iteration 200: Loss = -12375.896992775899
Iteration 300: Loss = -12108.632137821574
Iteration 400: Loss = -11875.272795622988
Iteration 500: Loss = -11741.731103048854
Iteration 600: Loss = -11704.622723371032
Iteration 700: Loss = -11668.273531324641
Iteration 800: Loss = -11654.750715499697
Iteration 900: Loss = -11650.791813409262
Iteration 1000: Loss = -11650.56068010916
Iteration 1100: Loss = -11650.366031507056
Iteration 1200: Loss = -11650.17239641149
Iteration 1300: Loss = -11650.069776775912
Iteration 1400: Loss = -11648.740925404165
Iteration 1500: Loss = -11648.582850098004
Iteration 1600: Loss = -11648.538479991557
Iteration 1700: Loss = -11648.49945014862
Iteration 1800: Loss = -11648.392416477012
Iteration 1900: Loss = -11648.277601936798
Iteration 2000: Loss = -11648.256994222955
Iteration 2100: Loss = -11648.24570960213
Iteration 2200: Loss = -11648.225025001839
Iteration 2300: Loss = -11648.212106114159
Iteration 2400: Loss = -11648.200819547024
Iteration 2500: Loss = -11648.194622094223
Iteration 2600: Loss = -11648.181906887812
Iteration 2700: Loss = -11648.17395817646
Iteration 2800: Loss = -11648.168519935392
Iteration 2900: Loss = -11648.160136387525
Iteration 3000: Loss = -11648.161506100232
1
Iteration 3100: Loss = -11648.148081901894
Iteration 3200: Loss = -11648.14188091528
Iteration 3300: Loss = -11648.133869663647
Iteration 3400: Loss = -11648.126366732697
Iteration 3500: Loss = -11648.120576908474
Iteration 3600: Loss = -11648.116342605346
Iteration 3700: Loss = -11648.112856583937
Iteration 3800: Loss = -11648.110059724859
Iteration 3900: Loss = -11648.107255377485
Iteration 4000: Loss = -11648.118520325655
1
Iteration 4100: Loss = -11648.102332372693
Iteration 4200: Loss = -11648.100174977693
Iteration 4300: Loss = -11648.103673983927
1
Iteration 4400: Loss = -11648.10347987172
2
Iteration 4500: Loss = -11648.094178640042
Iteration 4600: Loss = -11648.093758268624
Iteration 4700: Loss = -11648.090314575413
Iteration 4800: Loss = -11648.08829406663
Iteration 4900: Loss = -11648.086402494637
Iteration 5000: Loss = -11648.085027497886
Iteration 5100: Loss = -11648.084787914271
Iteration 5200: Loss = -11648.082707992526
Iteration 5300: Loss = -11648.08174167231
Iteration 5400: Loss = -11648.080979838975
Iteration 5500: Loss = -11648.079971681056
Iteration 5600: Loss = -11648.079275058322
Iteration 5700: Loss = -11648.078396377683
Iteration 5800: Loss = -11648.077676653957
Iteration 5900: Loss = -11648.077047517272
Iteration 6000: Loss = -11648.076363625698
Iteration 6100: Loss = -11648.075986208605
Iteration 6200: Loss = -11648.076221656931
1
Iteration 6300: Loss = -11648.077905362159
2
Iteration 6400: Loss = -11648.074219431646
Iteration 6500: Loss = -11648.074454756726
1
Iteration 6600: Loss = -11648.073492735117
Iteration 6700: Loss = -11648.072840750068
Iteration 6800: Loss = -11648.081151402195
1
Iteration 6900: Loss = -11648.076923393017
2
Iteration 7000: Loss = -11648.071714696942
Iteration 7100: Loss = -11648.071214143218
Iteration 7200: Loss = -11648.070724321375
Iteration 7300: Loss = -11648.070319948369
Iteration 7400: Loss = -11648.115829918212
1
Iteration 7500: Loss = -11648.070954354516
2
Iteration 7600: Loss = -11648.069132420886
Iteration 7700: Loss = -11648.08086007594
1
Iteration 7800: Loss = -11648.068197305714
Iteration 7900: Loss = -11648.06655248225
Iteration 8000: Loss = -11648.068829950742
1
Iteration 8100: Loss = -11648.069939741645
2
Iteration 8200: Loss = -11648.111204040293
3
Iteration 8300: Loss = -11648.068593083093
4
Iteration 8400: Loss = -11648.06535599834
Iteration 8500: Loss = -11648.065020742742
Iteration 8600: Loss = -11648.066573452832
1
Iteration 8700: Loss = -11648.076136644973
2
Iteration 8800: Loss = -11648.115652694367
3
Iteration 8900: Loss = -11648.06770758292
4
Iteration 9000: Loss = -11648.064307435294
Iteration 9100: Loss = -11648.064288062886
Iteration 9200: Loss = -11648.064710046392
1
Iteration 9300: Loss = -11648.0983032457
2
Iteration 9400: Loss = -11648.098789159345
3
Iteration 9500: Loss = -11648.064356067294
Iteration 9600: Loss = -11648.065457460249
1
Iteration 9700: Loss = -11648.064239807896
Iteration 9800: Loss = -11648.063421896168
Iteration 9900: Loss = -11648.151199581942
1
Iteration 10000: Loss = -11648.103809923294
2
Iteration 10100: Loss = -11648.085495298625
3
Iteration 10200: Loss = -11648.063187686987
Iteration 10300: Loss = -11648.062976561821
Iteration 10400: Loss = -11648.065960556083
1
Iteration 10500: Loss = -11648.062807539392
Iteration 10600: Loss = -11648.063468470831
1
Iteration 10700: Loss = -11648.145396797932
2
Iteration 10800: Loss = -11648.071829267054
3
Iteration 10900: Loss = -11648.06283664546
Iteration 11000: Loss = -11648.062679383364
Iteration 11100: Loss = -11648.067354427505
1
Iteration 11200: Loss = -11648.06273924809
Iteration 11300: Loss = -11648.062587877597
Iteration 11400: Loss = -11648.065220430008
1
Iteration 11500: Loss = -11648.083433597209
2
Iteration 11600: Loss = -11648.131862429584
3
Iteration 11700: Loss = -11648.083078096026
4
Iteration 11800: Loss = -11648.069431938855
5
Iteration 11900: Loss = -11648.06399890964
6
Iteration 12000: Loss = -11648.06577614966
7
Iteration 12100: Loss = -11648.062740161531
8
Iteration 12200: Loss = -11648.06274192901
9
Iteration 12300: Loss = -11648.079915474818
10
Iteration 12400: Loss = -11648.064145558423
11
Iteration 12500: Loss = -11648.062166665382
Iteration 12600: Loss = -11648.062371869426
1
Iteration 12700: Loss = -11648.075544282157
2
Iteration 12800: Loss = -11648.072986437617
3
Iteration 12900: Loss = -11648.07384311163
4
Iteration 13000: Loss = -11648.119812446268
5
Iteration 13100: Loss = -11648.067409896375
6
Iteration 13200: Loss = -11648.096053859335
7
Iteration 13300: Loss = -11648.063160163581
8
Iteration 13400: Loss = -11648.078937902303
9
Iteration 13500: Loss = -11648.062182682863
Iteration 13600: Loss = -11648.061984210837
Iteration 13700: Loss = -11648.062294585194
1
Iteration 13800: Loss = -11648.307468254648
2
Iteration 13900: Loss = -11648.061983112351
Iteration 14000: Loss = -11648.064604481697
1
Iteration 14100: Loss = -11648.063097089642
2
Iteration 14200: Loss = -11648.067995798296
3
Iteration 14300: Loss = -11648.065112082613
4
Iteration 14400: Loss = -11648.140506852293
5
Iteration 14500: Loss = -11648.06850583306
6
Iteration 14600: Loss = -11648.062731276976
7
Iteration 14700: Loss = -11648.116435601367
8
Iteration 14800: Loss = -11648.067384301274
9
Iteration 14900: Loss = -11648.065806500292
10
Iteration 15000: Loss = -11648.091065624372
11
Iteration 15100: Loss = -11648.062559807267
12
Iteration 15200: Loss = -11648.065414837407
13
Iteration 15300: Loss = -11648.063717127563
14
Iteration 15400: Loss = -11648.157123237277
15
Stopping early at iteration 15400 due to no improvement.
pi: tensor([[0.7621, 0.2379],
        [0.2512, 0.7488]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4991, 0.5009], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3921, 0.1010],
         [0.6836, 0.1966]],

        [[0.5858, 0.1147],
         [0.5160, 0.7038]],

        [[0.6059, 0.1016],
         [0.6616, 0.6738]],

        [[0.6166, 0.0981],
         [0.5418, 0.6096]],

        [[0.6377, 0.1049],
         [0.6675, 0.7017]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919998119331364
11655.353329662514
[0.48338696149095417, 0.99199998169963] [0.8408701313095369, 0.9919998119331364] [11773.282527756102, 11648.157123237277]
-------------------------------------
This iteration is 5
True Objective function: Loss = -11465.203812980853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22160.679259171393
Iteration 100: Loss = -12207.28080563371
Iteration 200: Loss = -12157.113786716114
Iteration 300: Loss = -11706.329019656156
Iteration 400: Loss = -11566.045848775495
Iteration 500: Loss = -11487.882743704293
Iteration 600: Loss = -11478.725781286039
Iteration 700: Loss = -11472.955895648352
Iteration 800: Loss = -11468.264045120155
Iteration 900: Loss = -11468.114101180849
Iteration 1000: Loss = -11466.962345157488
Iteration 1100: Loss = -11466.775532861306
Iteration 1200: Loss = -11465.685405185735
Iteration 1300: Loss = -11464.9863441848
Iteration 1400: Loss = -11462.58398331322
Iteration 1500: Loss = -11462.55502204223
Iteration 1600: Loss = -11462.53125689968
Iteration 1700: Loss = -11462.509522719512
Iteration 1800: Loss = -11462.467861799109
Iteration 1900: Loss = -11462.446561077644
Iteration 2000: Loss = -11462.443234524248
Iteration 2100: Loss = -11462.425369225175
Iteration 2200: Loss = -11462.416823000907
Iteration 2300: Loss = -11462.40931382459
Iteration 2400: Loss = -11462.402737423045
Iteration 2500: Loss = -11462.404365101756
1
Iteration 2600: Loss = -11462.391549171802
Iteration 2700: Loss = -11462.386797739207
Iteration 2800: Loss = -11462.382466119521
Iteration 2900: Loss = -11462.378570520206
Iteration 3000: Loss = -11462.374945115764
Iteration 3100: Loss = -11462.37165894387
Iteration 3200: Loss = -11462.376103875871
1
Iteration 3300: Loss = -11462.36601380837
Iteration 3400: Loss = -11462.363298757109
Iteration 3500: Loss = -11462.35990027634
Iteration 3600: Loss = -11462.323456057777
Iteration 3700: Loss = -11462.315641841898
Iteration 3800: Loss = -11462.31373911169
Iteration 3900: Loss = -11462.311918441696
Iteration 4000: Loss = -11462.3105343567
Iteration 4100: Loss = -11462.309127498393
Iteration 4200: Loss = -11462.30790901516
Iteration 4300: Loss = -11462.307687887142
Iteration 4400: Loss = -11462.306274966537
Iteration 4500: Loss = -11462.304905166544
Iteration 4600: Loss = -11462.306903485753
1
Iteration 4700: Loss = -11462.30292494365
Iteration 4800: Loss = -11462.304975997964
1
Iteration 4900: Loss = -11462.302331811627
Iteration 5000: Loss = -11462.300609613038
Iteration 5100: Loss = -11462.299911016555
Iteration 5200: Loss = -11462.2993624339
Iteration 5300: Loss = -11462.308403601708
1
Iteration 5400: Loss = -11462.297870608209
Iteration 5500: Loss = -11462.296139364715
Iteration 5600: Loss = -11462.295749916699
Iteration 5700: Loss = -11462.29939794864
1
Iteration 5800: Loss = -11462.29337224207
Iteration 5900: Loss = -11462.294648098152
1
Iteration 6000: Loss = -11462.29497554676
2
Iteration 6100: Loss = -11462.290556902604
Iteration 6200: Loss = -11462.290309024806
Iteration 6300: Loss = -11462.290240026854
Iteration 6400: Loss = -11462.289615987578
Iteration 6500: Loss = -11462.294062967994
1
Iteration 6600: Loss = -11462.289060498482
Iteration 6700: Loss = -11462.28883373698
Iteration 6800: Loss = -11462.288661889783
Iteration 6900: Loss = -11462.288395015285
Iteration 7000: Loss = -11462.300891931161
1
Iteration 7100: Loss = -11462.28828042324
Iteration 7200: Loss = -11462.287828183167
Iteration 7300: Loss = -11462.288878143154
1
Iteration 7400: Loss = -11462.287499005679
Iteration 7500: Loss = -11462.28819957762
1
Iteration 7600: Loss = -11462.287613590885
2
Iteration 7700: Loss = -11462.293542111958
3
Iteration 7800: Loss = -11462.287035264446
Iteration 7900: Loss = -11462.286786608998
Iteration 8000: Loss = -11462.30487398943
1
Iteration 8100: Loss = -11462.268648787045
Iteration 8200: Loss = -11462.2742724367
1
Iteration 8300: Loss = -11462.269194622895
2
Iteration 8400: Loss = -11462.27137058878
3
Iteration 8500: Loss = -11462.270182719107
4
Iteration 8600: Loss = -11462.268349298487
Iteration 8700: Loss = -11462.269766947305
1
Iteration 8800: Loss = -11462.26795335584
Iteration 8900: Loss = -11462.267988067895
Iteration 9000: Loss = -11462.270846872043
1
Iteration 9100: Loss = -11462.267774371829
Iteration 9200: Loss = -11462.318831572475
1
Iteration 9300: Loss = -11462.26962592968
2
Iteration 9400: Loss = -11462.269122336387
3
Iteration 9500: Loss = -11462.267712171482
Iteration 9600: Loss = -11462.26868949405
1
Iteration 9700: Loss = -11462.27023892458
2
Iteration 9800: Loss = -11462.267728553952
Iteration 9900: Loss = -11462.270737318955
1
Iteration 10000: Loss = -11462.269607601153
2
Iteration 10100: Loss = -11462.318612550085
3
Iteration 10200: Loss = -11462.269620952686
4
Iteration 10300: Loss = -11462.27957960352
5
Iteration 10400: Loss = -11462.268461017766
6
Iteration 10500: Loss = -11462.28120163944
7
Iteration 10600: Loss = -11462.279342256548
8
Iteration 10700: Loss = -11462.358976468007
9
Iteration 10800: Loss = -11462.281642853313
10
Iteration 10900: Loss = -11462.268314187026
11
Iteration 11000: Loss = -11462.272600194925
12
Iteration 11100: Loss = -11462.267320850713
Iteration 11200: Loss = -11462.278687973025
1
Iteration 11300: Loss = -11462.272147343849
2
Iteration 11400: Loss = -11462.27829164665
3
Iteration 11500: Loss = -11462.26938797495
4
Iteration 11600: Loss = -11462.269200528166
5
Iteration 11700: Loss = -11462.282145336363
6
Iteration 11800: Loss = -11462.26740808288
Iteration 11900: Loss = -11462.272482153194
1
Iteration 12000: Loss = -11462.270835829564
2
Iteration 12100: Loss = -11462.268388467066
3
Iteration 12200: Loss = -11462.270113581038
4
Iteration 12300: Loss = -11462.270229811826
5
Iteration 12400: Loss = -11462.279213344327
6
Iteration 12500: Loss = -11462.268068880609
7
Iteration 12600: Loss = -11462.277321856134
8
Iteration 12700: Loss = -11462.274747692298
9
Iteration 12800: Loss = -11462.318411810818
10
Iteration 12900: Loss = -11462.32662644236
11
Iteration 13000: Loss = -11462.26700960448
Iteration 13100: Loss = -11462.268739303856
1
Iteration 13200: Loss = -11462.272840810145
2
Iteration 13300: Loss = -11462.287249278914
3
Iteration 13400: Loss = -11462.271689297768
4
Iteration 13500: Loss = -11462.306796275147
5
Iteration 13600: Loss = -11462.27157461714
6
Iteration 13700: Loss = -11462.276997924346
7
Iteration 13800: Loss = -11462.28846759056
8
Iteration 13900: Loss = -11462.278442118675
9
Iteration 14000: Loss = -11462.274854607493
10
Iteration 14100: Loss = -11462.268268912514
11
Iteration 14200: Loss = -11462.273062642587
12
Iteration 14300: Loss = -11462.268303622546
13
Iteration 14400: Loss = -11462.28665139432
14
Iteration 14500: Loss = -11462.2680821533
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[0.7475, 0.2525],
        [0.2613, 0.7387]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4902, 0.5098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1922, 0.1005],
         [0.6618, 0.3960]],

        [[0.6627, 0.1003],
         [0.6862, 0.5815]],

        [[0.6517, 0.1020],
         [0.7212, 0.5587]],

        [[0.6168, 0.1013],
         [0.7133, 0.5196]],

        [[0.7136, 0.1002],
         [0.5216, 0.7207]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22125.979996611688
Iteration 100: Loss = -12206.014398947658
Iteration 200: Loss = -12181.706597449462
Iteration 300: Loss = -11881.990448041355
Iteration 400: Loss = -11807.802438216571
Iteration 500: Loss = -11780.881734909595
Iteration 600: Loss = -11764.764339481675
Iteration 700: Loss = -11756.11378787297
Iteration 800: Loss = -11756.025434144636
Iteration 900: Loss = -11755.96887918926
Iteration 1000: Loss = -11755.928919075732
Iteration 1100: Loss = -11755.89857419388
Iteration 1200: Loss = -11755.873418904923
Iteration 1300: Loss = -11755.845982163954
Iteration 1400: Loss = -11755.81118006018
Iteration 1500: Loss = -11755.776651743445
Iteration 1600: Loss = -11755.563231265027
Iteration 1700: Loss = -11749.4320227977
Iteration 1800: Loss = -11745.260225147587
Iteration 1900: Loss = -11744.934730046876
Iteration 2000: Loss = -11744.796557999895
Iteration 2100: Loss = -11743.575409602938
Iteration 2200: Loss = -11743.358896475142
Iteration 2300: Loss = -11739.180739497346
Iteration 2400: Loss = -11734.187085882593
Iteration 2500: Loss = -11731.273989966416
Iteration 2600: Loss = -11731.057011806439
Iteration 2700: Loss = -11731.01282326686
Iteration 2800: Loss = -11730.352678921314
Iteration 2900: Loss = -11730.240955741649
Iteration 3000: Loss = -11730.227504441704
Iteration 3100: Loss = -11730.21983482975
Iteration 3200: Loss = -11730.23071313931
1
Iteration 3300: Loss = -11730.217079057742
Iteration 3400: Loss = -11730.216072446226
Iteration 3500: Loss = -11730.230994863794
1
Iteration 3600: Loss = -11730.214151557211
Iteration 3700: Loss = -11730.213357666382
Iteration 3800: Loss = -11730.212794361027
Iteration 3900: Loss = -11730.212031418812
Iteration 4000: Loss = -11730.211637838273
Iteration 4100: Loss = -11730.210938987047
Iteration 4200: Loss = -11730.210494017781
Iteration 4300: Loss = -11730.230822127158
1
Iteration 4400: Loss = -11730.21137215635
2
Iteration 4500: Loss = -11730.23272742465
3
Iteration 4600: Loss = -11730.208897003937
Iteration 4700: Loss = -11730.210370771234
1
Iteration 4800: Loss = -11730.208241508988
Iteration 4900: Loss = -11730.207984954095
Iteration 5000: Loss = -11730.207719728287
Iteration 5100: Loss = -11730.207447391713
Iteration 5200: Loss = -11730.20728047983
Iteration 5300: Loss = -11730.207040267718
Iteration 5400: Loss = -11730.20855402565
1
Iteration 5500: Loss = -11730.206647843928
Iteration 5600: Loss = -11730.206534317573
Iteration 5700: Loss = -11730.206330099656
Iteration 5800: Loss = -11730.21056264896
1
Iteration 5900: Loss = -11730.206023442197
Iteration 6000: Loss = -11730.205854274816
Iteration 6100: Loss = -11730.205790642496
Iteration 6200: Loss = -11730.205674051751
Iteration 6300: Loss = -11730.205553497783
Iteration 6400: Loss = -11730.205558768557
Iteration 6500: Loss = -11730.205360385075
Iteration 6600: Loss = -11730.205269814483
Iteration 6700: Loss = -11730.205163962606
Iteration 6800: Loss = -11730.20507176464
Iteration 6900: Loss = -11730.232217134046
1
Iteration 7000: Loss = -11730.204925723663
Iteration 7100: Loss = -11730.204867720202
Iteration 7200: Loss = -11730.20537240395
1
Iteration 7300: Loss = -11730.204730118074
Iteration 7400: Loss = -11730.204681951998
Iteration 7500: Loss = -11730.20957530958
1
Iteration 7600: Loss = -11730.204614450487
Iteration 7700: Loss = -11730.207105528687
1
Iteration 7800: Loss = -11730.204511339314
Iteration 7900: Loss = -11730.20682728658
1
Iteration 8000: Loss = -11730.204391820824
Iteration 8100: Loss = -11730.264995329677
1
Iteration 8200: Loss = -11730.204324095042
Iteration 8300: Loss = -11730.204266982922
Iteration 8400: Loss = -11730.204187038156
Iteration 8500: Loss = -11730.203887241198
Iteration 8600: Loss = -11730.222153725683
1
Iteration 8700: Loss = -11730.20380145796
Iteration 8800: Loss = -11730.240174829774
1
Iteration 8900: Loss = -11730.205080010162
2
Iteration 9000: Loss = -11730.198245760794
Iteration 9100: Loss = -11730.198441419618
1
Iteration 9200: Loss = -11730.250866357152
2
Iteration 9300: Loss = -11730.198173322024
Iteration 9400: Loss = -11730.1995040012
1
Iteration 9500: Loss = -11730.198133451524
Iteration 9600: Loss = -11730.201780609781
1
Iteration 9700: Loss = -11730.197977854605
Iteration 9800: Loss = -11730.20001739817
1
Iteration 9900: Loss = -11730.282726773677
2
Iteration 10000: Loss = -11730.207859668086
3
Iteration 10100: Loss = -11730.200328260182
4
Iteration 10200: Loss = -11730.197858739404
Iteration 10300: Loss = -11730.200289554186
1
Iteration 10400: Loss = -11730.21627351456
2
Iteration 10500: Loss = -11730.197814170446
Iteration 10600: Loss = -11730.197817224858
Iteration 10700: Loss = -11730.206652863693
1
Iteration 10800: Loss = -11730.202398643863
2
Iteration 10900: Loss = -11730.196838571754
Iteration 11000: Loss = -11730.202826429804
1
Iteration 11100: Loss = -11730.203617233567
2
Iteration 11200: Loss = -11730.291334801092
3
Iteration 11300: Loss = -11730.196819297795
Iteration 11400: Loss = -11730.196890276327
Iteration 11500: Loss = -11730.19926103685
1
Iteration 11600: Loss = -11730.223953774723
2
Iteration 11700: Loss = -11730.250863407347
3
Iteration 11800: Loss = -11730.181190959907
Iteration 11900: Loss = -11730.181678952016
1
Iteration 12000: Loss = -11730.169106761923
Iteration 12100: Loss = -11730.169180523146
Iteration 12200: Loss = -11730.18746174632
1
Iteration 12300: Loss = -11730.16896696287
Iteration 12400: Loss = -11730.168310614588
Iteration 12500: Loss = -11730.175706065593
1
Iteration 12600: Loss = -11730.172585783755
2
Iteration 12700: Loss = -11730.168224638415
Iteration 12800: Loss = -11730.19636609123
1
Iteration 12900: Loss = -11730.1680508541
Iteration 13000: Loss = -11730.168975420569
1
Iteration 13100: Loss = -11730.177046039662
2
Iteration 13200: Loss = -11730.212240657745
3
Iteration 13300: Loss = -11730.168647539143
4
Iteration 13400: Loss = -11730.16817695646
5
Iteration 13500: Loss = -11730.169012732873
6
Iteration 13600: Loss = -11730.169353754954
7
Iteration 13700: Loss = -11730.178066828503
8
Iteration 13800: Loss = -11730.169236198988
9
Iteration 13900: Loss = -11730.172469604775
10
Iteration 14000: Loss = -11730.1814647538
11
Iteration 14100: Loss = -11730.168208274486
12
Iteration 14200: Loss = -11730.170698980935
13
Iteration 14300: Loss = -11730.179914460623
14
Iteration 14400: Loss = -11730.170913089916
15
Stopping early at iteration 14400 due to no improvement.
pi: tensor([[0.6749, 0.3251],
        [0.2989, 0.7011]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2511, 0.7489], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3803, 0.1072],
         [0.5826, 0.2214]],

        [[0.6255, 0.1023],
         [0.6763, 0.6445]],

        [[0.5343, 0.1020],
         [0.5161, 0.7287]],

        [[0.5220, 0.1006],
         [0.7018, 0.6048]],

        [[0.7021, 0.1002],
         [0.5315, 0.5448]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 76
Adjusted Rand Index: 0.26475869397925905
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 68
Adjusted Rand Index: 0.12080808080808081
time is 2
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.17819164276297333
Average Adjusted Rand Index: 0.6771133549574679
11465.203812980853
[1.0, 0.17819164276297333] [1.0, 0.6771133549574679] [11462.2680821533, 11730.170913089916]
-------------------------------------
This iteration is 6
True Objective function: Loss = -11556.441076620691
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22041.507559066613
Iteration 100: Loss = -11824.885428638669
Iteration 200: Loss = -11819.062066674833
Iteration 300: Loss = -11818.49153396457
Iteration 400: Loss = -11818.400960778701
Iteration 500: Loss = -11818.353076459078
Iteration 600: Loss = -11818.321026143463
Iteration 700: Loss = -11818.298430900375
Iteration 800: Loss = -11818.28543163307
Iteration 900: Loss = -11818.276053053005
Iteration 1000: Loss = -11818.269021691842
Iteration 1100: Loss = -11818.263587735548
Iteration 1200: Loss = -11818.259324715009
Iteration 1300: Loss = -11818.255877858126
Iteration 1400: Loss = -11818.253124541234
Iteration 1500: Loss = -11818.250799689486
Iteration 1600: Loss = -11818.248835389666
Iteration 1700: Loss = -11818.247189817994
Iteration 1800: Loss = -11818.24575492174
Iteration 1900: Loss = -11818.2445453927
Iteration 2000: Loss = -11818.249066639484
1
Iteration 2100: Loss = -11818.242569322938
Iteration 2200: Loss = -11818.24176048769
Iteration 2300: Loss = -11818.249367302044
1
Iteration 2400: Loss = -11818.24744413184
2
Iteration 2500: Loss = -11818.239844611637
Iteration 2600: Loss = -11818.24070009118
1
Iteration 2700: Loss = -11818.23900859951
Iteration 2800: Loss = -11818.238520964624
Iteration 2900: Loss = -11818.238157872973
Iteration 3000: Loss = -11818.237772134522
Iteration 3100: Loss = -11818.23764702112
Iteration 3200: Loss = -11818.237418677261
Iteration 3300: Loss = -11818.236927373559
Iteration 3400: Loss = -11818.236678175841
Iteration 3500: Loss = -11818.239093763394
1
Iteration 3600: Loss = -11818.236575499477
Iteration 3700: Loss = -11818.236082011796
Iteration 3800: Loss = -11818.236106321307
Iteration 3900: Loss = -11818.235940359966
Iteration 4000: Loss = -11818.23793840115
1
Iteration 4100: Loss = -11818.235501296813
Iteration 4200: Loss = -11818.236233513671
1
Iteration 4300: Loss = -11818.235340527837
Iteration 4400: Loss = -11818.23538090447
Iteration 4500: Loss = -11818.235047796405
Iteration 4600: Loss = -11818.234995981546
Iteration 4700: Loss = -11818.2540099106
1
Iteration 4800: Loss = -11818.2348170131
Iteration 4900: Loss = -11818.235264649471
1
Iteration 5000: Loss = -11818.234693513534
Iteration 5100: Loss = -11818.234640134857
Iteration 5200: Loss = -11818.237006407273
1
Iteration 5300: Loss = -11818.23453130038
Iteration 5400: Loss = -11818.234595427319
Iteration 5500: Loss = -11818.235108224415
1
Iteration 5600: Loss = -11818.241302980014
2
Iteration 5700: Loss = -11818.236164254167
3
Iteration 5800: Loss = -11818.23427944136
Iteration 5900: Loss = -11818.234365816843
Iteration 6000: Loss = -11818.2341884549
Iteration 6100: Loss = -11818.234244207244
Iteration 6200: Loss = -11818.234115146308
Iteration 6300: Loss = -11818.234670682807
1
Iteration 6400: Loss = -11818.23409574648
Iteration 6500: Loss = -11818.23487180921
1
Iteration 6600: Loss = -11818.233981315016
Iteration 6700: Loss = -11818.234160199218
1
Iteration 6800: Loss = -11818.233983625796
Iteration 6900: Loss = -11818.234028528688
Iteration 7000: Loss = -11818.23391303365
Iteration 7100: Loss = -11818.234904072768
1
Iteration 7200: Loss = -11818.23392639481
Iteration 7300: Loss = -11818.236292886837
1
Iteration 7400: Loss = -11818.233908931059
Iteration 7500: Loss = -11818.233860075256
Iteration 7600: Loss = -11818.23450830328
1
Iteration 7700: Loss = -11818.23384910659
Iteration 7800: Loss = -11818.236752107881
1
Iteration 7900: Loss = -11818.233837406591
Iteration 8000: Loss = -11818.233790612221
Iteration 8100: Loss = -11818.233983672399
1
Iteration 8200: Loss = -11818.233896144342
2
Iteration 8300: Loss = -11818.233778629286
Iteration 8400: Loss = -11818.265046552715
1
Iteration 8500: Loss = -11818.233770619296
Iteration 8600: Loss = -11818.235906689992
1
Iteration 8700: Loss = -11818.23373720767
Iteration 8800: Loss = -11818.233859370795
1
Iteration 8900: Loss = -11818.338373949231
2
Iteration 9000: Loss = -11818.233734703435
Iteration 9100: Loss = -11818.233719231743
Iteration 9200: Loss = -11818.240750279501
1
Iteration 9300: Loss = -11818.233683003085
Iteration 9400: Loss = -11818.233650802396
Iteration 9500: Loss = -11818.252329583676
1
Iteration 9600: Loss = -11818.233671043217
Iteration 9700: Loss = -11818.233671494925
Iteration 9800: Loss = -11818.260580215274
1
Iteration 9900: Loss = -11818.233700295312
Iteration 10000: Loss = -11818.233666207803
Iteration 10100: Loss = -11818.234530862506
1
Iteration 10200: Loss = -11818.238415277872
2
Iteration 10300: Loss = -11818.234157221668
3
Iteration 10400: Loss = -11818.238810439134
4
Iteration 10500: Loss = -11818.23633381869
5
Iteration 10600: Loss = -11818.271002081794
6
Iteration 10700: Loss = -11818.23862101689
7
Iteration 10800: Loss = -11818.233837734246
8
Iteration 10900: Loss = -11818.23961044449
9
Iteration 11000: Loss = -11818.33666305342
10
Iteration 11100: Loss = -11818.233867785215
11
Iteration 11200: Loss = -11818.234373205727
12
Iteration 11300: Loss = -11818.233719474161
Iteration 11400: Loss = -11818.237259609972
1
Iteration 11500: Loss = -11818.233692060998
Iteration 11600: Loss = -11818.263458774978
1
Iteration 11700: Loss = -11818.2404157696
2
Iteration 11800: Loss = -11818.233731639872
Iteration 11900: Loss = -11818.241764044355
1
Iteration 12000: Loss = -11818.335300453462
2
Iteration 12100: Loss = -11818.237494640976
3
Iteration 12200: Loss = -11818.235395739906
4
Iteration 12300: Loss = -11818.43566241786
5
Iteration 12400: Loss = -11818.234917401574
6
Iteration 12500: Loss = -11818.234508058023
7
Iteration 12600: Loss = -11818.272299620812
8
Iteration 12700: Loss = -11818.238097039712
9
Iteration 12800: Loss = -11818.23594536552
10
Iteration 12900: Loss = -11818.234810430335
11
Iteration 13000: Loss = -11818.237373948083
12
Iteration 13100: Loss = -11818.23390235446
13
Iteration 13200: Loss = -11818.319362045802
14
Iteration 13300: Loss = -11818.233948488865
15
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[0.7270, 0.2730],
        [0.4310, 0.5690]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5059, 0.4941], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2531, 0.0897],
         [0.6529, 0.3639]],

        [[0.5540, 0.0992],
         [0.5575, 0.6061]],

        [[0.6813, 0.1033],
         [0.5684, 0.6089]],

        [[0.6536, 0.0930],
         [0.6262, 0.5094]],

        [[0.6159, 0.0968],
         [0.6771, 0.6607]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 19
Adjusted Rand Index: 0.37903334270727623
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 16
Adjusted Rand Index: 0.45744585590509607
Global Adjusted Rand Index: 0.10847537373176028
Average Adjusted Rand Index: 0.7514564629534624
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22165.996026238114
Iteration 100: Loss = -12373.559690709351
Iteration 200: Loss = -12264.443408783796
Iteration 300: Loss = -11833.778742348923
Iteration 400: Loss = -11786.011583064002
Iteration 500: Loss = -11700.279137945383
Iteration 600: Loss = -11652.367636806886
Iteration 700: Loss = -11621.95901714505
Iteration 800: Loss = -11613.594494109178
Iteration 900: Loss = -11597.354394151154
Iteration 1000: Loss = -11589.683983508876
Iteration 1100: Loss = -11589.482565289418
Iteration 1200: Loss = -11571.63519635618
Iteration 1300: Loss = -11571.463011860855
Iteration 1400: Loss = -11565.251325000118
Iteration 1500: Loss = -11565.185211525451
Iteration 1600: Loss = -11552.10184018563
Iteration 1700: Loss = -11552.050737456671
Iteration 1800: Loss = -11552.015437986245
Iteration 1900: Loss = -11551.994384169355
Iteration 2000: Loss = -11551.977566879328
Iteration 2100: Loss = -11551.963435561242
Iteration 2200: Loss = -11551.95136380832
Iteration 2300: Loss = -11551.94098175732
Iteration 2400: Loss = -11551.931986273028
Iteration 2500: Loss = -11551.92418206687
Iteration 2600: Loss = -11551.917472533227
Iteration 2700: Loss = -11551.91550783967
Iteration 2800: Loss = -11551.906246538694
Iteration 2900: Loss = -11551.90146915337
Iteration 3000: Loss = -11551.89720691395
Iteration 3100: Loss = -11551.893750354693
Iteration 3200: Loss = -11551.891214263982
Iteration 3300: Loss = -11551.886762716615
Iteration 3400: Loss = -11551.898612331124
1
Iteration 3500: Loss = -11551.881397639738
Iteration 3600: Loss = -11551.87933891972
Iteration 3700: Loss = -11551.87690281761
Iteration 3800: Loss = -11551.87517896737
Iteration 3900: Loss = -11551.873170275556
Iteration 4000: Loss = -11551.872431905285
Iteration 4100: Loss = -11551.87022374793
Iteration 4200: Loss = -11551.868638461885
Iteration 4300: Loss = -11551.87859258359
1
Iteration 4400: Loss = -11551.866140755494
Iteration 4500: Loss = -11551.865255197232
Iteration 4600: Loss = -11551.863986904484
Iteration 4700: Loss = -11551.863268338991
Iteration 4800: Loss = -11551.862446456598
Iteration 4900: Loss = -11551.864871520784
1
Iteration 5000: Loss = -11551.860705544994
Iteration 5100: Loss = -11551.86106629749
1
Iteration 5200: Loss = -11551.859124081857
Iteration 5300: Loss = -11551.858463543176
Iteration 5400: Loss = -11551.85806647077
Iteration 5500: Loss = -11551.857295918118
Iteration 5600: Loss = -11551.857323436088
Iteration 5700: Loss = -11551.858138423368
1
Iteration 5800: Loss = -11551.856319983832
Iteration 5900: Loss = -11551.855433123514
Iteration 6000: Loss = -11551.855128169762
Iteration 6100: Loss = -11551.86064663439
1
Iteration 6200: Loss = -11551.85446641473
Iteration 6300: Loss = -11551.854007060925
Iteration 6400: Loss = -11551.853887848802
Iteration 6500: Loss = -11551.853317448675
Iteration 6600: Loss = -11551.853366864247
Iteration 6700: Loss = -11551.8556721427
1
Iteration 6800: Loss = -11551.854718980281
2
Iteration 6900: Loss = -11551.852384140422
Iteration 7000: Loss = -11551.852159110704
Iteration 7100: Loss = -11551.853544791678
1
Iteration 7200: Loss = -11551.842309886753
Iteration 7300: Loss = -11551.839229431907
Iteration 7400: Loss = -11551.84351336505
1
Iteration 7500: Loss = -11551.846347110131
2
Iteration 7600: Loss = -11551.840520697298
3
Iteration 7700: Loss = -11551.838901762128
Iteration 7800: Loss = -11551.957955913469
1
Iteration 7900: Loss = -11551.83813742041
Iteration 8000: Loss = -11551.84241437167
1
Iteration 8100: Loss = -11551.8410196719
2
Iteration 8200: Loss = -11551.837810130877
Iteration 8300: Loss = -11551.837735955676
Iteration 8400: Loss = -11551.85282084889
1
Iteration 8500: Loss = -11551.837470046614
Iteration 8600: Loss = -11551.847196457686
1
Iteration 8700: Loss = -11551.83729556361
Iteration 8800: Loss = -11551.842560828754
1
Iteration 8900: Loss = -11551.83715236266
Iteration 9000: Loss = -11551.837240094545
Iteration 9100: Loss = -11551.837017417613
Iteration 9200: Loss = -11551.857713467687
1
Iteration 9300: Loss = -11551.85279190027
2
Iteration 9400: Loss = -11551.836822406467
Iteration 9500: Loss = -11551.838219243813
1
Iteration 9600: Loss = -11551.843800633333
2
Iteration 9700: Loss = -11551.840262416099
3
Iteration 9800: Loss = -11551.849900015965
4
Iteration 9900: Loss = -11551.84411885949
5
Iteration 10000: Loss = -11551.854197746472
6
Iteration 10100: Loss = -11551.83784185994
7
Iteration 10200: Loss = -11551.840461663427
8
Iteration 10300: Loss = -11551.838443983252
9
Iteration 10400: Loss = -11551.840042835951
10
Iteration 10500: Loss = -11551.836660998073
Iteration 10600: Loss = -11551.838199421969
1
Iteration 10700: Loss = -11551.837608012465
2
Iteration 10800: Loss = -11551.83727985532
3
Iteration 10900: Loss = -11551.83658099556
Iteration 11000: Loss = -11551.8368849449
1
Iteration 11100: Loss = -11551.836926960028
2
Iteration 11200: Loss = -11551.838456345793
3
Iteration 11300: Loss = -11551.836748919875
4
Iteration 11400: Loss = -11551.847743192222
5
Iteration 11500: Loss = -11551.8423997132
6
Iteration 11600: Loss = -11551.837449264141
7
Iteration 11700: Loss = -11551.853636109166
8
Iteration 11800: Loss = -11551.866481830244
9
Iteration 11900: Loss = -11551.83820049017
10
Iteration 12000: Loss = -11551.836901926323
11
Iteration 12100: Loss = -11551.869201477062
12
Iteration 12200: Loss = -11551.840635592853
13
Iteration 12300: Loss = -11551.83771099222
14
Iteration 12400: Loss = -11551.83637620725
Iteration 12500: Loss = -11551.842498170996
1
Iteration 12600: Loss = -11551.849431419409
2
Iteration 12700: Loss = -11551.83872617
3
Iteration 12800: Loss = -11551.874226494321
4
Iteration 12900: Loss = -11551.850475622006
5
Iteration 13000: Loss = -11551.867359912198
6
Iteration 13100: Loss = -11551.857161600454
7
Iteration 13200: Loss = -11551.84922761557
8
Iteration 13300: Loss = -11551.836753270516
9
Iteration 13400: Loss = -11551.84384294834
10
Iteration 13500: Loss = -11551.841163456307
11
Iteration 13600: Loss = -11551.83976837517
12
Iteration 13700: Loss = -11551.837595229488
13
Iteration 13800: Loss = -11551.851706746093
14
Iteration 13900: Loss = -11551.862338522025
15
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[0.7316, 0.2684],
        [0.2481, 0.7519]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.0900],
         [0.6434, 0.4001]],

        [[0.5733, 0.0993],
         [0.6513, 0.6020]],

        [[0.5835, 0.1055],
         [0.6176, 0.6602]],

        [[0.5292, 0.0983],
         [0.5106, 0.5612]],

        [[0.6806, 0.1014],
         [0.6369, 0.5074]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320099978856
Average Adjusted Rand Index: 0.9839985580570193
11556.441076620691
[0.10847537373176028, 0.9840320099978856] [0.7514564629534624, 0.9839985580570193] [11818.233948488865, 11551.862338522025]
-------------------------------------
This iteration is 7
True Objective function: Loss = -11802.016312980853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22517.98694290101
Iteration 100: Loss = -12688.364713652121
Iteration 200: Loss = -12340.81355887733
Iteration 300: Loss = -12126.807643847163
Iteration 400: Loss = -12100.539668361023
Iteration 500: Loss = -12099.994582151065
Iteration 600: Loss = -12099.725317318707
Iteration 700: Loss = -12099.593469809553
Iteration 800: Loss = -12099.509708530117
Iteration 900: Loss = -12099.451557408454
Iteration 1000: Loss = -12099.409090058705
Iteration 1100: Loss = -12099.37686032522
Iteration 1200: Loss = -12099.351715226927
Iteration 1300: Loss = -12099.33188817028
Iteration 1400: Loss = -12099.315954783373
Iteration 1500: Loss = -12099.302904999993
Iteration 1600: Loss = -12099.292069177718
Iteration 1700: Loss = -12099.282930819083
Iteration 1800: Loss = -12099.275202676212
Iteration 1900: Loss = -12099.268562028452
Iteration 2000: Loss = -12099.262805819399
Iteration 2100: Loss = -12099.257799394882
Iteration 2200: Loss = -12099.277863714127
1
Iteration 2300: Loss = -12099.249615597519
Iteration 2400: Loss = -12099.246143251827
Iteration 2500: Loss = -12099.243104096531
Iteration 2600: Loss = -12099.240432712595
Iteration 2700: Loss = -12099.237894371323
Iteration 2800: Loss = -12099.236246049228
Iteration 2900: Loss = -12099.233695437886
Iteration 3000: Loss = -12099.248001173062
1
Iteration 3100: Loss = -12099.228762688044
Iteration 3200: Loss = -12099.179608581107
Iteration 3300: Loss = -12099.178154838322
Iteration 3400: Loss = -12099.176426732696
Iteration 3500: Loss = -12099.175101556682
Iteration 3600: Loss = -12099.17487246727
Iteration 3700: Loss = -12099.17217172678
Iteration 3800: Loss = -12099.170625876379
Iteration 3900: Loss = -12099.16850505378
Iteration 4000: Loss = -12099.169184442091
1
Iteration 4100: Loss = -12099.184273736097
2
Iteration 4200: Loss = -12099.177091751355
3
Iteration 4300: Loss = -12099.165720547291
Iteration 4400: Loss = -12099.165243512849
Iteration 4500: Loss = -12099.165809625381
1
Iteration 4600: Loss = -12099.164289309592
Iteration 4700: Loss = -12099.16369543764
Iteration 4800: Loss = -12099.164301885403
1
Iteration 4900: Loss = -12099.162891329446
Iteration 5000: Loss = -12099.16244245194
Iteration 5100: Loss = -12099.162363198047
Iteration 5200: Loss = -12099.161821261267
Iteration 5300: Loss = -12099.164239537431
1
Iteration 5400: Loss = -12099.161241527721
Iteration 5500: Loss = -12099.16402821632
1
Iteration 5600: Loss = -12099.163142890684
2
Iteration 5700: Loss = -12099.160483372998
Iteration 5800: Loss = -12099.160279135254
Iteration 5900: Loss = -12099.160032262858
Iteration 6000: Loss = -12099.16055828675
1
Iteration 6100: Loss = -12099.159652927965
Iteration 6200: Loss = -12099.161121207035
1
Iteration 6300: Loss = -12099.159259640497
Iteration 6400: Loss = -12099.159217992086
Iteration 6500: Loss = -12099.159425527014
1
Iteration 6600: Loss = -12099.160526637703
2
Iteration 6700: Loss = -12099.16099182722
3
Iteration 6800: Loss = -12099.159591125595
4
Iteration 6900: Loss = -12099.159070250244
Iteration 7000: Loss = -12099.16654607268
1
Iteration 7100: Loss = -12099.155142064108
Iteration 7200: Loss = -12099.195970712262
1
Iteration 7300: Loss = -12099.154887825684
Iteration 7400: Loss = -12099.154805258517
Iteration 7500: Loss = -12099.15839677332
1
Iteration 7600: Loss = -12099.154602138093
Iteration 7700: Loss = -12099.153367293102
Iteration 7800: Loss = -12099.136900815643
Iteration 7900: Loss = -12099.134687765345
Iteration 8000: Loss = -12099.134599597692
Iteration 8100: Loss = -12099.150031572592
1
Iteration 8200: Loss = -12099.1344936642
Iteration 8300: Loss = -12099.13443175466
Iteration 8400: Loss = -12099.425603981908
1
Iteration 8500: Loss = -12099.134361403283
Iteration 8600: Loss = -12099.134296369415
Iteration 8700: Loss = -12099.134238402186
Iteration 8800: Loss = -12099.134236551301
Iteration 8900: Loss = -12099.13417337608
Iteration 9000: Loss = -12099.386441620158
1
Iteration 9100: Loss = -12099.134102494187
Iteration 9200: Loss = -12099.134068450474
Iteration 9300: Loss = -12099.155589680551
1
Iteration 9400: Loss = -12099.133987225927
Iteration 9500: Loss = -12099.15683283902
1
Iteration 9600: Loss = -12099.138059037046
2
Iteration 9700: Loss = -12099.140913861404
3
Iteration 9800: Loss = -12099.133885690446
Iteration 9900: Loss = -12099.144435582106
1
Iteration 10000: Loss = -12099.133848172081
Iteration 10100: Loss = -12099.144175920306
1
Iteration 10200: Loss = -12099.192034136358
2
Iteration 10300: Loss = -12099.135594208768
3
Iteration 10400: Loss = -12099.136331367263
4
Iteration 10500: Loss = -12099.134454923204
5
Iteration 10600: Loss = -12099.137388085539
6
Iteration 10700: Loss = -12099.136429589274
7
Iteration 10800: Loss = -12099.13377810496
Iteration 10900: Loss = -12099.134740241958
1
Iteration 11000: Loss = -12099.134669362824
2
Iteration 11100: Loss = -12099.133679979675
Iteration 11200: Loss = -12099.389710578314
1
Iteration 11300: Loss = -12099.13370513267
Iteration 11400: Loss = -12099.153835769057
1
Iteration 11500: Loss = -12099.154169613084
2
Iteration 11600: Loss = -12099.13491505216
3
Iteration 11700: Loss = -12099.133648082085
Iteration 11800: Loss = -12099.147320849135
1
Iteration 11900: Loss = -12099.133635784287
Iteration 12000: Loss = -12099.136341084059
1
Iteration 12100: Loss = -12099.226060339039
2
Iteration 12200: Loss = -12099.137867194751
3
Iteration 12300: Loss = -12099.133940640033
4
Iteration 12400: Loss = -12099.144607097129
5
Iteration 12500: Loss = -12099.150722911294
6
Iteration 12600: Loss = -12099.133615173014
Iteration 12700: Loss = -12099.145633865644
1
Iteration 12800: Loss = -12099.133590083096
Iteration 12900: Loss = -12099.231105297029
1
Iteration 13000: Loss = -12099.133698103598
2
Iteration 13100: Loss = -12099.133591392056
Iteration 13200: Loss = -12099.138790382533
1
Iteration 13300: Loss = -12099.133594554527
Iteration 13400: Loss = -12099.13359112234
Iteration 13500: Loss = -12099.134019345633
1
Iteration 13600: Loss = -12099.13358751463
Iteration 13700: Loss = -12099.182720609015
1
Iteration 13800: Loss = -12099.133788362706
2
Iteration 13900: Loss = -12099.133601093277
Iteration 14000: Loss = -12099.454101889607
1
Iteration 14100: Loss = -12099.133595575371
Iteration 14200: Loss = -12099.13360041756
Iteration 14300: Loss = -12099.134676669304
1
Iteration 14400: Loss = -12099.134280784401
2
Iteration 14500: Loss = -12099.133681573523
Iteration 14600: Loss = -12099.133829403014
1
Iteration 14700: Loss = -12099.15107549447
2
Iteration 14800: Loss = -12099.133638866926
Iteration 14900: Loss = -12099.14817110399
1
Iteration 15000: Loss = -12099.13358372182
Iteration 15100: Loss = -12099.137474100731
1
Iteration 15200: Loss = -12099.133809591243
2
Iteration 15300: Loss = -12099.133582530574
Iteration 15400: Loss = -12099.137012242432
1
Iteration 15500: Loss = -12099.134300171298
2
Iteration 15600: Loss = -12099.142915392038
3
Iteration 15700: Loss = -12099.1436501064
4
Iteration 15800: Loss = -12099.136579407641
5
Iteration 15900: Loss = -12099.159943095521
6
Iteration 16000: Loss = -12099.135926686657
7
Iteration 16100: Loss = -12099.134404513758
8
Iteration 16200: Loss = -12099.151618004891
9
Iteration 16300: Loss = -12099.133783493306
10
Iteration 16400: Loss = -12099.226292586223
11
Iteration 16500: Loss = -12099.133596554173
Iteration 16600: Loss = -12099.134340571734
1
Iteration 16700: Loss = -12099.136769259278
2
Iteration 16800: Loss = -12099.1339395053
3
Iteration 16900: Loss = -12099.187424566784
4
Iteration 17000: Loss = -12099.133650814032
Iteration 17100: Loss = -12099.225092815126
1
Iteration 17200: Loss = -12099.133588523608
Iteration 17300: Loss = -12099.311878715449
1
Iteration 17400: Loss = -12099.13357739792
Iteration 17500: Loss = -12099.135205794088
1
Iteration 17600: Loss = -12099.133647100656
Iteration 17700: Loss = -12099.133590612833
Iteration 17800: Loss = -12099.134416399656
1
Iteration 17900: Loss = -12099.133607946356
Iteration 18000: Loss = -12099.235153972284
1
Iteration 18100: Loss = -12099.133584606036
Iteration 18200: Loss = -12099.133568123823
Iteration 18300: Loss = -12099.138397061004
1
Iteration 18400: Loss = -12099.133560956994
Iteration 18500: Loss = -12099.140450749399
1
Iteration 18600: Loss = -12099.13357582914
Iteration 18700: Loss = -12099.134612626081
1
Iteration 18800: Loss = -12099.134054359254
2
Iteration 18900: Loss = -12099.33914582401
3
Iteration 19000: Loss = -12099.134524469246
4
Iteration 19100: Loss = -12099.139947147361
5
Iteration 19200: Loss = -12099.141705182694
6
Iteration 19300: Loss = -12099.138829610118
7
Iteration 19400: Loss = -12099.133649595022
Iteration 19500: Loss = -12099.46110004969
1
Iteration 19600: Loss = -12099.133595513253
Iteration 19700: Loss = -12099.194845281545
1
Iteration 19800: Loss = -12099.285642627556
2
Iteration 19900: Loss = -12099.135103527373
3
pi: tensor([[0.4831, 0.5169],
        [0.3628, 0.6372]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5691, 0.4309], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3813, 0.0979],
         [0.6658, 0.2541]],

        [[0.5585, 0.1055],
         [0.7310, 0.6552]],

        [[0.7243, 0.0934],
         [0.7214, 0.5460]],

        [[0.6805, 0.0933],
         [0.6839, 0.6710]],

        [[0.5285, 0.1040],
         [0.5822, 0.5417]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 80
Adjusted Rand Index: 0.354551002206262
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 79
Adjusted Rand Index: 0.330853137499065
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.13071352163049216
Average Adjusted Rand Index: 0.7370808279410654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21446.725072799683
Iteration 100: Loss = -12720.695864354839
Iteration 200: Loss = -12642.501129075901
Iteration 300: Loss = -12012.841225738159
Iteration 400: Loss = -11857.559689372192
Iteration 500: Loss = -11831.674126426085
Iteration 600: Loss = -11819.170442918119
Iteration 700: Loss = -11804.267915866778
Iteration 800: Loss = -11800.34118129868
Iteration 900: Loss = -11800.159491660806
Iteration 1000: Loss = -11800.036894253251
Iteration 1100: Loss = -11799.948426139705
Iteration 1200: Loss = -11799.881725590514
Iteration 1300: Loss = -11799.82983053304
Iteration 1400: Loss = -11799.78840391748
Iteration 1500: Loss = -11799.754611301087
Iteration 1600: Loss = -11799.726458355368
Iteration 1700: Loss = -11799.70243553851
Iteration 1800: Loss = -11799.681377792456
Iteration 1900: Loss = -11799.66269552015
Iteration 2000: Loss = -11799.64716500595
Iteration 2100: Loss = -11799.634742605685
Iteration 2200: Loss = -11799.623470872075
Iteration 2300: Loss = -11799.613877235113
Iteration 2400: Loss = -11799.605378073806
Iteration 2500: Loss = -11799.597890615862
Iteration 2600: Loss = -11799.59216010732
Iteration 2700: Loss = -11799.585160231845
Iteration 2800: Loss = -11799.579863035311
Iteration 2900: Loss = -11799.574872099724
Iteration 3000: Loss = -11799.570436288408
Iteration 3100: Loss = -11799.56690335867
Iteration 3200: Loss = -11799.562817227372
Iteration 3300: Loss = -11799.559483972027
Iteration 3400: Loss = -11799.556487401016
Iteration 3500: Loss = -11799.553701041374
Iteration 3600: Loss = -11799.55111317797
Iteration 3700: Loss = -11799.555973275787
1
Iteration 3800: Loss = -11799.546630022638
Iteration 3900: Loss = -11799.54508563741
Iteration 4000: Loss = -11799.542788891124
Iteration 4100: Loss = -11799.541154439003
Iteration 4200: Loss = -11799.539448059319
Iteration 4300: Loss = -11799.538133473561
Iteration 4400: Loss = -11799.539321551809
1
Iteration 4500: Loss = -11799.535385710717
Iteration 4600: Loss = -11799.534184745735
Iteration 4700: Loss = -11799.533075905558
Iteration 4800: Loss = -11799.532086705061
Iteration 4900: Loss = -11799.531070056682
Iteration 5000: Loss = -11799.530154409475
Iteration 5100: Loss = -11799.530353384045
1
Iteration 5200: Loss = -11799.528487258489
Iteration 5300: Loss = -11799.527778816782
Iteration 5400: Loss = -11799.52707935041
Iteration 5500: Loss = -11799.526464868406
Iteration 5600: Loss = -11799.525830078188
Iteration 5700: Loss = -11799.525241171445
Iteration 5800: Loss = -11799.526625986135
1
Iteration 5900: Loss = -11799.524177344445
Iteration 6000: Loss = -11799.523722528638
Iteration 6100: Loss = -11799.524620426137
1
Iteration 6200: Loss = -11799.522921897029
Iteration 6300: Loss = -11799.522453299081
Iteration 6400: Loss = -11799.52210596568
Iteration 6500: Loss = -11799.614377925256
1
Iteration 6600: Loss = -11799.52139028615
Iteration 6700: Loss = -11799.521137853835
Iteration 6800: Loss = -11799.52208122499
1
Iteration 6900: Loss = -11799.520665629101
Iteration 7000: Loss = -11799.520307112114
Iteration 7100: Loss = -11799.520182768127
Iteration 7200: Loss = -11799.526046459809
1
Iteration 7300: Loss = -11799.523177224495
2
Iteration 7400: Loss = -11799.519480492769
Iteration 7500: Loss = -11799.520133511203
1
Iteration 7600: Loss = -11799.519079295418
Iteration 7700: Loss = -11799.523519935181
1
Iteration 7800: Loss = -11799.53523078884
2
Iteration 7900: Loss = -11799.541592549702
3
Iteration 8000: Loss = -11799.518429816562
Iteration 8100: Loss = -11799.58173302335
1
Iteration 8200: Loss = -11799.519401170288
2
Iteration 8300: Loss = -11799.543301782132
3
Iteration 8400: Loss = -11799.51790378095
Iteration 8500: Loss = -11799.517972809792
Iteration 8600: Loss = -11799.571667983193
1
Iteration 8700: Loss = -11799.518287791665
2
Iteration 8800: Loss = -11799.520617038406
3
Iteration 8900: Loss = -11799.527268579433
4
Iteration 9000: Loss = -11799.521895244558
5
Iteration 9100: Loss = -11799.518263429736
6
Iteration 9200: Loss = -11799.520023895951
7
Iteration 9300: Loss = -11799.518007142335
Iteration 9400: Loss = -11799.517232945325
Iteration 9500: Loss = -11799.532958037089
1
Iteration 9600: Loss = -11799.523193537269
2
Iteration 9700: Loss = -11799.517308053495
Iteration 9800: Loss = -11799.519322974123
1
Iteration 9900: Loss = -11799.528028975237
2
Iteration 10000: Loss = -11799.517065500622
Iteration 10100: Loss = -11799.521115550828
1
Iteration 10200: Loss = -11799.54544625561
2
Iteration 10300: Loss = -11799.518119554943
3
Iteration 10400: Loss = -11799.5174934369
4
Iteration 10500: Loss = -11799.519216361512
5
Iteration 10600: Loss = -11799.52118615538
6
Iteration 10700: Loss = -11799.537088188199
7
Iteration 10800: Loss = -11799.538702816
8
Iteration 10900: Loss = -11799.524082983577
9
Iteration 11000: Loss = -11799.520257579403
10
Iteration 11100: Loss = -11799.546859807248
11
Iteration 11200: Loss = -11799.590188830782
12
Iteration 11300: Loss = -11799.520393730527
13
Iteration 11400: Loss = -11799.516664910394
Iteration 11500: Loss = -11799.520004575583
1
Iteration 11600: Loss = -11799.53757116272
2
Iteration 11700: Loss = -11799.541764378386
3
Iteration 11800: Loss = -11799.518225288002
4
Iteration 11900: Loss = -11799.518714211285
5
Iteration 12000: Loss = -11799.530557721853
6
Iteration 12100: Loss = -11799.516086089421
Iteration 12200: Loss = -11799.517506185919
1
Iteration 12300: Loss = -11799.52762282442
2
Iteration 12400: Loss = -11799.54762927825
3
Iteration 12500: Loss = -11799.519292702731
4
Iteration 12600: Loss = -11799.51960688506
5
Iteration 12700: Loss = -11799.544299298184
6
Iteration 12800: Loss = -11799.542843360843
7
Iteration 12900: Loss = -11799.5173564215
8
Iteration 13000: Loss = -11799.544939346108
9
Iteration 13100: Loss = -11799.532000932122
10
Iteration 13200: Loss = -11799.544740630316
11
Iteration 13300: Loss = -11799.542346947186
12
Iteration 13400: Loss = -11799.52454948687
13
Iteration 13500: Loss = -11799.560042464307
14
Iteration 13600: Loss = -11799.59240874766
15
Stopping early at iteration 13600 due to no improvement.
pi: tensor([[0.7511, 0.2489],
        [0.2610, 0.7390]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5700, 0.4300], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4069, 0.0979],
         [0.6530, 0.1983]],

        [[0.7151, 0.1067],
         [0.6461, 0.7136]],

        [[0.6900, 0.1026],
         [0.6087, 0.7088]],

        [[0.6700, 0.0970],
         [0.5622, 0.5058]],

        [[0.7253, 0.1048],
         [0.5414, 0.5213]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11802.016312980853
[0.13071352163049216, 1.0] [0.7370808279410654, 1.0] [12099.13479990907, 11799.59240874766]
-------------------------------------
This iteration is 8
True Objective function: Loss = -11560.803258165199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23505.69522840063
Iteration 100: Loss = -11768.059716024747
Iteration 200: Loss = -11558.537483312202
Iteration 300: Loss = -11558.00141804497
Iteration 400: Loss = -11557.805337276668
Iteration 500: Loss = -11557.710130394738
Iteration 600: Loss = -11557.653170278076
Iteration 700: Loss = -11557.61586457564
Iteration 800: Loss = -11557.589974802573
Iteration 900: Loss = -11557.571128999403
Iteration 1000: Loss = -11557.557001281975
Iteration 1100: Loss = -11557.546021244172
Iteration 1200: Loss = -11557.537344784902
Iteration 1300: Loss = -11557.530373214095
Iteration 1400: Loss = -11557.524643090994
Iteration 1500: Loss = -11557.519952437244
Iteration 1600: Loss = -11557.51595488236
Iteration 1700: Loss = -11557.512608436129
Iteration 1800: Loss = -11557.509715054275
Iteration 1900: Loss = -11557.507215917718
Iteration 2000: Loss = -11557.505091031513
Iteration 2100: Loss = -11557.50322047262
Iteration 2200: Loss = -11557.501561739222
Iteration 2300: Loss = -11557.50007618816
Iteration 2400: Loss = -11557.498803945173
Iteration 2500: Loss = -11557.497646324995
Iteration 2600: Loss = -11557.496615571716
Iteration 2700: Loss = -11557.495707138261
Iteration 2800: Loss = -11557.494844409495
Iteration 2900: Loss = -11557.497769250007
1
Iteration 3000: Loss = -11557.493382612467
Iteration 3100: Loss = -11557.49280701841
Iteration 3200: Loss = -11557.492209587492
Iteration 3300: Loss = -11557.491714147936
Iteration 3400: Loss = -11557.491355887703
Iteration 3500: Loss = -11557.49074860319
Iteration 3600: Loss = -11557.490527829894
Iteration 3700: Loss = -11557.490005929707
Iteration 3800: Loss = -11557.491104728691
1
Iteration 3900: Loss = -11557.48944888945
Iteration 4000: Loss = -11557.49490404145
1
Iteration 4100: Loss = -11557.48884747593
Iteration 4200: Loss = -11557.488589689381
Iteration 4300: Loss = -11557.488536204368
Iteration 4400: Loss = -11557.488117190303
Iteration 4500: Loss = -11557.48793244223
Iteration 4600: Loss = -11557.488432022617
1
Iteration 4700: Loss = -11557.495302455434
2
Iteration 4800: Loss = -11557.487365459558
Iteration 4900: Loss = -11557.487360664456
Iteration 5000: Loss = -11557.487083738419
Iteration 5100: Loss = -11557.488326605317
1
Iteration 5200: Loss = -11557.486946168854
Iteration 5300: Loss = -11557.486780853596
Iteration 5400: Loss = -11557.492964701582
1
Iteration 5500: Loss = -11557.488112550063
2
Iteration 5600: Loss = -11557.489301361016
3
Iteration 5700: Loss = -11557.486329956178
Iteration 5800: Loss = -11557.486402772885
Iteration 5900: Loss = -11557.48675172134
1
Iteration 6000: Loss = -11557.485993694014
Iteration 6100: Loss = -11557.485937446885
Iteration 6200: Loss = -11557.486285315726
1
Iteration 6300: Loss = -11557.485653206193
Iteration 6400: Loss = -11557.492072877816
1
Iteration 6500: Loss = -11557.485565773346
Iteration 6600: Loss = -11557.485497128893
Iteration 6700: Loss = -11557.485470963215
Iteration 6800: Loss = -11557.485454284139
Iteration 6900: Loss = -11557.486057354326
1
Iteration 7000: Loss = -11557.485631635316
2
Iteration 7100: Loss = -11557.48566704432
3
Iteration 7200: Loss = -11557.487658655038
4
Iteration 7300: Loss = -11557.496621112654
5
Iteration 7400: Loss = -11557.48559384505
6
Iteration 7500: Loss = -11557.485126616344
Iteration 7600: Loss = -11557.485374860415
1
Iteration 7700: Loss = -11557.485083606514
Iteration 7800: Loss = -11557.486552017896
1
Iteration 7900: Loss = -11557.485046079611
Iteration 8000: Loss = -11557.485025805086
Iteration 8100: Loss = -11557.485444379112
1
Iteration 8200: Loss = -11557.49229326136
2
Iteration 8300: Loss = -11557.485094558271
Iteration 8400: Loss = -11557.488464635495
1
Iteration 8500: Loss = -11557.485040536856
Iteration 8600: Loss = -11557.485080521275
Iteration 8700: Loss = -11557.4859590947
1
Iteration 8800: Loss = -11557.491103156777
2
Iteration 8900: Loss = -11557.535058914782
3
Iteration 9000: Loss = -11557.4850506345
Iteration 9100: Loss = -11557.485226093138
1
Iteration 9200: Loss = -11557.4849276899
Iteration 9300: Loss = -11557.486041991795
1
Iteration 9400: Loss = -11557.484941298857
Iteration 9500: Loss = -11557.485724193077
1
Iteration 9600: Loss = -11557.59355292919
2
Iteration 9700: Loss = -11557.488842704077
3
Iteration 9800: Loss = -11557.484797438723
Iteration 9900: Loss = -11557.4870102889
1
Iteration 10000: Loss = -11557.484737492203
Iteration 10100: Loss = -11557.485864951796
1
Iteration 10200: Loss = -11557.484782606687
Iteration 10300: Loss = -11557.485274683842
1
Iteration 10400: Loss = -11557.487232380896
2
Iteration 10500: Loss = -11557.498372834112
3
Iteration 10600: Loss = -11557.492815435571
4
Iteration 10700: Loss = -11557.486471899894
5
Iteration 10800: Loss = -11557.490854635193
6
Iteration 10900: Loss = -11557.488040884266
7
Iteration 11000: Loss = -11557.485126666732
8
Iteration 11100: Loss = -11557.49480088924
9
Iteration 11200: Loss = -11557.489684273294
10
Iteration 11300: Loss = -11557.48517988258
11
Iteration 11400: Loss = -11557.49875458321
12
Iteration 11500: Loss = -11557.534135165568
13
Iteration 11600: Loss = -11557.488635573514
14
Iteration 11700: Loss = -11557.489976221643
15
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.7236, 0.2764],
        [0.2788, 0.7212]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5123, 0.4877], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3974, 0.1013],
         [0.5771, 0.2027]],

        [[0.5142, 0.0933],
         [0.5218, 0.7200]],

        [[0.7145, 0.0964],
         [0.5446, 0.6218]],

        [[0.5349, 0.1016],
         [0.6225, 0.6539]],

        [[0.5273, 0.1047],
         [0.5642, 0.5491]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21743.553654617543
Iteration 100: Loss = -11954.95109699172
Iteration 200: Loss = -11565.883934924146
Iteration 300: Loss = -11559.851981857553
Iteration 400: Loss = -11559.457233611074
Iteration 500: Loss = -11559.265236133964
Iteration 600: Loss = -11559.151083174798
Iteration 700: Loss = -11559.076095297143
Iteration 800: Loss = -11559.026686816682
Iteration 900: Loss = -11558.990781917866
Iteration 1000: Loss = -11558.939983463984
Iteration 1100: Loss = -11557.60803770821
Iteration 1200: Loss = -11557.59147895593
Iteration 1300: Loss = -11557.578481361428
Iteration 1400: Loss = -11557.567975607184
Iteration 1500: Loss = -11557.559238121272
Iteration 1600: Loss = -11557.551937564436
Iteration 1700: Loss = -11557.545786099072
Iteration 1800: Loss = -11557.540524916896
Iteration 1900: Loss = -11557.53603629356
Iteration 2000: Loss = -11557.532098183505
Iteration 2100: Loss = -11557.528655962877
Iteration 2200: Loss = -11557.525610888251
Iteration 2300: Loss = -11557.522958510219
Iteration 2400: Loss = -11557.522504519671
Iteration 2500: Loss = -11557.518409191449
Iteration 2600: Loss = -11557.516503687064
Iteration 2700: Loss = -11557.514853139199
Iteration 2800: Loss = -11557.513421934551
Iteration 2900: Loss = -11557.511906975371
Iteration 3000: Loss = -11557.510752840895
Iteration 3100: Loss = -11557.509591454183
Iteration 3200: Loss = -11557.51283950128
1
Iteration 3300: Loss = -11557.51435679618
2
Iteration 3400: Loss = -11557.507290488022
Iteration 3500: Loss = -11557.50603772502
Iteration 3600: Loss = -11557.511287573525
1
Iteration 3700: Loss = -11557.504719753933
Iteration 3800: Loss = -11557.503473175222
Iteration 3900: Loss = -11557.502603000261
Iteration 4000: Loss = -11557.49717194566
Iteration 4100: Loss = -11557.495952263396
Iteration 4200: Loss = -11557.49758526592
1
Iteration 4300: Loss = -11557.495148715543
Iteration 4400: Loss = -11557.496408418012
1
Iteration 4500: Loss = -11557.499061278548
2
Iteration 4600: Loss = -11557.49373118797
Iteration 4700: Loss = -11557.494002080548
1
Iteration 4800: Loss = -11557.49319838549
Iteration 4900: Loss = -11557.496741719642
1
Iteration 5000: Loss = -11557.494269099438
2
Iteration 5100: Loss = -11557.492488756985
Iteration 5200: Loss = -11557.492757259764
1
Iteration 5300: Loss = -11557.495335235575
2
Iteration 5400: Loss = -11557.491673593835
Iteration 5500: Loss = -11557.491686395186
Iteration 5600: Loss = -11557.491318291293
Iteration 5700: Loss = -11557.491182022712
Iteration 5800: Loss = -11557.492580857537
1
Iteration 5900: Loss = -11557.500763965329
2
Iteration 6000: Loss = -11557.490618893502
Iteration 6100: Loss = -11557.490543288377
Iteration 6200: Loss = -11557.490751810536
1
Iteration 6300: Loss = -11557.490291255086
Iteration 6400: Loss = -11557.490270825308
Iteration 6500: Loss = -11557.49015020288
Iteration 6600: Loss = -11557.490765898701
1
Iteration 6700: Loss = -11557.48976336439
Iteration 6800: Loss = -11557.489826077202
Iteration 6900: Loss = -11557.491116938349
1
Iteration 7000: Loss = -11557.489583478304
Iteration 7100: Loss = -11557.489819949598
1
Iteration 7200: Loss = -11557.489125241402
Iteration 7300: Loss = -11557.489172725538
Iteration 7400: Loss = -11557.489070429872
Iteration 7500: Loss = -11557.48900551852
Iteration 7600: Loss = -11557.488926363354
Iteration 7700: Loss = -11557.490272672967
1
Iteration 7800: Loss = -11557.491095209325
2
Iteration 7900: Loss = -11557.495220327106
3
Iteration 8000: Loss = -11557.489263641428
4
Iteration 8100: Loss = -11557.48942560716
5
Iteration 8200: Loss = -11557.492925292609
6
Iteration 8300: Loss = -11557.522880679491
7
Iteration 8400: Loss = -11557.488651409367
Iteration 8500: Loss = -11557.489194377156
1
Iteration 8600: Loss = -11557.51675579388
2
Iteration 8700: Loss = -11557.489203062034
3
Iteration 8800: Loss = -11557.489019043122
4
Iteration 8900: Loss = -11557.4897373218
5
Iteration 9000: Loss = -11557.489435747271
6
Iteration 9100: Loss = -11557.49156421982
7
Iteration 9200: Loss = -11557.489324262762
8
Iteration 9300: Loss = -11557.49158522689
9
Iteration 9400: Loss = -11557.488299453666
Iteration 9500: Loss = -11557.488730419936
1
Iteration 9600: Loss = -11557.51374263786
2
Iteration 9700: Loss = -11557.4881720722
Iteration 9800: Loss = -11557.487113328874
Iteration 9900: Loss = -11557.48713463208
Iteration 10000: Loss = -11557.491163274537
1
Iteration 10100: Loss = -11557.503719824575
2
Iteration 10200: Loss = -11557.486437869145
Iteration 10300: Loss = -11557.48725697882
1
Iteration 10400: Loss = -11557.485590757138
Iteration 10500: Loss = -11557.48609704411
1
Iteration 10600: Loss = -11557.504428233206
2
Iteration 10700: Loss = -11557.486966459997
3
Iteration 10800: Loss = -11557.489151039714
4
Iteration 10900: Loss = -11557.489356509112
5
Iteration 11000: Loss = -11557.485419012759
Iteration 11100: Loss = -11557.48572534543
1
Iteration 11200: Loss = -11557.497789829045
2
Iteration 11300: Loss = -11557.485410930502
Iteration 11400: Loss = -11557.49031208006
1
Iteration 11500: Loss = -11557.507460111565
2
Iteration 11600: Loss = -11557.533004637395
3
Iteration 11700: Loss = -11557.488151651423
4
Iteration 11800: Loss = -11557.488113217258
5
Iteration 11900: Loss = -11557.487999619398
6
Iteration 12000: Loss = -11557.485886106519
7
Iteration 12100: Loss = -11557.551323655758
8
Iteration 12200: Loss = -11557.487011116644
9
Iteration 12300: Loss = -11557.517500758842
10
Iteration 12400: Loss = -11557.493059325172
11
Iteration 12500: Loss = -11557.52963482739
12
Iteration 12600: Loss = -11557.507958798042
13
Iteration 12700: Loss = -11557.573898255581
14
Iteration 12800: Loss = -11557.601860797213
15
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[0.7256, 0.2744],
        [0.2806, 0.7194]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5139, 0.4861], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3957, 0.1009],
         [0.5500, 0.2039]],

        [[0.5242, 0.0931],
         [0.7125, 0.5189]],

        [[0.6368, 0.0961],
         [0.7072, 0.5589]],

        [[0.7069, 0.1009],
         [0.6544, 0.7003]],

        [[0.5994, 0.1037],
         [0.6730, 0.7135]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11560.803258165199
[1.0, 1.0] [1.0, 1.0] [11557.489976221643, 11557.601860797213]
-------------------------------------
This iteration is 9
True Objective function: Loss = -11942.787899975014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21261.615930312535
Iteration 100: Loss = -12895.32788252276
Iteration 200: Loss = -12645.252302331148
Iteration 300: Loss = -12216.907367251557
Iteration 400: Loss = -12153.810845892724
Iteration 500: Loss = -12131.295083934681
Iteration 600: Loss = -12104.620896640126
Iteration 700: Loss = -12077.894405670071
Iteration 800: Loss = -12038.269417013033
Iteration 900: Loss = -12024.04277118064
Iteration 1000: Loss = -12017.604625101738
Iteration 1100: Loss = -12000.194591363
Iteration 1200: Loss = -11997.504797580588
Iteration 1300: Loss = -11997.38730780375
Iteration 1400: Loss = -11984.195624647038
Iteration 1500: Loss = -11983.790728676646
Iteration 1600: Loss = -11975.297391204642
Iteration 1700: Loss = -11937.166778430166
Iteration 1800: Loss = -11937.103838080964
Iteration 1900: Loss = -11937.082657865836
Iteration 2000: Loss = -11937.066597314037
Iteration 2100: Loss = -11937.053663041439
Iteration 2200: Loss = -11937.043062318633
Iteration 2300: Loss = -11937.034069621435
Iteration 2400: Loss = -11937.026308485729
Iteration 2500: Loss = -11937.019580910523
Iteration 2600: Loss = -11937.013534133748
Iteration 2700: Loss = -11937.007839258267
Iteration 2800: Loss = -11937.002196647541
Iteration 2900: Loss = -11936.997107074081
Iteration 3000: Loss = -11936.993203199703
Iteration 3100: Loss = -11936.989915720877
Iteration 3200: Loss = -11936.986906261502
Iteration 3300: Loss = -11936.98431110807
Iteration 3400: Loss = -11936.983671584196
Iteration 3500: Loss = -11936.979802711687
Iteration 3600: Loss = -11936.97804479796
Iteration 3700: Loss = -11936.97604073608
Iteration 3800: Loss = -11936.97477144647
Iteration 3900: Loss = -11936.972899854405
Iteration 4000: Loss = -11936.977596007111
1
Iteration 4100: Loss = -11936.970219249926
Iteration 4200: Loss = -11936.969078017899
Iteration 4300: Loss = -11936.972590535266
1
Iteration 4400: Loss = -11936.96696220021
Iteration 4500: Loss = -11936.966283368574
Iteration 4600: Loss = -11936.965682297416
Iteration 4700: Loss = -11936.967274669796
1
Iteration 4800: Loss = -11936.963598987653
Iteration 4900: Loss = -11936.9628718074
Iteration 5000: Loss = -11936.962252458554
Iteration 5100: Loss = -11936.966423471697
1
Iteration 5200: Loss = -11936.961128769794
Iteration 5300: Loss = -11936.960659614382
Iteration 5400: Loss = -11936.960427226479
Iteration 5500: Loss = -11936.959530955215
Iteration 5600: Loss = -11936.959519244443
Iteration 5700: Loss = -11936.962660152052
1
Iteration 5800: Loss = -11936.959016064186
Iteration 5900: Loss = -11936.958000870254
Iteration 6000: Loss = -11936.957681844176
Iteration 6100: Loss = -11936.957447585744
Iteration 6200: Loss = -11936.957812478393
1
Iteration 6300: Loss = -11936.956677880642
Iteration 6400: Loss = -11936.95780492111
1
Iteration 6500: Loss = -11936.956242817978
Iteration 6600: Loss = -11936.955958941595
Iteration 6700: Loss = -11936.956961169499
1
Iteration 6800: Loss = -11936.955847139621
Iteration 6900: Loss = -11936.989056651479
1
Iteration 7000: Loss = -11936.956100982312
2
Iteration 7100: Loss = -11936.963319493987
3
Iteration 7200: Loss = -11937.026472669113
4
Iteration 7300: Loss = -11936.957270757024
5
Iteration 7400: Loss = -11936.954367244442
Iteration 7500: Loss = -11936.954579185523
1
Iteration 7600: Loss = -11936.953803440249
Iteration 7700: Loss = -11936.95375528448
Iteration 7800: Loss = -11936.953529479351
Iteration 7900: Loss = -11936.962274731346
1
Iteration 8000: Loss = -11936.953339743566
Iteration 8100: Loss = -11936.95319212976
Iteration 8200: Loss = -11936.953526784595
1
Iteration 8300: Loss = -11936.953017295365
Iteration 8400: Loss = -11936.956889477296
1
Iteration 8500: Loss = -11936.953550006168
2
Iteration 8600: Loss = -11936.952846866028
Iteration 8700: Loss = -11936.952714362638
Iteration 8800: Loss = -11936.958085053502
1
Iteration 8900: Loss = -11936.956931396673
2
Iteration 9000: Loss = -11936.952513793114
Iteration 9100: Loss = -11936.9576261173
1
Iteration 9200: Loss = -11936.95357588897
2
Iteration 9300: Loss = -11937.006064103223
3
Iteration 9400: Loss = -11936.95228618884
Iteration 9500: Loss = -11936.953376796835
1
Iteration 9600: Loss = -11936.952198389263
Iteration 9700: Loss = -11936.952365379731
1
Iteration 9800: Loss = -11936.955259937102
2
Iteration 9900: Loss = -11936.954203854093
3
Iteration 10000: Loss = -11936.961549003694
4
Iteration 10100: Loss = -11936.95456522084
5
Iteration 10200: Loss = -11936.968191442558
6
Iteration 10300: Loss = -11936.951938708075
Iteration 10400: Loss = -11936.953616554096
1
Iteration 10500: Loss = -11936.95285902598
2
Iteration 10600: Loss = -11936.973661658712
3
Iteration 10700: Loss = -11936.956030005573
4
Iteration 10800: Loss = -11936.957050367926
5
Iteration 10900: Loss = -11936.954976053707
6
Iteration 11000: Loss = -11936.958323422221
7
Iteration 11100: Loss = -11936.960329730502
8
Iteration 11200: Loss = -11936.969775857136
9
Iteration 11300: Loss = -11936.955546762709
10
Iteration 11400: Loss = -11936.967054129304
11
Iteration 11500: Loss = -11937.071652620514
12
Iteration 11600: Loss = -11936.952996699263
13
Iteration 11700: Loss = -11936.955567454876
14
Iteration 11800: Loss = -11936.958304035752
15
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[0.7968, 0.2032],
        [0.2798, 0.7202]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5105, 0.4895], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4111, 0.1017],
         [0.6257, 0.2031]],

        [[0.7211, 0.1036],
         [0.5050, 0.6004]],

        [[0.6865, 0.1049],
         [0.5284, 0.7255]],

        [[0.6773, 0.1005],
         [0.7260, 0.7308]],

        [[0.5641, 0.1065],
         [0.6698, 0.6273]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919971467023199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21664.782970200507
Iteration 100: Loss = -12894.668087712054
Iteration 200: Loss = -12749.955042260444
Iteration 300: Loss = -12269.80285345437
Iteration 400: Loss = -12177.40443721905
Iteration 500: Loss = -12160.238126718514
Iteration 600: Loss = -12144.178726468765
Iteration 700: Loss = -12128.292500465466
Iteration 800: Loss = -12120.286029052828
Iteration 900: Loss = -12120.176522635658
Iteration 1000: Loss = -12120.101250778493
Iteration 1100: Loss = -12120.045829262532
Iteration 1200: Loss = -12120.00360828555
Iteration 1300: Loss = -12119.970736028607
Iteration 1400: Loss = -12119.944458373222
Iteration 1500: Loss = -12119.922982668479
Iteration 1600: Loss = -12119.90525488752
Iteration 1700: Loss = -12119.890295919724
Iteration 1800: Loss = -12119.877637235504
Iteration 1900: Loss = -12119.866827423675
Iteration 2000: Loss = -12119.857476556293
Iteration 2100: Loss = -12119.849347044337
Iteration 2200: Loss = -12119.842202508495
Iteration 2300: Loss = -12119.835907914527
Iteration 2400: Loss = -12119.830343984526
Iteration 2500: Loss = -12119.82534493967
Iteration 2600: Loss = -12119.828459370698
1
Iteration 2700: Loss = -12119.816795623345
Iteration 2800: Loss = -12119.81311000979
Iteration 2900: Loss = -12119.809715064332
Iteration 3000: Loss = -12119.806541146378
Iteration 3100: Loss = -12119.803394833289
Iteration 3200: Loss = -12119.800292934131
Iteration 3300: Loss = -12119.80690263269
1
Iteration 3400: Loss = -12119.791798617878
Iteration 3500: Loss = -12119.777702880312
Iteration 3600: Loss = -12115.15377518188
Iteration 3700: Loss = -12109.312991751156
Iteration 3800: Loss = -12076.23593872855
Iteration 3900: Loss = -12031.569102752559
Iteration 4000: Loss = -12003.655111276075
Iteration 4100: Loss = -12000.197212814714
Iteration 4200: Loss = -11999.95668287695
Iteration 4300: Loss = -11999.951480058935
Iteration 4400: Loss = -11999.945796941198
Iteration 4500: Loss = -11999.927420651966
Iteration 4600: Loss = -11999.90823914585
Iteration 4700: Loss = -11986.762907740667
Iteration 4800: Loss = -11986.749855131766
Iteration 4900: Loss = -11986.74821543502
Iteration 5000: Loss = -11986.73567847961
Iteration 5100: Loss = -11984.451310359991
Iteration 5200: Loss = -11977.13622904425
Iteration 5300: Loss = -11961.061017368584
Iteration 5400: Loss = -11961.048036631058
Iteration 5500: Loss = -11961.046664789616
Iteration 5600: Loss = -11961.047571506848
1
Iteration 5700: Loss = -11947.73888916092
Iteration 5800: Loss = -11947.735982730255
Iteration 5900: Loss = -11947.732782792023
Iteration 6000: Loss = -11947.729796088488
Iteration 6100: Loss = -11947.735572570027
1
Iteration 6200: Loss = -11947.728852074668
Iteration 6300: Loss = -11947.728933557622
Iteration 6400: Loss = -11947.727765013804
Iteration 6500: Loss = -11947.724223942036
Iteration 6600: Loss = -11947.725531031754
1
Iteration 6700: Loss = -11947.723077041344
Iteration 6800: Loss = -11947.72540342161
1
Iteration 6900: Loss = -11947.722337791572
Iteration 7000: Loss = -11947.722091546817
Iteration 7100: Loss = -11947.721782293427
Iteration 7200: Loss = -11947.721238899157
Iteration 7300: Loss = -11947.716648511696
Iteration 7400: Loss = -11936.958808702662
Iteration 7500: Loss = -11936.954657102395
Iteration 7600: Loss = -11936.954990848499
1
Iteration 7700: Loss = -11936.954328881235
Iteration 7800: Loss = -11937.034718444876
1
Iteration 7900: Loss = -11936.954045334774
Iteration 8000: Loss = -11936.956530461248
1
Iteration 8100: Loss = -11936.954003240122
Iteration 8200: Loss = -11936.986139971184
1
Iteration 8300: Loss = -11936.953592241549
Iteration 8400: Loss = -11936.955256530682
1
Iteration 8500: Loss = -11936.953435120195
Iteration 8600: Loss = -11936.965193124763
1
Iteration 8700: Loss = -11936.953251586296
Iteration 8800: Loss = -11936.955993710038
1
Iteration 8900: Loss = -11936.954065337655
2
Iteration 9000: Loss = -11936.953115633445
Iteration 9100: Loss = -11936.953086624775
Iteration 9200: Loss = -11936.962419000887
1
Iteration 9300: Loss = -11936.954958017355
2
Iteration 9400: Loss = -11936.953702597732
3
Iteration 9500: Loss = -11936.952882345877
Iteration 9600: Loss = -11936.95533025924
1
Iteration 9700: Loss = -11936.95297247558
Iteration 9800: Loss = -11936.952962605472
Iteration 9900: Loss = -11936.95506931401
1
Iteration 10000: Loss = -11936.953815010827
2
Iteration 10100: Loss = -11937.017665198873
3
Iteration 10200: Loss = -11936.999241327982
4
Iteration 10300: Loss = -11936.956177600772
5
Iteration 10400: Loss = -11937.04264556902
6
Iteration 10500: Loss = -11936.968875166465
7
Iteration 10600: Loss = -11936.954405729499
8
Iteration 10700: Loss = -11936.952529260136
Iteration 10800: Loss = -11936.973734028425
1
Iteration 10900: Loss = -11936.955674047751
2
Iteration 11000: Loss = -11936.954543083039
3
Iteration 11100: Loss = -11936.955912830172
4
Iteration 11200: Loss = -11936.952530321063
Iteration 11300: Loss = -11936.96542038764
1
Iteration 11400: Loss = -11936.95292059517
2
Iteration 11500: Loss = -11936.95930304216
3
Iteration 11600: Loss = -11937.012611611117
4
Iteration 11700: Loss = -11936.955283558318
5
Iteration 11800: Loss = -11936.968883751666
6
Iteration 11900: Loss = -11936.955346338262
7
Iteration 12000: Loss = -11936.968218989236
8
Iteration 12100: Loss = -11937.055606138221
9
Iteration 12200: Loss = -11936.953224204353
10
Iteration 12300: Loss = -11936.952572319964
Iteration 12400: Loss = -11936.960347170976
1
Iteration 12500: Loss = -11936.95479724276
2
Iteration 12600: Loss = -11936.952133405208
Iteration 12700: Loss = -11936.953054289324
1
Iteration 12800: Loss = -11936.95765300024
2
Iteration 12900: Loss = -11936.95974806949
3
Iteration 13000: Loss = -11936.963365662255
4
Iteration 13100: Loss = -11936.953678522901
5
Iteration 13200: Loss = -11936.970124681466
6
Iteration 13300: Loss = -11936.970702802519
7
Iteration 13400: Loss = -11936.967758983805
8
Iteration 13500: Loss = -11936.964341687339
9
Iteration 13600: Loss = -11936.953459259892
10
Iteration 13700: Loss = -11936.952151343734
Iteration 13800: Loss = -11936.952639625866
1
Iteration 13900: Loss = -11936.971717888542
2
Iteration 14000: Loss = -11936.971605466695
3
Iteration 14100: Loss = -11936.95416844358
4
Iteration 14200: Loss = -11936.962344903322
5
Iteration 14300: Loss = -11936.956233648865
6
Iteration 14400: Loss = -11936.952993297074
7
Iteration 14500: Loss = -11936.981989168558
8
Iteration 14600: Loss = -11936.988990191141
9
Iteration 14700: Loss = -11936.953742424523
10
Iteration 14800: Loss = -11936.952481415274
11
Iteration 14900: Loss = -11936.956896070145
12
Iteration 15000: Loss = -11936.953065024902
13
Iteration 15100: Loss = -11937.001646959285
14
Iteration 15200: Loss = -11936.955609506495
15
Stopping early at iteration 15200 due to no improvement.
pi: tensor([[0.7961, 0.2039],
        [0.2796, 0.7204]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5101, 0.4899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4110, 0.1013],
         [0.5902, 0.2032]],

        [[0.5207, 0.1042],
         [0.5712, 0.6745]],

        [[0.5632, 0.1050],
         [0.5928, 0.7190]],

        [[0.5346, 0.1006],
         [0.6725, 0.5934]],

        [[0.5012, 0.1069],
         [0.5960, 0.6573]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919995362962712
Average Adjusted Rand Index: 0.9919971467023199
11942.787899975014
[0.9919995362962712, 0.9919995362962712] [0.9919971467023199, 0.9919971467023199] [11936.958304035752, 11936.955609506495]
-------------------------------------
This iteration is 10
True Objective function: Loss = -11608.16265368903
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21125.02441648451
Iteration 100: Loss = -12352.698393471192
Iteration 200: Loss = -12326.11699740367
Iteration 300: Loss = -11981.1143644853
Iteration 400: Loss = -11812.511410063527
Iteration 500: Loss = -11686.488827980173
Iteration 600: Loss = -11631.461444085244
Iteration 700: Loss = -11626.73967561475
Iteration 800: Loss = -11613.969570973222
Iteration 900: Loss = -11613.790363463777
Iteration 1000: Loss = -11608.920307539454
Iteration 1100: Loss = -11608.82846288159
Iteration 1200: Loss = -11608.76870757089
Iteration 1300: Loss = -11608.723624675533
Iteration 1400: Loss = -11608.688386698115
Iteration 1500: Loss = -11608.660319027676
Iteration 1600: Loss = -11608.637413622948
Iteration 1700: Loss = -11608.618038591874
Iteration 1800: Loss = -11608.597155121543
Iteration 1900: Loss = -11602.27726302112
Iteration 2000: Loss = -11602.251897656439
Iteration 2100: Loss = -11602.247924418083
Iteration 2200: Loss = -11602.232288005416
Iteration 2300: Loss = -11602.224536598847
Iteration 2400: Loss = -11602.217716721587
Iteration 2500: Loss = -11602.211718708602
Iteration 2600: Loss = -11602.206378051493
Iteration 2700: Loss = -11602.201518154783
Iteration 2800: Loss = -11602.197227913906
Iteration 2900: Loss = -11602.192567088254
Iteration 3000: Loss = -11602.186989050855
Iteration 3100: Loss = -11602.191482480403
1
Iteration 3200: Loss = -11602.17763927654
Iteration 3300: Loss = -11602.175104287755
Iteration 3400: Loss = -11602.182237179053
1
Iteration 3500: Loss = -11602.170595113928
Iteration 3600: Loss = -11602.168642361732
Iteration 3700: Loss = -11602.16820199333
Iteration 3800: Loss = -11602.165135186318
Iteration 3900: Loss = -11602.164185212785
Iteration 4000: Loss = -11602.162260914887
Iteration 4100: Loss = -11602.160803785653
Iteration 4200: Loss = -11602.172035178275
1
Iteration 4300: Loss = -11602.15848856333
Iteration 4400: Loss = -11602.157341371598
Iteration 4500: Loss = -11602.15682182946
Iteration 4600: Loss = -11602.15563949292
Iteration 4700: Loss = -11602.154609095383
Iteration 4800: Loss = -11602.160529016894
1
Iteration 4900: Loss = -11602.153159716801
Iteration 5000: Loss = -11602.15244717662
Iteration 5100: Loss = -11602.152056420382
Iteration 5200: Loss = -11602.15917537711
1
Iteration 5300: Loss = -11602.150679386252
Iteration 5400: Loss = -11602.150414250658
Iteration 5500: Loss = -11602.151771895766
1
Iteration 5600: Loss = -11602.149260299595
Iteration 5700: Loss = -11602.154205968694
1
Iteration 5800: Loss = -11602.14843684823
Iteration 5900: Loss = -11602.148040528476
Iteration 6000: Loss = -11602.150017075119
1
Iteration 6100: Loss = -11602.14739988337
Iteration 6200: Loss = -11602.147062653105
Iteration 6300: Loss = -11602.14795629954
1
Iteration 6400: Loss = -11602.14648248002
Iteration 6500: Loss = -11602.148128973708
1
Iteration 6600: Loss = -11602.147645154284
2
Iteration 6700: Loss = -11602.145711004443
Iteration 6800: Loss = -11602.145417292359
Iteration 6900: Loss = -11602.145392679271
Iteration 7000: Loss = -11602.144692166632
Iteration 7100: Loss = -11602.143509866508
Iteration 7200: Loss = -11602.1415724527
Iteration 7300: Loss = -11602.149466914472
1
Iteration 7400: Loss = -11602.14043878219
Iteration 7500: Loss = -11602.140016846579
Iteration 7600: Loss = -11602.139105962693
Iteration 7700: Loss = -11602.13876709638
Iteration 7800: Loss = -11602.1391836318
1
Iteration 7900: Loss = -11602.138322797222
Iteration 8000: Loss = -11602.14633411859
1
Iteration 8100: Loss = -11602.139697546081
2
Iteration 8200: Loss = -11602.158053768686
3
Iteration 8300: Loss = -11602.138280560921
Iteration 8400: Loss = -11602.138246337612
Iteration 8500: Loss = -11602.138409850468
1
Iteration 8600: Loss = -11602.142920468814
2
Iteration 8700: Loss = -11602.137611090273
Iteration 8800: Loss = -11602.137742455656
1
Iteration 8900: Loss = -11602.137443568565
Iteration 9000: Loss = -11602.137462324938
Iteration 9100: Loss = -11602.149102159387
1
Iteration 9200: Loss = -11602.137446846036
Iteration 9300: Loss = -11602.141650381784
1
Iteration 9400: Loss = -11602.138360867877
2
Iteration 9500: Loss = -11602.137248117986
Iteration 9600: Loss = -11602.171260021489
1
Iteration 9700: Loss = -11602.141036005594
2
Iteration 9800: Loss = -11602.166181926736
3
Iteration 9900: Loss = -11602.138247041828
4
Iteration 10000: Loss = -11602.137029382508
Iteration 10100: Loss = -11602.140252031915
1
Iteration 10200: Loss = -11602.19941142134
2
Iteration 10300: Loss = -11602.137178699431
3
Iteration 10400: Loss = -11602.137390719332
4
Iteration 10500: Loss = -11602.137039197456
Iteration 10600: Loss = -11602.137223882488
1
Iteration 10700: Loss = -11602.137517566282
2
Iteration 10800: Loss = -11602.136962937335
Iteration 10900: Loss = -11602.141447368913
1
Iteration 11000: Loss = -11602.198661493909
2
Iteration 11100: Loss = -11602.13756594682
3
Iteration 11200: Loss = -11602.138887113246
4
Iteration 11300: Loss = -11602.151635220298
5
Iteration 11400: Loss = -11602.136554811508
Iteration 11500: Loss = -11602.136874708101
1
Iteration 11600: Loss = -11602.136559954657
Iteration 11700: Loss = -11602.137004096598
1
Iteration 11800: Loss = -11602.16025008723
2
Iteration 11900: Loss = -11602.13699928233
3
Iteration 12000: Loss = -11602.13651895578
Iteration 12100: Loss = -11602.137365853174
1
Iteration 12200: Loss = -11602.136619838813
2
Iteration 12300: Loss = -11602.137452321542
3
Iteration 12400: Loss = -11602.142956008927
4
Iteration 12500: Loss = -11602.1462541075
5
Iteration 12600: Loss = -11602.137385949309
6
Iteration 12700: Loss = -11602.138253509664
7
Iteration 12800: Loss = -11602.142184728338
8
Iteration 12900: Loss = -11602.137640945388
9
Iteration 13000: Loss = -11602.137797676402
10
Iteration 13100: Loss = -11602.145084710848
11
Iteration 13200: Loss = -11602.180607691704
12
Iteration 13300: Loss = -11602.143055228336
13
Iteration 13400: Loss = -11602.145839607569
14
Iteration 13500: Loss = -11602.146364021239
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7277, 0.2723],
        [0.2810, 0.7190]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5004, 0.4996], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3903, 0.1003],
         [0.7164, 0.2012]],

        [[0.5364, 0.1046],
         [0.7064, 0.7046]],

        [[0.6158, 0.1039],
         [0.6377, 0.6274]],

        [[0.7258, 0.0933],
         [0.6653, 0.7206]],

        [[0.6078, 0.1023],
         [0.7023, 0.7303]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9841601267189862
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20221.033048946523
Iteration 100: Loss = -12325.044499714824
Iteration 200: Loss = -12275.687521159018
Iteration 300: Loss = -12065.245643561473
Iteration 400: Loss = -11985.201897294515
Iteration 500: Loss = -11938.655525264956
Iteration 600: Loss = -11922.829546671615
Iteration 700: Loss = -11918.261480018835
Iteration 800: Loss = -11903.125244383606
Iteration 900: Loss = -11898.631359877454
Iteration 1000: Loss = -11886.246834917392
Iteration 1100: Loss = -11886.170442895578
Iteration 1200: Loss = -11879.362484564765
Iteration 1300: Loss = -11879.257577750172
Iteration 1400: Loss = -11879.151892920632
Iteration 1500: Loss = -11879.134948349962
Iteration 1600: Loss = -11877.379301156496
Iteration 1700: Loss = -11871.961238444102
Iteration 1800: Loss = -11862.14538291331
Iteration 1900: Loss = -11862.113439464718
Iteration 2000: Loss = -11862.116605501971
1
Iteration 2100: Loss = -11862.108273050217
Iteration 2200: Loss = -11862.116353368827
1
Iteration 2300: Loss = -11862.104987685838
Iteration 2400: Loss = -11862.103675221091
Iteration 2500: Loss = -11862.102479681429
Iteration 2600: Loss = -11862.10567949445
1
Iteration 2700: Loss = -11862.100078588424
Iteration 2800: Loss = -11862.098021959477
Iteration 2900: Loss = -11854.222794430862
Iteration 3000: Loss = -11850.553686051466
Iteration 3100: Loss = -11850.552804392823
Iteration 3200: Loss = -11850.552131090855
Iteration 3300: Loss = -11850.551534150933
Iteration 3400: Loss = -11850.565673044503
1
Iteration 3500: Loss = -11850.550629874157
Iteration 3600: Loss = -11850.550246587414
Iteration 3700: Loss = -11850.550644213785
1
Iteration 3800: Loss = -11850.549561658747
Iteration 3900: Loss = -11850.549329199706
Iteration 4000: Loss = -11850.54935325129
Iteration 4100: Loss = -11850.549143442746
Iteration 4200: Loss = -11850.549265330996
1
Iteration 4300: Loss = -11850.549251519707
2
Iteration 4400: Loss = -11850.548546917129
Iteration 4500: Loss = -11850.54853254877
Iteration 4600: Loss = -11850.551842337098
1
Iteration 4700: Loss = -11850.55027515679
2
Iteration 4800: Loss = -11850.547567306594
Iteration 4900: Loss = -11850.5481977064
1
Iteration 5000: Loss = -11850.54789836492
2
Iteration 5100: Loss = -11850.547185749416
Iteration 5200: Loss = -11850.552655331989
1
Iteration 5300: Loss = -11850.560918614747
2
Iteration 5400: Loss = -11850.553589184301
3
Iteration 5500: Loss = -11850.546757556293
Iteration 5600: Loss = -11850.546853382526
Iteration 5700: Loss = -11850.547484485245
1
Iteration 5800: Loss = -11850.551376264611
2
Iteration 5900: Loss = -11850.546049375902
Iteration 6000: Loss = -11850.545980512903
Iteration 6100: Loss = -11850.54453659479
Iteration 6200: Loss = -11850.546928977761
1
Iteration 6300: Loss = -11850.545845940236
2
Iteration 6400: Loss = -11850.544206779337
Iteration 6500: Loss = -11850.544473409009
1
Iteration 6600: Loss = -11850.544737059588
2
Iteration 6700: Loss = -11850.544063753765
Iteration 6800: Loss = -11850.544565372998
1
Iteration 6900: Loss = -11850.544000459251
Iteration 7000: Loss = -11850.57426822216
1
Iteration 7100: Loss = -11850.543908471815
Iteration 7200: Loss = -11850.54384626366
Iteration 7300: Loss = -11850.546339277542
1
Iteration 7400: Loss = -11850.543790354646
Iteration 7500: Loss = -11850.543774882175
Iteration 7600: Loss = -11850.543753585684
Iteration 7700: Loss = -11850.547192917264
1
Iteration 7800: Loss = -11850.543711661207
Iteration 7900: Loss = -11850.543685838018
Iteration 8000: Loss = -11850.561507035269
1
Iteration 8100: Loss = -11850.543652497576
Iteration 8200: Loss = -11850.54362756212
Iteration 8300: Loss = -11850.544453098128
1
Iteration 8400: Loss = -11850.5514888038
2
Iteration 8500: Loss = -11850.5435454031
Iteration 8600: Loss = -11850.54393585358
1
Iteration 8700: Loss = -11850.543523223887
Iteration 8800: Loss = -11850.592841946402
1
Iteration 8900: Loss = -11850.543509287632
Iteration 9000: Loss = -11850.544894723775
1
Iteration 9100: Loss = -11850.543507158518
Iteration 9200: Loss = -11850.611399516209
1
Iteration 9300: Loss = -11850.55099358122
2
Iteration 9400: Loss = -11850.543546915997
Iteration 9500: Loss = -11850.548147874317
1
Iteration 9600: Loss = -11850.544838759997
2
Iteration 9700: Loss = -11850.543478871145
Iteration 9800: Loss = -11850.550906667333
1
Iteration 9900: Loss = -11850.543324466635
Iteration 10000: Loss = -11850.551175999777
1
Iteration 10100: Loss = -11850.579523226588
2
Iteration 10200: Loss = -11850.543511066218
3
Iteration 10300: Loss = -11850.63810202639
4
Iteration 10400: Loss = -11850.553050159193
5
Iteration 10500: Loss = -11850.547039027033
6
Iteration 10600: Loss = -11850.546121743986
7
Iteration 10700: Loss = -11850.545483436816
8
Iteration 10800: Loss = -11850.544786340866
9
Iteration 10900: Loss = -11850.547349743129
10
Iteration 11000: Loss = -11850.544155600574
11
Iteration 11100: Loss = -11850.543710961132
12
Iteration 11200: Loss = -11850.54480319592
13
Iteration 11300: Loss = -11850.543426794406
14
Iteration 11400: Loss = -11850.545125354294
15
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.5489, 0.4511],
        [0.6296, 0.3704]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4914, 0.5086], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2660, 0.0991],
         [0.5184, 0.3423]],

        [[0.7241, 0.0979],
         [0.6647, 0.5319]],

        [[0.5356, 0.0968],
         [0.6271, 0.6064]],

        [[0.6685, 0.0926],
         [0.6240, 0.5251]],

        [[0.7242, 0.1009],
         [0.6301, 0.5467]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 13
Adjusted Rand Index: 0.543336255159078
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 11
Adjusted Rand Index: 0.6042568328689156
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.08345204015382175
Average Adjusted Rand Index: 0.821517833935328
11608.16265368903
[0.9840320165194881, 0.08345204015382175] [0.9841601267189862, 0.821517833935328] [11602.146364021239, 11850.545125354294]
-------------------------------------
This iteration is 11
True Objective function: Loss = -11268.300738954336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21630.376309042305
Iteration 100: Loss = -11908.832246131684
Iteration 200: Loss = -11906.932287300137
Iteration 300: Loss = -11895.699220086562
Iteration 400: Loss = -11890.00121587893
Iteration 500: Loss = -11608.908653490325
Iteration 600: Loss = -11565.44749462724
Iteration 700: Loss = -11564.944428277126
Iteration 800: Loss = -11548.789260108013
Iteration 900: Loss = -11544.53881781114
Iteration 1000: Loss = -11537.529471323582
Iteration 1100: Loss = -11533.8744252498
Iteration 1200: Loss = -11533.718555102627
Iteration 1300: Loss = -11533.682052450513
Iteration 1400: Loss = -11533.62662711328
Iteration 1500: Loss = -11527.223422958952
Iteration 1600: Loss = -11526.896166908422
Iteration 1700: Loss = -11526.874404630033
Iteration 1800: Loss = -11526.880770853606
1
Iteration 1900: Loss = -11525.296396931797
Iteration 2000: Loss = -11524.771868636635
Iteration 2100: Loss = -11524.699492129646
Iteration 2200: Loss = -11524.692733148984
Iteration 2300: Loss = -11524.622037338788
Iteration 2400: Loss = -11524.61834633333
Iteration 2500: Loss = -11524.616460961788
Iteration 2600: Loss = -11524.614386732057
Iteration 2700: Loss = -11520.972258200207
Iteration 2800: Loss = -11520.667426488797
Iteration 2900: Loss = -11520.653886366705
Iteration 3000: Loss = -11514.463822892438
Iteration 3100: Loss = -11514.44041514496
Iteration 3200: Loss = -11514.436748176342
Iteration 3300: Loss = -11514.397630999965
Iteration 3400: Loss = -11514.39611092346
Iteration 3500: Loss = -11514.395195676896
Iteration 3600: Loss = -11514.39458474579
Iteration 3700: Loss = -11514.393375200905
Iteration 3800: Loss = -11514.395456860837
1
Iteration 3900: Loss = -11514.395116189688
2
Iteration 4000: Loss = -11514.391306762725
Iteration 4100: Loss = -11514.391049853837
Iteration 4200: Loss = -11514.39081990295
Iteration 4300: Loss = -11514.390343838542
Iteration 4400: Loss = -11514.390073447803
Iteration 4500: Loss = -11514.390011520509
Iteration 4600: Loss = -11514.391377909438
1
Iteration 4700: Loss = -11514.408380593419
2
Iteration 4800: Loss = -11514.388940058589
Iteration 4900: Loss = -11514.392240599027
1
Iteration 5000: Loss = -11514.388488025144
Iteration 5100: Loss = -11514.388607813815
1
Iteration 5200: Loss = -11514.387878552985
Iteration 5300: Loss = -11514.387561984082
Iteration 5400: Loss = -11514.38696014385
Iteration 5500: Loss = -11514.420668540619
1
Iteration 5600: Loss = -11514.386255608048
Iteration 5700: Loss = -11514.390525276709
1
Iteration 5800: Loss = -11514.38645930783
2
Iteration 5900: Loss = -11514.400769019481
3
Iteration 6000: Loss = -11514.38987193421
4
Iteration 6100: Loss = -11514.385751833697
Iteration 6200: Loss = -11514.385715300044
Iteration 6300: Loss = -11514.385584919488
Iteration 6400: Loss = -11514.38554762024
Iteration 6500: Loss = -11514.386149859203
1
Iteration 6600: Loss = -11514.385472726466
Iteration 6700: Loss = -11514.385391551114
Iteration 6800: Loss = -11514.385282731404
Iteration 6900: Loss = -11514.385143835403
Iteration 7000: Loss = -11514.258675147159
Iteration 7100: Loss = -11514.243585520566
Iteration 7200: Loss = -11514.234960431995
Iteration 7300: Loss = -11514.235065862318
1
Iteration 7400: Loss = -11514.234865955046
Iteration 7500: Loss = -11514.23472343377
Iteration 7600: Loss = -11508.788461595004
Iteration 7700: Loss = -11508.784996225513
Iteration 7800: Loss = -11508.785137246665
1
Iteration 7900: Loss = -11508.784155927437
Iteration 8000: Loss = -11508.783281590915
Iteration 8100: Loss = -11508.783121800752
Iteration 8200: Loss = -11508.785179897797
1
Iteration 8300: Loss = -11508.782733724376
Iteration 8400: Loss = -11508.779590574799
Iteration 8500: Loss = -11508.780035877797
1
Iteration 8600: Loss = -11508.779524771213
Iteration 8700: Loss = -11508.741770976432
Iteration 8800: Loss = -11508.739383578742
Iteration 8900: Loss = -11508.73311454871
Iteration 9000: Loss = -11508.73311943618
Iteration 9100: Loss = -11508.733093436776
Iteration 9200: Loss = -11508.733034902918
Iteration 9300: Loss = -11508.73700477782
1
Iteration 9400: Loss = -11508.732941428003
Iteration 9500: Loss = -11508.737480801588
1
Iteration 9600: Loss = -11508.732798807534
Iteration 9700: Loss = -11508.75796293048
1
Iteration 9800: Loss = -11508.739509811876
2
Iteration 9900: Loss = -11508.735375175393
3
Iteration 10000: Loss = -11508.73300156945
4
Iteration 10100: Loss = -11508.757172434634
5
Iteration 10200: Loss = -11508.733423429332
6
Iteration 10300: Loss = -11508.734150332963
7
Iteration 10400: Loss = -11508.748134564592
8
Iteration 10500: Loss = -11508.740021353256
9
Iteration 10600: Loss = -11508.732759173667
Iteration 10700: Loss = -11508.762629043922
1
Iteration 10800: Loss = -11508.735841875075
2
Iteration 10900: Loss = -11508.732704426326
Iteration 11000: Loss = -11508.73366334038
1
Iteration 11100: Loss = -11508.740900977631
2
Iteration 11200: Loss = -11508.733171488484
3
Iteration 11300: Loss = -11508.732446360824
Iteration 11400: Loss = -11508.732082250634
Iteration 11500: Loss = -11508.732309050583
1
Iteration 11600: Loss = -11508.732089217137
Iteration 11700: Loss = -11508.733377790431
1
Iteration 11800: Loss = -11508.732072406163
Iteration 11900: Loss = -11508.732757772063
1
Iteration 12000: Loss = -11508.771145740537
2
Iteration 12100: Loss = -11508.732067145254
Iteration 12200: Loss = -11508.876120774692
1
Iteration 12300: Loss = -11508.734960798074
2
Iteration 12400: Loss = -11508.72983092816
Iteration 12500: Loss = -11508.730145355628
1
Iteration 12600: Loss = -11508.72986603495
Iteration 12700: Loss = -11508.730099720482
1
Iteration 12800: Loss = -11508.957317312454
2
Iteration 12900: Loss = -11508.730275983526
3
Iteration 13000: Loss = -11508.732604206982
4
Iteration 13100: Loss = -11508.746182268249
5
Iteration 13200: Loss = -11508.729821433946
Iteration 13300: Loss = -11508.730352430359
1
Iteration 13400: Loss = -11508.742519568683
2
Iteration 13500: Loss = -11508.729810040893
Iteration 13600: Loss = -11508.754061126232
1
Iteration 13700: Loss = -11508.730371659789
2
Iteration 13800: Loss = -11508.731180771012
3
Iteration 13900: Loss = -11508.75252722568
4
Iteration 14000: Loss = -11508.729749081485
Iteration 14100: Loss = -11508.735544285493
1
Iteration 14200: Loss = -11508.823724451755
2
Iteration 14300: Loss = -11508.728748370246
Iteration 14400: Loss = -11508.74027516689
1
Iteration 14500: Loss = -11508.813318015973
2
Iteration 14600: Loss = -11508.728384451095
Iteration 14700: Loss = -11508.727930565485
Iteration 14800: Loss = -11508.734044964422
1
Iteration 14900: Loss = -11508.884321960368
2
Iteration 15000: Loss = -11508.727834396286
Iteration 15100: Loss = -11508.728095441287
1
Iteration 15200: Loss = -11508.727926850303
Iteration 15300: Loss = -11508.728229797847
1
Iteration 15400: Loss = -11508.731327832054
2
Iteration 15500: Loss = -11508.728036061033
3
Iteration 15600: Loss = -11508.72826081634
4
Iteration 15700: Loss = -11508.811675488074
5
Iteration 15800: Loss = -11508.727812577514
Iteration 15900: Loss = -11508.731681984882
1
Iteration 16000: Loss = -11508.727829842228
Iteration 16100: Loss = -11508.728870574949
1
Iteration 16200: Loss = -11508.727816821802
Iteration 16300: Loss = -11508.756029844419
1
Iteration 16400: Loss = -11508.727826502321
Iteration 16500: Loss = -11508.731457468917
1
Iteration 16600: Loss = -11508.72782228049
Iteration 16700: Loss = -11508.729373083786
1
Iteration 16800: Loss = -11508.764070535997
2
Iteration 16900: Loss = -11508.73657721244
3
Iteration 17000: Loss = -11508.729225434397
4
Iteration 17100: Loss = -11508.728610184673
5
Iteration 17200: Loss = -11508.83222589147
6
Iteration 17300: Loss = -11508.81109582243
7
Iteration 17400: Loss = -11508.727615952384
Iteration 17500: Loss = -11508.728403826575
1
Iteration 17600: Loss = -11508.727946558205
2
Iteration 17700: Loss = -11508.727528209516
Iteration 17800: Loss = -11509.013813062988
1
Iteration 17900: Loss = -11508.72751594199
Iteration 18000: Loss = -11508.747859999052
1
Iteration 18100: Loss = -11508.72759522532
Iteration 18200: Loss = -11508.727695505037
1
Iteration 18300: Loss = -11508.82855192196
2
Iteration 18400: Loss = -11508.730458129892
3
Iteration 18500: Loss = -11508.729217325706
4
Iteration 18600: Loss = -11508.72754980938
Iteration 18700: Loss = -11508.64995380352
Iteration 18800: Loss = -11508.578536568606
Iteration 18900: Loss = -11508.608052928179
1
Iteration 19000: Loss = -11508.551276078786
Iteration 19100: Loss = -11508.55645184595
1
Iteration 19200: Loss = -11508.564555774401
2
Iteration 19300: Loss = -11508.550474173651
Iteration 19400: Loss = -11508.628419659999
1
Iteration 19500: Loss = -11508.551183049909
2
Iteration 19600: Loss = -11508.550496125035
Iteration 19700: Loss = -11508.554384057334
1
Iteration 19800: Loss = -11508.59185552052
2
Iteration 19900: Loss = -11508.550384754573
pi: tensor([[0.4310, 0.5690],
        [0.6935, 0.3065]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5799, 0.4201], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2504, 0.1003],
         [0.7196, 0.3141]],

        [[0.6894, 0.0891],
         [0.6581, 0.6715]],

        [[0.5700, 0.0909],
         [0.5719, 0.6326]],

        [[0.6861, 0.1029],
         [0.5109, 0.6746]],

        [[0.6593, 0.1011],
         [0.7253, 0.5982]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 19
Adjusted Rand Index: 0.3778917675973077
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.772151675588645
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.772151675588645
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208025343189018
Global Adjusted Rand Index: 0.06771066682493947
Average Adjusted Rand Index: 0.7605995306186999
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20733.7484841106
Iteration 100: Loss = -11904.757088290895
Iteration 200: Loss = -11895.311224176876
Iteration 300: Loss = -11728.451278591318
Iteration 400: Loss = -11459.453437477478
Iteration 500: Loss = -11427.73182797461
Iteration 600: Loss = -11426.351986286003
Iteration 700: Loss = -11410.655681655713
Iteration 800: Loss = -11403.041329241129
Iteration 900: Loss = -11394.108083001445
Iteration 1000: Loss = -11394.013644902407
Iteration 1100: Loss = -11393.945468054684
Iteration 1200: Loss = -11393.892424976959
Iteration 1300: Loss = -11393.850806442171
Iteration 1400: Loss = -11393.814278583426
Iteration 1500: Loss = -11393.760803771698
Iteration 1600: Loss = -11379.70242282092
Iteration 1700: Loss = -11379.651952917959
Iteration 1800: Loss = -11379.596364770898
Iteration 1900: Loss = -11379.532207955279
Iteration 2000: Loss = -11379.481713234638
Iteration 2100: Loss = -11372.938585159805
Iteration 2200: Loss = -11372.917884247214
Iteration 2300: Loss = -11372.9075172068
Iteration 2400: Loss = -11372.8997538988
Iteration 2500: Loss = -11372.893312309823
Iteration 2600: Loss = -11372.887660310736
Iteration 2700: Loss = -11372.88257777244
Iteration 2800: Loss = -11372.877808751617
Iteration 2900: Loss = -11372.872880171275
Iteration 3000: Loss = -11372.86569434985
Iteration 3100: Loss = -11372.852179728367
Iteration 3200: Loss = -11372.852296656809
1
Iteration 3300: Loss = -11372.83495818832
Iteration 3400: Loss = -11372.799149916365
Iteration 3500: Loss = -11372.786085006257
Iteration 3600: Loss = -11372.777480185357
Iteration 3700: Loss = -11372.773187301706
Iteration 3800: Loss = -11372.77003386539
Iteration 3900: Loss = -11372.767538194023
Iteration 4000: Loss = -11372.765267972469
Iteration 4100: Loss = -11372.76351117276
Iteration 4200: Loss = -11372.76189130107
Iteration 4300: Loss = -11372.760490108787
Iteration 4400: Loss = -11372.76113844729
1
Iteration 4500: Loss = -11372.75827181457
Iteration 4600: Loss = -11372.757284073548
Iteration 4700: Loss = -11372.756483418745
Iteration 4800: Loss = -11372.75563795712
Iteration 4900: Loss = -11372.7557110131
Iteration 5000: Loss = -11372.754263789522
Iteration 5100: Loss = -11372.75534090539
1
Iteration 5200: Loss = -11372.752971551578
Iteration 5300: Loss = -11372.75265594984
Iteration 5400: Loss = -11372.75181365442
Iteration 5500: Loss = -11372.758684271195
1
Iteration 5600: Loss = -11372.74990127379
Iteration 5700: Loss = -11372.747590982264
Iteration 5800: Loss = -11372.735187110948
Iteration 5900: Loss = -11372.731667907783
Iteration 6000: Loss = -11372.728499323548
Iteration 6100: Loss = -11372.727718103868
Iteration 6200: Loss = -11372.726809563148
Iteration 6300: Loss = -11372.725188846094
Iteration 6400: Loss = -11372.718120159452
Iteration 6500: Loss = -11372.653737369217
Iteration 6600: Loss = -11372.640559715754
Iteration 6700: Loss = -11372.644319371597
1
Iteration 6800: Loss = -11372.639461336928
Iteration 6900: Loss = -11372.639061364183
Iteration 7000: Loss = -11372.638810059272
Iteration 7100: Loss = -11372.63927382444
1
Iteration 7200: Loss = -11372.640172483989
2
Iteration 7300: Loss = -11372.649242040477
3
Iteration 7400: Loss = -11372.637995845009
Iteration 7500: Loss = -11372.637825932345
Iteration 7600: Loss = -11372.63818892934
1
Iteration 7700: Loss = -11372.646574519533
2
Iteration 7800: Loss = -11372.637284652243
Iteration 7900: Loss = -11372.637063222854
Iteration 8000: Loss = -11372.637128245908
Iteration 8100: Loss = -11372.636061903044
Iteration 8200: Loss = -11372.635711082034
Iteration 8300: Loss = -11372.717172177083
1
Iteration 8400: Loss = -11372.627534606307
Iteration 8500: Loss = -11372.628848682893
1
Iteration 8600: Loss = -11372.626383872393
Iteration 8700: Loss = -11372.625690827543
Iteration 8800: Loss = -11372.629945918627
1
Iteration 8900: Loss = -11372.627806637202
2
Iteration 9000: Loss = -11372.505335696645
Iteration 9100: Loss = -11372.504179325048
Iteration 9200: Loss = -11372.498898905294
Iteration 9300: Loss = -11372.500407571088
1
Iteration 9400: Loss = -11372.498805074309
Iteration 9500: Loss = -11372.57797964469
1
Iteration 9600: Loss = -11372.499134642068
2
Iteration 9700: Loss = -11372.563157693417
3
Iteration 9800: Loss = -11372.629736965444
4
Iteration 9900: Loss = -11372.498187203475
Iteration 10000: Loss = -11372.498251358567
Iteration 10100: Loss = -11372.547288940863
1
Iteration 10200: Loss = -11372.539604947055
2
Iteration 10300: Loss = -11372.501380379528
3
Iteration 10400: Loss = -11372.497750138764
Iteration 10500: Loss = -11372.50752430298
1
Iteration 10600: Loss = -11372.495595769302
Iteration 10700: Loss = -11372.490835968727
Iteration 10800: Loss = -11372.526732878503
1
Iteration 10900: Loss = -11372.523353378961
2
Iteration 11000: Loss = -11372.497871521755
3
Iteration 11100: Loss = -11372.507917204783
4
Iteration 11200: Loss = -11372.490749832048
Iteration 11300: Loss = -11372.4973865286
1
Iteration 11400: Loss = -11372.490856193213
2
Iteration 11500: Loss = -11372.500175230345
3
Iteration 11600: Loss = -11372.497381960864
4
Iteration 11700: Loss = -11372.4902756387
Iteration 11800: Loss = -11372.490424015115
1
Iteration 11900: Loss = -11372.503359344082
2
Iteration 12000: Loss = -11372.49638383827
3
Iteration 12100: Loss = -11372.4875891311
Iteration 12200: Loss = -11372.494999125218
1
Iteration 12300: Loss = -11372.489334251848
2
Iteration 12400: Loss = -11372.61543957315
3
Iteration 12500: Loss = -11372.488012919397
4
Iteration 12600: Loss = -11372.487684671727
Iteration 12700: Loss = -11372.52073348769
1
Iteration 12800: Loss = -11372.48842199059
2
Iteration 12900: Loss = -11372.489762227351
3
Iteration 13000: Loss = -11372.496792243795
4
Iteration 13100: Loss = -11372.488495845837
5
Iteration 13200: Loss = -11372.490355913722
6
Iteration 13300: Loss = -11372.480911699684
Iteration 13400: Loss = -11372.497041447748
1
Iteration 13500: Loss = -11372.511997992871
2
Iteration 13600: Loss = -11372.482382912198
3
Iteration 13700: Loss = -11372.471390604893
Iteration 13800: Loss = -11372.469604055563
Iteration 13900: Loss = -11372.47424224051
1
Iteration 14000: Loss = -11372.46920478383
Iteration 14100: Loss = -11372.468941595438
Iteration 14200: Loss = -11372.46906202136
1
Iteration 14300: Loss = -11372.472756976755
2
Iteration 14400: Loss = -11372.473223002138
3
Iteration 14500: Loss = -11372.477414447188
4
Iteration 14600: Loss = -11372.471443104314
5
Iteration 14700: Loss = -11372.47587154512
6
Iteration 14800: Loss = -11372.472191288383
7
Iteration 14900: Loss = -11372.471351643673
8
Iteration 15000: Loss = -11372.474809201754
9
Iteration 15100: Loss = -11372.47394289213
10
Iteration 15200: Loss = -11372.501351386696
11
Iteration 15300: Loss = -11372.478211314452
12
Iteration 15400: Loss = -11372.489642002018
13
Iteration 15500: Loss = -11372.475321858972
14
Iteration 15600: Loss = -11372.473165612826
15
Stopping early at iteration 15600 due to no improvement.
pi: tensor([[0.7063, 0.2937],
        [0.2574, 0.7426]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9740, 0.0260], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1914, 0.1412],
         [0.5248, 0.3944]],

        [[0.5733, 0.0897],
         [0.5851, 0.6164]],

        [[0.6594, 0.0911],
         [0.6645, 0.6225]],

        [[0.6079, 0.1060],
         [0.6145, 0.7176]],

        [[0.7046, 0.1033],
         [0.6088, 0.6613]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 62
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.711463472943519
Average Adjusted Rand Index: 0.7919971467023198
11268.300738954336
[0.06771066682493947, 0.711463472943519] [0.7605995306186999, 0.7919971467023198] [11508.553813738577, 11372.473165612826]
-------------------------------------
This iteration is 12
True Objective function: Loss = -11667.316261335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20271.939567122947
Iteration 100: Loss = -12582.025015828363
Iteration 200: Loss = -12557.676974928303
Iteration 300: Loss = -11924.128530973703
Iteration 400: Loss = -11810.28784041183
Iteration 500: Loss = -11795.881207807624
Iteration 600: Loss = -11795.525023048272
Iteration 700: Loss = -11795.32860341732
Iteration 800: Loss = -11794.0121020466
Iteration 900: Loss = -11793.821209079095
Iteration 1000: Loss = -11793.762302108054
Iteration 1100: Loss = -11793.714449632114
Iteration 1200: Loss = -11793.63236670955
Iteration 1300: Loss = -11793.600368168129
Iteration 1400: Loss = -11793.579787683897
Iteration 1500: Loss = -11793.56302053555
Iteration 1600: Loss = -11793.549082500305
Iteration 1700: Loss = -11793.537218437554
Iteration 1800: Loss = -11793.527089736812
Iteration 1900: Loss = -11793.518279957854
Iteration 2000: Loss = -11793.510597422877
Iteration 2100: Loss = -11793.50357310136
Iteration 2200: Loss = -11793.497100873252
Iteration 2300: Loss = -11793.490591547657
Iteration 2400: Loss = -11793.483212945499
Iteration 2500: Loss = -11793.472783740095
Iteration 2600: Loss = -11793.46480114379
Iteration 2700: Loss = -11793.333441775068
Iteration 2800: Loss = -11791.426712647346
Iteration 2900: Loss = -11786.910199643062
Iteration 3000: Loss = -11785.406673522635
Iteration 3100: Loss = -11785.177729715757
Iteration 3200: Loss = -11785.13109646325
Iteration 3300: Loss = -11785.10787448848
Iteration 3400: Loss = -11785.098586683305
Iteration 3500: Loss = -11785.081336283189
Iteration 3600: Loss = -11784.785690341012
Iteration 3700: Loss = -11784.769443234734
Iteration 3800: Loss = -11784.763534475573
Iteration 3900: Loss = -11784.760379508607
Iteration 4000: Loss = -11784.758482436173
Iteration 4100: Loss = -11784.759179970244
1
Iteration 4200: Loss = -11784.756167929165
Iteration 4300: Loss = -11784.755204740066
Iteration 4400: Loss = -11784.756222755585
1
Iteration 4500: Loss = -11784.753649351798
Iteration 4600: Loss = -11784.761683758363
1
Iteration 4700: Loss = -11784.752378738118
Iteration 4800: Loss = -11784.75241650368
Iteration 4900: Loss = -11784.751313246978
Iteration 5000: Loss = -11784.751060196133
Iteration 5100: Loss = -11784.750379291687
Iteration 5200: Loss = -11784.752928252807
1
Iteration 5300: Loss = -11784.749635922455
Iteration 5400: Loss = -11784.75320634758
1
Iteration 5500: Loss = -11784.748917991612
Iteration 5600: Loss = -11784.748666405696
Iteration 5700: Loss = -11784.748478354324
Iteration 5800: Loss = -11784.74854982412
Iteration 5900: Loss = -11784.752272198046
1
Iteration 6000: Loss = -11784.74761025426
Iteration 6100: Loss = -11784.74739393831
Iteration 6200: Loss = -11784.747173914075
Iteration 6300: Loss = -11784.747071142561
Iteration 6400: Loss = -11784.747028285452
Iteration 6500: Loss = -11784.747677399208
1
Iteration 6600: Loss = -11784.747223063076
2
Iteration 6700: Loss = -11784.746263622208
Iteration 6800: Loss = -11784.746113673998
Iteration 6900: Loss = -11784.758558192172
1
Iteration 7000: Loss = -11784.74816097258
2
Iteration 7100: Loss = -11784.753201060981
3
Iteration 7200: Loss = -11784.745917572984
Iteration 7300: Loss = -11784.746434415309
1
Iteration 7400: Loss = -11784.745431284036
Iteration 7500: Loss = -11784.745495993375
Iteration 7600: Loss = -11784.74521398163
Iteration 7700: Loss = -11784.745571253221
1
Iteration 7800: Loss = -11784.749035291929
2
Iteration 7900: Loss = -11784.77339283164
3
Iteration 8000: Loss = -11784.750680162702
4
Iteration 8100: Loss = -11784.759548053775
5
Iteration 8200: Loss = -11784.752256774873
6
Iteration 8300: Loss = -11784.747127499006
7
Iteration 8400: Loss = -11784.754264617613
8
Iteration 8500: Loss = -11784.751533881972
9
Iteration 8600: Loss = -11784.744965560329
Iteration 8700: Loss = -11784.745355426194
1
Iteration 8800: Loss = -11784.757833292568
2
Iteration 8900: Loss = -11784.74440063807
Iteration 9000: Loss = -11784.745137820217
1
Iteration 9100: Loss = -11784.7443340907
Iteration 9200: Loss = -11784.744400184429
Iteration 9300: Loss = -11784.748093985016
1
Iteration 9400: Loss = -11784.744198087828
Iteration 9500: Loss = -11784.74409650598
Iteration 9600: Loss = -11784.744292199246
1
Iteration 9700: Loss = -11784.746467951913
2
Iteration 9800: Loss = -11784.747459819915
3
Iteration 9900: Loss = -11784.74435491713
4
Iteration 10000: Loss = -11784.748316566645
5
Iteration 10100: Loss = -11784.744543359107
6
Iteration 10200: Loss = -11784.743999543813
Iteration 10300: Loss = -11784.757256419462
1
Iteration 10400: Loss = -11784.745107788904
2
Iteration 10500: Loss = -11784.743806313854
Iteration 10600: Loss = -11784.745071621735
1
Iteration 10700: Loss = -11784.745487671094
2
Iteration 10800: Loss = -11784.74388257886
Iteration 10900: Loss = -11784.74376245456
Iteration 11000: Loss = -11784.765269928905
1
Iteration 11100: Loss = -11784.747113907615
2
Iteration 11200: Loss = -11784.743566673296
Iteration 11300: Loss = -11784.748682664593
1
Iteration 11400: Loss = -11784.78296788005
2
Iteration 11500: Loss = -11784.745619562
3
Iteration 11600: Loss = -11784.743680694319
4
Iteration 11700: Loss = -11784.744566666599
5
Iteration 11800: Loss = -11784.743548062303
Iteration 11900: Loss = -11784.75026510857
1
Iteration 12000: Loss = -11784.840433515325
2
Iteration 12100: Loss = -11784.744071637528
3
Iteration 12200: Loss = -11784.745524917
4
Iteration 12300: Loss = -11784.75209029794
5
Iteration 12400: Loss = -11784.74455050483
6
Iteration 12500: Loss = -11784.748810371113
7
Iteration 12600: Loss = -11784.748314282027
8
Iteration 12700: Loss = -11784.75696641805
9
Iteration 12800: Loss = -11784.743509756934
Iteration 12900: Loss = -11784.744000240342
1
Iteration 13000: Loss = -11784.74889802295
2
Iteration 13100: Loss = -11784.744795559001
3
Iteration 13200: Loss = -11784.743488489532
Iteration 13300: Loss = -11784.743632055593
1
Iteration 13400: Loss = -11784.754491618394
2
Iteration 13500: Loss = -11784.774903723513
3
Iteration 13600: Loss = -11784.807478167819
4
Iteration 13700: Loss = -11784.766467922089
5
Iteration 13800: Loss = -11784.753391923143
6
Iteration 13900: Loss = -11784.742989769922
Iteration 14000: Loss = -11784.743200129551
1
Iteration 14100: Loss = -11784.745997322105
2
Iteration 14200: Loss = -11784.74486461436
3
Iteration 14300: Loss = -11784.748940563928
4
Iteration 14400: Loss = -11784.748346849145
5
Iteration 14500: Loss = -11784.875656361157
6
Iteration 14600: Loss = -11784.746011419915
7
Iteration 14700: Loss = -11784.74381793459
8
Iteration 14800: Loss = -11784.743055977378
Iteration 14900: Loss = -11784.762237084984
1
Iteration 15000: Loss = -11784.75207787387
2
Iteration 15100: Loss = -11784.931935906805
3
Iteration 15200: Loss = -11784.743342734482
4
Iteration 15300: Loss = -11784.743502597707
5
Iteration 15400: Loss = -11784.785390213672
6
Iteration 15500: Loss = -11784.749546114033
7
Iteration 15600: Loss = -11784.765239473423
8
Iteration 15700: Loss = -11784.748480143853
9
Iteration 15800: Loss = -11784.74362746174
10
Iteration 15900: Loss = -11784.743559872713
11
Iteration 16000: Loss = -11784.743042405456
Iteration 16100: Loss = -11784.743249512567
1
Iteration 16200: Loss = -11784.76985248016
2
Iteration 16300: Loss = -11784.783446732477
3
Iteration 16400: Loss = -11784.743057611127
Iteration 16500: Loss = -11784.743439302441
1
Iteration 16600: Loss = -11784.745905270454
2
Iteration 16700: Loss = -11784.746743002514
3
Iteration 16800: Loss = -11784.751311945203
4
Iteration 16900: Loss = -11784.749252959922
5
Iteration 17000: Loss = -11784.786221395532
6
Iteration 17100: Loss = -11784.745918978133
7
Iteration 17200: Loss = -11784.743511946042
8
Iteration 17300: Loss = -11784.74483580488
9
Iteration 17400: Loss = -11784.746690018068
10
Iteration 17500: Loss = -11784.7429922605
Iteration 17600: Loss = -11784.742894590003
Iteration 17700: Loss = -11784.74304580319
1
Iteration 17800: Loss = -11784.742968201943
Iteration 17900: Loss = -11784.744885245034
1
Iteration 18000: Loss = -11784.858583309844
2
Iteration 18100: Loss = -11784.745899201273
3
Iteration 18200: Loss = -11784.742999355622
Iteration 18300: Loss = -11784.751081515242
1
Iteration 18400: Loss = -11784.744860034823
2
Iteration 18500: Loss = -11784.762082449643
3
Iteration 18600: Loss = -11784.74847934912
4
Iteration 18700: Loss = -11784.742988890537
Iteration 18800: Loss = -11784.744216932055
1
Iteration 18900: Loss = -11784.909597151247
2
Iteration 19000: Loss = -11784.74295046093
Iteration 19100: Loss = -11784.743739645255
1
Iteration 19200: Loss = -11784.93307784863
2
Iteration 19300: Loss = -11784.742895075568
Iteration 19400: Loss = -11784.744771484777
1
Iteration 19500: Loss = -11784.811595701369
2
Iteration 19600: Loss = -11784.742934879385
Iteration 19700: Loss = -11784.74665418894
1
Iteration 19800: Loss = -11784.836663071468
2
Iteration 19900: Loss = -11784.750533800176
3
pi: tensor([[0.7607, 0.2393],
        [0.3855, 0.6145]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1089, 0.8911], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3988, 0.1025],
         [0.5237, 0.2088]],

        [[0.5733, 0.0914],
         [0.6223, 0.5676]],

        [[0.7155, 0.0930],
         [0.6182, 0.6734]],

        [[0.7228, 0.1011],
         [0.6848, 0.5734]],

        [[0.6346, 0.0923],
         [0.5597, 0.6385]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.008319455648947949
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.601374792000983
Average Adjusted Rand Index: 0.8016638911297896
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22471.02886966651
Iteration 100: Loss = -12586.134590939573
Iteration 200: Loss = -12104.541966311497
Iteration 300: Loss = -11898.795372193334
Iteration 400: Loss = -11809.387684940391
Iteration 500: Loss = -11797.77684068982
Iteration 600: Loss = -11791.137272626791
Iteration 700: Loss = -11788.387677761453
Iteration 800: Loss = -11781.836470224129
Iteration 900: Loss = -11771.133719444873
Iteration 1000: Loss = -11758.646421087835
Iteration 1100: Loss = -11741.759522655328
Iteration 1200: Loss = -11738.647524289276
Iteration 1300: Loss = -11717.010396949327
Iteration 1400: Loss = -11704.513544461028
Iteration 1500: Loss = -11687.413211445608
Iteration 1600: Loss = -11672.73643495563
Iteration 1700: Loss = -11669.701866574696
Iteration 1800: Loss = -11661.804926254837
Iteration 1900: Loss = -11661.774126862376
Iteration 2000: Loss = -11661.751606047823
Iteration 2100: Loss = -11661.734514002932
Iteration 2200: Loss = -11661.722646113252
Iteration 2300: Loss = -11661.707477071031
Iteration 2400: Loss = -11661.69348824286
Iteration 2500: Loss = -11661.681012716315
Iteration 2600: Loss = -11661.665797396408
Iteration 2700: Loss = -11661.65937480686
Iteration 2800: Loss = -11661.654173514915
Iteration 2900: Loss = -11661.649492503664
Iteration 3000: Loss = -11661.646840594562
Iteration 3100: Loss = -11661.64177065868
Iteration 3200: Loss = -11661.650711624437
1
Iteration 3300: Loss = -11661.638138911172
Iteration 3400: Loss = -11661.635935390157
Iteration 3500: Loss = -11661.628866560795
Iteration 3600: Loss = -11661.63277891083
1
Iteration 3700: Loss = -11661.188448358138
Iteration 3800: Loss = -11660.671750126918
Iteration 3900: Loss = -11660.66950448941
Iteration 4000: Loss = -11660.667498762516
Iteration 4100: Loss = -11660.666005907062
Iteration 4200: Loss = -11660.664797830867
Iteration 4300: Loss = -11660.6632854855
Iteration 4400: Loss = -11660.662543551993
Iteration 4500: Loss = -11660.66101417491
Iteration 4600: Loss = -11660.660069995381
Iteration 4700: Loss = -11660.660144621083
Iteration 4800: Loss = -11660.658235622499
Iteration 4900: Loss = -11660.657620097845
Iteration 5000: Loss = -11660.662508911846
1
Iteration 5100: Loss = -11660.65603664707
Iteration 5200: Loss = -11660.655450973687
Iteration 5300: Loss = -11660.654762252656
Iteration 5400: Loss = -11660.654252677588
Iteration 5500: Loss = -11660.655154357133
1
Iteration 5600: Loss = -11660.653331994003
Iteration 5700: Loss = -11660.653991563893
1
Iteration 5800: Loss = -11660.652257302985
Iteration 5900: Loss = -11660.652495300645
1
Iteration 6000: Loss = -11660.652883144581
2
Iteration 6100: Loss = -11660.651097919279
Iteration 6200: Loss = -11660.650885058003
Iteration 6300: Loss = -11660.65074781889
Iteration 6400: Loss = -11660.651035015446
1
Iteration 6500: Loss = -11660.649787030183
Iteration 6600: Loss = -11660.649512907903
Iteration 6700: Loss = -11660.649447820517
Iteration 6800: Loss = -11660.669136901435
1
Iteration 6900: Loss = -11660.662234602374
2
Iteration 7000: Loss = -11660.649466974754
Iteration 7100: Loss = -11660.649845008764
1
Iteration 7200: Loss = -11660.64832188477
Iteration 7300: Loss = -11660.648187545656
Iteration 7400: Loss = -11660.653854720398
1
Iteration 7500: Loss = -11660.652961546384
2
Iteration 7600: Loss = -11660.648150500298
Iteration 7700: Loss = -11660.647522691837
Iteration 7800: Loss = -11660.649700330416
1
Iteration 7900: Loss = -11660.647256201431
Iteration 8000: Loss = -11660.649765956783
1
Iteration 8100: Loss = -11660.647031011995
Iteration 8200: Loss = -11660.71563606097
1
Iteration 8300: Loss = -11660.646828305435
Iteration 8400: Loss = -11660.662342384978
1
Iteration 8500: Loss = -11660.647081254006
2
Iteration 8600: Loss = -11660.64797855988
3
Iteration 8700: Loss = -11660.648378645914
4
Iteration 8800: Loss = -11660.651169738601
5
Iteration 8900: Loss = -11660.67872590635
6
Iteration 9000: Loss = -11660.648790876063
7
Iteration 9100: Loss = -11660.647614665051
8
Iteration 9200: Loss = -11660.649146268262
9
Iteration 9300: Loss = -11660.80623524576
10
Iteration 9400: Loss = -11660.653584563574
11
Iteration 9500: Loss = -11660.6538961719
12
Iteration 9600: Loss = -11660.655231426412
13
Iteration 9700: Loss = -11660.662647948602
14
Iteration 9800: Loss = -11660.646487246775
Iteration 9900: Loss = -11660.646141337376
Iteration 10000: Loss = -11660.655563520088
1
Iteration 10100: Loss = -11660.648436831048
2
Iteration 10200: Loss = -11660.646648428317
3
Iteration 10300: Loss = -11660.646203533697
Iteration 10400: Loss = -11660.646243115323
Iteration 10500: Loss = -11660.680711982672
1
Iteration 10600: Loss = -11660.657732375004
2
Iteration 10700: Loss = -11660.648675836639
3
Iteration 10800: Loss = -11660.72425010733
4
Iteration 10900: Loss = -11660.64640711567
5
Iteration 11000: Loss = -11660.648955263112
6
Iteration 11100: Loss = -11660.67349454097
7
Iteration 11200: Loss = -11660.669780500813
8
Iteration 11300: Loss = -11660.674590567101
9
Iteration 11400: Loss = -11660.675329823964
10
Iteration 11500: Loss = -11660.725483887272
11
Iteration 11600: Loss = -11660.668892273692
12
Iteration 11700: Loss = -11660.653808669584
13
Iteration 11800: Loss = -11660.646543844214
14
Iteration 11900: Loss = -11660.645804917576
Iteration 12000: Loss = -11660.64569698297
Iteration 12100: Loss = -11660.652944864265
1
Iteration 12200: Loss = -11660.653571154327
2
Iteration 12300: Loss = -11660.651230184092
3
Iteration 12400: Loss = -11660.659619464685
4
Iteration 12500: Loss = -11660.645739083066
Iteration 12600: Loss = -11660.645422735271
Iteration 12700: Loss = -11660.64616306891
1
Iteration 12800: Loss = -11660.645363987562
Iteration 12900: Loss = -11660.657458229369
1
Iteration 13000: Loss = -11660.646462598632
2
Iteration 13100: Loss = -11660.654257169539
3
Iteration 13200: Loss = -11660.815454142803
4
Iteration 13300: Loss = -11660.657189602947
5
Iteration 13400: Loss = -11660.647626586853
6
Iteration 13500: Loss = -11660.650525929444
7
Iteration 13600: Loss = -11660.651130525788
8
Iteration 13700: Loss = -11660.646648217627
9
Iteration 13800: Loss = -11660.668077726654
10
Iteration 13900: Loss = -11660.649141381893
11
Iteration 14000: Loss = -11660.645331616857
Iteration 14100: Loss = -11660.646995847175
1
Iteration 14200: Loss = -11660.711186884993
2
Iteration 14300: Loss = -11660.6480608361
3
Iteration 14400: Loss = -11660.649944744828
4
Iteration 14500: Loss = -11660.698542004162
5
Iteration 14600: Loss = -11660.6535707272
6
Iteration 14700: Loss = -11660.64831281277
7
Iteration 14800: Loss = -11660.645670908829
8
Iteration 14900: Loss = -11660.676423408664
9
Iteration 15000: Loss = -11660.664919924304
10
Iteration 15100: Loss = -11660.646449496975
11
Iteration 15200: Loss = -11660.646134259334
12
Iteration 15300: Loss = -11660.649224685336
13
Iteration 15400: Loss = -11660.648876238194
14
Iteration 15500: Loss = -11660.64646798142
15
Stopping early at iteration 15500 due to no improvement.
pi: tensor([[0.7213, 0.2787],
        [0.2084, 0.7916]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5415, 0.4585], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.1092],
         [0.5530, 0.3984]],

        [[0.7302, 0.0910],
         [0.6975, 0.5143]],

        [[0.5703, 0.0928],
         [0.7035, 0.6185]],

        [[0.5128, 0.1009],
         [0.5348, 0.5855]],

        [[0.7241, 0.0927],
         [0.7216, 0.6603]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11667.316261335178
[0.601374792000983, 1.0] [0.8016638911297896, 1.0] [11784.744228699985, 11660.64646798142]
-------------------------------------
This iteration is 13
True Objective function: Loss = -11600.002723201182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19753.32393390011
Iteration 100: Loss = -12423.726428438022
Iteration 200: Loss = -12421.268561388928
Iteration 300: Loss = -12330.134863857227
Iteration 400: Loss = -11614.558336078993
Iteration 500: Loss = -11594.880867942102
Iteration 600: Loss = -11590.61813058596
Iteration 700: Loss = -11590.414306683224
Iteration 800: Loss = -11589.319090208906
Iteration 900: Loss = -11589.256296109601
Iteration 1000: Loss = -11589.209603239731
Iteration 1100: Loss = -11589.178249248527
Iteration 1200: Loss = -11589.155252383822
Iteration 1300: Loss = -11589.137058183878
Iteration 1400: Loss = -11589.122319572862
Iteration 1500: Loss = -11589.10995373593
Iteration 1600: Loss = -11589.099252535929
Iteration 1700: Loss = -11589.089303665398
Iteration 1800: Loss = -11589.079643306753
Iteration 1900: Loss = -11589.073112393575
Iteration 2000: Loss = -11589.0678514083
Iteration 2100: Loss = -11589.06309851707
Iteration 2200: Loss = -11589.058897859586
Iteration 2300: Loss = -11589.054801486667
Iteration 2400: Loss = -11589.048852229544
Iteration 2500: Loss = -11586.404088151148
Iteration 2600: Loss = -11586.39221288151
Iteration 2700: Loss = -11586.400976720797
1
Iteration 2800: Loss = -11586.387228052523
Iteration 2900: Loss = -11586.385128071124
Iteration 3000: Loss = -11586.383516180982
Iteration 3100: Loss = -11586.381508385479
Iteration 3200: Loss = -11586.379951358518
Iteration 3300: Loss = -11586.378492875006
Iteration 3400: Loss = -11586.376876128215
Iteration 3500: Loss = -11586.378397040773
1
Iteration 3600: Loss = -11586.338414851032
Iteration 3700: Loss = -11586.31398861403
Iteration 3800: Loss = -11586.313175679126
Iteration 3900: Loss = -11586.312400347078
Iteration 4000: Loss = -11586.311699101245
Iteration 4100: Loss = -11586.311382170918
Iteration 4200: Loss = -11586.310393229838
Iteration 4300: Loss = -11586.326803956204
1
Iteration 4400: Loss = -11586.309343769683
Iteration 4500: Loss = -11586.308804986129
Iteration 4600: Loss = -11586.309614835867
1
Iteration 4700: Loss = -11586.307916981244
Iteration 4800: Loss = -11586.307506232473
Iteration 4900: Loss = -11586.30724734045
Iteration 5000: Loss = -11586.306768973573
Iteration 5100: Loss = -11586.30643369659
Iteration 5200: Loss = -11586.306143087137
Iteration 5300: Loss = -11586.305809775477
Iteration 5400: Loss = -11586.305505401035
Iteration 5500: Loss = -11586.30526058442
Iteration 5600: Loss = -11586.30490148857
Iteration 5700: Loss = -11586.304647900542
Iteration 5800: Loss = -11586.304768321148
1
Iteration 5900: Loss = -11586.304084295618
Iteration 6000: Loss = -11586.303836473939
Iteration 6100: Loss = -11586.303598106822
Iteration 6200: Loss = -11586.303307757946
Iteration 6300: Loss = -11586.302954342274
Iteration 6400: Loss = -11586.317888163472
1
Iteration 6500: Loss = -11586.177882588066
Iteration 6600: Loss = -11586.17812614902
1
Iteration 6700: Loss = -11586.17762726942
Iteration 6800: Loss = -11586.180759805044
1
Iteration 6900: Loss = -11586.176784633217
Iteration 7000: Loss = -11586.17645287958
Iteration 7100: Loss = -11586.180127169973
1
Iteration 7200: Loss = -11586.176181869449
Iteration 7300: Loss = -11586.176114333903
Iteration 7400: Loss = -11586.177826421695
1
Iteration 7500: Loss = -11586.17591992951
Iteration 7600: Loss = -11586.175854858404
Iteration 7700: Loss = -11586.1835835522
1
Iteration 7800: Loss = -11586.17574109745
Iteration 7900: Loss = -11586.17688312499
1
Iteration 8000: Loss = -11586.17688289904
2
Iteration 8100: Loss = -11586.175552571842
Iteration 8200: Loss = -11586.175540003207
Iteration 8300: Loss = -11586.184465296215
1
Iteration 8400: Loss = -11586.175603722246
Iteration 8500: Loss = -11586.175461684823
Iteration 8600: Loss = -11586.17630774573
1
Iteration 8700: Loss = -11586.185307664895
2
Iteration 8800: Loss = -11586.182942646115
3
Iteration 8900: Loss = -11586.178473869631
4
Iteration 9000: Loss = -11586.17547320833
Iteration 9100: Loss = -11586.175888585041
1
Iteration 9200: Loss = -11586.178042331601
2
Iteration 9300: Loss = -11586.17976856378
3
Iteration 9400: Loss = -11586.179756485035
4
Iteration 9500: Loss = -11586.177765610935
5
Iteration 9600: Loss = -11586.230870410678
6
Iteration 9700: Loss = -11586.174831692535
Iteration 9800: Loss = -11586.171355235907
Iteration 9900: Loss = -11586.180805737027
1
Iteration 10000: Loss = -11586.171819137833
2
Iteration 10100: Loss = -11586.170068181314
Iteration 10200: Loss = -11586.168470215793
Iteration 10300: Loss = -11586.169336613453
1
Iteration 10400: Loss = -11586.169235465097
2
Iteration 10500: Loss = -11586.170468057313
3
Iteration 10600: Loss = -11586.171252441962
4
Iteration 10700: Loss = -11586.177474389318
5
Iteration 10800: Loss = -11586.182640353549
6
Iteration 10900: Loss = -11586.18138497725
7
Iteration 11000: Loss = -11586.159392532862
Iteration 11100: Loss = -11586.17521259379
1
Iteration 11200: Loss = -11586.158630666592
Iteration 11300: Loss = -11586.166342758681
1
Iteration 11400: Loss = -11586.159171368541
2
Iteration 11500: Loss = -11586.161196879448
3
Iteration 11600: Loss = -11586.158948119448
4
Iteration 11700: Loss = -11586.16585741041
5
Iteration 11800: Loss = -11586.174945615843
6
Iteration 11900: Loss = -11586.167539563668
7
Iteration 12000: Loss = -11586.160234085151
8
Iteration 12100: Loss = -11586.158792571347
9
Iteration 12200: Loss = -11586.159006466363
10
Iteration 12300: Loss = -11586.164021758716
11
Iteration 12400: Loss = -11586.167866507933
12
Iteration 12500: Loss = -11586.223483419644
13
Iteration 12600: Loss = -11586.251124478888
14
Iteration 12700: Loss = -11586.160155462509
15
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.7381, 0.2619],
        [0.2429, 0.7571]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5186, 0.4814], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1993, 0.1075],
         [0.6835, 0.4007]],

        [[0.7139, 0.0933],
         [0.5165, 0.6662]],

        [[0.5964, 0.0985],
         [0.5791, 0.5506]],

        [[0.6178, 0.0979],
         [0.5373, 0.5658]],

        [[0.5032, 0.1026],
         [0.6444, 0.7159]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9839991536604262
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22473.206043925526
Iteration 100: Loss = -12424.250086312099
Iteration 200: Loss = -12418.990403730815
Iteration 300: Loss = -12159.405116484566
Iteration 400: Loss = -11965.926190295033
Iteration 500: Loss = -11937.246921991264
Iteration 600: Loss = -11926.141248159895
Iteration 700: Loss = -11921.330650821365
Iteration 800: Loss = -11921.123996881674
Iteration 900: Loss = -11920.986014820319
Iteration 1000: Loss = -11920.871657046499
Iteration 1100: Loss = -11920.669315051535
Iteration 1200: Loss = -11920.547960528093
Iteration 1300: Loss = -11920.083817229819
Iteration 1400: Loss = -11914.077555333668
Iteration 1500: Loss = -11894.261814857278
Iteration 1600: Loss = -11886.370208097975
Iteration 1700: Loss = -11875.931423791833
Iteration 1800: Loss = -11875.073079205698
Iteration 1900: Loss = -11871.730130405056
Iteration 2000: Loss = -11869.831469431238
Iteration 2100: Loss = -11864.018323539358
Iteration 2200: Loss = -11863.96124137449
Iteration 2300: Loss = -11859.512549310326
Iteration 2400: Loss = -11859.46449508394
Iteration 2500: Loss = -11859.423722751637
Iteration 2600: Loss = -11859.404155821281
Iteration 2700: Loss = -11859.41286925923
1
Iteration 2800: Loss = -11859.388946516816
Iteration 2900: Loss = -11859.382633840612
Iteration 3000: Loss = -11859.376820072976
Iteration 3100: Loss = -11859.372144448158
Iteration 3200: Loss = -11859.368546492029
Iteration 3300: Loss = -11859.365890472362
Iteration 3400: Loss = -11859.362910952814
Iteration 3500: Loss = -11859.369539332325
1
Iteration 3600: Loss = -11859.358777276331
Iteration 3700: Loss = -11859.357510458503
Iteration 3800: Loss = -11859.356999243195
Iteration 3900: Loss = -11859.355999341466
Iteration 4000: Loss = -11859.352663709202
Iteration 4100: Loss = -11859.349815498601
Iteration 4200: Loss = -11859.285175880075
Iteration 4300: Loss = -11859.298877232048
1
Iteration 4400: Loss = -11859.282360686879
Iteration 4500: Loss = -11859.281659175163
Iteration 4600: Loss = -11859.281186403423
Iteration 4700: Loss = -11859.294271705348
1
Iteration 4800: Loss = -11859.2791078654
Iteration 4900: Loss = -11859.278785612205
Iteration 5000: Loss = -11859.277555868788
Iteration 5100: Loss = -11859.276520554402
Iteration 5200: Loss = -11859.243383962368
Iteration 5300: Loss = -11856.814370517277
Iteration 5400: Loss = -11856.806683700737
Iteration 5500: Loss = -11856.80616644347
Iteration 5600: Loss = -11856.807348847491
1
Iteration 5700: Loss = -11856.805138718959
Iteration 5800: Loss = -11856.804823735263
Iteration 5900: Loss = -11856.804755906456
Iteration 6000: Loss = -11856.811458266031
1
Iteration 6100: Loss = -11856.803804809293
Iteration 6200: Loss = -11856.803536279114
Iteration 6300: Loss = -11856.82702273882
1
Iteration 6400: Loss = -11856.802948727029
Iteration 6500: Loss = -11856.805812943394
1
Iteration 6600: Loss = -11856.802476345529
Iteration 6700: Loss = -11856.804887914368
1
Iteration 6800: Loss = -11856.801983340381
Iteration 6900: Loss = -11856.801832989446
Iteration 7000: Loss = -11856.801536274565
Iteration 7100: Loss = -11856.801172865027
Iteration 7200: Loss = -11856.800599637763
Iteration 7300: Loss = -11856.791411639686
Iteration 7400: Loss = -11856.739810787003
Iteration 7500: Loss = -11856.739690001996
Iteration 7600: Loss = -11856.73954334906
Iteration 7700: Loss = -11856.739715022262
1
Iteration 7800: Loss = -11856.739299663426
Iteration 7900: Loss = -11856.739380249766
Iteration 8000: Loss = -11856.739669886756
1
Iteration 8100: Loss = -11856.747416659842
2
Iteration 8200: Loss = -11856.738918838748
Iteration 8300: Loss = -11856.738830067938
Iteration 8400: Loss = -11856.738761300558
Iteration 8500: Loss = -11856.738692734136
Iteration 8600: Loss = -11856.738550853102
Iteration 8700: Loss = -11856.738730927274
1
Iteration 8800: Loss = -11856.738468530462
Iteration 8900: Loss = -11856.73840217027
Iteration 9000: Loss = -11856.738359807643
Iteration 9100: Loss = -11856.738454491537
Iteration 9200: Loss = -11856.738265749056
Iteration 9300: Loss = -11856.790532789628
1
Iteration 9400: Loss = -11856.738320434155
Iteration 9500: Loss = -11856.738167515348
Iteration 9600: Loss = -11856.738151784139
Iteration 9700: Loss = -11856.738033030215
Iteration 9800: Loss = -11856.769836792917
1
Iteration 9900: Loss = -11856.71499933669
Iteration 10000: Loss = -11856.876740943735
1
Iteration 10100: Loss = -11856.713737249163
Iteration 10200: Loss = -11856.72037541381
1
Iteration 10300: Loss = -11856.7168468195
2
Iteration 10400: Loss = -11856.713856538181
3
Iteration 10500: Loss = -11856.716896085269
4
Iteration 10600: Loss = -11856.713707846671
Iteration 10700: Loss = -11856.715701131414
1
Iteration 10800: Loss = -11856.716758245446
2
Iteration 10900: Loss = -11856.71442392229
3
Iteration 11000: Loss = -11856.714253129123
4
Iteration 11100: Loss = -11856.792271770244
5
Iteration 11200: Loss = -11856.724111368603
6
Iteration 11300: Loss = -11856.713917400946
7
Iteration 11400: Loss = -11856.714146592603
8
Iteration 11500: Loss = -11856.724609135998
9
Iteration 11600: Loss = -11856.714170438403
10
Iteration 11700: Loss = -11856.71400874784
11
Iteration 11800: Loss = -11856.715552598695
12
Iteration 11900: Loss = -11856.72210583274
13
Iteration 12000: Loss = -11856.715721432804
14
Iteration 12100: Loss = -11856.714756270974
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.6218, 0.3782],
        [0.3680, 0.6320]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2827, 0.7173], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3713, 0.1061],
         [0.6332, 0.2460]],

        [[0.6613, 0.0925],
         [0.6551, 0.5433]],

        [[0.5104, 0.0984],
         [0.7252, 0.7164]],

        [[0.5107, 0.0978],
         [0.6585, 0.6990]],

        [[0.7293, 0.1015],
         [0.6587, 0.6327]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 77
Adjusted Rand Index: 0.28546347370166186
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 87
Adjusted Rand Index: 0.5433434070303802
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.11113733587735841
Average Adjusted Rand Index: 0.7497605298068348
11600.002723201182
[0.9840320230862317, 0.11113733587735841] [0.9839991536604262, 0.7497605298068348] [11586.160155462509, 11856.714756270974]
-------------------------------------
This iteration is 14
True Objective function: Loss = -11591.886348073027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21306.215999157434
Iteration 100: Loss = -12316.41866171511
Iteration 200: Loss = -12290.763706274543
Iteration 300: Loss = -11837.243565996807
Iteration 400: Loss = -11632.137532584786
Iteration 500: Loss = -11601.641704387162
Iteration 600: Loss = -11601.223281505005
Iteration 700: Loss = -11601.034676134595
Iteration 800: Loss = -11600.921946047241
Iteration 900: Loss = -11600.845454284632
Iteration 1000: Loss = -11600.786909748835
Iteration 1100: Loss = -11600.7440654998
Iteration 1200: Loss = -11600.713454346955
Iteration 1300: Loss = -11600.689704685912
Iteration 1400: Loss = -11600.669746058453
Iteration 1500: Loss = -11592.249298709734
Iteration 1600: Loss = -11592.179334558243
Iteration 1700: Loss = -11592.03314321622
Iteration 1800: Loss = -11592.022954657827
Iteration 1900: Loss = -11592.014639436999
Iteration 2000: Loss = -11592.007559025138
Iteration 2100: Loss = -11592.001500449387
Iteration 2200: Loss = -11591.996171968462
Iteration 2300: Loss = -11591.991511049247
Iteration 2400: Loss = -11591.987419464474
Iteration 2500: Loss = -11591.983819236906
Iteration 2600: Loss = -11591.980567880995
Iteration 2700: Loss = -11591.983352617082
1
Iteration 2800: Loss = -11591.975172054585
Iteration 2900: Loss = -11591.974959132169
Iteration 3000: Loss = -11591.97073041243
Iteration 3100: Loss = -11591.968871479938
Iteration 3200: Loss = -11591.966776293264
Iteration 3300: Loss = -11587.834287848795
Iteration 3400: Loss = -11587.83144779589
Iteration 3500: Loss = -11587.827667345711
Iteration 3600: Loss = -11587.782493812023
Iteration 3700: Loss = -11587.78114161009
Iteration 3800: Loss = -11587.785321828225
1
Iteration 3900: Loss = -11587.779074636996
Iteration 4000: Loss = -11587.780681436334
1
Iteration 4100: Loss = -11587.777591219592
Iteration 4200: Loss = -11587.776703113528
Iteration 4300: Loss = -11587.776012858123
Iteration 4400: Loss = -11587.775258566719
Iteration 4500: Loss = -11587.776188576265
1
Iteration 4600: Loss = -11587.773429939018
Iteration 4700: Loss = -11587.771640828294
Iteration 4800: Loss = -11587.771012266718
Iteration 4900: Loss = -11587.770647891324
Iteration 5000: Loss = -11587.77003273261
Iteration 5100: Loss = -11587.76972265463
Iteration 5200: Loss = -11587.769230410411
Iteration 5300: Loss = -11587.769108749228
Iteration 5400: Loss = -11587.768494233482
Iteration 5500: Loss = -11587.768290615008
Iteration 5600: Loss = -11587.76826275856
Iteration 5700: Loss = -11587.76799762387
Iteration 5800: Loss = -11587.76881364421
1
Iteration 5900: Loss = -11587.766827553725
Iteration 6000: Loss = -11587.766844408896
Iteration 6100: Loss = -11587.766933993555
Iteration 6200: Loss = -11587.774632683235
1
Iteration 6300: Loss = -11587.766368053692
Iteration 6400: Loss = -11587.76597794808
Iteration 6500: Loss = -11587.765538478025
Iteration 6600: Loss = -11587.777023037921
1
Iteration 6700: Loss = -11587.767544288048
2
Iteration 6800: Loss = -11587.767471373138
3
Iteration 6900: Loss = -11587.8049049538
4
Iteration 7000: Loss = -11587.76546976689
Iteration 7100: Loss = -11587.763140145886
Iteration 7200: Loss = -11587.79018351891
1
Iteration 7300: Loss = -11587.762606832966
Iteration 7400: Loss = -11587.780196416405
1
Iteration 7500: Loss = -11587.762510669201
Iteration 7600: Loss = -11587.76220842068
Iteration 7700: Loss = -11587.804809730836
1
Iteration 7800: Loss = -11587.750363614683
Iteration 7900: Loss = -11587.748562505207
Iteration 8000: Loss = -11587.764520007137
1
Iteration 8100: Loss = -11587.748380969508
Iteration 8200: Loss = -11587.75664741881
1
Iteration 8300: Loss = -11587.748612967167
2
Iteration 8400: Loss = -11587.748382188496
Iteration 8500: Loss = -11587.75402956838
1
Iteration 8600: Loss = -11587.748043492222
Iteration 8700: Loss = -11587.748850580645
1
Iteration 8800: Loss = -11587.74791179957
Iteration 8900: Loss = -11587.7505072932
1
Iteration 9000: Loss = -11587.754995504956
2
Iteration 9100: Loss = -11587.749033377697
3
Iteration 9200: Loss = -11587.74880809228
4
Iteration 9300: Loss = -11587.75020975403
5
Iteration 9400: Loss = -11587.750114410808
6
Iteration 9500: Loss = -11587.754888120005
7
Iteration 9600: Loss = -11587.753404376223
8
Iteration 9700: Loss = -11587.850885622747
9
Iteration 9800: Loss = -11587.752798549507
10
Iteration 9900: Loss = -11587.747683764424
Iteration 10000: Loss = -11587.747728456081
Iteration 10100: Loss = -11587.783835879238
1
Iteration 10200: Loss = -11587.760687394657
2
Iteration 10300: Loss = -11587.74961270805
3
Iteration 10400: Loss = -11587.74787013092
4
Iteration 10500: Loss = -11587.75213226268
5
Iteration 10600: Loss = -11587.75899538183
6
Iteration 10700: Loss = -11587.750896084399
7
Iteration 10800: Loss = -11587.748878919561
8
Iteration 10900: Loss = -11587.747612779265
Iteration 11000: Loss = -11587.750411078347
1
Iteration 11100: Loss = -11587.75243508158
2
Iteration 11200: Loss = -11587.770524356747
3
Iteration 11300: Loss = -11587.748017936428
4
Iteration 11400: Loss = -11587.717163489846
Iteration 11500: Loss = -11587.71086675274
Iteration 11600: Loss = -11587.787653367039
1
Iteration 11700: Loss = -11587.728081180312
2
Iteration 11800: Loss = -11587.773399548421
3
Iteration 11900: Loss = -11587.7132906993
4
Iteration 12000: Loss = -11587.802212575512
5
Iteration 12100: Loss = -11587.713384712408
6
Iteration 12200: Loss = -11587.717509215456
7
Iteration 12300: Loss = -11587.71239405847
8
Iteration 12400: Loss = -11587.765063797506
9
Iteration 12500: Loss = -11587.73791840127
10
Iteration 12600: Loss = -11587.733789843338
11
Iteration 12700: Loss = -11587.713962319429
12
Iteration 12800: Loss = -11587.762598490704
13
Iteration 12900: Loss = -11587.71301617646
14
Iteration 13000: Loss = -11587.735805025673
15
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.7153, 0.2847],
        [0.2467, 0.7533]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5182, 0.4818], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4000, 0.1047],
         [0.6834, 0.2099]],

        [[0.6258, 0.1010],
         [0.6211, 0.5378]],

        [[0.5844, 0.0936],
         [0.6690, 0.6359]],

        [[0.5795, 0.1038],
         [0.6295, 0.5676]],

        [[0.5523, 0.1005],
         [0.7208, 0.6691]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24108.917747763026
Iteration 100: Loss = -12270.532408784153
Iteration 200: Loss = -11589.772815264278
Iteration 300: Loss = -11588.535802954315
Iteration 400: Loss = -11588.188946470256
Iteration 500: Loss = -11588.017516815136
Iteration 600: Loss = -11587.91731770222
Iteration 700: Loss = -11587.85493957327
Iteration 800: Loss = -11587.813276074397
Iteration 900: Loss = -11587.78357085973
Iteration 1000: Loss = -11587.761454257745
Iteration 1100: Loss = -11587.744488231372
Iteration 1200: Loss = -11587.731118429052
Iteration 1300: Loss = -11587.72045993699
Iteration 1400: Loss = -11587.71168118804
Iteration 1500: Loss = -11587.70447120296
Iteration 1600: Loss = -11587.698435972228
Iteration 1700: Loss = -11587.693298465096
Iteration 1800: Loss = -11587.688964400855
Iteration 1900: Loss = -11587.685185808225
Iteration 2000: Loss = -11587.681952716894
Iteration 2100: Loss = -11587.67909456413
Iteration 2200: Loss = -11587.676585212206
Iteration 2300: Loss = -11587.674420500547
Iteration 2400: Loss = -11587.672439210011
Iteration 2500: Loss = -11587.67073249437
Iteration 2600: Loss = -11587.669164249923
Iteration 2700: Loss = -11587.667735646477
Iteration 2800: Loss = -11587.66649113061
Iteration 2900: Loss = -11587.66535796523
Iteration 3000: Loss = -11587.664329297122
Iteration 3100: Loss = -11587.66340103212
Iteration 3200: Loss = -11587.664652983189
1
Iteration 3300: Loss = -11587.661808705376
Iteration 3400: Loss = -11587.661073397125
Iteration 3500: Loss = -11587.681886643204
1
Iteration 3600: Loss = -11587.659821105035
Iteration 3700: Loss = -11587.659270671127
Iteration 3800: Loss = -11587.658757865924
Iteration 3900: Loss = -11587.663406070442
1
Iteration 4000: Loss = -11587.658057713563
Iteration 4100: Loss = -11587.657469599528
Iteration 4200: Loss = -11587.657938351538
1
Iteration 4300: Loss = -11587.65674193745
Iteration 4400: Loss = -11587.656421614938
Iteration 4500: Loss = -11587.656544505435
1
Iteration 4600: Loss = -11587.656035112861
Iteration 4700: Loss = -11587.655677366009
Iteration 4800: Loss = -11587.655467481572
Iteration 4900: Loss = -11587.655209606128
Iteration 5000: Loss = -11587.654970407128
Iteration 5100: Loss = -11587.655046558883
Iteration 5200: Loss = -11587.654760316158
Iteration 5300: Loss = -11587.654387585588
Iteration 5400: Loss = -11587.655525687444
1
Iteration 5500: Loss = -11587.657813132082
2
Iteration 5600: Loss = -11587.654266115027
Iteration 5700: Loss = -11587.657763428302
1
Iteration 5800: Loss = -11587.653677550115
Iteration 5900: Loss = -11587.653666900502
Iteration 6000: Loss = -11587.65345201572
Iteration 6100: Loss = -11587.653491630406
Iteration 6200: Loss = -11587.653295529755
Iteration 6300: Loss = -11587.653157066972
Iteration 6400: Loss = -11587.653546388974
1
Iteration 6500: Loss = -11587.658694288117
2
Iteration 6600: Loss = -11587.653076233806
Iteration 6700: Loss = -11587.65284509054
Iteration 6800: Loss = -11587.652761679758
Iteration 6900: Loss = -11587.652877861972
1
Iteration 7000: Loss = -11587.652612672908
Iteration 7100: Loss = -11587.652640404052
Iteration 7200: Loss = -11587.655154228443
1
Iteration 7300: Loss = -11587.652480364213
Iteration 7400: Loss = -11587.65264784455
1
Iteration 7500: Loss = -11587.661747265807
2
Iteration 7600: Loss = -11587.652305246645
Iteration 7700: Loss = -11587.653285907742
1
Iteration 7800: Loss = -11587.654316894486
2
Iteration 7900: Loss = -11587.660849144464
3
Iteration 8000: Loss = -11587.652210727922
Iteration 8100: Loss = -11587.652962806871
1
Iteration 8200: Loss = -11587.652118297774
Iteration 8300: Loss = -11587.652222892808
1
Iteration 8400: Loss = -11587.671381459615
2
Iteration 8500: Loss = -11587.659874244826
3
Iteration 8600: Loss = -11587.652347101304
4
Iteration 8700: Loss = -11587.660473068529
5
Iteration 8800: Loss = -11587.656973800851
6
Iteration 8900: Loss = -11587.652131234569
Iteration 9000: Loss = -11587.652049898823
Iteration 9100: Loss = -11587.652115395158
Iteration 9200: Loss = -11587.651911187137
Iteration 9300: Loss = -11587.656803746984
1
Iteration 9400: Loss = -11587.658210594494
2
Iteration 9500: Loss = -11587.664778651428
3
Iteration 9600: Loss = -11587.651869729692
Iteration 9700: Loss = -11587.65256175088
1
Iteration 9800: Loss = -11587.662195319866
2
Iteration 9900: Loss = -11587.686248358337
3
Iteration 10000: Loss = -11587.655525911916
4
Iteration 10100: Loss = -11587.658277442284
5
Iteration 10200: Loss = -11587.693486840284
6
Iteration 10300: Loss = -11587.651999425674
7
Iteration 10400: Loss = -11587.664866426381
8
Iteration 10500: Loss = -11587.673562557258
9
Iteration 10600: Loss = -11587.652995863093
10
Iteration 10700: Loss = -11587.655935528684
11
Iteration 10800: Loss = -11587.685993551539
12
Iteration 10900: Loss = -11587.656975083708
13
Iteration 11000: Loss = -11587.655143356676
14
Iteration 11100: Loss = -11587.670268925718
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.7151, 0.2849],
        [0.2435, 0.7565]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5217, 0.4783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4002, 0.1045],
         [0.5869, 0.2096]],

        [[0.6448, 0.1014],
         [0.5944, 0.6708]],

        [[0.5587, 0.0936],
         [0.5776, 0.5031]],

        [[0.6121, 0.1032],
         [0.7252, 0.6573]],

        [[0.7237, 0.1000],
         [0.6338, 0.5813]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11591.886348073027
[1.0, 1.0] [1.0, 1.0] [11587.735805025673, 11587.670268925718]
-------------------------------------
This iteration is 15
True Objective function: Loss = -11507.976581775833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22900.042428695808
Iteration 100: Loss = -12295.826910464782
Iteration 200: Loss = -11803.12650067204
Iteration 300: Loss = -11690.869769940062
Iteration 400: Loss = -11684.118826081163
Iteration 500: Loss = -11683.683374914892
Iteration 600: Loss = -11683.474433203517
Iteration 700: Loss = -11683.351835325548
Iteration 800: Loss = -11683.273064810846
Iteration 900: Loss = -11683.218582634596
Iteration 1000: Loss = -11683.1783974096
Iteration 1100: Loss = -11683.14729971016
Iteration 1200: Loss = -11683.122017803174
Iteration 1300: Loss = -11683.100553195603
Iteration 1400: Loss = -11683.081044911165
Iteration 1500: Loss = -11683.061485481565
Iteration 1600: Loss = -11683.037896020347
Iteration 1700: Loss = -11682.998005391712
Iteration 1800: Loss = -11682.869283792725
Iteration 1900: Loss = -11682.456061873838
Iteration 2000: Loss = -11682.373024344583
Iteration 2100: Loss = -11682.34771736096
Iteration 2200: Loss = -11682.32468037221
Iteration 2300: Loss = -11682.29550614239
Iteration 2400: Loss = -11682.283509027939
Iteration 2500: Loss = -11682.27755017428
Iteration 2600: Loss = -11682.282593768248
1
Iteration 2700: Loss = -11682.269747164488
Iteration 2800: Loss = -11682.26682360277
Iteration 2900: Loss = -11682.264359458828
Iteration 3000: Loss = -11682.262222167934
Iteration 3100: Loss = -11682.26034614184
Iteration 3200: Loss = -11682.258626673702
Iteration 3300: Loss = -11682.257453806915
Iteration 3400: Loss = -11682.255793458322
Iteration 3500: Loss = -11682.259214577227
1
Iteration 3600: Loss = -11682.253695884036
Iteration 3700: Loss = -11682.254742091658
1
Iteration 3800: Loss = -11682.25158874496
Iteration 3900: Loss = -11682.25069777429
Iteration 4000: Loss = -11682.252098564097
1
Iteration 4100: Loss = -11682.249267668247
Iteration 4200: Loss = -11682.248581389036
Iteration 4300: Loss = -11682.247909191436
Iteration 4400: Loss = -11682.247291447975
Iteration 4500: Loss = -11682.272796761228
1
Iteration 4600: Loss = -11682.246235114744
Iteration 4700: Loss = -11682.258032061667
1
Iteration 4800: Loss = -11682.245310998744
Iteration 4900: Loss = -11682.245400305277
Iteration 5000: Loss = -11682.24447830053
Iteration 5100: Loss = -11682.246750425717
1
Iteration 5200: Loss = -11682.243839840667
Iteration 5300: Loss = -11682.243493113181
Iteration 5400: Loss = -11682.243247957525
Iteration 5500: Loss = -11682.243156247134
Iteration 5600: Loss = -11682.242677318452
Iteration 5700: Loss = -11682.242447772842
Iteration 5800: Loss = -11682.242205652245
Iteration 5900: Loss = -11682.241992569907
Iteration 6000: Loss = -11682.24175109805
Iteration 6100: Loss = -11682.242877227747
1
Iteration 6200: Loss = -11682.241307036124
Iteration 6300: Loss = -11682.241077639177
Iteration 6400: Loss = -11682.242004141897
1
Iteration 6500: Loss = -11682.240693848775
Iteration 6600: Loss = -11682.24205722784
1
Iteration 6700: Loss = -11682.240424800715
Iteration 6800: Loss = -11682.24211600914
1
Iteration 6900: Loss = -11682.240114199869
Iteration 7000: Loss = -11682.244800413635
1
Iteration 7100: Loss = -11682.23983983826
Iteration 7200: Loss = -11682.239726045358
Iteration 7300: Loss = -11682.241363216115
1
Iteration 7400: Loss = -11682.23932783828
Iteration 7500: Loss = -11682.238848187673
Iteration 7600: Loss = -11682.237224750288
Iteration 7700: Loss = -11682.236733095837
Iteration 7800: Loss = -11682.236575575524
Iteration 7900: Loss = -11682.236562591912
Iteration 8000: Loss = -11682.236458780108
Iteration 8100: Loss = -11682.238505654352
1
Iteration 8200: Loss = -11682.236318463745
Iteration 8300: Loss = -11682.236266843724
Iteration 8400: Loss = -11682.236897141216
1
Iteration 8500: Loss = -11682.23658602652
2
Iteration 8600: Loss = -11682.23654347737
3
Iteration 8700: Loss = -11682.23606337697
Iteration 8800: Loss = -11682.237966406705
1
Iteration 8900: Loss = -11682.236050069669
Iteration 9000: Loss = -11682.235978701287
Iteration 9100: Loss = -11682.237349294028
1
Iteration 9200: Loss = -11682.235947202167
Iteration 9300: Loss = -11682.235921279924
Iteration 9400: Loss = -11682.236152162543
1
Iteration 9500: Loss = -11682.235857538735
Iteration 9600: Loss = -11682.239311369653
1
Iteration 9700: Loss = -11682.299993311964
2
Iteration 9800: Loss = -11682.393871934155
3
Iteration 9900: Loss = -11682.235799313024
Iteration 10000: Loss = -11682.239385687177
1
Iteration 10100: Loss = -11682.235805195138
Iteration 10200: Loss = -11682.23585491572
Iteration 10300: Loss = -11682.301994431402
1
Iteration 10400: Loss = -11682.235718879287
Iteration 10500: Loss = -11682.247178224332
1
Iteration 10600: Loss = -11682.235707696345
Iteration 10700: Loss = -11682.246412222536
1
Iteration 10800: Loss = -11682.236956914647
2
Iteration 10900: Loss = -11682.238418713574
3
Iteration 11000: Loss = -11682.252783314994
4
Iteration 11100: Loss = -11682.23590355849
5
Iteration 11200: Loss = -11682.236511233694
6
Iteration 11300: Loss = -11682.265809071676
7
Iteration 11400: Loss = -11682.238686636165
8
Iteration 11500: Loss = -11682.237109232014
9
Iteration 11600: Loss = -11682.23579259238
Iteration 11700: Loss = -11682.238498277122
1
Iteration 11800: Loss = -11682.260269609129
2
Iteration 11900: Loss = -11682.235505198953
Iteration 12000: Loss = -11682.23570824383
1
Iteration 12100: Loss = -11682.242013048053
2
Iteration 12200: Loss = -11682.240218092762
3
Iteration 12300: Loss = -11682.235789183782
4
Iteration 12400: Loss = -11682.260241188289
5
Iteration 12500: Loss = -11682.2354864441
Iteration 12600: Loss = -11682.250023426337
1
Iteration 12700: Loss = -11682.241643902635
2
Iteration 12800: Loss = -11682.250442926786
3
Iteration 12900: Loss = -11682.236527888821
4
Iteration 13000: Loss = -11682.247277536224
5
Iteration 13100: Loss = -11682.291889044976
6
Iteration 13200: Loss = -11682.235385075915
Iteration 13300: Loss = -11682.23821036857
1
Iteration 13400: Loss = -11682.235393331375
Iteration 13500: Loss = -11682.237927566533
1
Iteration 13600: Loss = -11682.235459881947
Iteration 13700: Loss = -11682.254116010885
1
Iteration 13800: Loss = -11682.26632884625
2
Iteration 13900: Loss = -11682.267623035457
3
Iteration 14000: Loss = -11682.237674974296
4
Iteration 14100: Loss = -11682.2358959998
5
Iteration 14200: Loss = -11682.235409801177
Iteration 14300: Loss = -11682.235880701324
1
Iteration 14400: Loss = -11682.27760433966
2
Iteration 14500: Loss = -11682.236201931977
3
Iteration 14600: Loss = -11682.236940275854
4
Iteration 14700: Loss = -11682.270729860194
5
Iteration 14800: Loss = -11682.235462018138
Iteration 14900: Loss = -11682.247390581631
1
Iteration 15000: Loss = -11682.237493200157
2
Iteration 15100: Loss = -11682.243581910514
3
Iteration 15200: Loss = -11682.261944599972
4
Iteration 15300: Loss = -11682.256773892175
5
Iteration 15400: Loss = -11682.235409069019
Iteration 15500: Loss = -11682.245834439722
1
Iteration 15600: Loss = -11682.235454132258
Iteration 15700: Loss = -11682.27014371638
1
Iteration 15800: Loss = -11682.24357522215
2
Iteration 15900: Loss = -11682.237847157405
3
Iteration 16000: Loss = -11682.339716536082
4
Iteration 16100: Loss = -11682.235382549476
Iteration 16200: Loss = -11682.255852001425
1
Iteration 16300: Loss = -11682.235401262174
Iteration 16400: Loss = -11682.240355943712
1
Iteration 16500: Loss = -11682.235393343857
Iteration 16600: Loss = -11682.23537410134
Iteration 16700: Loss = -11682.235852748714
1
Iteration 16800: Loss = -11682.235373359741
Iteration 16900: Loss = -11682.239387665291
1
Iteration 17000: Loss = -11682.303745582236
2
Iteration 17100: Loss = -11682.237005706667
3
Iteration 17200: Loss = -11682.236741938843
4
Iteration 17300: Loss = -11682.23571603097
5
Iteration 17400: Loss = -11682.241884429994
6
Iteration 17500: Loss = -11682.26044509973
7
Iteration 17600: Loss = -11682.235414464243
Iteration 17700: Loss = -11682.242610790205
1
Iteration 17800: Loss = -11682.384494671955
2
Iteration 17900: Loss = -11682.23546004452
Iteration 18000: Loss = -11682.239022359097
1
Iteration 18100: Loss = -11682.235451868846
Iteration 18200: Loss = -11682.235443795687
Iteration 18300: Loss = -11682.407742682148
1
Iteration 18400: Loss = -11682.238692025601
2
Iteration 18500: Loss = -11682.266322567339
3
Iteration 18600: Loss = -11682.235398291232
Iteration 18700: Loss = -11682.237275458112
1
Iteration 18800: Loss = -11682.235364645338
Iteration 18900: Loss = -11682.236224569206
1
Iteration 19000: Loss = -11682.23535316589
Iteration 19100: Loss = -11682.237457440699
1
Iteration 19200: Loss = -11682.235364157474
Iteration 19300: Loss = -11682.236114951871
1
Iteration 19400: Loss = -11682.371600532468
2
Iteration 19500: Loss = -11682.235407378563
Iteration 19600: Loss = -11682.239622425997
1
Iteration 19700: Loss = -11682.236973533774
2
Iteration 19800: Loss = -11682.238662281736
3
Iteration 19900: Loss = -11682.250001858165
4
pi: tensor([[0.8033, 0.1967],
        [0.3263, 0.6737]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0188, 0.9812], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3918, 0.0984],
         [0.5780, 0.1992]],

        [[0.6827, 0.1031],
         [0.7215, 0.7219]],

        [[0.5740, 0.0992],
         [0.5651, 0.6280]],

        [[0.6003, 0.1010],
         [0.5555, 0.5774]],

        [[0.6189, 0.0954],
         [0.5988, 0.6973]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.645732017922289
Average Adjusted Rand Index: 0.7915468653711935
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23585.571492036983
Iteration 100: Loss = -12301.729672420019
Iteration 200: Loss = -12211.03752179195
Iteration 300: Loss = -11811.381310365487
Iteration 400: Loss = -11656.554441956037
Iteration 500: Loss = -11601.133077298606
Iteration 600: Loss = -11577.704680170547
Iteration 700: Loss = -11552.998335285296
Iteration 800: Loss = -11535.93957544849
Iteration 900: Loss = -11535.588394105627
Iteration 1000: Loss = -11528.039962315535
Iteration 1100: Loss = -11526.853783656534
Iteration 1200: Loss = -11526.794633311394
Iteration 1300: Loss = -11526.727114209427
Iteration 1400: Loss = -11520.164563729093
Iteration 1500: Loss = -11520.134384103876
Iteration 1600: Loss = -11520.110968730642
Iteration 1700: Loss = -11520.090421181214
Iteration 1800: Loss = -11520.058796926265
Iteration 1900: Loss = -11510.597137008459
Iteration 2000: Loss = -11510.581472849208
Iteration 2100: Loss = -11510.570522694234
Iteration 2200: Loss = -11510.561357423267
Iteration 2300: Loss = -11510.553680818495
Iteration 2400: Loss = -11510.54647936037
Iteration 2500: Loss = -11510.540363855784
Iteration 2600: Loss = -11510.534707731333
Iteration 2700: Loss = -11510.529528139221
Iteration 2800: Loss = -11510.53407511022
1
Iteration 2900: Loss = -11510.518556296236
Iteration 3000: Loss = -11510.511346147016
Iteration 3100: Loss = -11499.512858019016
Iteration 3200: Loss = -11499.469879270888
Iteration 3300: Loss = -11499.470972462119
1
Iteration 3400: Loss = -11499.464379804802
Iteration 3500: Loss = -11499.462206304406
Iteration 3600: Loss = -11499.461050736652
Iteration 3700: Loss = -11499.458369337732
Iteration 3800: Loss = -11499.460534234951
1
Iteration 3900: Loss = -11499.457199406315
Iteration 4000: Loss = -11499.454283382664
Iteration 4100: Loss = -11499.45112912299
Iteration 4200: Loss = -11499.449985968278
Iteration 4300: Loss = -11499.44868036332
Iteration 4400: Loss = -11499.447356038878
Iteration 4500: Loss = -11499.446232776441
Iteration 4600: Loss = -11499.45008440337
1
Iteration 4700: Loss = -11499.444381315694
Iteration 4800: Loss = -11499.444139807305
Iteration 4900: Loss = -11499.44288959733
Iteration 5000: Loss = -11499.44322865474
1
Iteration 5100: Loss = -11499.441623267214
Iteration 5200: Loss = -11499.44109212515
Iteration 5300: Loss = -11499.4407602566
Iteration 5400: Loss = -11499.440900159041
1
Iteration 5500: Loss = -11499.439516918574
Iteration 5600: Loss = -11499.43915580538
Iteration 5700: Loss = -11499.43921153984
Iteration 5800: Loss = -11499.438306353837
Iteration 5900: Loss = -11499.438040847097
Iteration 6000: Loss = -11499.437523730601
Iteration 6100: Loss = -11499.437287561459
Iteration 6200: Loss = -11499.440151503843
1
Iteration 6300: Loss = -11499.436640678448
Iteration 6400: Loss = -11499.437544571541
1
Iteration 6500: Loss = -11499.43612130422
Iteration 6600: Loss = -11499.436153503471
Iteration 6700: Loss = -11499.43970112952
1
Iteration 6800: Loss = -11499.436018650926
Iteration 6900: Loss = -11499.435217646642
Iteration 7000: Loss = -11499.45835601359
1
Iteration 7100: Loss = -11499.435143475435
Iteration 7200: Loss = -11499.434817086281
Iteration 7300: Loss = -11499.434593601685
Iteration 7400: Loss = -11499.45686476044
1
Iteration 7500: Loss = -11499.434252565621
Iteration 7600: Loss = -11499.481362474096
1
Iteration 7700: Loss = -11499.43397341135
Iteration 7800: Loss = -11499.433955536055
Iteration 7900: Loss = -11499.477830957503
1
Iteration 8000: Loss = -11499.433592717773
Iteration 8100: Loss = -11499.45785716545
1
Iteration 8200: Loss = -11499.433370226867
Iteration 8300: Loss = -11499.434644929757
1
Iteration 8400: Loss = -11499.484587546054
2
Iteration 8500: Loss = -11499.491621785339
3
Iteration 8600: Loss = -11499.433005122528
Iteration 8700: Loss = -11499.433638581151
1
Iteration 8800: Loss = -11499.432868936752
Iteration 8900: Loss = -11499.441063725588
1
Iteration 9000: Loss = -11499.43274744854
Iteration 9100: Loss = -11499.445558426649
1
Iteration 9200: Loss = -11499.432890544686
2
Iteration 9300: Loss = -11499.432576850411
Iteration 9400: Loss = -11499.432477246677
Iteration 9500: Loss = -11499.432592814286
1
Iteration 9600: Loss = -11499.432266613352
Iteration 9700: Loss = -11499.434976201843
1
Iteration 9800: Loss = -11499.432890130805
2
Iteration 9900: Loss = -11499.432576470674
3
Iteration 10000: Loss = -11499.455876026192
4
Iteration 10100: Loss = -11499.475425672426
5
Iteration 10200: Loss = -11499.431917620488
Iteration 10300: Loss = -11499.432399773641
1
Iteration 10400: Loss = -11499.43506974513
2
Iteration 10500: Loss = -11499.46108396509
3
Iteration 10600: Loss = -11499.435011455007
4
Iteration 10700: Loss = -11499.432518608908
5
Iteration 10800: Loss = -11499.396053710308
Iteration 10900: Loss = -11499.425611859697
1
Iteration 11000: Loss = -11499.433716165076
2
Iteration 11100: Loss = -11499.39238620102
Iteration 11200: Loss = -11499.38574409922
Iteration 11300: Loss = -11499.395150177996
1
Iteration 11400: Loss = -11499.432212125737
2
Iteration 11500: Loss = -11499.387661936315
3
Iteration 11600: Loss = -11499.38829005302
4
Iteration 11700: Loss = -11499.571616656573
5
Iteration 11800: Loss = -11499.384131681962
Iteration 11900: Loss = -11499.38464173173
1
Iteration 12000: Loss = -11499.385107915123
2
Iteration 12100: Loss = -11499.386372072462
3
Iteration 12200: Loss = -11499.384247851991
4
Iteration 12300: Loss = -11499.384153793195
Iteration 12400: Loss = -11499.384474490738
1
Iteration 12500: Loss = -11499.390756115681
2
Iteration 12600: Loss = -11499.384170462083
Iteration 12700: Loss = -11499.384226236696
Iteration 12800: Loss = -11499.385629461442
1
Iteration 12900: Loss = -11499.393801517299
2
Iteration 13000: Loss = -11499.383942320737
Iteration 13100: Loss = -11499.38442755282
1
Iteration 13200: Loss = -11499.476989157949
2
Iteration 13300: Loss = -11499.389979701251
3
Iteration 13400: Loss = -11499.394526939288
4
Iteration 13500: Loss = -11499.395598517125
5
Iteration 13600: Loss = -11499.38420804592
6
Iteration 13700: Loss = -11499.3841980898
7
Iteration 13800: Loss = -11499.38513088724
8
Iteration 13900: Loss = -11499.385660551858
9
Iteration 14000: Loss = -11499.393004951788
10
Iteration 14100: Loss = -11499.38391622045
Iteration 14200: Loss = -11499.385230436803
1
Iteration 14300: Loss = -11499.388398214433
2
Iteration 14400: Loss = -11499.384721304928
3
Iteration 14500: Loss = -11499.387669001393
4
Iteration 14600: Loss = -11499.384637246643
5
Iteration 14700: Loss = -11499.385332328176
6
Iteration 14800: Loss = -11499.38414221607
7
Iteration 14900: Loss = -11499.388421263659
8
Iteration 15000: Loss = -11499.384044113502
9
Iteration 15100: Loss = -11499.391021046977
10
Iteration 15200: Loss = -11499.411639107622
11
Iteration 15300: Loss = -11499.399362299968
12
Iteration 15400: Loss = -11499.42882065026
13
Iteration 15500: Loss = -11499.384467765247
14
Iteration 15600: Loss = -11499.383979717037
Iteration 15700: Loss = -11499.391103993643
1
Iteration 15800: Loss = -11499.42901944312
2
Iteration 15900: Loss = -11499.48840544128
3
Iteration 16000: Loss = -11499.3843438767
4
Iteration 16100: Loss = -11499.384173389664
5
Iteration 16200: Loss = -11499.384978345115
6
Iteration 16300: Loss = -11499.390017313975
7
Iteration 16400: Loss = -11499.38427108685
8
Iteration 16500: Loss = -11499.385894569474
9
Iteration 16600: Loss = -11499.390529337163
10
Iteration 16700: Loss = -11499.384101616644
11
Iteration 16800: Loss = -11499.405991617507
12
Iteration 16900: Loss = -11499.415668226253
13
Iteration 17000: Loss = -11499.49375456499
14
Iteration 17100: Loss = -11499.385842999665
15
Stopping early at iteration 17100 due to no improvement.
pi: tensor([[0.7732, 0.2268],
        [0.2050, 0.7950]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5289, 0.4711], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2029, 0.0922],
         [0.6384, 0.3956]],

        [[0.6419, 0.1038],
         [0.5671, 0.6748]],

        [[0.6643, 0.0997],
         [0.6717, 0.6832]],

        [[0.6305, 0.1010],
         [0.5790, 0.5302]],

        [[0.5489, 0.0952],
         [0.5869, 0.6642]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320050933414
Average Adjusted Rand Index: 0.9841616161616162
11507.976581775833
[0.645732017922289, 0.9840320050933414] [0.7915468653711935, 0.9841616161616162] [11682.237781166348, 11499.385842999665]
-------------------------------------
This iteration is 16
True Objective function: Loss = -11477.899125480853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21588.17213482445
Iteration 100: Loss = -12217.05699748664
Iteration 200: Loss = -12215.525814769886
Iteration 300: Loss = -12171.978020328193
Iteration 400: Loss = -11715.760775220459
Iteration 500: Loss = -11669.469180679433
Iteration 600: Loss = -11657.713079753956
Iteration 700: Loss = -11501.215045143414
Iteration 800: Loss = -11490.218953013957
Iteration 900: Loss = -11489.803631189847
Iteration 1000: Loss = -11475.515558931382
Iteration 1100: Loss = -11475.418068824918
Iteration 1200: Loss = -11475.35348367367
Iteration 1300: Loss = -11475.305698722977
Iteration 1400: Loss = -11475.27056736348
Iteration 1500: Loss = -11475.24457193904
Iteration 1600: Loss = -11475.223819172628
Iteration 1700: Loss = -11475.206968440883
Iteration 1800: Loss = -11475.193274625412
Iteration 1900: Loss = -11475.181829895324
Iteration 2000: Loss = -11475.172203060507
Iteration 2100: Loss = -11475.163928611135
Iteration 2200: Loss = -11475.16913145427
1
Iteration 2300: Loss = -11475.1504967966
Iteration 2400: Loss = -11475.145152428058
Iteration 2500: Loss = -11475.140136253654
Iteration 2600: Loss = -11475.135796211975
Iteration 2700: Loss = -11475.150562701567
1
Iteration 2800: Loss = -11475.128496238664
Iteration 2900: Loss = -11475.125400599227
Iteration 3000: Loss = -11475.125015714955
Iteration 3100: Loss = -11475.120096261373
Iteration 3200: Loss = -11475.120241585555
1
Iteration 3300: Loss = -11475.115715352404
Iteration 3400: Loss = -11475.121794837878
1
Iteration 3500: Loss = -11475.112389638436
Iteration 3600: Loss = -11475.11064593309
Iteration 3700: Loss = -11475.108923955233
Iteration 3800: Loss = -11475.11281041318
1
Iteration 3900: Loss = -11475.105594275035
Iteration 4000: Loss = -11475.10255029322
Iteration 4100: Loss = -11475.102300901217
Iteration 4200: Loss = -11475.098801128792
Iteration 4300: Loss = -11475.097716525603
Iteration 4400: Loss = -11475.09679557931
Iteration 4500: Loss = -11475.095809653485
Iteration 4600: Loss = -11475.095124971123
Iteration 4700: Loss = -11475.09424023054
Iteration 4800: Loss = -11475.097289126066
1
Iteration 4900: Loss = -11475.092849353763
Iteration 5000: Loss = -11475.092094649524
Iteration 5100: Loss = -11475.101727190173
1
Iteration 5200: Loss = -11475.090815092135
Iteration 5300: Loss = -11475.090333176462
Iteration 5400: Loss = -11475.095600383624
1
Iteration 5500: Loss = -11475.089470804702
Iteration 5600: Loss = -11475.089138463381
Iteration 5700: Loss = -11475.09549766365
1
Iteration 5800: Loss = -11475.09220280153
2
Iteration 5900: Loss = -11475.08814275156
Iteration 6000: Loss = -11475.08926579696
1
Iteration 6100: Loss = -11475.087575095411
Iteration 6200: Loss = -11475.087662617196
Iteration 6300: Loss = -11475.087384571394
Iteration 6400: Loss = -11475.086884072905
Iteration 6500: Loss = -11475.086688074402
Iteration 6600: Loss = -11475.08657130609
Iteration 6700: Loss = -11475.08632403374
Iteration 6800: Loss = -11475.08611840391
Iteration 6900: Loss = -11475.085947176884
Iteration 7000: Loss = -11475.086096103294
1
Iteration 7100: Loss = -11475.085641695792
Iteration 7200: Loss = -11475.085511015905
Iteration 7300: Loss = -11475.089552359968
1
Iteration 7400: Loss = -11475.260876615026
2
Iteration 7500: Loss = -11475.085136960473
Iteration 7600: Loss = -11475.08506115427
Iteration 7700: Loss = -11475.084953074971
Iteration 7800: Loss = -11475.0848415852
Iteration 7900: Loss = -11475.093535912043
1
Iteration 8000: Loss = -11475.091039825118
2
Iteration 8100: Loss = -11475.084585720228
Iteration 8200: Loss = -11475.084528726236
Iteration 8300: Loss = -11475.085760426777
1
Iteration 8400: Loss = -11475.088823088274
2
Iteration 8500: Loss = -11475.084423103695
Iteration 8600: Loss = -11475.137959921294
1
Iteration 8700: Loss = -11475.084210783185
Iteration 8800: Loss = -11475.099445987384
1
Iteration 8900: Loss = -11475.089731825192
2
Iteration 9000: Loss = -11475.084004366654
Iteration 9100: Loss = -11475.0879632666
1
Iteration 9200: Loss = -11475.083987126764
Iteration 9300: Loss = -11475.084360409404
1
Iteration 9400: Loss = -11475.085048702078
2
Iteration 9500: Loss = -11475.087928184075
3
Iteration 9600: Loss = -11475.083954187876
Iteration 9700: Loss = -11475.084205974757
1
Iteration 9800: Loss = -11475.091043640698
2
Iteration 9900: Loss = -11475.089406803114
3
Iteration 10000: Loss = -11475.09328553337
4
Iteration 10100: Loss = -11475.087386972606
5
Iteration 10200: Loss = -11475.09666830132
6
Iteration 10300: Loss = -11475.09761080537
7
Iteration 10400: Loss = -11475.089723951001
8
Iteration 10500: Loss = -11475.083677951758
Iteration 10600: Loss = -11475.085024898093
1
Iteration 10700: Loss = -11475.09144064241
2
Iteration 10800: Loss = -11475.081635788954
Iteration 10900: Loss = -11475.083136389107
1
Iteration 11000: Loss = -11475.097669443845
2
Iteration 11100: Loss = -11475.108508040157
3
Iteration 11200: Loss = -11475.0865958512
4
Iteration 11300: Loss = -11475.093499460198
5
Iteration 11400: Loss = -11475.086799188019
6
Iteration 11500: Loss = -11475.086001256988
7
Iteration 11600: Loss = -11475.10795003348
8
Iteration 11700: Loss = -11475.103101087445
9
Iteration 11800: Loss = -11475.097103954664
10
Iteration 11900: Loss = -11475.089153753654
11
Iteration 12000: Loss = -11475.139466597066
12
Iteration 12100: Loss = -11475.088225759759
13
Iteration 12200: Loss = -11475.094787857986
14
Iteration 12300: Loss = -11475.091127542595
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[0.7236, 0.2764],
        [0.2341, 0.7659]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5098, 0.4902], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3997, 0.0945],
         [0.6168, 0.1968]],

        [[0.6264, 0.1006],
         [0.7269, 0.6404]],

        [[0.5237, 0.1024],
         [0.6206, 0.6646]],

        [[0.5681, 0.1098],
         [0.6251, 0.6452]],

        [[0.6974, 0.0976],
         [0.5799, 0.6165]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22504.913328535284
Iteration 100: Loss = -11672.251641613873
Iteration 200: Loss = -11476.97946198955
Iteration 300: Loss = -11476.004183774221
Iteration 400: Loss = -11475.654091272254
Iteration 500: Loss = -11475.478507274343
Iteration 600: Loss = -11475.375242694325
Iteration 700: Loss = -11475.308469834119
Iteration 800: Loss = -11475.262450983617
Iteration 900: Loss = -11475.2292666543
Iteration 1000: Loss = -11475.20452475404
Iteration 1100: Loss = -11475.185473513564
Iteration 1200: Loss = -11475.170512057668
Iteration 1300: Loss = -11475.158506847256
Iteration 1400: Loss = -11475.14871704365
Iteration 1500: Loss = -11475.14060891687
Iteration 1600: Loss = -11475.133819675699
Iteration 1700: Loss = -11475.12808479636
Iteration 1800: Loss = -11475.123152931064
Iteration 1900: Loss = -11475.118946613595
Iteration 2000: Loss = -11475.115269727099
Iteration 2100: Loss = -11475.115127396872
Iteration 2200: Loss = -11475.109305581995
Iteration 2300: Loss = -11475.106810825
Iteration 2400: Loss = -11475.104657509817
Iteration 2500: Loss = -11475.102717914488
Iteration 2600: Loss = -11475.10566040013
1
Iteration 2700: Loss = -11475.099405400337
Iteration 2800: Loss = -11475.098040654477
Iteration 2900: Loss = -11475.096768531777
Iteration 3000: Loss = -11475.09564567155
Iteration 3100: Loss = -11475.095814995035
1
Iteration 3200: Loss = -11475.093667754985
Iteration 3300: Loss = -11475.092789253213
Iteration 3400: Loss = -11475.092940342693
1
Iteration 3500: Loss = -11475.09130021088
Iteration 3600: Loss = -11475.09062010394
Iteration 3700: Loss = -11475.089973234355
Iteration 3800: Loss = -11475.089429664888
Iteration 3900: Loss = -11475.088874122372
Iteration 4000: Loss = -11475.089395617697
1
Iteration 4100: Loss = -11475.08804938293
Iteration 4200: Loss = -11475.087602660506
Iteration 4300: Loss = -11475.087182868909
Iteration 4400: Loss = -11475.087510445146
1
Iteration 4500: Loss = -11475.086516667005
Iteration 4600: Loss = -11475.086214227022
Iteration 4700: Loss = -11475.087697261746
1
Iteration 4800: Loss = -11475.085646719543
Iteration 4900: Loss = -11475.085425978315
Iteration 5000: Loss = -11475.085579632558
1
Iteration 5100: Loss = -11475.085196391597
Iteration 5200: Loss = -11475.08477780752
Iteration 5300: Loss = -11475.08455756009
Iteration 5400: Loss = -11475.08609423099
1
Iteration 5500: Loss = -11475.086116618359
2
Iteration 5600: Loss = -11475.084102094173
Iteration 5700: Loss = -11475.084414540597
1
Iteration 5800: Loss = -11475.096594134515
2
Iteration 5900: Loss = -11475.083690570817
Iteration 6000: Loss = -11475.08354398508
Iteration 6100: Loss = -11475.0841314979
1
Iteration 6200: Loss = -11475.083321422917
Iteration 6300: Loss = -11475.083219029255
Iteration 6400: Loss = -11475.091018123083
1
Iteration 6500: Loss = -11475.083053540639
Iteration 6600: Loss = -11475.082993045266
Iteration 6700: Loss = -11475.082879892487
Iteration 6800: Loss = -11475.083101159033
1
Iteration 6900: Loss = -11475.08272382707
Iteration 7000: Loss = -11475.082668981315
Iteration 7100: Loss = -11475.082657583263
Iteration 7200: Loss = -11475.082510020799
Iteration 7300: Loss = -11475.08247989407
Iteration 7400: Loss = -11475.085008604576
1
Iteration 7500: Loss = -11475.082403822775
Iteration 7600: Loss = -11475.082323668448
Iteration 7700: Loss = -11475.082346853253
Iteration 7800: Loss = -11475.082232951545
Iteration 7900: Loss = -11475.08222929108
Iteration 8000: Loss = -11475.083375272843
1
Iteration 8100: Loss = -11475.08214701704
Iteration 8200: Loss = -11475.0820993178
Iteration 8300: Loss = -11475.094325839013
1
Iteration 8400: Loss = -11475.08249935431
2
Iteration 8500: Loss = -11475.082022956
Iteration 8600: Loss = -11475.083009434205
1
Iteration 8700: Loss = -11475.082427340945
2
Iteration 8800: Loss = -11475.08212578674
3
Iteration 8900: Loss = -11475.084760244394
4
Iteration 9000: Loss = -11475.083529556161
5
Iteration 9100: Loss = -11475.0819559831
Iteration 9200: Loss = -11475.083061555508
1
Iteration 9300: Loss = -11475.08188434934
Iteration 9400: Loss = -11475.08220281611
1
Iteration 9500: Loss = -11475.093164311265
2
Iteration 9600: Loss = -11475.098061007535
3
Iteration 9700: Loss = -11475.086128664623
4
Iteration 9800: Loss = -11475.14332466002
5
Iteration 9900: Loss = -11475.09650806985
6
Iteration 10000: Loss = -11475.088043864771
7
Iteration 10100: Loss = -11475.085821405582
8
Iteration 10200: Loss = -11475.084190368503
9
Iteration 10300: Loss = -11475.085018409653
10
Iteration 10400: Loss = -11475.083931546442
11
Iteration 10500: Loss = -11475.085105758506
12
Iteration 10600: Loss = -11475.101903290995
13
Iteration 10700: Loss = -11475.088568570894
14
Iteration 10800: Loss = -11475.085087786905
15
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[0.7668, 0.2332],
        [0.2771, 0.7229]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4900, 0.5100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1967, 0.0944],
         [0.5191, 0.3999]],

        [[0.7250, 0.1002],
         [0.6502, 0.5000]],

        [[0.6051, 0.1022],
         [0.6529, 0.5961]],

        [[0.5691, 0.1098],
         [0.6124, 0.6306]],

        [[0.7247, 0.0985],
         [0.7282, 0.6404]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11477.899125480853
[1.0, 1.0] [1.0, 1.0] [11475.091127542595, 11475.085087786905]
-------------------------------------
This iteration is 17
True Objective function: Loss = -11625.506444736695
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19980.913827793465
Iteration 100: Loss = -12391.200144616141
Iteration 200: Loss = -12375.201425100384
Iteration 300: Loss = -12007.755133895764
Iteration 400: Loss = -11650.102874496899
Iteration 500: Loss = -11629.771581612287
Iteration 600: Loss = -11629.374081526363
Iteration 700: Loss = -11629.164868848144
Iteration 800: Loss = -11617.176875304185
Iteration 900: Loss = -11615.6785203002
Iteration 1000: Loss = -11615.619748389687
Iteration 1100: Loss = -11615.576375389983
Iteration 1200: Loss = -11615.54310295596
Iteration 1300: Loss = -11615.51690245933
Iteration 1400: Loss = -11615.495823321804
Iteration 1500: Loss = -11615.478705742427
Iteration 1600: Loss = -11615.46447734382
Iteration 1700: Loss = -11615.452617111774
Iteration 1800: Loss = -11615.442617060815
Iteration 1900: Loss = -11615.434377031741
Iteration 2000: Loss = -11615.426757006957
Iteration 2100: Loss = -11615.420424169466
Iteration 2200: Loss = -11615.414876066254
Iteration 2300: Loss = -11615.409906007937
Iteration 2400: Loss = -11615.40559097968
Iteration 2500: Loss = -11615.401725984068
Iteration 2600: Loss = -11615.399356372143
Iteration 2700: Loss = -11615.39514167605
Iteration 2800: Loss = -11615.392324208364
Iteration 2900: Loss = -11615.41648293408
1
Iteration 3000: Loss = -11615.387412754917
Iteration 3100: Loss = -11615.38534619475
Iteration 3200: Loss = -11615.383405854902
Iteration 3300: Loss = -11615.381668516671
Iteration 3400: Loss = -11615.379996488917
Iteration 3500: Loss = -11615.378457849049
Iteration 3600: Loss = -11615.388899475342
1
Iteration 3700: Loss = -11615.375574908392
Iteration 3800: Loss = -11615.37413374841
Iteration 3900: Loss = -11615.372571858998
Iteration 4000: Loss = -11615.37070582095
Iteration 4100: Loss = -11615.36901940839
Iteration 4200: Loss = -11615.368011708413
Iteration 4300: Loss = -11615.367268784144
Iteration 4400: Loss = -11615.366330933599
Iteration 4500: Loss = -11615.365594161245
Iteration 4600: Loss = -11615.374106810494
1
Iteration 4700: Loss = -11615.364195532564
Iteration 4800: Loss = -11615.364630465434
1
Iteration 4900: Loss = -11615.362832079923
Iteration 5000: Loss = -11615.365375385632
1
Iteration 5100: Loss = -11615.36441788321
2
Iteration 5200: Loss = -11615.360635496832
Iteration 5300: Loss = -11615.362299315111
1
Iteration 5400: Loss = -11615.359715831919
Iteration 5500: Loss = -11615.364937983333
1
Iteration 5600: Loss = -11615.35895132498
Iteration 5700: Loss = -11615.359441006594
1
Iteration 5800: Loss = -11615.362253643605
2
Iteration 5900: Loss = -11615.35998100062
3
Iteration 6000: Loss = -11615.359796178145
4
Iteration 6100: Loss = -11615.391761935469
5
Iteration 6200: Loss = -11615.35908171576
6
Iteration 6300: Loss = -11615.361552485101
7
Iteration 6400: Loss = -11615.366430148348
8
Iteration 6500: Loss = -11615.357057244204
Iteration 6600: Loss = -11615.35913545598
1
Iteration 6700: Loss = -11615.356257930769
Iteration 6800: Loss = -11615.360048829185
1
Iteration 6900: Loss = -11615.357959398174
2
Iteration 7000: Loss = -11615.356026610894
Iteration 7100: Loss = -11615.365710017191
1
Iteration 7200: Loss = -11615.35548445229
Iteration 7300: Loss = -11615.35583219449
1
Iteration 7400: Loss = -11615.355374074234
Iteration 7500: Loss = -11615.355328238184
Iteration 7600: Loss = -11615.355315007744
Iteration 7700: Loss = -11615.354939269995
Iteration 7800: Loss = -11615.35509779652
1
Iteration 7900: Loss = -11615.358670436442
2
Iteration 8000: Loss = -11615.354777966533
Iteration 8100: Loss = -11615.354845873228
Iteration 8200: Loss = -11615.355967904112
1
Iteration 8300: Loss = -11615.357332245512
2
Iteration 8400: Loss = -11615.354556431932
Iteration 8500: Loss = -11615.360116023385
1
Iteration 8600: Loss = -11615.354371796784
Iteration 8700: Loss = -11615.354481798026
1
Iteration 8800: Loss = -11615.367045966466
2
Iteration 8900: Loss = -11615.358655937478
3
Iteration 9000: Loss = -11615.354128887126
Iteration 9100: Loss = -11615.355851059405
1
Iteration 9200: Loss = -11615.355288645409
2
Iteration 9300: Loss = -11615.356293424695
3
Iteration 9400: Loss = -11615.354070636502
Iteration 9500: Loss = -11615.354280423679
1
Iteration 9600: Loss = -11615.356418117073
2
Iteration 9700: Loss = -11615.373782851699
3
Iteration 9800: Loss = -11615.373016475001
4
Iteration 9900: Loss = -11615.3555070571
5
Iteration 10000: Loss = -11615.361052848986
6
Iteration 10100: Loss = -11615.354227023576
7
Iteration 10200: Loss = -11615.35697786872
8
Iteration 10300: Loss = -11615.357334556285
9
Iteration 10400: Loss = -11615.357048794045
10
Iteration 10500: Loss = -11615.373517978422
11
Iteration 10600: Loss = -11615.353271480115
Iteration 10700: Loss = -11615.353295806726
Iteration 10800: Loss = -11615.423933688136
1
Iteration 10900: Loss = -11615.35349474949
2
Iteration 11000: Loss = -11615.375149950863
3
Iteration 11100: Loss = -11615.358755843006
4
Iteration 11200: Loss = -11615.353164355576
Iteration 11300: Loss = -11615.3569484611
1
Iteration 11400: Loss = -11615.393545085299
2
Iteration 11500: Loss = -11615.352908361761
Iteration 11600: Loss = -11615.353318725447
1
Iteration 11700: Loss = -11615.419113515116
2
Iteration 11800: Loss = -11615.396462856177
3
Iteration 11900: Loss = -11615.3571782999
4
Iteration 12000: Loss = -11615.361367605434
5
Iteration 12100: Loss = -11615.355481375484
6
Iteration 12200: Loss = -11615.352562171389
Iteration 12300: Loss = -11615.35237683861
Iteration 12400: Loss = -11615.35632008266
1
Iteration 12500: Loss = -11615.361043407127
2
Iteration 12600: Loss = -11615.409385701685
3
Iteration 12700: Loss = -11615.352606304154
4
Iteration 12800: Loss = -11615.353817359735
5
Iteration 12900: Loss = -11615.353413747504
6
Iteration 13000: Loss = -11615.385978629858
7
Iteration 13100: Loss = -11615.353502093745
8
Iteration 13200: Loss = -11615.357187556585
9
Iteration 13300: Loss = -11615.431998917487
10
Iteration 13400: Loss = -11615.355295152835
11
Iteration 13500: Loss = -11615.36081158253
12
Iteration 13600: Loss = -11615.363103042255
13
Iteration 13700: Loss = -11615.35600425149
14
Iteration 13800: Loss = -11615.35225010308
Iteration 13900: Loss = -11615.382656060936
1
Iteration 14000: Loss = -11615.360209754203
2
Iteration 14100: Loss = -11615.380559243276
3
Iteration 14200: Loss = -11615.353075336767
4
Iteration 14300: Loss = -11615.363622824456
5
Iteration 14400: Loss = -11615.352436051948
6
Iteration 14500: Loss = -11615.353352662398
7
Iteration 14600: Loss = -11615.35189642695
Iteration 14700: Loss = -11615.353836687244
1
Iteration 14800: Loss = -11615.352607766168
2
Iteration 14900: Loss = -11615.369648133394
3
Iteration 15000: Loss = -11615.352749163372
4
Iteration 15100: Loss = -11615.352250744148
5
Iteration 15200: Loss = -11615.35278905977
6
Iteration 15300: Loss = -11615.372430968306
7
Iteration 15400: Loss = -11615.35729990193
8
Iteration 15500: Loss = -11615.371118369087
9
Iteration 15600: Loss = -11615.354518279173
10
Iteration 15700: Loss = -11615.35315152698
11
Iteration 15800: Loss = -11615.35419297808
12
Iteration 15900: Loss = -11615.357284444235
13
Iteration 16000: Loss = -11615.353124653875
14
Iteration 16100: Loss = -11615.36791131593
15
Stopping early at iteration 16100 due to no improvement.
pi: tensor([[0.7263, 0.2737],
        [0.2637, 0.7363]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5319, 0.4681], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3922, 0.1080],
         [0.5961, 0.1985]],

        [[0.5813, 0.1152],
         [0.6351, 0.6718]],

        [[0.5610, 0.0952],
         [0.7203, 0.7165]],

        [[0.5446, 0.0994],
         [0.5608, 0.6457]],

        [[0.7128, 0.0882],
         [0.6489, 0.5775]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24031.77680513947
Iteration 100: Loss = -11729.61021393427
Iteration 200: Loss = -11616.879501822767
Iteration 300: Loss = -11616.119170370783
Iteration 400: Loss = -11615.829802064552
Iteration 500: Loss = -11615.681848112281
Iteration 600: Loss = -11615.594304889073
Iteration 700: Loss = -11615.537659192083
Iteration 800: Loss = -11615.498584593228
Iteration 900: Loss = -11615.47033225625
Iteration 1000: Loss = -11615.449216172787
Iteration 1100: Loss = -11615.43301863036
Iteration 1200: Loss = -11615.420244060619
Iteration 1300: Loss = -11615.410017853694
Iteration 1400: Loss = -11615.401623896552
Iteration 1500: Loss = -11615.394746968579
Iteration 1600: Loss = -11615.388927007829
Iteration 1700: Loss = -11615.384051681629
Iteration 1800: Loss = -11615.37988588693
Iteration 1900: Loss = -11615.376277777277
Iteration 2000: Loss = -11615.37315660054
Iteration 2100: Loss = -11615.370432374277
Iteration 2200: Loss = -11615.368047079184
Iteration 2300: Loss = -11615.365934145802
Iteration 2400: Loss = -11615.364047600366
Iteration 2500: Loss = -11615.362380435052
Iteration 2600: Loss = -11615.36085698189
Iteration 2700: Loss = -11615.35952786134
Iteration 2800: Loss = -11615.358268456464
Iteration 2900: Loss = -11615.357075556967
Iteration 3000: Loss = -11615.35609603264
Iteration 3100: Loss = -11615.355184638338
Iteration 3200: Loss = -11615.354308814054
Iteration 3300: Loss = -11615.353562777686
Iteration 3400: Loss = -11615.355306668273
1
Iteration 3500: Loss = -11615.352277761416
Iteration 3600: Loss = -11615.3516742218
Iteration 3700: Loss = -11615.353654083314
1
Iteration 3800: Loss = -11615.350684862648
Iteration 3900: Loss = -11615.350422587151
Iteration 4000: Loss = -11615.349795285298
Iteration 4100: Loss = -11615.352212140055
1
Iteration 4200: Loss = -11615.34906844533
Iteration 4300: Loss = -11615.34875435481
Iteration 4400: Loss = -11615.348710567576
Iteration 4500: Loss = -11615.348128429916
Iteration 4600: Loss = -11615.347915650134
Iteration 4700: Loss = -11615.34908946202
1
Iteration 4800: Loss = -11615.347424169538
Iteration 4900: Loss = -11615.347951607984
1
Iteration 5000: Loss = -11615.346994023912
Iteration 5100: Loss = -11615.346947596321
Iteration 5200: Loss = -11615.34665073623
Iteration 5300: Loss = -11615.347029042725
1
Iteration 5400: Loss = -11615.346331925482
Iteration 5500: Loss = -11615.347675930912
1
Iteration 5600: Loss = -11615.346045758804
Iteration 5700: Loss = -11615.362293279159
1
Iteration 5800: Loss = -11615.345804587
Iteration 5900: Loss = -11615.34594670805
1
Iteration 6000: Loss = -11615.350316332568
2
Iteration 6100: Loss = -11615.347342050238
3
Iteration 6200: Loss = -11615.346259521202
4
Iteration 6300: Loss = -11615.345276209611
Iteration 6400: Loss = -11615.345401949206
1
Iteration 6500: Loss = -11615.345427195223
2
Iteration 6600: Loss = -11615.346368765206
3
Iteration 6700: Loss = -11615.345024016884
Iteration 6800: Loss = -11615.346102995474
1
Iteration 6900: Loss = -11615.344902314782
Iteration 7000: Loss = -11615.344861616253
Iteration 7100: Loss = -11615.34565912155
1
Iteration 7200: Loss = -11615.349421381734
2
Iteration 7300: Loss = -11615.346978328935
3
Iteration 7400: Loss = -11615.346865984946
4
Iteration 7500: Loss = -11615.350805393327
5
Iteration 7600: Loss = -11615.344558543178
Iteration 7700: Loss = -11615.344597575442
Iteration 7800: Loss = -11615.344549894093
Iteration 7900: Loss = -11615.34499302363
1
Iteration 8000: Loss = -11615.344629829697
Iteration 8100: Loss = -11615.34457322655
Iteration 8200: Loss = -11615.344695504084
1
Iteration 8300: Loss = -11615.345029872464
2
Iteration 8400: Loss = -11615.34760028678
3
Iteration 8500: Loss = -11615.344616179766
Iteration 8600: Loss = -11615.345934737548
1
Iteration 8700: Loss = -11615.344633579689
Iteration 8800: Loss = -11615.345602067664
1
Iteration 8900: Loss = -11615.34426713632
Iteration 9000: Loss = -11615.378823225152
1
Iteration 9100: Loss = -11615.353983000907
2
Iteration 9200: Loss = -11615.346886733734
3
Iteration 9300: Loss = -11615.34494422084
4
Iteration 9400: Loss = -11615.344526499319
5
Iteration 9500: Loss = -11615.346800599214
6
Iteration 9600: Loss = -11615.387229844358
7
Iteration 9700: Loss = -11615.346860980371
8
Iteration 9800: Loss = -11615.34627620721
9
Iteration 9900: Loss = -11615.347278620618
10
Iteration 10000: Loss = -11615.347928283534
11
Iteration 10100: Loss = -11615.388576091102
12
Iteration 10200: Loss = -11615.346819274193
13
Iteration 10300: Loss = -11615.349187774958
14
Iteration 10400: Loss = -11615.349069963886
15
Stopping early at iteration 10400 due to no improvement.
pi: tensor([[0.7269, 0.2731],
        [0.2627, 0.7373]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5300, 0.4700], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3932, 0.1084],
         [0.6706, 0.1980]],

        [[0.5008, 0.1153],
         [0.5847, 0.6568]],

        [[0.5712, 0.0952],
         [0.6905, 0.7189]],

        [[0.5930, 0.0999],
         [0.6517, 0.6082]],

        [[0.6637, 0.0882],
         [0.5997, 0.6971]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919998119331364
11625.506444736695
[0.99199998169963, 0.99199998169963] [0.9919998119331364, 0.9919998119331364] [11615.36791131593, 11615.349069963886]
-------------------------------------
This iteration is 18
True Objective function: Loss = -11324.104388320342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23286.83877089716
Iteration 100: Loss = -12035.223541317528
Iteration 200: Loss = -12016.43433636118
Iteration 300: Loss = -11725.233334499957
Iteration 400: Loss = -11590.194539793632
Iteration 500: Loss = -11508.421745877842
Iteration 600: Loss = -11438.915413831935
Iteration 700: Loss = -11405.088069230598
Iteration 800: Loss = -11391.89296469733
Iteration 900: Loss = -11370.092148804006
Iteration 1000: Loss = -11366.291901798142
Iteration 1100: Loss = -11366.043678700573
Iteration 1200: Loss = -11357.855999474092
Iteration 1300: Loss = -11357.773768524983
Iteration 1400: Loss = -11348.415600541128
Iteration 1500: Loss = -11344.358333944749
Iteration 1600: Loss = -11332.61518553485
Iteration 1700: Loss = -11332.578797662922
Iteration 1800: Loss = -11332.552462141126
Iteration 1900: Loss = -11332.528957716288
Iteration 2000: Loss = -11332.35730551251
Iteration 2100: Loss = -11322.110797045869
Iteration 2200: Loss = -11322.09465623507
Iteration 2300: Loss = -11321.172368126501
Iteration 2400: Loss = -11320.997390425173
Iteration 2500: Loss = -11320.989244704186
Iteration 2600: Loss = -11320.98211015844
Iteration 2700: Loss = -11320.975898247243
Iteration 2800: Loss = -11320.970245138293
Iteration 2900: Loss = -11320.964935253161
Iteration 3000: Loss = -11320.95983377226
Iteration 3100: Loss = -11320.951173471438
Iteration 3200: Loss = -11318.081205950235
Iteration 3300: Loss = -11318.084894425998
1
Iteration 3400: Loss = -11318.069904190792
Iteration 3500: Loss = -11318.063622158665
Iteration 3600: Loss = -11318.034217375893
Iteration 3700: Loss = -11317.569353482315
Iteration 3800: Loss = -11317.559073175838
Iteration 3900: Loss = -11317.55995359821
1
Iteration 4000: Loss = -11317.555089328665
Iteration 4100: Loss = -11317.568489624224
1
Iteration 4200: Loss = -11317.5520970345
Iteration 4300: Loss = -11317.550781363747
Iteration 4400: Loss = -11317.551430765525
1
Iteration 4500: Loss = -11317.548444428194
Iteration 4600: Loss = -11317.54741412901
Iteration 4700: Loss = -11317.546420961844
Iteration 4800: Loss = -11317.545673159198
Iteration 4900: Loss = -11317.544670342897
Iteration 5000: Loss = -11317.543962928548
Iteration 5100: Loss = -11317.543115949315
Iteration 5200: Loss = -11317.542537645157
Iteration 5300: Loss = -11317.5447379524
1
Iteration 5400: Loss = -11317.542670501283
2
Iteration 5500: Loss = -11317.540514876282
Iteration 5600: Loss = -11317.539935317676
Iteration 5700: Loss = -11317.5401427114
1
Iteration 5800: Loss = -11317.538459539808
Iteration 5900: Loss = -11317.543848747935
1
Iteration 6000: Loss = -11317.530325536425
Iteration 6100: Loss = -11317.529943703772
Iteration 6200: Loss = -11317.53793971885
1
Iteration 6300: Loss = -11317.529865553215
Iteration 6400: Loss = -11317.527505871232
Iteration 6500: Loss = -11317.526959461931
Iteration 6600: Loss = -11317.525952315538
Iteration 6700: Loss = -11317.52788294248
1
Iteration 6800: Loss = -11317.521970640539
Iteration 6900: Loss = -11316.371013318654
Iteration 7000: Loss = -11316.327248303987
Iteration 7100: Loss = -11316.32654438556
Iteration 7200: Loss = -11316.326346002912
Iteration 7300: Loss = -11316.326113084693
Iteration 7400: Loss = -11316.324980670908
Iteration 7500: Loss = -11316.32515881765
1
Iteration 7600: Loss = -11316.325127015074
2
Iteration 7700: Loss = -11316.334266736287
3
Iteration 7800: Loss = -11316.324502899035
Iteration 7900: Loss = -11316.324048304494
Iteration 8000: Loss = -11316.329236622989
1
Iteration 8100: Loss = -11316.321551830319
Iteration 8200: Loss = -11316.32911231057
1
Iteration 8300: Loss = -11316.328368552364
2
Iteration 8400: Loss = -11316.322073836807
3
Iteration 8500: Loss = -11316.34821093955
4
Iteration 8600: Loss = -11316.365471028867
5
Iteration 8700: Loss = -11316.320629302478
Iteration 8800: Loss = -11316.320504031084
Iteration 8900: Loss = -11316.324416841406
1
Iteration 9000: Loss = -11316.33787804366
2
Iteration 9100: Loss = -11316.32364534929
3
Iteration 9200: Loss = -11316.320269305623
Iteration 9300: Loss = -11316.320814872906
1
Iteration 9400: Loss = -11316.320197157234
Iteration 9500: Loss = -11316.34733306272
1
Iteration 9600: Loss = -11316.321833999831
2
Iteration 9700: Loss = -11316.320261892797
Iteration 9800: Loss = -11316.320045231303
Iteration 9900: Loss = -11316.31972517249
Iteration 10000: Loss = -11316.320816813919
1
Iteration 10100: Loss = -11316.324033514593
2
Iteration 10200: Loss = -11316.319732736392
Iteration 10300: Loss = -11316.321394179931
1
Iteration 10400: Loss = -11316.319797788645
Iteration 10500: Loss = -11316.32121868906
1
Iteration 10600: Loss = -11316.3196967795
Iteration 10700: Loss = -11316.318738892172
Iteration 10800: Loss = -11316.319225564039
1
Iteration 10900: Loss = -11316.350444828446
2
Iteration 11000: Loss = -11316.3207791591
3
Iteration 11100: Loss = -11316.332092075996
4
Iteration 11200: Loss = -11316.318583977325
Iteration 11300: Loss = -11316.322202304533
1
Iteration 11400: Loss = -11316.321159712532
2
Iteration 11500: Loss = -11316.318556743983
Iteration 11600: Loss = -11316.318463260262
Iteration 11700: Loss = -11316.319522455973
1
Iteration 11800: Loss = -11316.337423254137
2
Iteration 11900: Loss = -11316.322969840152
3
Iteration 12000: Loss = -11316.32436538266
4
Iteration 12100: Loss = -11316.319021345826
5
Iteration 12200: Loss = -11316.32436290602
6
Iteration 12300: Loss = -11316.442659478033
7
Iteration 12400: Loss = -11316.325940275097
8
Iteration 12500: Loss = -11316.31852686962
Iteration 12600: Loss = -11316.33000823053
1
Iteration 12700: Loss = -11316.322237922293
2
Iteration 12800: Loss = -11316.325659352622
3
Iteration 12900: Loss = -11316.366913204049
4
Iteration 13000: Loss = -11316.32341398941
5
Iteration 13100: Loss = -11316.317120503583
Iteration 13200: Loss = -11316.32502879562
1
Iteration 13300: Loss = -11316.318677506717
2
Iteration 13400: Loss = -11316.322089667376
3
Iteration 13500: Loss = -11316.317298080034
4
Iteration 13600: Loss = -11316.315858827693
Iteration 13700: Loss = -11316.315989817576
1
Iteration 13800: Loss = -11316.316565379024
2
Iteration 13900: Loss = -11316.317329529904
3
Iteration 14000: Loss = -11316.318215974823
4
Iteration 14100: Loss = -11316.316691067988
5
Iteration 14200: Loss = -11316.318370013636
6
Iteration 14300: Loss = -11316.31693656343
7
Iteration 14400: Loss = -11316.317634760004
8
Iteration 14500: Loss = -11316.322710293074
9
Iteration 14600: Loss = -11316.316200974486
10
Iteration 14700: Loss = -11316.315958171841
Iteration 14800: Loss = -11316.364742771251
1
Iteration 14900: Loss = -11316.31751447302
2
Iteration 15000: Loss = -11316.322014174068
3
Iteration 15100: Loss = -11316.330009236954
4
Iteration 15200: Loss = -11316.318678234235
5
Iteration 15300: Loss = -11316.319124566257
6
Iteration 15400: Loss = -11316.319220554748
7
Iteration 15500: Loss = -11316.320042234947
8
Iteration 15600: Loss = -11316.316036136157
Iteration 15700: Loss = -11316.315687702832
Iteration 15800: Loss = -11316.324341191172
1
Iteration 15900: Loss = -11316.362610639093
2
Iteration 16000: Loss = -11316.315509645205
Iteration 16100: Loss = -11316.320837209205
1
Iteration 16200: Loss = -11316.326925372865
2
Iteration 16300: Loss = -11316.45010963389
3
Iteration 16400: Loss = -11316.35396206196
4
Iteration 16500: Loss = -11316.321436891543
5
Iteration 16600: Loss = -11316.335831321794
6
Iteration 16700: Loss = -11316.317066828167
7
Iteration 16800: Loss = -11316.316184902296
8
Iteration 16900: Loss = -11316.315604466534
Iteration 17000: Loss = -11316.315822170576
1
Iteration 17100: Loss = -11316.315781371251
2
Iteration 17200: Loss = -11316.315782835256
3
Iteration 17300: Loss = -11316.316040763504
4
Iteration 17400: Loss = -11316.322476833482
5
Iteration 17500: Loss = -11316.31563589308
Iteration 17600: Loss = -11316.339607892425
1
Iteration 17700: Loss = -11316.380478806439
2
Iteration 17800: Loss = -11316.344473257199
3
Iteration 17900: Loss = -11316.315663669477
Iteration 18000: Loss = -11316.316299465303
1
Iteration 18100: Loss = -11316.317822980825
2
Iteration 18200: Loss = -11316.354014535917
3
Iteration 18300: Loss = -11316.333687879065
4
Iteration 18400: Loss = -11316.33439020749
5
Iteration 18500: Loss = -11316.317700831954
6
Iteration 18600: Loss = -11316.352444820148
7
Iteration 18700: Loss = -11316.315746003103
Iteration 18800: Loss = -11316.317311486286
1
Iteration 18900: Loss = -11316.34323463484
2
Iteration 19000: Loss = -11316.322675993411
3
Iteration 19100: Loss = -11316.321670133171
4
Iteration 19200: Loss = -11316.320074888896
5
Iteration 19300: Loss = -11316.387374095102
6
Iteration 19400: Loss = -11316.338827297144
7
Iteration 19500: Loss = -11316.322024320538
8
Iteration 19600: Loss = -11316.327876676756
9
Iteration 19700: Loss = -11316.31582155821
Iteration 19800: Loss = -11316.31618546146
1
Iteration 19900: Loss = -11316.330357437122
2
pi: tensor([[0.7904, 0.2096],
        [0.2668, 0.7332]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5627, 0.4373], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1944, 0.1063],
         [0.6365, 0.4138]],

        [[0.5676, 0.0984],
         [0.5283, 0.5165]],

        [[0.5562, 0.0952],
         [0.6351, 0.7303]],

        [[0.7158, 0.1010],
         [0.6124, 0.7291]],

        [[0.6882, 0.1034],
         [0.5751, 0.6244]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21196.283828566062
Iteration 100: Loss = -12035.128902202061
Iteration 200: Loss = -12020.955035995152
Iteration 300: Loss = -11702.396858952392
Iteration 400: Loss = -11580.105929884046
Iteration 500: Loss = -11474.985322764587
Iteration 600: Loss = -11406.8010528342
Iteration 700: Loss = -11382.735514132779
Iteration 800: Loss = -11381.451174274369
Iteration 900: Loss = -11380.871454769032
Iteration 1000: Loss = -11380.61572096638
Iteration 1100: Loss = -11373.958835927611
Iteration 1200: Loss = -11370.363106285093
Iteration 1300: Loss = -11352.539918516402
Iteration 1400: Loss = -11352.374557073192
Iteration 1500: Loss = -11348.034976963669
Iteration 1600: Loss = -11338.43989559064
Iteration 1700: Loss = -11338.385796162785
Iteration 1800: Loss = -11338.355949337942
Iteration 1900: Loss = -11338.333563774053
Iteration 2000: Loss = -11338.315490627727
Iteration 2100: Loss = -11338.300347603412
Iteration 2200: Loss = -11338.287046500402
Iteration 2300: Loss = -11338.274799794324
Iteration 2400: Loss = -11338.26241426024
Iteration 2500: Loss = -11338.243214031012
Iteration 2600: Loss = -11330.26873874106
Iteration 2700: Loss = -11321.695305390596
Iteration 2800: Loss = -11321.641017797585
Iteration 2900: Loss = -11321.629938707692
Iteration 3000: Loss = -11321.623271824727
Iteration 3100: Loss = -11321.61806977437
Iteration 3200: Loss = -11321.613516004538
Iteration 3300: Loss = -11321.612795361849
Iteration 3400: Loss = -11321.6051764073
Iteration 3500: Loss = -11321.597768298487
Iteration 3600: Loss = -11320.504211054073
Iteration 3700: Loss = -11320.50049961817
Iteration 3800: Loss = -11320.497384649068
Iteration 3900: Loss = -11320.496231699755
Iteration 4000: Loss = -11320.491255999055
Iteration 4100: Loss = -11320.490085341726
Iteration 4200: Loss = -11320.499485189037
1
Iteration 4300: Loss = -11320.477704797302
Iteration 4400: Loss = -11320.478496799698
1
Iteration 4500: Loss = -11320.472208858138
Iteration 4600: Loss = -11320.470792237826
Iteration 4700: Loss = -11320.469596455774
Iteration 4800: Loss = -11320.473147341194
1
Iteration 4900: Loss = -11320.465869922518
Iteration 5000: Loss = -11320.458647945778
Iteration 5100: Loss = -11317.578087008333
Iteration 5200: Loss = -11317.575137552627
Iteration 5300: Loss = -11317.574567384943
Iteration 5400: Loss = -11317.57250579741
Iteration 5500: Loss = -11317.572331468495
Iteration 5600: Loss = -11317.570741631142
Iteration 5700: Loss = -11317.569618540245
Iteration 5800: Loss = -11317.567969580823
Iteration 5900: Loss = -11316.981598863082
Iteration 6000: Loss = -11316.386902712738
Iteration 6100: Loss = -11316.374749287197
Iteration 6200: Loss = -11316.333237407725
Iteration 6300: Loss = -11316.324366982306
Iteration 6400: Loss = -11316.324228641968
Iteration 6500: Loss = -11316.32366841227
Iteration 6600: Loss = -11316.3250316257
1
Iteration 6700: Loss = -11316.323020234548
Iteration 6800: Loss = -11316.322706716177
Iteration 6900: Loss = -11316.322524982306
Iteration 7000: Loss = -11316.323270071869
1
Iteration 7100: Loss = -11316.323389266061
2
Iteration 7200: Loss = -11316.321526887255
Iteration 7300: Loss = -11316.321258960328
Iteration 7400: Loss = -11316.327833949625
1
Iteration 7500: Loss = -11316.324646119074
2
Iteration 7600: Loss = -11316.321083624465
Iteration 7700: Loss = -11316.321053753169
Iteration 7800: Loss = -11316.318644112538
Iteration 7900: Loss = -11316.318064386442
Iteration 8000: Loss = -11316.319082914779
1
Iteration 8100: Loss = -11316.320771029064
2
Iteration 8200: Loss = -11316.318035394523
Iteration 8300: Loss = -11316.325520463415
1
Iteration 8400: Loss = -11316.317072621317
Iteration 8500: Loss = -11316.317808806316
1
Iteration 8600: Loss = -11316.317111715513
Iteration 8700: Loss = -11316.317587273672
1
Iteration 8800: Loss = -11316.318853129957
2
Iteration 8900: Loss = -11316.31735903469
3
Iteration 9000: Loss = -11316.317578713357
4
Iteration 9100: Loss = -11316.321356372331
5
Iteration 9200: Loss = -11316.341431223584
6
Iteration 9300: Loss = -11316.319105773211
7
Iteration 9400: Loss = -11316.355584225235
8
Iteration 9500: Loss = -11316.320576266437
9
Iteration 9600: Loss = -11316.326759590436
10
Iteration 9700: Loss = -11316.335157469044
11
Iteration 9800: Loss = -11316.319201573984
12
Iteration 9900: Loss = -11316.316914534233
Iteration 10000: Loss = -11316.31724954309
1
Iteration 10100: Loss = -11316.320041328592
2
Iteration 10200: Loss = -11316.382548525613
3
Iteration 10300: Loss = -11316.329864569207
4
Iteration 10400: Loss = -11316.316563227583
Iteration 10500: Loss = -11316.360213358364
1
Iteration 10600: Loss = -11316.319012650945
2
Iteration 10700: Loss = -11316.376191578138
3
Iteration 10800: Loss = -11316.317750782451
4
Iteration 10900: Loss = -11316.315182514854
Iteration 11000: Loss = -11316.316016132612
1
Iteration 11100: Loss = -11316.322526858326
2
Iteration 11200: Loss = -11316.321503446876
3
Iteration 11300: Loss = -11316.361375204267
4
Iteration 11400: Loss = -11316.315622769775
5
Iteration 11500: Loss = -11316.31437877514
Iteration 11600: Loss = -11316.31518860104
1
Iteration 11700: Loss = -11316.316385193033
2
Iteration 11800: Loss = -11316.319604668346
3
Iteration 11900: Loss = -11316.353100536544
4
Iteration 12000: Loss = -11316.321593552777
5
Iteration 12100: Loss = -11316.31933118706
6
Iteration 12200: Loss = -11316.317513321364
7
Iteration 12300: Loss = -11316.31620342153
8
Iteration 12400: Loss = -11316.315687194212
9
Iteration 12500: Loss = -11316.314626524074
10
Iteration 12600: Loss = -11316.325027186898
11
Iteration 12700: Loss = -11316.319399309898
12
Iteration 12800: Loss = -11316.318616034368
13
Iteration 12900: Loss = -11316.316154858428
14
Iteration 13000: Loss = -11316.314082705934
Iteration 13100: Loss = -11316.314079186322
Iteration 13200: Loss = -11316.314461139633
1
Iteration 13300: Loss = -11316.315064090983
2
Iteration 13400: Loss = -11316.314391717046
3
Iteration 13500: Loss = -11316.316914082958
4
Iteration 13600: Loss = -11316.369677667062
5
Iteration 13700: Loss = -11316.336822306277
6
Iteration 13800: Loss = -11316.353363961776
7
Iteration 13900: Loss = -11316.314852422052
8
Iteration 14000: Loss = -11316.317176462164
9
Iteration 14100: Loss = -11316.318188899015
10
Iteration 14200: Loss = -11316.315186495742
11
Iteration 14300: Loss = -11316.314071089511
Iteration 14400: Loss = -11316.320225923066
1
Iteration 14500: Loss = -11316.315742991728
2
Iteration 14600: Loss = -11316.315278247457
3
Iteration 14700: Loss = -11316.331530604493
4
Iteration 14800: Loss = -11316.425063484843
5
Iteration 14900: Loss = -11316.315076005734
6
Iteration 15000: Loss = -11316.320476057472
7
Iteration 15100: Loss = -11316.31489073747
8
Iteration 15200: Loss = -11316.342088400594
9
Iteration 15300: Loss = -11316.314618510454
10
Iteration 15400: Loss = -11316.31428891839
11
Iteration 15500: Loss = -11316.313762565693
Iteration 15600: Loss = -11316.313781015011
Iteration 15700: Loss = -11316.318110344295
1
Iteration 15800: Loss = -11316.314568289441
2
Iteration 15900: Loss = -11316.391512244156
3
Iteration 16000: Loss = -11316.316497902057
4
Iteration 16100: Loss = -11316.314170430509
5
Iteration 16200: Loss = -11316.321958303013
6
Iteration 16300: Loss = -11316.35229646695
7
Iteration 16400: Loss = -11316.32625751656
8
Iteration 16500: Loss = -11316.317988807634
9
Iteration 16600: Loss = -11316.313750479338
Iteration 16700: Loss = -11316.315780330382
1
Iteration 16800: Loss = -11316.316326146225
2
Iteration 16900: Loss = -11316.319027201662
3
Iteration 17000: Loss = -11316.375112344693
4
Iteration 17100: Loss = -11316.333928219396
5
Iteration 17200: Loss = -11316.32629248797
6
Iteration 17300: Loss = -11316.313704422522
Iteration 17400: Loss = -11316.315039160692
1
Iteration 17500: Loss = -11316.320516093845
2
Iteration 17600: Loss = -11316.330416191715
3
Iteration 17700: Loss = -11316.345848947753
4
Iteration 17800: Loss = -11316.315287195184
5
Iteration 17900: Loss = -11316.314516222215
6
Iteration 18000: Loss = -11316.320151493928
7
Iteration 18100: Loss = -11316.339503187577
8
Iteration 18200: Loss = -11316.313642561028
Iteration 18300: Loss = -11316.31360007029
Iteration 18400: Loss = -11316.314286803716
1
Iteration 18500: Loss = -11316.314487327383
2
Iteration 18600: Loss = -11316.314257382068
3
Iteration 18700: Loss = -11316.316577843594
4
Iteration 18800: Loss = -11316.362359649596
5
Iteration 18900: Loss = -11316.322773580006
6
Iteration 19000: Loss = -11316.313614441158
Iteration 19100: Loss = -11316.313937138531
1
Iteration 19200: Loss = -11316.354862056642
2
Iteration 19300: Loss = -11316.314080491944
3
Iteration 19400: Loss = -11316.373440534417
4
Iteration 19500: Loss = -11316.3481864745
5
Iteration 19600: Loss = -11316.358568256894
6
Iteration 19700: Loss = -11316.327514332823
7
Iteration 19800: Loss = -11316.317245287759
8
Iteration 19900: Loss = -11316.31557045846
9
pi: tensor([[0.7902, 0.2098],
        [0.2671, 0.7329]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5634, 0.4366], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1945, 0.1065],
         [0.5105, 0.4140]],

        [[0.7190, 0.0983],
         [0.5720, 0.5500]],

        [[0.7088, 0.0952],
         [0.6733, 0.6436]],

        [[0.5741, 0.1010],
         [0.5412, 0.6370]],

        [[0.5741, 0.1030],
         [0.6649, 0.5464]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919988588426287
Average Adjusted Rand Index: 0.9919995611635631
11324.104388320342
[0.9919988588426287, 0.9919988588426287] [0.9919995611635631, 0.9919995611635631] [11316.317061716883, 11316.314730776745]
-------------------------------------
This iteration is 19
True Objective function: Loss = -11756.119910701182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22771.06215959859
Iteration 100: Loss = -12521.81322560706
Iteration 200: Loss = -12418.435703657693
Iteration 300: Loss = -12195.121354209488
Iteration 400: Loss = -11758.613987104667
Iteration 500: Loss = -11751.596577234348
Iteration 600: Loss = -11751.399162062571
Iteration 700: Loss = -11751.301091585443
Iteration 800: Loss = -11751.242042691389
Iteration 900: Loss = -11751.202661052183
Iteration 1000: Loss = -11751.174652613658
Iteration 1100: Loss = -11751.153767847032
Iteration 1200: Loss = -11751.137626156024
Iteration 1300: Loss = -11751.124873895924
Iteration 1400: Loss = -11751.114526396002
Iteration 1500: Loss = -11751.106076890346
Iteration 1600: Loss = -11751.09899497879
Iteration 1700: Loss = -11751.092986592763
Iteration 1800: Loss = -11751.087872615828
Iteration 1900: Loss = -11751.083487136764
Iteration 2000: Loss = -11751.079647553841
Iteration 2100: Loss = -11751.07632376309
Iteration 2200: Loss = -11751.073377141773
Iteration 2300: Loss = -11751.07077278393
Iteration 2400: Loss = -11751.068464815142
Iteration 2500: Loss = -11751.06638474789
Iteration 2600: Loss = -11751.064524023182
Iteration 2700: Loss = -11751.062906213943
Iteration 2800: Loss = -11751.061416351424
Iteration 2900: Loss = -11751.060011251648
Iteration 3000: Loss = -11751.05884543659
Iteration 3100: Loss = -11751.05769329882
Iteration 3200: Loss = -11751.056660497985
Iteration 3300: Loss = -11751.056551224865
Iteration 3400: Loss = -11751.054901625328
Iteration 3500: Loss = -11751.05411889946
Iteration 3600: Loss = -11751.053363146735
Iteration 3700: Loss = -11751.052672273288
Iteration 3800: Loss = -11751.05263427235
Iteration 3900: Loss = -11751.051494238396
Iteration 4000: Loss = -11751.051115882978
Iteration 4100: Loss = -11751.050446066536
Iteration 4200: Loss = -11751.050875156154
1
Iteration 4300: Loss = -11751.049592677326
Iteration 4400: Loss = -11751.049303283038
Iteration 4500: Loss = -11751.050898760257
1
Iteration 4600: Loss = -11751.048704642966
Iteration 4700: Loss = -11751.048524688798
Iteration 4800: Loss = -11751.047892677578
Iteration 4900: Loss = -11751.047943777825
Iteration 5000: Loss = -11751.053351918992
1
Iteration 5100: Loss = -11751.047113572691
Iteration 5200: Loss = -11751.047766448604
1
Iteration 5300: Loss = -11751.046683014141
Iteration 5400: Loss = -11751.046556758422
Iteration 5500: Loss = -11751.047489180355
1
Iteration 5600: Loss = -11751.046100785643
Iteration 5700: Loss = -11751.047015329948
1
Iteration 5800: Loss = -11751.045761814239
Iteration 5900: Loss = -11751.045679635974
Iteration 6000: Loss = -11751.045467220443
Iteration 6100: Loss = -11751.045361200018
Iteration 6200: Loss = -11751.045205872046
Iteration 6300: Loss = -11751.04520873416
Iteration 6400: Loss = -11751.045112838552
Iteration 6500: Loss = -11751.045509778502
1
Iteration 6600: Loss = -11751.044813673201
Iteration 6700: Loss = -11751.046586343247
1
Iteration 6800: Loss = -11751.04476808814
Iteration 6900: Loss = -11751.061558988871
1
Iteration 7000: Loss = -11751.044543869419
Iteration 7100: Loss = -11751.046834178574
1
Iteration 7200: Loss = -11751.044614463703
Iteration 7300: Loss = -11751.04481136707
1
Iteration 7400: Loss = -11751.04471039258
Iteration 7500: Loss = -11751.049077324913
1
Iteration 7600: Loss = -11751.052608824288
2
Iteration 7700: Loss = -11751.044214071011
Iteration 7800: Loss = -11751.04413071532
Iteration 7900: Loss = -11751.051755096434
1
Iteration 8000: Loss = -11751.044692931957
2
Iteration 8100: Loss = -11751.046249387271
3
Iteration 8200: Loss = -11751.056325613878
4
Iteration 8300: Loss = -11751.045076160273
5
Iteration 8400: Loss = -11751.053344144351
6
Iteration 8500: Loss = -11751.043761997418
Iteration 8600: Loss = -11751.044495929695
1
Iteration 8700: Loss = -11751.043686440085
Iteration 8800: Loss = -11751.04409057887
1
Iteration 8900: Loss = -11751.045094479168
2
Iteration 9000: Loss = -11751.043980755525
3
Iteration 9100: Loss = -11751.04362572117
Iteration 9200: Loss = -11751.045263729306
1
Iteration 9300: Loss = -11751.04466963563
2
Iteration 9400: Loss = -11751.044070951473
3
Iteration 9500: Loss = -11751.08071440144
4
Iteration 9600: Loss = -11751.043594144412
Iteration 9700: Loss = -11751.043900168059
1
Iteration 9800: Loss = -11751.049072375976
2
Iteration 9900: Loss = -11751.043798513369
3
Iteration 10000: Loss = -11751.043665702236
Iteration 10100: Loss = -11751.054115125315
1
Iteration 10200: Loss = -11751.043418141247
Iteration 10300: Loss = -11751.04540651899
1
Iteration 10400: Loss = -11751.04338042444
Iteration 10500: Loss = -11751.051585937319
1
Iteration 10600: Loss = -11751.044719488562
2
Iteration 10700: Loss = -11751.04485581836
3
Iteration 10800: Loss = -11751.048548649975
4
Iteration 10900: Loss = -11751.150684885555
5
Iteration 11000: Loss = -11751.058151334479
6
Iteration 11100: Loss = -11751.045086671216
7
Iteration 11200: Loss = -11751.049248536609
8
Iteration 11300: Loss = -11751.094630030962
9
Iteration 11400: Loss = -11751.049251972225
10
Iteration 11500: Loss = -11751.043852577766
11
Iteration 11600: Loss = -11751.048110773216
12
Iteration 11700: Loss = -11751.064394600597
13
Iteration 11800: Loss = -11751.069345543649
14
Iteration 11900: Loss = -11751.052171384206
15
Stopping early at iteration 11900 due to no improvement.
pi: tensor([[0.7238, 0.2762],
        [0.2229, 0.7771]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4902, 0.5098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.0993],
         [0.5861, 0.3995]],

        [[0.6020, 0.1013],
         [0.5776, 0.6995]],

        [[0.5531, 0.1001],
         [0.6305, 0.7263]],

        [[0.6978, 0.1006],
         [0.6758, 0.5529]],

        [[0.6844, 0.1055],
         [0.5318, 0.6792]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9919998665493138
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21673.109384837167
Iteration 100: Loss = -12622.030826647131
Iteration 200: Loss = -12206.776139616166
Iteration 300: Loss = -11871.124256592955
Iteration 400: Loss = -11817.504301856188
Iteration 500: Loss = -11786.802894105058
Iteration 600: Loss = -11786.511711740312
Iteration 700: Loss = -11786.087742566431
Iteration 800: Loss = -11777.873813007403
Iteration 900: Loss = -11777.816241000703
Iteration 1000: Loss = -11777.774876187606
Iteration 1100: Loss = -11777.743753724782
Iteration 1200: Loss = -11777.71965866387
Iteration 1300: Loss = -11777.700289593404
Iteration 1400: Loss = -11777.684080213121
Iteration 1500: Loss = -11777.666357028515
Iteration 1600: Loss = -11756.528026854518
Iteration 1700: Loss = -11756.510730085954
Iteration 1800: Loss = -11756.501517465676
Iteration 1900: Loss = -11756.493906564674
Iteration 2000: Loss = -11756.487553062145
Iteration 2100: Loss = -11756.482146638486
Iteration 2200: Loss = -11756.477407380338
Iteration 2300: Loss = -11756.473337186006
Iteration 2400: Loss = -11756.469753570555
Iteration 2500: Loss = -11756.46655125975
Iteration 2600: Loss = -11756.463747771877
Iteration 2700: Loss = -11756.461236510013
Iteration 2800: Loss = -11756.458968309942
Iteration 2900: Loss = -11756.459547959621
1
Iteration 3000: Loss = -11756.455109937064
Iteration 3100: Loss = -11756.453405263728
Iteration 3200: Loss = -11756.455453881035
1
Iteration 3300: Loss = -11756.450583302132
Iteration 3400: Loss = -11756.453249849614
1
Iteration 3500: Loss = -11756.451648180815
2
Iteration 3600: Loss = -11756.447095788302
Iteration 3700: Loss = -11756.446114774113
Iteration 3800: Loss = -11756.44532251708
Iteration 3900: Loss = -11756.44436830324
Iteration 4000: Loss = -11756.443612816005
Iteration 4100: Loss = -11756.442885355284
Iteration 4200: Loss = -11756.442287061322
Iteration 4300: Loss = -11756.44171797749
Iteration 4400: Loss = -11756.441080106095
Iteration 4500: Loss = -11756.444363188935
1
Iteration 4600: Loss = -11756.440126951291
Iteration 4700: Loss = -11756.439627110878
Iteration 4800: Loss = -11756.442551572649
1
Iteration 4900: Loss = -11756.43877219213
Iteration 5000: Loss = -11756.438418728027
Iteration 5100: Loss = -11756.438070523604
Iteration 5200: Loss = -11756.454744252382
1
Iteration 5300: Loss = -11756.437520896397
Iteration 5400: Loss = -11756.437157775423
Iteration 5500: Loss = -11756.437781789542
1
Iteration 5600: Loss = -11756.436575940532
Iteration 5700: Loss = -11756.436312204527
Iteration 5800: Loss = -11756.440339970679
1
Iteration 5900: Loss = -11756.435840904876
Iteration 6000: Loss = -11756.435597438824
Iteration 6100: Loss = -11756.43533945417
Iteration 6200: Loss = -11756.434832266172
Iteration 6300: Loss = -11751.26149786507
Iteration 6400: Loss = -11751.261163970648
Iteration 6500: Loss = -11751.293127060664
1
Iteration 6600: Loss = -11751.260828757937
Iteration 6700: Loss = -11751.260685974814
Iteration 6800: Loss = -11751.263660133407
1
Iteration 6900: Loss = -11751.055116753825
Iteration 7000: Loss = -11751.05670258117
1
Iteration 7100: Loss = -11751.053454028723
Iteration 7200: Loss = -11751.053210560734
Iteration 7300: Loss = -11751.053529853954
1
Iteration 7400: Loss = -11751.053103698176
Iteration 7500: Loss = -11751.053990051809
1
Iteration 7600: Loss = -11751.053567884359
2
Iteration 7700: Loss = -11751.052889947878
Iteration 7800: Loss = -11751.052770461343
Iteration 7900: Loss = -11751.05835283994
1
Iteration 8000: Loss = -11751.052580360216
Iteration 8100: Loss = -11751.052646672199
Iteration 8200: Loss = -11751.103741667534
1
Iteration 8300: Loss = -11751.052368178935
Iteration 8400: Loss = -11751.06143615699
1
Iteration 8500: Loss = -11751.052377073662
Iteration 8600: Loss = -11751.145816126971
1
Iteration 8700: Loss = -11751.052114212258
Iteration 8800: Loss = -11751.052647916158
1
Iteration 8900: Loss = -11751.08443391631
2
Iteration 9000: Loss = -11751.051365002178
Iteration 9100: Loss = -11751.051201338118
Iteration 9200: Loss = -11751.074996323468
1
Iteration 9300: Loss = -11751.05659253478
2
Iteration 9400: Loss = -11751.051368630253
3
Iteration 9500: Loss = -11751.05714473495
4
Iteration 9600: Loss = -11751.050159101713
Iteration 9700: Loss = -11751.056286000412
1
Iteration 9800: Loss = -11751.053183502385
2
Iteration 9900: Loss = -11751.051529283532
3
Iteration 10000: Loss = -11751.051313614344
4
Iteration 10100: Loss = -11751.074502460198
5
Iteration 10200: Loss = -11751.060075457286
6
Iteration 10300: Loss = -11751.0544260095
7
Iteration 10400: Loss = -11751.051991085804
8
Iteration 10500: Loss = -11751.08369878774
9
Iteration 10600: Loss = -11751.06677480105
10
Iteration 10700: Loss = -11751.051107564634
11
Iteration 10800: Loss = -11751.05007412633
Iteration 10900: Loss = -11751.059555370608
1
Iteration 11000: Loss = -11751.079448978247
2
Iteration 11100: Loss = -11751.116982944062
3
Iteration 11200: Loss = -11751.125666901475
4
Iteration 11300: Loss = -11751.053929517944
5
Iteration 11400: Loss = -11751.054989635359
6
Iteration 11500: Loss = -11751.062220764396
7
Iteration 11600: Loss = -11751.059503765844
8
Iteration 11700: Loss = -11751.06312892257
9
Iteration 11800: Loss = -11751.051216129632
10
Iteration 11900: Loss = -11751.080140150158
11
Iteration 12000: Loss = -11751.055409430477
12
Iteration 12100: Loss = -11751.052137916831
13
Iteration 12200: Loss = -11751.050573929855
14
Iteration 12300: Loss = -11751.061365091384
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[0.7236, 0.2764],
        [0.2218, 0.7782]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4902, 0.5098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.0994],
         [0.7009, 0.3993]],

        [[0.6438, 0.1002],
         [0.6898, 0.5697]],

        [[0.5175, 0.1000],
         [0.5704, 0.6617]],

        [[0.5265, 0.1011],
         [0.6393, 0.5099]],

        [[0.5584, 0.1053],
         [0.5436, 0.5606]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9919998665493138
Average Adjusted Rand Index: 0.9919998119331364
11756.119910701182
[0.9919998665493138, 0.9919998665493138] [0.9919998119331364, 0.9919998119331364] [11751.052171384206, 11751.061365091384]
-------------------------------------
This iteration is 20
True Objective function: Loss = -11714.192135278518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21471.742251894284
Iteration 100: Loss = -12524.524678445063
Iteration 200: Loss = -12486.248706699353
Iteration 300: Loss = -12092.612899171969
Iteration 400: Loss = -11889.364880084397
Iteration 500: Loss = -11866.818841187764
Iteration 600: Loss = -11843.35987200148
Iteration 700: Loss = -11835.713878450872
Iteration 800: Loss = -11815.622179404878
Iteration 900: Loss = -11807.725445346596
Iteration 1000: Loss = -11807.115683206986
Iteration 1100: Loss = -11803.244944661494
Iteration 1200: Loss = -11803.165474567495
Iteration 1300: Loss = -11802.999018200417
Iteration 1400: Loss = -11799.299439301463
Iteration 1500: Loss = -11789.644030591304
Iteration 1600: Loss = -11771.933795529696
Iteration 1700: Loss = -11769.934575519672
Iteration 1800: Loss = -11763.793786627642
Iteration 1900: Loss = -11763.733672756587
Iteration 2000: Loss = -11749.447408862448
Iteration 2100: Loss = -11749.399897482801
Iteration 2200: Loss = -11737.205036390596
Iteration 2300: Loss = -11737.050476731642
Iteration 2400: Loss = -11723.846782529705
Iteration 2500: Loss = -11723.815615297664
Iteration 2600: Loss = -11723.800956759253
Iteration 2700: Loss = -11723.68044158319
Iteration 2800: Loss = -11710.042471586396
Iteration 2900: Loss = -11710.031417931878
Iteration 3000: Loss = -11710.016360140113
Iteration 3100: Loss = -11710.00810620517
Iteration 3200: Loss = -11710.001870388107
Iteration 3300: Loss = -11709.998083030208
Iteration 3400: Loss = -11709.994040030222
Iteration 3500: Loss = -11709.994092935258
Iteration 3600: Loss = -11709.987987905895
Iteration 3700: Loss = -11709.985083948943
Iteration 3800: Loss = -11709.982867787292
Iteration 3900: Loss = -11709.986757183467
1
Iteration 4000: Loss = -11709.970466212402
Iteration 4100: Loss = -11709.946389682607
Iteration 4200: Loss = -11709.944580353424
Iteration 4300: Loss = -11709.94033445656
Iteration 4400: Loss = -11709.938800134878
Iteration 4500: Loss = -11709.946038739961
1
Iteration 4600: Loss = -11709.936409694596
Iteration 4700: Loss = -11709.935476165152
Iteration 4800: Loss = -11709.935606351803
1
Iteration 4900: Loss = -11709.933786922886
Iteration 5000: Loss = -11709.932555021052
Iteration 5100: Loss = -11709.932429992245
Iteration 5200: Loss = -11709.940577377522
1
Iteration 5300: Loss = -11709.93008524359
Iteration 5400: Loss = -11709.930051622116
Iteration 5500: Loss = -11709.928440349142
Iteration 5600: Loss = -11709.927830799792
Iteration 5700: Loss = -11709.927171995332
Iteration 5800: Loss = -11709.926868452552
Iteration 5900: Loss = -11709.926238202072
Iteration 6000: Loss = -11709.932568028351
1
Iteration 6100: Loss = -11709.925421856962
Iteration 6200: Loss = -11709.927414625387
1
Iteration 6300: Loss = -11709.924693380834
Iteration 6400: Loss = -11709.925114737334
1
Iteration 6500: Loss = -11709.924266110656
Iteration 6600: Loss = -11709.94886452559
1
Iteration 6700: Loss = -11709.923490819336
Iteration 6800: Loss = -11709.927354785674
1
Iteration 6900: Loss = -11709.925041596636
2
Iteration 7000: Loss = -11709.925570041914
3
Iteration 7100: Loss = -11709.923570479257
Iteration 7200: Loss = -11709.966415275549
1
Iteration 7300: Loss = -11709.92378020759
2
Iteration 7400: Loss = -11709.92221036462
Iteration 7500: Loss = -11709.925859831983
1
Iteration 7600: Loss = -11709.922150951777
Iteration 7700: Loss = -11709.921403134751
Iteration 7800: Loss = -11709.921688429149
1
Iteration 7900: Loss = -11709.92116672291
Iteration 8000: Loss = -11709.921005464357
Iteration 8100: Loss = -11709.926437161232
1
Iteration 8200: Loss = -11709.920802451956
Iteration 8300: Loss = -11709.9205434147
Iteration 8400: Loss = -11709.92722117541
1
Iteration 8500: Loss = -11709.920308228537
Iteration 8600: Loss = -11709.973240146423
1
Iteration 8700: Loss = -11709.92004667347
Iteration 8800: Loss = -11709.92003558636
Iteration 8900: Loss = -11709.931309992726
1
Iteration 9000: Loss = -11709.923522966736
2
Iteration 9100: Loss = -11709.920645446276
3
Iteration 9200: Loss = -11709.919931243927
Iteration 9300: Loss = -11709.91948236991
Iteration 9400: Loss = -11709.921822149678
1
Iteration 9500: Loss = -11709.919321755895
Iteration 9600: Loss = -11709.919585157839
1
Iteration 9700: Loss = -11709.925709069286
2
Iteration 9800: Loss = -11709.930493485926
3
Iteration 9900: Loss = -11709.975599727877
4
Iteration 10000: Loss = -11709.948421484323
5
Iteration 10100: Loss = -11710.013680424314
6
Iteration 10200: Loss = -11709.944219529505
7
Iteration 10300: Loss = -11709.922513976966
8
Iteration 10400: Loss = -11709.92313795752
9
Iteration 10500: Loss = -11709.919350585864
Iteration 10600: Loss = -11709.935375408531
1
Iteration 10700: Loss = -11709.919292176983
Iteration 10800: Loss = -11709.919114435197
Iteration 10900: Loss = -11709.93634550815
1
Iteration 11000: Loss = -11709.922321798149
2
Iteration 11100: Loss = -11709.920588388795
3
Iteration 11200: Loss = -11709.976587233677
4
Iteration 11300: Loss = -11709.921600145244
5
Iteration 11400: Loss = -11709.939428680844
6
Iteration 11500: Loss = -11709.971474835475
7
Iteration 11600: Loss = -11709.937844917637
8
Iteration 11700: Loss = -11709.92411798114
9
Iteration 11800: Loss = -11709.934256633722
10
Iteration 11900: Loss = -11709.935501416287
11
Iteration 12000: Loss = -11709.918800046467
Iteration 12100: Loss = -11709.91933759224
1
Iteration 12200: Loss = -11709.92775385145
2
Iteration 12300: Loss = -11710.037698719545
3
Iteration 12400: Loss = -11709.935828516587
4
Iteration 12500: Loss = -11709.942189879466
5
Iteration 12600: Loss = -11710.045118617518
6
Iteration 12700: Loss = -11709.919096623285
7
Iteration 12800: Loss = -11709.935510310297
8
Iteration 12900: Loss = -11709.920045765093
9
Iteration 13000: Loss = -11709.955379969555
10
Iteration 13100: Loss = -11709.926985508013
11
Iteration 13200: Loss = -11709.919513896566
12
Iteration 13300: Loss = -11709.91932655827
13
Iteration 13400: Loss = -11709.946802799277
14
Iteration 13500: Loss = -11709.926221416328
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7679, 0.2321],
        [0.2712, 0.7288]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4746, 0.5254], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4004, 0.1040],
         [0.6382, 0.2066]],

        [[0.5125, 0.0976],
         [0.5214, 0.6897]],

        [[0.6917, 0.0962],
         [0.5125, 0.5313]],

        [[0.5092, 0.1082],
         [0.6866, 0.6874]],

        [[0.6368, 0.1041],
         [0.6939, 0.5753]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23083.111918559352
Iteration 100: Loss = -12506.106669974088
Iteration 200: Loss = -12235.568285091573
Iteration 300: Loss = -12071.994605245201
Iteration 400: Loss = -11968.402497791656
Iteration 500: Loss = -11910.919279979327
Iteration 600: Loss = -11868.867397659902
Iteration 700: Loss = -11855.344010084096
Iteration 800: Loss = -11853.055940328726
Iteration 900: Loss = -11852.932841265643
Iteration 1000: Loss = -11852.843772632312
Iteration 1100: Loss = -11852.765520916399
Iteration 1200: Loss = -11852.724287365403
Iteration 1300: Loss = -11852.694517257292
Iteration 1400: Loss = -11852.6712083341
Iteration 1500: Loss = -11852.652397086154
Iteration 1600: Loss = -11852.637028501835
Iteration 1700: Loss = -11852.624234867406
Iteration 1800: Loss = -11852.61346542606
Iteration 1900: Loss = -11852.604285899737
Iteration 2000: Loss = -11852.602367707223
Iteration 2100: Loss = -11852.589726264801
Iteration 2200: Loss = -11852.583797436106
Iteration 2300: Loss = -11852.578633983205
Iteration 2400: Loss = -11852.574077343048
Iteration 2500: Loss = -11852.570041182867
Iteration 2600: Loss = -11852.568669364307
Iteration 2700: Loss = -11852.563486606114
Iteration 2800: Loss = -11852.56058615528
Iteration 2900: Loss = -11852.557839541394
Iteration 3000: Loss = -11852.555274347242
Iteration 3100: Loss = -11852.553535581408
Iteration 3200: Loss = -11852.551192797337
Iteration 3300: Loss = -11852.549761234384
Iteration 3400: Loss = -11852.548171592958
Iteration 3500: Loss = -11852.546263856724
Iteration 3600: Loss = -11852.544883086312
Iteration 3700: Loss = -11852.549381262561
1
Iteration 3800: Loss = -11852.542481457675
Iteration 3900: Loss = -11852.541486198012
Iteration 4000: Loss = -11852.544501238515
1
Iteration 4100: Loss = -11852.539542504943
Iteration 4200: Loss = -11852.538940881606
Iteration 4300: Loss = -11852.537882153365
Iteration 4400: Loss = -11852.537292926592
Iteration 4500: Loss = -11852.53641203206
Iteration 4600: Loss = -11852.537166524327
1
Iteration 4700: Loss = -11852.535202439025
Iteration 4800: Loss = -11852.534763441548
Iteration 4900: Loss = -11852.535731973041
1
Iteration 5000: Loss = -11852.533666624786
Iteration 5100: Loss = -11852.543991530367
1
Iteration 5200: Loss = -11852.53281901192
Iteration 5300: Loss = -11852.532455994353
Iteration 5400: Loss = -11852.532022445255
Iteration 5500: Loss = -11852.531711573096
Iteration 5600: Loss = -11852.536762865659
1
Iteration 5700: Loss = -11852.531057197279
Iteration 5800: Loss = -11852.5312937448
1
Iteration 5900: Loss = -11852.530980442378
Iteration 6000: Loss = -11852.530784938243
Iteration 6100: Loss = -11852.529923988306
Iteration 6200: Loss = -11852.529780339195
Iteration 6300: Loss = -11852.529512566216
Iteration 6400: Loss = -11852.52952737777
Iteration 6500: Loss = -11852.529583883725
Iteration 6600: Loss = -11852.540831599337
1
Iteration 6700: Loss = -11852.52880434441
Iteration 6800: Loss = -11852.530336066993
1
Iteration 6900: Loss = -11852.528824285868
Iteration 7000: Loss = -11852.529042037813
1
Iteration 7100: Loss = -11852.529090423359
2
Iteration 7200: Loss = -11852.540561088115
3
Iteration 7300: Loss = -11852.545710068742
4
Iteration 7400: Loss = -11852.568637352546
5
Iteration 7500: Loss = -11852.52775923092
Iteration 7600: Loss = -11852.52803329248
1
Iteration 7700: Loss = -11852.527572441084
Iteration 7800: Loss = -11852.527574791953
Iteration 7900: Loss = -11852.529071997196
1
Iteration 8000: Loss = -11852.54966691784
2
Iteration 8100: Loss = -11852.527245758845
Iteration 8200: Loss = -11852.52766355075
1
Iteration 8300: Loss = -11852.52967605141
2
Iteration 8400: Loss = -11852.536770605506
3
Iteration 8500: Loss = -11852.526980208851
Iteration 8600: Loss = -11852.574931089819
1
Iteration 8700: Loss = -11852.526933874187
Iteration 8800: Loss = -11852.524988208228
Iteration 8900: Loss = -11852.517638501884
Iteration 9000: Loss = -11852.513682103096
Iteration 9100: Loss = -11852.516316694764
1
Iteration 9200: Loss = -11852.514097208741
2
Iteration 9300: Loss = -11852.513158351132
Iteration 9400: Loss = -11852.516907524843
1
Iteration 9500: Loss = -11852.51613020042
2
Iteration 9600: Loss = -11852.513270415808
3
Iteration 9700: Loss = -11852.51336334254
4
Iteration 9800: Loss = -11852.514572149235
5
Iteration 9900: Loss = -11852.521266189802
6
Iteration 10000: Loss = -11852.521437961397
7
Iteration 10100: Loss = -11852.538911213664
8
Iteration 10200: Loss = -11852.526072446899
9
Iteration 10300: Loss = -11852.556696198775
10
Iteration 10400: Loss = -11852.516817336262
11
Iteration 10500: Loss = -11852.512986924976
Iteration 10600: Loss = -11852.519199587403
1
Iteration 10700: Loss = -11852.463594266195
Iteration 10800: Loss = -11852.453952247304
Iteration 10900: Loss = -11852.451093511676
Iteration 11000: Loss = -11852.4473503246
Iteration 11100: Loss = -11852.448129984889
1
Iteration 11200: Loss = -11852.447073167194
Iteration 11300: Loss = -11852.459366752575
1
Iteration 11400: Loss = -11852.447281163073
2
Iteration 11500: Loss = -11852.44669063825
Iteration 11600: Loss = -11852.448702115016
1
Iteration 11700: Loss = -11852.566266519787
2
Iteration 11800: Loss = -11852.447999981543
3
Iteration 11900: Loss = -11852.449492494945
4
Iteration 12000: Loss = -11852.45381531772
5
Iteration 12100: Loss = -11852.446476834273
Iteration 12200: Loss = -11852.446853488387
1
Iteration 12300: Loss = -11852.459995714149
2
Iteration 12400: Loss = -11852.447434031617
3
Iteration 12500: Loss = -11852.446519476705
Iteration 12600: Loss = -11852.448923640008
1
Iteration 12700: Loss = -11852.53731564735
2
Iteration 12800: Loss = -11852.457850723358
3
Iteration 12900: Loss = -11852.452465111619
4
Iteration 13000: Loss = -11852.413977582071
Iteration 13100: Loss = -11852.411495622746
Iteration 13200: Loss = -11852.462189484306
1
Iteration 13300: Loss = -11852.428314406148
2
Iteration 13400: Loss = -11852.40865343349
Iteration 13500: Loss = -11852.409892929465
1
Iteration 13600: Loss = -11852.413905683004
2
Iteration 13700: Loss = -11852.421644735841
3
Iteration 13800: Loss = -11852.446431789784
4
Iteration 13900: Loss = -11852.415174113023
5
Iteration 14000: Loss = -11852.409158297725
6
Iteration 14100: Loss = -11852.41078948883
7
Iteration 14200: Loss = -11852.41000625508
8
Iteration 14300: Loss = -11852.409534907505
9
Iteration 14400: Loss = -11852.41593230236
10
Iteration 14500: Loss = -11852.411924302232
11
Iteration 14600: Loss = -11852.41105924099
12
Iteration 14700: Loss = -11852.41094515048
13
Iteration 14800: Loss = -11852.423961726787
14
Iteration 14900: Loss = -11852.40879386018
15
Stopping early at iteration 14900 due to no improvement.
pi: tensor([[0.5364, 0.4636],
        [0.4187, 0.5813]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4786, 0.5214], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3971, 0.1033],
         [0.5054, 0.2225]],

        [[0.5082, 0.0977],
         [0.5908, 0.6814]],

        [[0.5197, 0.0968],
         [0.5810, 0.7192]],

        [[0.7129, 0.1070],
         [0.5229, 0.7201]],

        [[0.5123, 0.1041],
         [0.5818, 0.6148]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 78
Adjusted Rand Index: 0.3077948741518568
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.46679055220811594
Average Adjusted Rand Index: 0.8535585359939344
11714.192135278518
[1.0, 0.46679055220811594] [1.0, 0.8535585359939344] [11709.926221416328, 11852.40879386018]
-------------------------------------
This iteration is 21
True Objective function: Loss = -11663.62749743903
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22039.457544289162
Iteration 100: Loss = -12433.650647955792
Iteration 200: Loss = -12394.217130707588
Iteration 300: Loss = -11803.601609386535
Iteration 400: Loss = -11733.562343004369
Iteration 500: Loss = -11724.663774133978
Iteration 600: Loss = -11707.693323719926
Iteration 700: Loss = -11695.90050888964
Iteration 800: Loss = -11678.343993520879
Iteration 900: Loss = -11678.21814786902
Iteration 1000: Loss = -11674.286421362185
Iteration 1100: Loss = -11663.63026996883
Iteration 1200: Loss = -11663.594194567831
Iteration 1300: Loss = -11663.566814721698
Iteration 1400: Loss = -11663.544667593074
Iteration 1500: Loss = -11663.525208031288
Iteration 1600: Loss = -11663.503290529878
Iteration 1700: Loss = -11663.458795569853
Iteration 1800: Loss = -11663.441814766653
Iteration 1900: Loss = -11656.993422360661
Iteration 2000: Loss = -11656.978834321457
Iteration 2100: Loss = -11656.969511154195
Iteration 2200: Loss = -11656.963692279563
Iteration 2300: Loss = -11656.958811453513
Iteration 2400: Loss = -11656.954596794292
Iteration 2500: Loss = -11656.950868370075
Iteration 2600: Loss = -11656.947557119467
Iteration 2700: Loss = -11656.945175639175
Iteration 2800: Loss = -11656.9420079642
Iteration 2900: Loss = -11656.93968769778
Iteration 3000: Loss = -11656.939700349889
Iteration 3100: Loss = -11656.935592776219
Iteration 3200: Loss = -11656.940578523025
1
Iteration 3300: Loss = -11656.932653021151
Iteration 3400: Loss = -11656.930815603308
Iteration 3500: Loss = -11656.9324640152
1
Iteration 3600: Loss = -11656.928307678078
Iteration 3700: Loss = -11656.927268482532
Iteration 3800: Loss = -11656.926204591771
Iteration 3900: Loss = -11656.925199398034
Iteration 4000: Loss = -11656.934466023537
1
Iteration 4100: Loss = -11656.929457919603
2
Iteration 4200: Loss = -11656.92291184518
Iteration 4300: Loss = -11656.922094090192
Iteration 4400: Loss = -11656.921418642
Iteration 4500: Loss = -11656.92121704116
Iteration 4600: Loss = -11656.920256207648
Iteration 4700: Loss = -11656.919709283799
Iteration 4800: Loss = -11656.919239630524
Iteration 4900: Loss = -11656.92007875358
1
Iteration 5000: Loss = -11656.91951047058
2
Iteration 5100: Loss = -11656.917917429197
Iteration 5200: Loss = -11656.917538397463
Iteration 5300: Loss = -11656.918249539869
1
Iteration 5400: Loss = -11656.916852847902
Iteration 5500: Loss = -11656.916538101408
Iteration 5600: Loss = -11656.91622654588
Iteration 5700: Loss = -11656.915904148133
Iteration 5800: Loss = -11656.91942504982
1
Iteration 5900: Loss = -11656.915341909722
Iteration 6000: Loss = -11656.915120144944
Iteration 6100: Loss = -11656.914858076623
Iteration 6200: Loss = -11656.914607627992
Iteration 6300: Loss = -11656.925409758487
1
Iteration 6400: Loss = -11656.914221478959
Iteration 6500: Loss = -11656.914014140071
Iteration 6600: Loss = -11656.916608623978
1
Iteration 6700: Loss = -11656.913753478717
Iteration 6800: Loss = -11656.916934977806
1
Iteration 6900: Loss = -11656.91351013496
Iteration 7000: Loss = -11656.913296826404
Iteration 7100: Loss = -11656.916829603733
1
Iteration 7200: Loss = -11656.913105976248
Iteration 7300: Loss = -11656.912942531902
Iteration 7400: Loss = -11656.912895241696
Iteration 7500: Loss = -11656.91331262337
1
Iteration 7600: Loss = -11656.91263445111
Iteration 7700: Loss = -11657.014476812441
1
Iteration 7800: Loss = -11656.912363076211
Iteration 7900: Loss = -11656.91136860236
Iteration 8000: Loss = -11656.912345950564
1
Iteration 8100: Loss = -11656.911312944063
Iteration 8200: Loss = -11656.911154095991
Iteration 8300: Loss = -11656.912267702806
1
Iteration 8400: Loss = -11656.913869068747
2
Iteration 8500: Loss = -11656.910916735049
Iteration 8600: Loss = -11656.960896410783
1
Iteration 8700: Loss = -11656.910805627911
Iteration 8800: Loss = -11656.910796378685
Iteration 8900: Loss = -11656.91195749484
1
Iteration 9000: Loss = -11656.911103039885
2
Iteration 9100: Loss = -11656.930571893108
3
Iteration 9200: Loss = -11656.924712666603
4
Iteration 9300: Loss = -11656.913542896687
5
Iteration 9400: Loss = -11656.928453008404
6
Iteration 9500: Loss = -11656.911927119752
7
Iteration 9600: Loss = -11656.926128415207
8
Iteration 9700: Loss = -11656.920085006552
9
Iteration 9800: Loss = -11656.915024202599
10
Iteration 9900: Loss = -11656.91038497647
Iteration 10000: Loss = -11656.966068225462
1
Iteration 10100: Loss = -11656.916630058671
2
Iteration 10200: Loss = -11656.91097102295
3
Iteration 10300: Loss = -11656.969648703332
4
Iteration 10400: Loss = -11656.91606895377
5
Iteration 10500: Loss = -11656.934809680506
6
Iteration 10600: Loss = -11656.916631024953
7
Iteration 10700: Loss = -11656.910033222153
Iteration 10800: Loss = -11656.91032864834
1
Iteration 10900: Loss = -11656.953892866226
2
Iteration 11000: Loss = -11656.910150037149
3
Iteration 11100: Loss = -11656.929316370779
4
Iteration 11200: Loss = -11656.913461597698
5
Iteration 11300: Loss = -11656.939554271265
6
Iteration 11400: Loss = -11656.961102493644
7
Iteration 11500: Loss = -11657.070143137264
8
Iteration 11600: Loss = -11656.910022120734
Iteration 11700: Loss = -11656.91040820263
1
Iteration 11800: Loss = -11656.926018284028
2
Iteration 11900: Loss = -11656.923596756471
3
Iteration 12000: Loss = -11656.910458593766
4
Iteration 12100: Loss = -11656.910294290283
5
Iteration 12200: Loss = -11656.910400593433
6
Iteration 12300: Loss = -11656.942785318068
7
Iteration 12400: Loss = -11656.99987310118
8
Iteration 12500: Loss = -11656.911511609389
9
Iteration 12600: Loss = -11656.919854320866
10
Iteration 12700: Loss = -11656.918849463007
11
Iteration 12800: Loss = -11656.911464745353
12
Iteration 12900: Loss = -11656.90982081373
Iteration 13000: Loss = -11656.921400920843
1
Iteration 13100: Loss = -11656.954859121963
2
Iteration 13200: Loss = -11656.927974421034
3
Iteration 13300: Loss = -11656.916498541214
4
Iteration 13400: Loss = -11656.927998339766
5
Iteration 13500: Loss = -11656.91457890263
6
Iteration 13600: Loss = -11656.909642211716
Iteration 13700: Loss = -11656.913132009478
1
Iteration 13800: Loss = -11656.920402011261
2
Iteration 13900: Loss = -11657.022265642177
3
Iteration 14000: Loss = -11656.91092701871
4
Iteration 14100: Loss = -11656.909963794276
5
Iteration 14200: Loss = -11656.930516272962
6
Iteration 14300: Loss = -11656.915338250692
7
Iteration 14400: Loss = -11656.910138054775
8
Iteration 14500: Loss = -11656.94974997922
9
Iteration 14600: Loss = -11656.908967995649
Iteration 14700: Loss = -11656.909580029655
1
Iteration 14800: Loss = -11656.910044485727
2
Iteration 14900: Loss = -11656.909572522605
3
Iteration 15000: Loss = -11656.910879773008
4
Iteration 15100: Loss = -11656.913060317454
5
Iteration 15200: Loss = -11656.927262591977
6
Iteration 15300: Loss = -11656.91000535998
7
Iteration 15400: Loss = -11656.919000390742
8
Iteration 15500: Loss = -11656.911729626563
9
Iteration 15600: Loss = -11656.958979600919
10
Iteration 15700: Loss = -11656.908931977661
Iteration 15800: Loss = -11656.924137283595
1
Iteration 15900: Loss = -11656.925382756343
2
Iteration 16000: Loss = -11656.920892525786
3
Iteration 16100: Loss = -11656.930505654484
4
Iteration 16200: Loss = -11656.914878573458
5
Iteration 16300: Loss = -11656.91432553281
6
Iteration 16400: Loss = -11656.9101840333
7
Iteration 16500: Loss = -11656.9098291706
8
Iteration 16600: Loss = -11656.90936577416
9
Iteration 16700: Loss = -11656.910842806452
10
Iteration 16800: Loss = -11656.91009348069
11
Iteration 16900: Loss = -11656.910150186963
12
Iteration 17000: Loss = -11656.91450470642
13
Iteration 17100: Loss = -11656.955921111561
14
Iteration 17200: Loss = -11656.917197240342
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[0.7191, 0.2809],
        [0.2678, 0.7322]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4866, 0.5134], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2055, 0.0996],
         [0.6897, 0.3926]],

        [[0.5241, 0.1009],
         [0.7280, 0.5711]],

        [[0.6262, 0.0888],
         [0.5030, 0.5853]],

        [[0.6990, 0.1074],
         [0.7195, 0.6598]],

        [[0.5417, 0.1078],
         [0.6191, 0.6386]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21676.049198657834
Iteration 100: Loss = -12430.193673328875
Iteration 200: Loss = -12328.419955405434
Iteration 300: Loss = -11813.072807519913
Iteration 400: Loss = -11672.612753158799
Iteration 500: Loss = -11671.13896859558
Iteration 600: Loss = -11667.410321489378
Iteration 700: Loss = -11667.145454808473
Iteration 800: Loss = -11666.981071695824
Iteration 900: Loss = -11666.857334558501
Iteration 1000: Loss = -11666.782392721561
Iteration 1100: Loss = -11666.72662856647
Iteration 1200: Loss = -11666.68433690775
Iteration 1300: Loss = -11666.65108580702
Iteration 1400: Loss = -11666.62413660605
Iteration 1500: Loss = -11666.601372507264
Iteration 1600: Loss = -11666.580309021861
Iteration 1700: Loss = -11666.544455763511
Iteration 1800: Loss = -11663.39054853255
Iteration 1900: Loss = -11663.375879378089
Iteration 2000: Loss = -11663.365008833869
Iteration 2100: Loss = -11663.355836223795
Iteration 2200: Loss = -11663.347702580433
Iteration 2300: Loss = -11663.340035520578
Iteration 2400: Loss = -11663.331185518098
Iteration 2500: Loss = -11663.297931191622
Iteration 2600: Loss = -11663.292787782777
Iteration 2700: Loss = -11663.288315889262
Iteration 2800: Loss = -11663.28401802843
Iteration 2900: Loss = -11663.279734252646
Iteration 3000: Loss = -11663.276030845509
Iteration 3100: Loss = -11663.273285296706
Iteration 3200: Loss = -11663.270570239361
Iteration 3300: Loss = -11663.268644067604
Iteration 3400: Loss = -11663.266245920604
Iteration 3500: Loss = -11663.264376066873
Iteration 3600: Loss = -11663.267691498038
1
Iteration 3700: Loss = -11663.265671805339
2
Iteration 3800: Loss = -11663.259601727857
Iteration 3900: Loss = -11663.258275625016
Iteration 4000: Loss = -11663.26280067062
1
Iteration 4100: Loss = -11663.272261856448
2
Iteration 4200: Loss = -11663.25485852913
Iteration 4300: Loss = -11663.253781176
Iteration 4400: Loss = -11663.252791928997
Iteration 4500: Loss = -11663.254476272201
1
Iteration 4600: Loss = -11663.251137369525
Iteration 4700: Loss = -11663.250771005412
Iteration 4800: Loss = -11663.24973540304
Iteration 4900: Loss = -11663.252524713025
1
Iteration 5000: Loss = -11663.248500040958
Iteration 5100: Loss = -11663.254373987837
1
Iteration 5200: Loss = -11663.248215811296
Iteration 5300: Loss = -11663.246926339025
Iteration 5400: Loss = -11663.251307482926
1
Iteration 5500: Loss = -11663.246021323002
Iteration 5600: Loss = -11663.249679053826
1
Iteration 5700: Loss = -11663.245299148677
Iteration 5800: Loss = -11663.250048252357
1
Iteration 5900: Loss = -11663.244613889636
Iteration 6000: Loss = -11663.244348954582
Iteration 6100: Loss = -11663.244037372937
Iteration 6200: Loss = -11663.244010762366
Iteration 6300: Loss = -11663.243470961526
Iteration 6400: Loss = -11663.243290328219
Iteration 6500: Loss = -11663.242948152414
Iteration 6600: Loss = -11663.244781599034
1
Iteration 6700: Loss = -11663.242978967983
Iteration 6800: Loss = -11663.274460543238
1
Iteration 6900: Loss = -11663.242398451217
Iteration 7000: Loss = -11663.242213662897
Iteration 7100: Loss = -11663.244161082053
1
Iteration 7200: Loss = -11663.241699554897
Iteration 7300: Loss = -11663.241942714341
1
Iteration 7400: Loss = -11663.24141450819
Iteration 7500: Loss = -11663.242377991537
1
Iteration 7600: Loss = -11663.249274588088
2
Iteration 7700: Loss = -11663.246688468082
3
Iteration 7800: Loss = -11663.244695170108
4
Iteration 7900: Loss = -11663.241491748604
Iteration 8000: Loss = -11663.269171477903
1
Iteration 8100: Loss = -11663.240622084422
Iteration 8200: Loss = -11663.243073704129
1
Iteration 8300: Loss = -11663.24152704679
2
Iteration 8400: Loss = -11663.240399776767
Iteration 8500: Loss = -11663.24258699671
1
Iteration 8600: Loss = -11663.249860551025
2
Iteration 8700: Loss = -11663.240169817795
Iteration 8800: Loss = -11663.24026511166
Iteration 8900: Loss = -11663.268808666844
1
Iteration 9000: Loss = -11663.239880731144
Iteration 9100: Loss = -11663.25121667917
1
Iteration 9200: Loss = -11663.239620134473
Iteration 9300: Loss = -11663.239667484238
Iteration 9400: Loss = -11663.299720916957
1
Iteration 9500: Loss = -11663.24711776349
2
Iteration 9600: Loss = -11663.263041677717
3
Iteration 9700: Loss = -11663.257125588152
4
Iteration 9800: Loss = -11663.249487179175
5
Iteration 9900: Loss = -11663.247366851858
6
Iteration 10000: Loss = -11663.240875496018
7
Iteration 10100: Loss = -11663.251716735482
8
Iteration 10200: Loss = -11663.238489453879
Iteration 10300: Loss = -11663.239528075603
1
Iteration 10400: Loss = -11663.24217789879
2
Iteration 10500: Loss = -11663.238424246492
Iteration 10600: Loss = -11663.239487838366
1
Iteration 10700: Loss = -11663.239070351017
2
Iteration 10800: Loss = -11663.238510112893
Iteration 10900: Loss = -11663.244660802648
1
Iteration 11000: Loss = -11663.238572240878
Iteration 11100: Loss = -11663.240221527116
1
Iteration 11200: Loss = -11663.245621771764
2
Iteration 11300: Loss = -11663.383589683614
3
Iteration 11400: Loss = -11663.239524039856
4
Iteration 11500: Loss = -11663.239661719745
5
Iteration 11600: Loss = -11663.238630175729
Iteration 11700: Loss = -11663.239839735763
1
Iteration 11800: Loss = -11663.237890516488
Iteration 11900: Loss = -11663.238993002675
1
Iteration 12000: Loss = -11663.245632857186
2
Iteration 12100: Loss = -11663.238121665418
3
Iteration 12200: Loss = -11663.237952424633
Iteration 12300: Loss = -11663.238244058843
1
Iteration 12400: Loss = -11663.278616408032
2
Iteration 12500: Loss = -11663.23900691545
3
Iteration 12600: Loss = -11663.242491367771
4
Iteration 12700: Loss = -11663.239482259442
5
Iteration 12800: Loss = -11663.259357522194
6
Iteration 12900: Loss = -11663.238145127947
7
Iteration 13000: Loss = -11663.23993001135
8
Iteration 13100: Loss = -11663.242177398046
9
Iteration 13200: Loss = -11663.247949187129
10
Iteration 13300: Loss = -11663.241318378135
11
Iteration 13400: Loss = -11663.300490891696
12
Iteration 13500: Loss = -11663.237862188553
Iteration 13600: Loss = -11663.24955354536
1
Iteration 13700: Loss = -11663.2471660679
2
Iteration 13800: Loss = -11663.24384619091
3
Iteration 13900: Loss = -11663.349599452085
4
Iteration 14000: Loss = -11663.25025834453
5
Iteration 14100: Loss = -11663.245395153132
6
Iteration 14200: Loss = -11663.386785620884
7
Iteration 14300: Loss = -11663.25175776832
8
Iteration 14400: Loss = -11663.24001795696
9
Iteration 14500: Loss = -11663.275293291641
10
Iteration 14600: Loss = -11663.240983861526
11
Iteration 14700: Loss = -11656.9213944561
Iteration 14800: Loss = -11656.933808675556
1
Iteration 14900: Loss = -11656.916993462213
Iteration 15000: Loss = -11656.917379876948
1
Iteration 15100: Loss = -11656.922410067476
2
Iteration 15200: Loss = -11656.921845971507
3
Iteration 15300: Loss = -11656.928374545321
4
Iteration 15400: Loss = -11656.921682954226
5
Iteration 15500: Loss = -11656.91834883264
6
Iteration 15600: Loss = -11656.917140118829
7
Iteration 15700: Loss = -11656.91818093252
8
Iteration 15800: Loss = -11656.957154916247
9
Iteration 15900: Loss = -11656.916977160214
Iteration 16000: Loss = -11656.917507893533
1
Iteration 16100: Loss = -11656.92685557593
2
Iteration 16200: Loss = -11656.94513464468
3
Iteration 16300: Loss = -11656.974238817756
4
Iteration 16400: Loss = -11657.007084676898
5
Iteration 16500: Loss = -11657.012046338474
6
Iteration 16600: Loss = -11656.928431396156
7
Iteration 16700: Loss = -11656.916966834528
Iteration 16800: Loss = -11657.000728685827
1
Iteration 16900: Loss = -11656.920320902631
2
Iteration 17000: Loss = -11657.006139935602
3
Iteration 17100: Loss = -11656.933706922866
4
Iteration 17200: Loss = -11657.01289683041
5
Iteration 17300: Loss = -11656.940252862754
6
Iteration 17400: Loss = -11656.92788785787
7
Iteration 17500: Loss = -11656.916536297766
Iteration 17600: Loss = -11657.017678454667
1
Iteration 17700: Loss = -11656.928440088854
2
Iteration 17800: Loss = -11656.926866855587
3
Iteration 17900: Loss = -11656.964541008283
4
Iteration 18000: Loss = -11656.944695689292
5
Iteration 18100: Loss = -11656.92904204514
6
Iteration 18200: Loss = -11656.929547883317
7
Iteration 18300: Loss = -11656.928624066713
8
Iteration 18400: Loss = -11656.917843416752
9
Iteration 18500: Loss = -11656.918352168987
10
Iteration 18600: Loss = -11656.935394602175
11
Iteration 18700: Loss = -11656.918213880259
12
Iteration 18800: Loss = -11656.925945568193
13
Iteration 18900: Loss = -11656.966799989781
14
Iteration 19000: Loss = -11656.920793260115
15
Stopping early at iteration 19000 due to no improvement.
pi: tensor([[0.7326, 0.2674],
        [0.2783, 0.7217]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5100, 0.4900], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3934, 0.0996],
         [0.5307, 0.2053]],

        [[0.5323, 0.1010],
         [0.6626, 0.6343]],

        [[0.6003, 0.0888],
         [0.5246, 0.5802]],

        [[0.7180, 0.1073],
         [0.5807, 0.6786]],

        [[0.7200, 0.1087],
         [0.5888, 0.5113]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.992
11663.62749743903
[0.99199998169963, 0.99199998169963] [0.992, 0.992] [11656.917197240342, 11656.920793260115]
-------------------------------------
This iteration is 22
True Objective function: Loss = -11619.66253054604
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21363.73628671898
Iteration 100: Loss = -12398.763617127861
Iteration 200: Loss = -11941.514525970866
Iteration 300: Loss = -11814.126249986246
Iteration 400: Loss = -11761.121074570789
Iteration 500: Loss = -11738.31227821328
Iteration 600: Loss = -11738.148580733496
Iteration 700: Loss = -11738.059324535217
Iteration 800: Loss = -11738.002906608252
Iteration 900: Loss = -11737.96415919821
Iteration 1000: Loss = -11737.936056308316
Iteration 1100: Loss = -11737.914827701046
Iteration 1200: Loss = -11737.898431035106
Iteration 1300: Loss = -11737.8852912195
Iteration 1400: Loss = -11737.874716365712
Iteration 1500: Loss = -11737.866032476997
Iteration 1600: Loss = -11737.858818315603
Iteration 1700: Loss = -11737.852751506578
Iteration 1800: Loss = -11737.847578768717
Iteration 1900: Loss = -11737.843048889034
Iteration 2000: Loss = -11737.839132971132
Iteration 2100: Loss = -11737.835670914172
Iteration 2200: Loss = -11737.83195556714
Iteration 2300: Loss = -11737.811050202205
Iteration 2400: Loss = -11737.80831074625
Iteration 2500: Loss = -11737.806260403459
Iteration 2600: Loss = -11737.80436430018
Iteration 2700: Loss = -11737.80270848326
Iteration 2800: Loss = -11737.801213992008
Iteration 2900: Loss = -11737.800108664576
Iteration 3000: Loss = -11737.800713886687
1
Iteration 3100: Loss = -11737.797544702535
Iteration 3200: Loss = -11737.796690628338
Iteration 3300: Loss = -11737.795676189768
Iteration 3400: Loss = -11737.794734217496
Iteration 3500: Loss = -11737.793928231986
Iteration 3600: Loss = -11737.79330825531
Iteration 3700: Loss = -11737.7930595989
Iteration 3800: Loss = -11737.808888733021
1
Iteration 3900: Loss = -11737.794013719551
2
Iteration 4000: Loss = -11737.783441353891
Iteration 4100: Loss = -11737.712851544446
Iteration 4200: Loss = -11737.71213917797
Iteration 4300: Loss = -11737.716911892056
1
Iteration 4400: Loss = -11737.711230441784
Iteration 4500: Loss = -11737.71088931405
Iteration 4600: Loss = -11737.711125999636
1
Iteration 4700: Loss = -11737.711368338314
2
Iteration 4800: Loss = -11737.710083013855
Iteration 4900: Loss = -11737.711902444331
1
Iteration 5000: Loss = -11737.713768934436
2
Iteration 5100: Loss = -11737.71817247698
3
Iteration 5200: Loss = -11737.709126551812
Iteration 5300: Loss = -11737.708894537402
Iteration 5400: Loss = -11737.710465508142
1
Iteration 5500: Loss = -11737.708327913939
Iteration 5600: Loss = -11737.708214014347
Iteration 5700: Loss = -11737.708175292799
Iteration 5800: Loss = -11737.707853161297
Iteration 5900: Loss = -11737.707682661729
Iteration 6000: Loss = -11737.70764946759
Iteration 6100: Loss = -11737.707768885193
1
Iteration 6200: Loss = -11737.707299125805
Iteration 6300: Loss = -11737.707149433985
Iteration 6400: Loss = -11737.707291204633
1
Iteration 6500: Loss = -11737.707001551633
Iteration 6600: Loss = -11737.7070856218
Iteration 6700: Loss = -11737.706676364309
Iteration 6800: Loss = -11737.709025209282
1
Iteration 6900: Loss = -11737.706980089379
2
Iteration 7000: Loss = -11737.706143147396
Iteration 7100: Loss = -11737.710360212197
1
Iteration 7200: Loss = -11737.705978586951
Iteration 7300: Loss = -11737.709170127166
1
Iteration 7400: Loss = -11737.71094230364
2
Iteration 7500: Loss = -11737.707991265563
3
Iteration 7600: Loss = -11737.707814963696
4
Iteration 7700: Loss = -11737.714572101788
5
Iteration 7800: Loss = -11737.707908016819
6
Iteration 7900: Loss = -11737.705347154113
Iteration 8000: Loss = -11737.80632765571
1
Iteration 8100: Loss = -11737.704961009702
Iteration 8200: Loss = -11737.715881461821
1
Iteration 8300: Loss = -11737.70466396334
Iteration 8400: Loss = -11737.703650209853
Iteration 8500: Loss = -11737.723037197617
1
Iteration 8600: Loss = -11737.695679828174
Iteration 8700: Loss = -11737.708930565017
1
Iteration 8800: Loss = -11737.689560415192
Iteration 8900: Loss = -11737.694000974272
1
Iteration 9000: Loss = -11737.694293124467
2
Iteration 9100: Loss = -11737.691659277918
3
Iteration 9200: Loss = -11737.683592111094
Iteration 9300: Loss = -11737.68866808337
1
Iteration 9400: Loss = -11737.68194364575
Iteration 9500: Loss = -11737.657269763658
Iteration 9600: Loss = -11737.67131363329
1
Iteration 9700: Loss = -11737.672144099712
2
Iteration 9800: Loss = -11737.671517613931
3
Iteration 9900: Loss = -11737.6611303462
4
Iteration 10000: Loss = -11737.66247232647
5
Iteration 10100: Loss = -11737.583263507733
Iteration 10200: Loss = -11737.564788729906
Iteration 10300: Loss = -11737.582164908497
1
Iteration 10400: Loss = -11737.576115289388
2
Iteration 10500: Loss = -11737.566436999203
3
Iteration 10600: Loss = -11737.588554042195
4
Iteration 10700: Loss = -11737.588660520049
5
Iteration 10800: Loss = -11737.572769854849
6
Iteration 10900: Loss = -11737.580045623632
7
Iteration 11000: Loss = -11737.566470041695
8
Iteration 11100: Loss = -11728.912944122838
Iteration 11200: Loss = -11728.83082352267
Iteration 11300: Loss = -11728.827022012692
Iteration 11400: Loss = -11728.826589323755
Iteration 11500: Loss = -11728.827062857372
1
Iteration 11600: Loss = -11728.749183387938
Iteration 11700: Loss = -11728.747739093089
Iteration 11800: Loss = -11728.749248171316
1
Iteration 11900: Loss = -11728.738357653428
Iteration 12000: Loss = -11728.74322101569
1
Iteration 12100: Loss = -11728.738071675485
Iteration 12200: Loss = -11728.738095650848
Iteration 12300: Loss = -11728.737937528955
Iteration 12400: Loss = -11728.73218049037
Iteration 12500: Loss = -11728.705621330635
Iteration 12600: Loss = -11728.707957300305
1
Iteration 12700: Loss = -11728.718327847077
2
Iteration 12800: Loss = -11728.70515432214
Iteration 12900: Loss = -11728.705360532082
1
Iteration 13000: Loss = -11728.7095882631
2
Iteration 13100: Loss = -11728.712349179572
3
Iteration 13200: Loss = -11728.828141210055
4
Iteration 13300: Loss = -11728.705177474245
Iteration 13400: Loss = -11728.70447114844
Iteration 13500: Loss = -11728.706498424337
1
Iteration 13600: Loss = -11728.706178217199
2
Iteration 13700: Loss = -11728.706831562713
3
Iteration 13800: Loss = -11728.718964155087
4
Iteration 13900: Loss = -11728.706044307843
5
Iteration 14000: Loss = -11728.704489057222
Iteration 14100: Loss = -11728.704805505797
1
Iteration 14200: Loss = -11728.704403424708
Iteration 14300: Loss = -11728.70533048638
1
Iteration 14400: Loss = -11728.707167427188
2
Iteration 14500: Loss = -11728.707501527177
3
Iteration 14600: Loss = -11728.707827537846
4
Iteration 14700: Loss = -11728.755148664211
5
Iteration 14800: Loss = -11728.704229745952
Iteration 14900: Loss = -11728.705040378407
1
Iteration 15000: Loss = -11728.702937663955
Iteration 15100: Loss = -11728.70207231279
Iteration 15200: Loss = -11728.70205007758
Iteration 15300: Loss = -11728.702466981633
1
Iteration 15400: Loss = -11728.697185982031
Iteration 15500: Loss = -11728.66692032139
Iteration 15600: Loss = -11728.727255021138
1
Iteration 15700: Loss = -11728.665701858448
Iteration 15800: Loss = -11728.663811426062
Iteration 15900: Loss = -11728.664547167527
1
Iteration 16000: Loss = -11728.69198156275
2
Iteration 16100: Loss = -11728.870456030672
3
Iteration 16200: Loss = -11728.676982799807
4
Iteration 16300: Loss = -11728.663906027014
Iteration 16400: Loss = -11728.663720521767
Iteration 16500: Loss = -11728.665543867462
1
Iteration 16600: Loss = -11728.66670178347
2
Iteration 16700: Loss = -11728.663347663418
Iteration 16800: Loss = -11728.666958119826
1
Iteration 16900: Loss = -11728.669301117194
2
Iteration 17000: Loss = -11728.711780341493
3
Iteration 17100: Loss = -11728.69604822909
4
Iteration 17200: Loss = -11728.688130516473
5
Iteration 17300: Loss = -11728.689665531481
6
Iteration 17400: Loss = -11728.68652197788
7
Iteration 17500: Loss = -11728.66922854255
8
Iteration 17600: Loss = -11728.666794106639
9
Iteration 17700: Loss = -11728.671824260595
10
Iteration 17800: Loss = -11728.664274137955
11
Iteration 17900: Loss = -11728.663238911915
Iteration 18000: Loss = -11728.663127036909
Iteration 18100: Loss = -11728.664509092898
1
Iteration 18200: Loss = -11728.737228501144
2
Iteration 18300: Loss = -11728.66623353506
3
Iteration 18400: Loss = -11728.665936931451
4
Iteration 18500: Loss = -11728.664377017369
5
Iteration 18600: Loss = -11728.663553489307
6
Iteration 18700: Loss = -11728.703386846382
7
Iteration 18800: Loss = -11728.694647882776
8
Iteration 18900: Loss = -11728.700205488842
9
Iteration 19000: Loss = -11728.669617409183
10
Iteration 19100: Loss = -11728.67546778318
11
Iteration 19200: Loss = -11728.677497430821
12
Iteration 19300: Loss = -11728.679131934925
13
Iteration 19400: Loss = -11728.71471271992
14
Iteration 19500: Loss = -11728.669027494236
15
Stopping early at iteration 19500 due to no improvement.
pi: tensor([[0.6109, 0.3891],
        [0.2865, 0.7135]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8942, 0.1058], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2003, 0.1003],
         [0.6975, 0.4026]],

        [[0.7063, 0.0943],
         [0.6201, 0.6596]],

        [[0.5496, 0.0956],
         [0.6766, 0.5859]],

        [[0.6893, 0.1138],
         [0.6227, 0.6083]],

        [[0.5956, 0.1003],
         [0.6724, 0.5349]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.002427152448917675
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6076227342221008
Average Adjusted Rand Index: 0.8004854304897835
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21717.48091984873
Iteration 100: Loss = -12429.60221177218
Iteration 200: Loss = -12266.856417295965
Iteration 300: Loss = -11920.005832165807
Iteration 400: Loss = -11814.244256107298
Iteration 500: Loss = -11739.543748634796
Iteration 600: Loss = -11738.744703040438
Iteration 700: Loss = -11738.416315006813
Iteration 800: Loss = -11738.23234775233
Iteration 900: Loss = -11738.114261971372
Iteration 1000: Loss = -11738.03122916863
Iteration 1100: Loss = -11737.96760199613
Iteration 1200: Loss = -11737.913917154314
Iteration 1300: Loss = -11737.872295833846
Iteration 1400: Loss = -11737.837207845083
Iteration 1500: Loss = -11737.805225159544
Iteration 1600: Loss = -11737.779050240919
Iteration 1700: Loss = -11737.75515293123
Iteration 1800: Loss = -11737.729912710267
Iteration 1900: Loss = -11737.69893006049
Iteration 2000: Loss = -11737.652374245941
Iteration 2100: Loss = -11737.568777015558
Iteration 2200: Loss = -11737.47123258095
Iteration 2300: Loss = -11732.529279370941
Iteration 2400: Loss = -11730.892000329193
Iteration 2500: Loss = -11728.850002013067
Iteration 2600: Loss = -11728.78739780589
Iteration 2700: Loss = -11728.769438754602
Iteration 2800: Loss = -11728.75337797007
Iteration 2900: Loss = -11728.747817799705
Iteration 3000: Loss = -11728.74164018603
Iteration 3100: Loss = -11728.732528054112
Iteration 3200: Loss = -11728.697653614992
Iteration 3300: Loss = -11728.68932129533
Iteration 3400: Loss = -11728.688330162402
Iteration 3500: Loss = -11728.684441938882
Iteration 3600: Loss = -11728.682479165884
Iteration 3700: Loss = -11728.680777479618
Iteration 3800: Loss = -11728.679213517858
Iteration 3900: Loss = -11728.677797161085
Iteration 4000: Loss = -11728.68033407936
1
Iteration 4100: Loss = -11728.675478876308
Iteration 4200: Loss = -11728.674201307853
Iteration 4300: Loss = -11728.673605920336
Iteration 4400: Loss = -11728.681186817068
1
Iteration 4500: Loss = -11728.672013694946
Iteration 4600: Loss = -11728.670827484551
Iteration 4700: Loss = -11728.680328240704
1
Iteration 4800: Loss = -11728.669848236783
Iteration 4900: Loss = -11728.679417343368
1
Iteration 5000: Loss = -11728.668318352136
Iteration 5100: Loss = -11728.669731798773
1
Iteration 5200: Loss = -11728.669307084283
2
Iteration 5300: Loss = -11728.666847515804
Iteration 5400: Loss = -11728.669607616059
1
Iteration 5500: Loss = -11728.666809886032
Iteration 5600: Loss = -11728.666195035248
Iteration 5700: Loss = -11728.665827454884
Iteration 5800: Loss = -11728.66503411347
Iteration 5900: Loss = -11728.664738837131
Iteration 6000: Loss = -11728.66443748158
Iteration 6100: Loss = -11728.664254112347
Iteration 6200: Loss = -11728.664159457563
Iteration 6300: Loss = -11728.665036572436
1
Iteration 6400: Loss = -11728.66467103058
2
Iteration 6500: Loss = -11728.665239468397
3
Iteration 6600: Loss = -11728.665352141663
4
Iteration 6700: Loss = -11728.663414762384
Iteration 6800: Loss = -11728.664004303568
1
Iteration 6900: Loss = -11728.663004200524
Iteration 7000: Loss = -11728.662611806292
Iteration 7100: Loss = -11728.662445226619
Iteration 7200: Loss = -11728.662198589842
Iteration 7300: Loss = -11728.662031517597
Iteration 7400: Loss = -11728.666931357744
1
Iteration 7500: Loss = -11728.66379651966
2
Iteration 7600: Loss = -11728.66448401317
3
Iteration 7700: Loss = -11728.661688181412
Iteration 7800: Loss = -11728.661641034287
Iteration 7900: Loss = -11728.662624348648
1
Iteration 8000: Loss = -11728.694243567426
2
Iteration 8100: Loss = -11728.661935839136
3
Iteration 8200: Loss = -11728.665258133182
4
Iteration 8300: Loss = -11728.663241375376
5
Iteration 8400: Loss = -11728.670941879616
6
Iteration 8500: Loss = -11728.6621126726
7
Iteration 8600: Loss = -11728.660882341286
Iteration 8700: Loss = -11728.661370293186
1
Iteration 8800: Loss = -11728.661068911588
2
Iteration 8900: Loss = -11728.661242957607
3
Iteration 9000: Loss = -11728.662247898124
4
Iteration 9100: Loss = -11728.664647101075
5
Iteration 9200: Loss = -11728.662741966657
6
Iteration 9300: Loss = -11728.665259059806
7
Iteration 9400: Loss = -11728.661272732048
8
Iteration 9500: Loss = -11728.660520855581
Iteration 9600: Loss = -11728.664075560098
1
Iteration 9700: Loss = -11728.67564913275
2
Iteration 9800: Loss = -11728.660487749066
Iteration 9900: Loss = -11728.783827550253
1
Iteration 10000: Loss = -11728.660331737687
Iteration 10100: Loss = -11728.669403300717
1
Iteration 10200: Loss = -11728.66026328316
Iteration 10300: Loss = -11728.6646055231
1
Iteration 10400: Loss = -11728.66176944654
2
Iteration 10500: Loss = -11728.662081961416
3
Iteration 10600: Loss = -11728.661371843735
4
Iteration 10700: Loss = -11728.666732105592
5
Iteration 10800: Loss = -11728.669118323578
6
Iteration 10900: Loss = -11728.663458898982
7
Iteration 11000: Loss = -11728.661382276696
8
Iteration 11100: Loss = -11728.660278013693
Iteration 11200: Loss = -11728.661042531296
1
Iteration 11300: Loss = -11728.663395309546
2
Iteration 11400: Loss = -11728.705042082667
3
Iteration 11500: Loss = -11728.664460139662
4
Iteration 11600: Loss = -11728.682595327487
5
Iteration 11700: Loss = -11728.660160022688
Iteration 11800: Loss = -11728.661201149444
1
Iteration 11900: Loss = -11728.660263697728
2
Iteration 12000: Loss = -11728.678405157194
3
Iteration 12100: Loss = -11728.662554395807
4
Iteration 12200: Loss = -11728.662205062923
5
Iteration 12300: Loss = -11728.660548653297
6
Iteration 12400: Loss = -11728.660552946465
7
Iteration 12500: Loss = -11728.660920242271
8
Iteration 12600: Loss = -11728.682138668144
9
Iteration 12700: Loss = -11728.659694669073
Iteration 12800: Loss = -11728.661215981178
1
Iteration 12900: Loss = -11728.665983215917
2
Iteration 13000: Loss = -11728.673312315208
3
Iteration 13100: Loss = -11728.664200318177
4
Iteration 13200: Loss = -11728.659775779031
Iteration 13300: Loss = -11728.660906648747
1
Iteration 13400: Loss = -11728.670666839374
2
Iteration 13500: Loss = -11728.659659639225
Iteration 13600: Loss = -11728.67377558998
1
Iteration 13700: Loss = -11728.671389205432
2
Iteration 13800: Loss = -11728.659649851788
Iteration 13900: Loss = -11728.666379792277
1
Iteration 14000: Loss = -11728.660973720158
2
Iteration 14100: Loss = -11728.661503441912
3
Iteration 14200: Loss = -11728.66130525278
4
Iteration 14300: Loss = -11728.694696690869
5
Iteration 14400: Loss = -11728.754005533328
6
Iteration 14500: Loss = -11728.689854044946
7
Iteration 14600: Loss = -11728.659882921886
8
Iteration 14700: Loss = -11728.659515473131
Iteration 14800: Loss = -11728.661268427946
1
Iteration 14900: Loss = -11728.69928141404
2
Iteration 15000: Loss = -11728.659924856987
3
Iteration 15100: Loss = -11728.659798339575
4
Iteration 15200: Loss = -11728.661843452883
5
Iteration 15300: Loss = -11728.659510363468
Iteration 15400: Loss = -11728.835714581117
1
Iteration 15500: Loss = -11728.659567923809
Iteration 15600: Loss = -11728.66456373511
1
Iteration 15700: Loss = -11728.670370902722
2
Iteration 15800: Loss = -11728.671784628119
3
Iteration 15900: Loss = -11728.664622723567
4
Iteration 16000: Loss = -11728.677225985295
5
Iteration 16100: Loss = -11728.670508067114
6
Iteration 16200: Loss = -11728.66709995004
7
Iteration 16300: Loss = -11728.737841971242
8
Iteration 16400: Loss = -11728.674685655433
9
Iteration 16500: Loss = -11728.661096377411
10
Iteration 16600: Loss = -11728.659734482246
11
Iteration 16700: Loss = -11728.66843256965
12
Iteration 16800: Loss = -11728.659671847192
13
Iteration 16900: Loss = -11728.6596634412
Iteration 17000: Loss = -11728.772217724643
1
Iteration 17100: Loss = -11728.660946508857
2
Iteration 17200: Loss = -11728.660364818958
3
Iteration 17300: Loss = -11728.666505746527
4
Iteration 17400: Loss = -11728.711379582874
5
Iteration 17500: Loss = -11728.660672504817
6
Iteration 17600: Loss = -11728.660523538776
7
Iteration 17700: Loss = -11728.671386113332
8
Iteration 17800: Loss = -11728.746943126633
9
Iteration 17900: Loss = -11728.731538140039
10
Iteration 18000: Loss = -11728.66159655223
11
Iteration 18100: Loss = -11728.65958995003
Iteration 18200: Loss = -11728.668476971778
1
Iteration 18300: Loss = -11728.66334252399
2
Iteration 18400: Loss = -11728.864577612705
3
Iteration 18500: Loss = -11728.659562477067
Iteration 18600: Loss = -11728.674011503685
1
Iteration 18700: Loss = -11728.744027107585
2
Iteration 18800: Loss = -11728.673479349809
3
Iteration 18900: Loss = -11728.660770914455
4
Iteration 19000: Loss = -11728.659723191857
5
Iteration 19100: Loss = -11728.718995287076
6
Iteration 19200: Loss = -11728.659434156518
Iteration 19300: Loss = -11728.660086078533
1
Iteration 19400: Loss = -11728.659447754666
Iteration 19500: Loss = -11728.66064526352
1
Iteration 19600: Loss = -11728.662056940468
2
Iteration 19700: Loss = -11728.66095196007
3
Iteration 19800: Loss = -11728.659859141144
4
Iteration 19900: Loss = -11728.662381829101
5
pi: tensor([[0.7094, 0.2906],
        [0.3909, 0.6091]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1076, 0.8924], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4015, 0.1011],
         [0.5714, 0.2019]],

        [[0.7004, 0.0943],
         [0.5142, 0.6670]],

        [[0.6733, 0.0950],
         [0.6611, 0.7124]],

        [[0.7211, 0.1136],
         [0.6669, 0.6662]],

        [[0.7238, 0.1000],
         [0.6130, 0.6891]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.002427152448917675
time is 1
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6076227342221008
Average Adjusted Rand Index: 0.8004854304897835
11619.66253054604
[0.6076227342221008, 0.6076227342221008] [0.8004854304897835, 0.8004854304897835] [11728.669027494236, 11728.735231230818]
-------------------------------------
This iteration is 23
True Objective function: Loss = -11618.1123034395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20373.667827221212
Iteration 100: Loss = -12192.205987302266
Iteration 200: Loss = -11667.29290643158
Iteration 300: Loss = -11643.539468433139
Iteration 400: Loss = -11624.096815246781
Iteration 500: Loss = -11613.565485614727
Iteration 600: Loss = -11613.282351824995
Iteration 700: Loss = -11611.94618405461
Iteration 800: Loss = -11611.86800725286
Iteration 900: Loss = -11611.812549977125
Iteration 1000: Loss = -11611.77155041954
Iteration 1100: Loss = -11611.740097876283
Iteration 1200: Loss = -11611.715402473536
Iteration 1300: Loss = -11611.695446466603
Iteration 1400: Loss = -11611.67890328031
Iteration 1500: Loss = -11611.66456625317
Iteration 1600: Loss = -11611.649968784528
Iteration 1700: Loss = -11611.617290277094
Iteration 1800: Loss = -11611.603790791632
Iteration 1900: Loss = -11611.49597974943
Iteration 2000: Loss = -11611.348627726944
Iteration 2100: Loss = -11611.34312892477
Iteration 2200: Loss = -11611.338397229321
Iteration 2300: Loss = -11611.33425092636
Iteration 2400: Loss = -11611.330584211546
Iteration 2500: Loss = -11611.327319614098
Iteration 2600: Loss = -11611.324401010095
Iteration 2700: Loss = -11611.321855923663
Iteration 2800: Loss = -11611.319455960138
Iteration 2900: Loss = -11611.317316958879
Iteration 3000: Loss = -11611.315377326182
Iteration 3100: Loss = -11611.313589851787
Iteration 3200: Loss = -11611.311974171404
Iteration 3300: Loss = -11611.310512709728
Iteration 3400: Loss = -11611.334111053604
1
Iteration 3500: Loss = -11611.307978798734
Iteration 3600: Loss = -11611.306800548675
Iteration 3700: Loss = -11611.305783574413
Iteration 3800: Loss = -11611.304914759261
Iteration 3900: Loss = -11611.303933589837
Iteration 4000: Loss = -11611.303097925333
Iteration 4100: Loss = -11611.310746907964
1
Iteration 4200: Loss = -11611.301554591453
Iteration 4300: Loss = -11611.300931615628
Iteration 4400: Loss = -11611.301514056267
1
Iteration 4500: Loss = -11611.299756327493
Iteration 4600: Loss = -11611.29926377053
Iteration 4700: Loss = -11611.299108718715
Iteration 4800: Loss = -11611.29823566046
Iteration 4900: Loss = -11611.297790185536
Iteration 5000: Loss = -11611.29742827998
Iteration 5100: Loss = -11611.297133260972
Iteration 5200: Loss = -11611.296649526435
Iteration 5300: Loss = -11611.307589597256
1
Iteration 5400: Loss = -11611.295960428657
Iteration 5500: Loss = -11611.29622907888
1
Iteration 5600: Loss = -11611.295263423042
Iteration 5700: Loss = -11611.294832475038
Iteration 5800: Loss = -11611.294737602302
Iteration 5900: Loss = -11611.293964700617
Iteration 6000: Loss = -11611.294728988547
1
Iteration 6100: Loss = -11611.29352792021
Iteration 6200: Loss = -11611.306993863016
1
Iteration 6300: Loss = -11611.295731766742
2
Iteration 6400: Loss = -11611.296849733862
3
Iteration 6500: Loss = -11611.29277904894
Iteration 6600: Loss = -11611.292710190313
Iteration 6700: Loss = -11611.29343652868
1
Iteration 6800: Loss = -11611.292290064086
Iteration 6900: Loss = -11611.292231317835
Iteration 7000: Loss = -11611.293430624706
1
Iteration 7100: Loss = -11611.292008136455
Iteration 7200: Loss = -11611.292322065665
1
Iteration 7300: Loss = -11611.325633115845
2
Iteration 7400: Loss = -11611.294587520355
3
Iteration 7500: Loss = -11611.287108520866
Iteration 7600: Loss = -11611.285354294756
Iteration 7700: Loss = -11611.286500202286
1
Iteration 7800: Loss = -11611.288171982933
2
Iteration 7900: Loss = -11611.285109158953
Iteration 8000: Loss = -11611.300761924376
1
Iteration 8100: Loss = -11611.2851091089
Iteration 8200: Loss = -11611.28503346154
Iteration 8300: Loss = -11611.288619714242
1
Iteration 8400: Loss = -11611.284752070469
Iteration 8500: Loss = -11611.288702341024
1
Iteration 8600: Loss = -11611.28490595675
2
Iteration 8700: Loss = -11611.29566035513
3
Iteration 8800: Loss = -11611.2874905801
4
Iteration 8900: Loss = -11611.287175071287
5
Iteration 9000: Loss = -11611.284615308123
Iteration 9100: Loss = -11611.286411244762
1
Iteration 9200: Loss = -11611.304523503613
2
Iteration 9300: Loss = -11611.286719527448
3
Iteration 9400: Loss = -11611.291058714016
4
Iteration 9500: Loss = -11611.28560292203
5
Iteration 9600: Loss = -11611.283956980822
Iteration 9700: Loss = -11611.284030694907
Iteration 9800: Loss = -11611.288252112627
1
Iteration 9900: Loss = -11611.298099261085
2
Iteration 10000: Loss = -11611.284277526292
3
Iteration 10100: Loss = -11611.283386582787
Iteration 10200: Loss = -11611.313806520586
1
Iteration 10300: Loss = -11611.291111222281
2
Iteration 10400: Loss = -11611.28361043077
3
Iteration 10500: Loss = -11611.284805925929
4
Iteration 10600: Loss = -11611.283581681775
5
Iteration 10700: Loss = -11611.290053209494
6
Iteration 10800: Loss = -11611.335933475995
7
Iteration 10900: Loss = -11611.375844248261
8
Iteration 11000: Loss = -11611.267712874182
Iteration 11100: Loss = -11611.268928293413
1
Iteration 11200: Loss = -11611.269221660688
2
Iteration 11300: Loss = -11611.267053226999
Iteration 11400: Loss = -11611.267630006292
1
Iteration 11500: Loss = -11611.277244757644
2
Iteration 11600: Loss = -11611.268152409384
3
Iteration 11700: Loss = -11611.268793734353
4
Iteration 11800: Loss = -11611.267409466558
5
Iteration 11900: Loss = -11611.269338615253
6
Iteration 12000: Loss = -11611.267104943698
Iteration 12100: Loss = -11611.270520319133
1
Iteration 12200: Loss = -11611.268417623012
2
Iteration 12300: Loss = -11611.269595988686
3
Iteration 12400: Loss = -11611.276233133749
4
Iteration 12500: Loss = -11611.267810547057
5
Iteration 12600: Loss = -11611.268283630749
6
Iteration 12700: Loss = -11611.274818224072
7
Iteration 12800: Loss = -11611.270699135639
8
Iteration 12900: Loss = -11611.37416615976
9
Iteration 13000: Loss = -11611.289247176159
10
Iteration 13100: Loss = -11611.268906116018
11
Iteration 13200: Loss = -11611.267725807937
12
Iteration 13300: Loss = -11611.270618597215
13
Iteration 13400: Loss = -11611.27114041978
14
Iteration 13500: Loss = -11611.266811122798
Iteration 13600: Loss = -11611.267049486845
1
Iteration 13700: Loss = -11611.273902449591
2
Iteration 13800: Loss = -11611.266933173349
3
Iteration 13900: Loss = -11611.284885159184
4
Iteration 14000: Loss = -11611.3388153709
5
Iteration 14100: Loss = -11611.267446306694
6
Iteration 14200: Loss = -11611.269089932688
7
Iteration 14300: Loss = -11611.272915025222
8
Iteration 14400: Loss = -11611.278447156326
9
Iteration 14500: Loss = -11611.267063861194
10
Iteration 14600: Loss = -11611.27723338589
11
Iteration 14700: Loss = -11611.271030588276
12
Iteration 14800: Loss = -11611.287641328128
13
Iteration 14900: Loss = -11611.28262794434
14
Iteration 15000: Loss = -11611.268024212148
15
Stopping early at iteration 15000 due to no improvement.
pi: tensor([[0.7662, 0.2338],
        [0.2041, 0.7959]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5770, 0.4230], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4070, 0.1038],
         [0.6378, 0.2055]],

        [[0.5884, 0.1002],
         [0.6734, 0.7167]],

        [[0.6257, 0.0928],
         [0.5244, 0.5539]],

        [[0.6683, 0.0938],
         [0.6149, 0.6084]],

        [[0.6991, 0.0930],
         [0.6642, 0.6891]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22258.449593841968
Iteration 100: Loss = -12558.664188329656
Iteration 200: Loss = -12354.402574847347
Iteration 300: Loss = -12012.619987351081
Iteration 400: Loss = -11965.028058416874
Iteration 500: Loss = -11938.637591292523
Iteration 600: Loss = -11910.331383039571
Iteration 700: Loss = -11863.99599987089
Iteration 800: Loss = -11822.780231520723
Iteration 900: Loss = -11806.410079320854
Iteration 1000: Loss = -11806.306893918641
Iteration 1100: Loss = -11806.264028797441
Iteration 1200: Loss = -11806.232244407805
Iteration 1300: Loss = -11806.207987668033
Iteration 1400: Loss = -11806.191624223675
Iteration 1500: Loss = -11806.178881184507
Iteration 1600: Loss = -11806.16853798234
Iteration 1700: Loss = -11806.159994663174
Iteration 1800: Loss = -11806.152771238325
Iteration 1900: Loss = -11806.146679072548
Iteration 2000: Loss = -11806.14137582856
Iteration 2100: Loss = -11806.157436837182
1
Iteration 2200: Loss = -11806.132908989384
Iteration 2300: Loss = -11806.12954313218
Iteration 2400: Loss = -11806.126516703353
Iteration 2500: Loss = -11806.123916901954
Iteration 2600: Loss = -11806.121501352489
Iteration 2700: Loss = -11806.119376740266
Iteration 2800: Loss = -11806.117783300002
Iteration 2900: Loss = -11806.115776107088
Iteration 3000: Loss = -11806.114219396248
Iteration 3100: Loss = -11806.113393660913
Iteration 3200: Loss = -11806.118015674056
1
Iteration 3300: Loss = -11806.115327177899
2
Iteration 3400: Loss = -11806.108333503858
Iteration 3500: Loss = -11806.106062672781
Iteration 3600: Loss = -11806.103988941388
Iteration 3700: Loss = -11806.10252544814
Iteration 3800: Loss = -11806.101174935195
Iteration 3900: Loss = -11806.115022514772
1
Iteration 4000: Loss = -11806.091951720919
Iteration 4100: Loss = -11806.091374501419
Iteration 4200: Loss = -11806.092111293194
1
Iteration 4300: Loss = -11806.092239372736
2
Iteration 4400: Loss = -11806.089673079618
Iteration 4500: Loss = -11806.092238124485
1
Iteration 4600: Loss = -11806.089222978715
Iteration 4700: Loss = -11806.088368714794
Iteration 4800: Loss = -11806.087917214909
Iteration 4900: Loss = -11806.088069256677
1
Iteration 5000: Loss = -11806.088001583576
Iteration 5100: Loss = -11806.086968956566
Iteration 5200: Loss = -11806.086943354705
Iteration 5300: Loss = -11806.08640794397
Iteration 5400: Loss = -11806.086326272527
Iteration 5500: Loss = -11806.086118242452
Iteration 5600: Loss = -11806.085723345413
Iteration 5700: Loss = -11806.085495343448
Iteration 5800: Loss = -11806.085342887434
Iteration 5900: Loss = -11806.085879065404
1
Iteration 6000: Loss = -11806.086660936366
2
Iteration 6100: Loss = -11806.08575953657
3
Iteration 6200: Loss = -11806.084641674144
Iteration 6300: Loss = -11806.106993510995
1
Iteration 6400: Loss = -11806.0843880866
Iteration 6500: Loss = -11806.084229728553
Iteration 6600: Loss = -11806.084148130203
Iteration 6700: Loss = -11806.083990863019
Iteration 6800: Loss = -11806.092525546572
1
Iteration 6900: Loss = -11806.083840888614
Iteration 7000: Loss = -11806.084598907873
1
Iteration 7100: Loss = -11806.084323062641
2
Iteration 7200: Loss = -11806.08403854626
3
Iteration 7300: Loss = -11806.083490227786
Iteration 7400: Loss = -11806.083848231334
1
Iteration 7500: Loss = -11806.088242454714
2
Iteration 7600: Loss = -11806.092726324363
3
Iteration 7700: Loss = -11806.083716556113
4
Iteration 7800: Loss = -11806.083074669345
Iteration 7900: Loss = -11806.083613153072
1
Iteration 8000: Loss = -11806.083032155851
Iteration 8100: Loss = -11806.094800217035
1
Iteration 8200: Loss = -11806.082900536716
Iteration 8300: Loss = -11806.083730409318
1
Iteration 8400: Loss = -11806.083444545386
2
Iteration 8500: Loss = -11806.082781393892
Iteration 8600: Loss = -11806.116248849965
1
Iteration 8700: Loss = -11806.08266366679
Iteration 8800: Loss = -11806.082641098545
Iteration 8900: Loss = -11806.084834643472
1
Iteration 9000: Loss = -11806.082664774718
Iteration 9100: Loss = -11806.098967342732
1
Iteration 9200: Loss = -11806.093213572809
2
Iteration 9300: Loss = -11806.085613165484
3
Iteration 9400: Loss = -11806.084092639523
4
Iteration 9500: Loss = -11806.082862331274
5
Iteration 9600: Loss = -11806.082845456443
6
Iteration 9700: Loss = -11806.090207558822
7
Iteration 9800: Loss = -11806.082599475885
Iteration 9900: Loss = -11806.085796184512
1
Iteration 10000: Loss = -11806.082360483555
Iteration 10100: Loss = -11806.344715867526
1
Iteration 10200: Loss = -11806.082314336034
Iteration 10300: Loss = -11806.133617520596
1
Iteration 10400: Loss = -11806.08447337389
2
Iteration 10500: Loss = -11806.108570901082
3
Iteration 10600: Loss = -11806.086333908435
4
Iteration 10700: Loss = -11806.082348584085
Iteration 10800: Loss = -11806.091776358791
1
Iteration 10900: Loss = -11806.083091117667
2
Iteration 11000: Loss = -11806.098355721802
3
Iteration 11100: Loss = -11806.082339285398
Iteration 11200: Loss = -11806.082509771439
1
Iteration 11300: Loss = -11806.085628202449
2
Iteration 11400: Loss = -11806.08246040045
3
Iteration 11500: Loss = -11806.123542706544
4
Iteration 11600: Loss = -11806.130658673348
5
Iteration 11700: Loss = -11806.082151256129
Iteration 11800: Loss = -11806.08360669743
1
Iteration 11900: Loss = -11806.08360534663
2
Iteration 12000: Loss = -11806.084735229302
3
Iteration 12100: Loss = -11806.086481424041
4
Iteration 12200: Loss = -11806.082075100303
Iteration 12300: Loss = -11806.082801161823
1
Iteration 12400: Loss = -11806.082551194753
2
Iteration 12500: Loss = -11806.084242726956
3
Iteration 12600: Loss = -11806.08403183124
4
Iteration 12700: Loss = -11806.082115732705
Iteration 12800: Loss = -11806.082640177092
1
Iteration 12900: Loss = -11806.08205348245
Iteration 13000: Loss = -11806.098630651928
1
Iteration 13100: Loss = -11806.08331805032
2
Iteration 13200: Loss = -11806.082566915222
3
Iteration 13300: Loss = -11806.266151501852
4
Iteration 13400: Loss = -11806.082002086263
Iteration 13500: Loss = -11806.085371422983
1
Iteration 13600: Loss = -11806.085813270753
2
Iteration 13700: Loss = -11806.093887757508
3
Iteration 13800: Loss = -11806.091853584801
4
Iteration 13900: Loss = -11806.085808957074
5
Iteration 14000: Loss = -11806.093424855377
6
Iteration 14100: Loss = -11806.236776317355
7
Iteration 14200: Loss = -11806.082107420229
8
Iteration 14300: Loss = -11806.091638114081
9
Iteration 14400: Loss = -11806.0827610276
10
Iteration 14500: Loss = -11806.084034535535
11
Iteration 14600: Loss = -11806.214682321539
12
Iteration 14700: Loss = -11806.083140368342
13
Iteration 14800: Loss = -11806.083138330096
14
Iteration 14900: Loss = -11806.082224034632
15
Stopping early at iteration 14900 due to no improvement.
pi: tensor([[0.6086, 0.3914],
        [0.4792, 0.5208]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4194, 0.5806], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2287, 0.1037],
         [0.5328, 0.3982]],

        [[0.7106, 0.0999],
         [0.6867, 0.5604]],

        [[0.6027, 0.0934],
         [0.5998, 0.5682]],

        [[0.5658, 0.0929],
         [0.5760, 0.6878]],

        [[0.6206, 0.0930],
         [0.5988, 0.5908]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 22
Adjusted Rand Index: 0.3079630444346678
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4722886415814607
Average Adjusted Rand Index: 0.8615926088869337
11618.1123034395
[1.0, 0.4722886415814607] [1.0, 0.8615926088869337] [11611.268024212148, 11806.082224034632]
-------------------------------------
This iteration is 24
True Objective function: Loss = -11563.677343174195
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22085.085326235156
Iteration 100: Loss = -12281.820724691903
Iteration 200: Loss = -12073.983168006742
Iteration 300: Loss = -11848.115955770263
Iteration 400: Loss = -11746.673614872967
Iteration 500: Loss = -11739.451999669089
Iteration 600: Loss = -11718.038363464988
Iteration 700: Loss = -11717.079393234066
Iteration 800: Loss = -11716.979127952236
Iteration 900: Loss = -11716.847565071828
Iteration 1000: Loss = -11716.815935971967
Iteration 1100: Loss = -11716.792995855578
Iteration 1200: Loss = -11716.775569719903
Iteration 1300: Loss = -11716.761711167957
Iteration 1400: Loss = -11716.750860296426
Iteration 1500: Loss = -11716.740248715636
Iteration 1600: Loss = -11716.731814654995
Iteration 1700: Loss = -11716.725594409989
Iteration 1800: Loss = -11716.72061046927
Iteration 1900: Loss = -11716.716333924172
Iteration 2000: Loss = -11716.712709382315
Iteration 2100: Loss = -11716.709837090793
Iteration 2200: Loss = -11716.70659494802
Iteration 2300: Loss = -11716.706424888142
Iteration 2400: Loss = -11716.701339666013
Iteration 2500: Loss = -11716.699689463752
Iteration 2600: Loss = -11716.667375404824
Iteration 2700: Loss = -11716.666358813616
Iteration 2800: Loss = -11716.663224735645
Iteration 2900: Loss = -11716.66075405986
Iteration 3000: Loss = -11716.66295376447
1
Iteration 3100: Loss = -11716.656087975007
Iteration 3200: Loss = -11716.65848825692
1
Iteration 3300: Loss = -11716.655506256087
Iteration 3400: Loss = -11716.65350179323
Iteration 3500: Loss = -11716.652810682852
Iteration 3600: Loss = -11716.653123312279
1
Iteration 3700: Loss = -11716.652709106073
Iteration 3800: Loss = -11716.653204353403
1
Iteration 3900: Loss = -11716.650889350907
Iteration 4000: Loss = -11716.650384757473
Iteration 4100: Loss = -11716.649804995028
Iteration 4200: Loss = -11716.649315045235
Iteration 4300: Loss = -11716.649190639922
Iteration 4400: Loss = -11716.64975483775
1
Iteration 4500: Loss = -11716.648657453288
Iteration 4600: Loss = -11716.648349741514
Iteration 4700: Loss = -11716.649258155256
1
Iteration 4800: Loss = -11716.657412310937
2
Iteration 4900: Loss = -11716.647188607043
Iteration 5000: Loss = -11716.646946687519
Iteration 5100: Loss = -11716.647033506051
Iteration 5200: Loss = -11716.64689826613
Iteration 5300: Loss = -11716.650295542164
1
Iteration 5400: Loss = -11716.649225368215
2
Iteration 5500: Loss = -11716.646268834327
Iteration 5600: Loss = -11716.646571084082
1
Iteration 5700: Loss = -11716.645982951459
Iteration 5800: Loss = -11716.645763563884
Iteration 5900: Loss = -11716.64602075418
1
Iteration 6000: Loss = -11716.645612410939
Iteration 6100: Loss = -11716.64655074284
1
Iteration 6200: Loss = -11716.645522684274
Iteration 6300: Loss = -11716.650362080067
1
Iteration 6400: Loss = -11716.647141448997
2
Iteration 6500: Loss = -11716.648163543312
3
Iteration 6600: Loss = -11716.645012431549
Iteration 6700: Loss = -11716.645027800629
Iteration 6800: Loss = -11716.644884648054
Iteration 6900: Loss = -11716.654789921453
1
Iteration 7000: Loss = -11716.644753789024
Iteration 7100: Loss = -11716.6447399495
Iteration 7200: Loss = -11716.645916491667
1
Iteration 7300: Loss = -11716.654716152623
2
Iteration 7400: Loss = -11716.644592824
Iteration 7500: Loss = -11716.64513740998
1
Iteration 7600: Loss = -11716.751708312147
2
Iteration 7700: Loss = -11716.644453371111
Iteration 7800: Loss = -11716.647236471452
1
Iteration 7900: Loss = -11716.644686218833
2
Iteration 8000: Loss = -11716.644329945555
Iteration 8100: Loss = -11716.645844372306
1
Iteration 8200: Loss = -11716.64422830475
Iteration 8300: Loss = -11716.645204016479
1
Iteration 8400: Loss = -11716.644076097145
Iteration 8500: Loss = -11716.656456490045
1
Iteration 8600: Loss = -11716.644046401327
Iteration 8700: Loss = -11716.643987958585
Iteration 8800: Loss = -11716.644050549205
Iteration 8900: Loss = -11716.643963042794
Iteration 9000: Loss = -11716.663048506896
1
Iteration 9100: Loss = -11716.643926506264
Iteration 9200: Loss = -11716.643901171028
Iteration 9300: Loss = -11716.686455295274
1
Iteration 9400: Loss = -11716.64389245407
Iteration 9500: Loss = -11716.662451239232
1
Iteration 9600: Loss = -11716.644100338806
2
Iteration 9700: Loss = -11716.644536955819
3
Iteration 9800: Loss = -11716.644278404452
4
Iteration 9900: Loss = -11716.645109728475
5
Iteration 10000: Loss = -11716.698832239614
6
Iteration 10100: Loss = -11716.650854170051
7
Iteration 10200: Loss = -11716.645003940574
8
Iteration 10300: Loss = -11716.646958430218
9
Iteration 10400: Loss = -11716.643844871176
Iteration 10500: Loss = -11716.685026095025
1
Iteration 10600: Loss = -11716.64566906131
2
Iteration 10700: Loss = -11716.668832220153
3
Iteration 10800: Loss = -11716.642966105892
Iteration 10900: Loss = -11716.640946443309
Iteration 11000: Loss = -11716.640953175845
Iteration 11100: Loss = -11716.641376291183
1
Iteration 11200: Loss = -11716.641768530302
2
Iteration 11300: Loss = -11716.681975664686
3
Iteration 11400: Loss = -11716.645754408122
4
Iteration 11500: Loss = -11716.652445513373
5
Iteration 11600: Loss = -11716.640936165819
Iteration 11700: Loss = -11716.646181795311
1
Iteration 11800: Loss = -11716.64491238971
2
Iteration 11900: Loss = -11716.642662454802
3
Iteration 12000: Loss = -11716.64256843358
4
Iteration 12100: Loss = -11716.644654445225
5
Iteration 12200: Loss = -11716.641276883802
6
Iteration 12300: Loss = -11716.64432020496
7
Iteration 12400: Loss = -11716.739861090877
8
Iteration 12500: Loss = -11716.82329731867
9
Iteration 12600: Loss = -11716.642250242141
10
Iteration 12700: Loss = -11716.641031802854
Iteration 12800: Loss = -11716.641488821204
1
Iteration 12900: Loss = -11716.648666962208
2
Iteration 13000: Loss = -11716.644729578646
3
Iteration 13100: Loss = -11716.641612337095
4
Iteration 13200: Loss = -11716.640847578426
Iteration 13300: Loss = -11716.640727718446
Iteration 13400: Loss = -11716.653269338705
1
Iteration 13500: Loss = -11716.642554610276
2
Iteration 13600: Loss = -11716.643027637127
3
Iteration 13700: Loss = -11716.781160133638
4
Iteration 13800: Loss = -11716.64142594036
5
Iteration 13900: Loss = -11716.640992128201
6
Iteration 14000: Loss = -11716.64538210358
7
Iteration 14100: Loss = -11716.725059278113
8
Iteration 14200: Loss = -11716.640785800217
Iteration 14300: Loss = -11716.643071647028
1
Iteration 14400: Loss = -11716.662251022643
2
Iteration 14500: Loss = -11716.64107243394
3
Iteration 14600: Loss = -11716.64123151069
4
Iteration 14700: Loss = -11716.640880738178
Iteration 14800: Loss = -11716.695416054452
1
Iteration 14900: Loss = -11716.65104822823
2
Iteration 15000: Loss = -11716.644320267247
3
Iteration 15100: Loss = -11716.684969364363
4
Iteration 15200: Loss = -11716.649326735193
5
Iteration 15300: Loss = -11716.931696160089
6
Iteration 15400: Loss = -11716.641907496241
7
Iteration 15500: Loss = -11716.667959851686
8
Iteration 15600: Loss = -11716.760214064778
9
Iteration 15700: Loss = -11716.642256312261
10
Iteration 15800: Loss = -11716.645885230357
11
Iteration 15900: Loss = -11716.651073350135
12
Iteration 16000: Loss = -11716.643512126278
13
Iteration 16100: Loss = -11716.640752914547
Iteration 16200: Loss = -11716.6410369074
1
Iteration 16300: Loss = -11716.668277067947
2
Iteration 16400: Loss = -11716.645556380874
3
Iteration 16500: Loss = -11716.641239634739
4
Iteration 16600: Loss = -11716.640268363795
Iteration 16700: Loss = -11716.64069577755
1
Iteration 16800: Loss = -11716.734643891567
2
Iteration 16900: Loss = -11716.642088368917
3
Iteration 17000: Loss = -11716.643474033885
4
Iteration 17100: Loss = -11716.642775377004
5
Iteration 17200: Loss = -11716.640395913164
6
Iteration 17300: Loss = -11716.640763396214
7
Iteration 17400: Loss = -11716.670832485674
8
Iteration 17500: Loss = -11716.640577371829
9
Iteration 17600: Loss = -11716.648674191269
10
Iteration 17700: Loss = -11716.640295958034
Iteration 17800: Loss = -11716.640706145634
1
Iteration 17900: Loss = -11716.653958206929
2
Iteration 18000: Loss = -11716.640798320315
3
Iteration 18100: Loss = -11716.640456234449
4
Iteration 18200: Loss = -11716.650505645015
5
Iteration 18300: Loss = -11716.641521432563
6
Iteration 18400: Loss = -11716.642822664808
7
Iteration 18500: Loss = -11716.641489746527
8
Iteration 18600: Loss = -11716.652431452185
9
Iteration 18700: Loss = -11716.671349867669
10
Iteration 18800: Loss = -11716.648102042873
11
Iteration 18900: Loss = -11716.640291371796
Iteration 19000: Loss = -11716.640634164714
1
Iteration 19100: Loss = -11716.72708636318
2
Iteration 19200: Loss = -11716.645862428368
3
Iteration 19300: Loss = -11716.640522775071
4
Iteration 19400: Loss = -11716.6423791924
5
Iteration 19500: Loss = -11716.646906586475
6
Iteration 19600: Loss = -11716.64169002706
7
Iteration 19700: Loss = -11716.643311190855
8
Iteration 19800: Loss = -11716.650339548833
9
Iteration 19900: Loss = -11716.640507638547
10
pi: tensor([[0.6184, 0.3816],
        [0.2609, 0.7391]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4199, 0.5801], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3952, 0.1090],
         [0.5028, 0.2142]],

        [[0.5610, 0.1015],
         [0.5227, 0.5160]],

        [[0.6320, 0.0978],
         [0.7114, 0.7229]],

        [[0.6681, 0.1060],
         [0.6907, 0.5069]],

        [[0.7247, 0.1023],
         [0.6348, 0.6075]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 78
Adjusted Rand Index: 0.30732918817043314
Global Adjusted Rand Index: 0.4613381274003537
Average Adjusted Rand Index: 0.8454623260636964
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22792.55409276456
Iteration 100: Loss = -12292.54002377276
Iteration 200: Loss = -12238.699397897617
Iteration 300: Loss = -11905.982117891821
Iteration 400: Loss = -11756.497847449467
Iteration 500: Loss = -11683.471343073235
Iteration 600: Loss = -11665.478588444295
Iteration 700: Loss = -11625.028029145986
Iteration 800: Loss = -11587.083665311424
Iteration 900: Loss = -11577.951282895647
Iteration 1000: Loss = -11577.730061316413
Iteration 1100: Loss = -11569.769801437225
Iteration 1200: Loss = -11569.665926527308
Iteration 1300: Loss = -11569.548577491787
Iteration 1400: Loss = -11565.166765134556
Iteration 1500: Loss = -11565.04321222321
Iteration 1600: Loss = -11554.577744842863
Iteration 1700: Loss = -11554.53013452229
Iteration 1800: Loss = -11554.50226761043
Iteration 1900: Loss = -11554.480873882578
Iteration 2000: Loss = -11554.463540777699
Iteration 2100: Loss = -11554.4490105185
Iteration 2200: Loss = -11554.436602211395
Iteration 2300: Loss = -11554.425875903107
Iteration 2400: Loss = -11554.41652157022
Iteration 2500: Loss = -11554.408324942342
Iteration 2600: Loss = -11554.401057352847
Iteration 2700: Loss = -11554.394677639912
Iteration 2800: Loss = -11554.390309261988
Iteration 2900: Loss = -11554.383713304604
Iteration 3000: Loss = -11554.378884013957
Iteration 3100: Loss = -11554.381748574846
1
Iteration 3200: Loss = -11554.369851452428
Iteration 3300: Loss = -11554.364323140251
Iteration 3400: Loss = -11554.33567267038
Iteration 3500: Loss = -11554.317267337161
Iteration 3600: Loss = -11554.314073420263
Iteration 3700: Loss = -11554.311560818964
Iteration 3800: Loss = -11554.309178097114
Iteration 3900: Loss = -11554.307075844728
Iteration 4000: Loss = -11554.30516423612
Iteration 4100: Loss = -11554.303334225864
Iteration 4200: Loss = -11554.311206977387
1
Iteration 4300: Loss = -11554.300027545629
Iteration 4400: Loss = -11554.309782728431
1
Iteration 4500: Loss = -11554.296961599975
Iteration 4600: Loss = -11554.295716102513
Iteration 4700: Loss = -11554.293964764654
Iteration 4800: Loss = -11554.292645660502
Iteration 4900: Loss = -11554.288945885943
Iteration 5000: Loss = -11554.287955921001
Iteration 5100: Loss = -11554.283676258501
Iteration 5200: Loss = -11554.279631344858
Iteration 5300: Loss = -11554.278254530329
Iteration 5400: Loss = -11554.280034786883
1
Iteration 5500: Loss = -11554.275905373981
Iteration 5600: Loss = -11554.276159576453
1
Iteration 5700: Loss = -11554.272748844323
Iteration 5800: Loss = -11554.275045233153
1
Iteration 5900: Loss = -11554.271470472997
Iteration 6000: Loss = -11554.271353509694
Iteration 6100: Loss = -11554.27020292571
Iteration 6200: Loss = -11554.27345954839
1
Iteration 6300: Loss = -11554.2691082323
Iteration 6400: Loss = -11554.27094055188
1
Iteration 6500: Loss = -11554.269385006033
2
Iteration 6600: Loss = -11554.268961129883
Iteration 6700: Loss = -11554.267516118787
Iteration 6800: Loss = -11554.26748803928
Iteration 6900: Loss = -11554.272506646208
1
Iteration 7000: Loss = -11554.266743397871
Iteration 7100: Loss = -11554.267646382623
1
Iteration 7200: Loss = -11554.266275586893
Iteration 7300: Loss = -11554.266401934276
1
Iteration 7400: Loss = -11554.267036535999
2
Iteration 7500: Loss = -11554.265777954211
Iteration 7600: Loss = -11554.266264322812
1
Iteration 7700: Loss = -11554.266467153486
2
Iteration 7800: Loss = -11554.266598708886
3
Iteration 7900: Loss = -11554.26531236205
Iteration 8000: Loss = -11554.264940779969
Iteration 8100: Loss = -11554.265770682707
1
Iteration 8200: Loss = -11554.277956151185
2
Iteration 8300: Loss = -11554.283681492403
3
Iteration 8400: Loss = -11554.271717002708
4
Iteration 8500: Loss = -11554.264443877932
Iteration 8600: Loss = -11554.269849169508
1
Iteration 8700: Loss = -11554.262565339848
Iteration 8800: Loss = -11554.262672622684
1
Iteration 8900: Loss = -11554.261838603315
Iteration 9000: Loss = -11554.263229244307
1
Iteration 9100: Loss = -11554.26227161514
2
Iteration 9200: Loss = -11554.263810597764
3
Iteration 9300: Loss = -11554.261438050191
Iteration 9400: Loss = -11554.261967547738
1
Iteration 9500: Loss = -11554.440438354484
2
Iteration 9600: Loss = -11554.261644078802
3
Iteration 9700: Loss = -11554.293735581776
4
Iteration 9800: Loss = -11554.262705052799
5
Iteration 9900: Loss = -11554.261135081248
Iteration 10000: Loss = -11554.261566696881
1
Iteration 10100: Loss = -11554.26454897572
2
Iteration 10200: Loss = -11554.270818500208
3
Iteration 10300: Loss = -11554.266382548907
4
Iteration 10400: Loss = -11554.258596785447
Iteration 10500: Loss = -11554.262519088881
1
Iteration 10600: Loss = -11554.301291930826
2
Iteration 10700: Loss = -11554.259803521776
3
Iteration 10800: Loss = -11554.262609483158
4
Iteration 10900: Loss = -11554.275203826777
5
Iteration 11000: Loss = -11554.382065643043
6
Iteration 11100: Loss = -11554.276148279232
7
Iteration 11200: Loss = -11554.268923767822
8
Iteration 11300: Loss = -11554.260899459703
9
Iteration 11400: Loss = -11554.258505757836
Iteration 11500: Loss = -11554.261563337399
1
Iteration 11600: Loss = -11554.272304670185
2
Iteration 11700: Loss = -11554.262116502754
3
Iteration 11800: Loss = -11554.261292097564
4
Iteration 11900: Loss = -11554.277860716013
5
Iteration 12000: Loss = -11554.280713328824
6
Iteration 12100: Loss = -11554.258510647056
Iteration 12200: Loss = -11554.260221741375
1
Iteration 12300: Loss = -11554.215294920958
Iteration 12400: Loss = -11554.246659151435
1
Iteration 12500: Loss = -11554.223128616453
2
Iteration 12600: Loss = -11554.209098176816
Iteration 12700: Loss = -11554.211585373914
1
Iteration 12800: Loss = -11554.20968413171
2
Iteration 12900: Loss = -11554.22329896829
3
Iteration 13000: Loss = -11554.217978299308
4
Iteration 13100: Loss = -11554.217667737397
5
Iteration 13200: Loss = -11554.211079900379
6
Iteration 13300: Loss = -11554.206227344974
Iteration 13400: Loss = -11554.212826935465
1
Iteration 13500: Loss = -11554.208306049799
2
Iteration 13600: Loss = -11554.205597102622
Iteration 13700: Loss = -11554.207080835931
1
Iteration 13800: Loss = -11554.205514100933
Iteration 13900: Loss = -11554.205930540635
1
Iteration 14000: Loss = -11554.218367944168
2
Iteration 14100: Loss = -11554.206022449296
3
Iteration 14200: Loss = -11554.210060302132
4
Iteration 14300: Loss = -11554.226003950993
5
Iteration 14400: Loss = -11554.254563998535
6
Iteration 14500: Loss = -11554.214265149732
7
Iteration 14600: Loss = -11554.242358490706
8
Iteration 14700: Loss = -11554.22402902021
9
Iteration 14800: Loss = -11554.213434627909
10
Iteration 14900: Loss = -11554.208886263601
11
Iteration 15000: Loss = -11554.208374700223
12
Iteration 15100: Loss = -11554.205493929634
Iteration 15200: Loss = -11554.207346295898
1
Iteration 15300: Loss = -11554.209908605178
2
Iteration 15400: Loss = -11554.215459980756
3
Iteration 15500: Loss = -11554.206632421408
4
Iteration 15600: Loss = -11554.217161838073
5
Iteration 15700: Loss = -11554.205658639592
6
Iteration 15800: Loss = -11554.208618484388
7
Iteration 15900: Loss = -11554.20733092393
8
Iteration 16000: Loss = -11554.21240723574
9
Iteration 16100: Loss = -11554.204645718615
Iteration 16200: Loss = -11554.205619485248
1
Iteration 16300: Loss = -11554.206894773277
2
Iteration 16400: Loss = -11554.206432878409
3
Iteration 16500: Loss = -11554.20836114147
4
Iteration 16600: Loss = -11554.211216186219
5
Iteration 16700: Loss = -11554.2376888734
6
Iteration 16800: Loss = -11554.217924507919
7
Iteration 16900: Loss = -11554.220198157134
8
Iteration 17000: Loss = -11554.210557612998
9
Iteration 17100: Loss = -11554.206539347537
10
Iteration 17200: Loss = -11554.212660193614
11
Iteration 17300: Loss = -11554.20844206843
12
Iteration 17400: Loss = -11554.256404611713
13
Iteration 17500: Loss = -11554.25481639473
14
Iteration 17600: Loss = -11554.224587356734
15
Stopping early at iteration 17600 due to no improvement.
pi: tensor([[0.7149, 0.2851],
        [0.2430, 0.7570]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5749, 0.4251], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1897, 0.1084],
         [0.6662, 0.3966]],

        [[0.7194, 0.1014],
         [0.6509, 0.6179]],

        [[0.5150, 0.0977],
         [0.7208, 0.7197]],

        [[0.6375, 0.1061],
         [0.5042, 0.7287]],

        [[0.5747, 0.1050],
         [0.6876, 0.5947]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320089020589
Average Adjusted Rand Index: 0.9839964884296097
11563.677343174195
[0.4613381274003537, 0.9840320089020589] [0.8454623260636964, 0.9839964884296097] [11716.646966036202, 11554.224587356734]
-------------------------------------
This iteration is 25
True Objective function: Loss = -11567.952856942185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20962.205722542993
Iteration 100: Loss = -12329.824880340097
Iteration 200: Loss = -12166.9447977055
Iteration 300: Loss = -11604.842047462185
Iteration 400: Loss = -11563.455895473837
Iteration 500: Loss = -11562.361894743308
Iteration 600: Loss = -11561.90860149173
Iteration 700: Loss = -11561.656060221883
Iteration 800: Loss = -11561.49642533529
Iteration 900: Loss = -11561.38730038229
Iteration 1000: Loss = -11561.308496633083
Iteration 1100: Loss = -11561.24936788703
Iteration 1200: Loss = -11561.203437029933
Iteration 1300: Loss = -11561.166817638077
Iteration 1400: Loss = -11561.136948001924
Iteration 1500: Loss = -11561.111787546974
Iteration 1600: Loss = -11561.089272802434
Iteration 1700: Loss = -11561.064691474212
Iteration 1800: Loss = -11561.033123146557
Iteration 1900: Loss = -11561.0178050351
Iteration 2000: Loss = -11561.005902540397
Iteration 2100: Loss = -11560.995574098883
Iteration 2200: Loss = -11560.98650118031
Iteration 2300: Loss = -11560.995403824394
1
Iteration 2400: Loss = -11560.971731083859
Iteration 2500: Loss = -11560.965706636362
Iteration 2600: Loss = -11560.960259179581
Iteration 2700: Loss = -11560.955317199758
Iteration 2800: Loss = -11560.950493407146
Iteration 2900: Loss = -11560.946012961938
Iteration 3000: Loss = -11560.94141806024
Iteration 3100: Loss = -11560.936555568152
Iteration 3200: Loss = -11560.932350128242
Iteration 3300: Loss = -11560.928852544825
Iteration 3400: Loss = -11560.925915677533
Iteration 3500: Loss = -11560.923846609232
Iteration 3600: Loss = -11560.92144485355
Iteration 3700: Loss = -11560.923746777602
1
Iteration 3800: Loss = -11560.917852082199
Iteration 3900: Loss = -11560.91629660165
Iteration 4000: Loss = -11560.91495172665
Iteration 4100: Loss = -11560.913718128253
Iteration 4200: Loss = -11560.913317319219
Iteration 4300: Loss = -11560.911111957048
Iteration 4400: Loss = -11560.910205172393
Iteration 4500: Loss = -11560.909053059251
Iteration 4600: Loss = -11560.908153786148
Iteration 4700: Loss = -11560.907320268934
Iteration 4800: Loss = -11560.906468587436
Iteration 4900: Loss = -11560.910451630723
1
Iteration 5000: Loss = -11560.904998366688
Iteration 5100: Loss = -11560.904752474271
Iteration 5200: Loss = -11560.90371103222
Iteration 5300: Loss = -11560.903063794278
Iteration 5400: Loss = -11560.93135366828
1
Iteration 5500: Loss = -11560.901925178194
Iteration 5600: Loss = -11560.906820143091
1
Iteration 5700: Loss = -11560.901091466032
Iteration 5800: Loss = -11560.900508082479
Iteration 5900: Loss = -11560.900181334651
Iteration 6000: Loss = -11560.8999297649
Iteration 6100: Loss = -11560.899338616584
Iteration 6200: Loss = -11560.899678461457
1
Iteration 6300: Loss = -11560.90204358131
2
Iteration 6400: Loss = -11560.898451547744
Iteration 6500: Loss = -11560.898819170596
1
Iteration 6600: Loss = -11560.914882715766
2
Iteration 6700: Loss = -11560.897952286461
Iteration 6800: Loss = -11560.8974088803
Iteration 6900: Loss = -11560.899727954595
1
Iteration 7000: Loss = -11560.896995040406
Iteration 7100: Loss = -11560.89931714262
1
Iteration 7200: Loss = -11560.896627500144
Iteration 7300: Loss = -11560.896520243738
Iteration 7400: Loss = -11560.912972736127
1
Iteration 7500: Loss = -11560.897087831816
2
Iteration 7600: Loss = -11560.897996531487
3
Iteration 7700: Loss = -11560.896125799842
Iteration 7800: Loss = -11560.896668615445
1
Iteration 7900: Loss = -11560.895716311656
Iteration 8000: Loss = -11560.895533369106
Iteration 8100: Loss = -11560.904855808272
1
Iteration 8200: Loss = -11560.89529631507
Iteration 8300: Loss = -11560.913001263581
1
Iteration 8400: Loss = -11560.895120005249
Iteration 8500: Loss = -11560.895014920294
Iteration 8600: Loss = -11560.895248401514
1
Iteration 8700: Loss = -11560.894859958602
Iteration 8800: Loss = -11560.97289362778
1
Iteration 8900: Loss = -11560.89475767757
Iteration 9000: Loss = -11560.902641801484
1
Iteration 9100: Loss = -11560.910204488444
2
Iteration 9200: Loss = -11560.93038478799
3
Iteration 9300: Loss = -11560.894538426302
Iteration 9400: Loss = -11560.89511008301
1
Iteration 9500: Loss = -11560.894727957966
2
Iteration 9600: Loss = -11560.897299519862
3
Iteration 9700: Loss = -11560.894401063946
Iteration 9800: Loss = -11560.896701973359
1
Iteration 9900: Loss = -11561.070478707024
2
Iteration 10000: Loss = -11560.894186009544
Iteration 10100: Loss = -11560.90305846466
1
Iteration 10200: Loss = -11560.894116436162
Iteration 10300: Loss = -11560.90383052313
1
Iteration 10400: Loss = -11560.911774800486
2
Iteration 10500: Loss = -11560.89972880935
3
Iteration 10600: Loss = -11560.909468046237
4
Iteration 10700: Loss = -11560.895442819014
5
Iteration 10800: Loss = -11560.933143674523
6
Iteration 10900: Loss = -11560.896109842854
7
Iteration 11000: Loss = -11560.896472703787
8
Iteration 11100: Loss = -11560.896789104463
9
Iteration 11200: Loss = -11560.894229408954
10
Iteration 11300: Loss = -11560.894100485231
Iteration 11400: Loss = -11560.896580464983
1
Iteration 11500: Loss = -11560.895065500235
2
Iteration 11600: Loss = -11560.901020980766
3
Iteration 11700: Loss = -11560.899722350208
4
Iteration 11800: Loss = -11560.894710787721
5
Iteration 11900: Loss = -11560.900596713514
6
Iteration 12000: Loss = -11560.909480958255
7
Iteration 12100: Loss = -11560.901960195582
8
Iteration 12200: Loss = -11560.915564111136
9
Iteration 12300: Loss = -11560.893791603663
Iteration 12400: Loss = -11560.896160427026
1
Iteration 12500: Loss = -11560.896232715615
2
Iteration 12600: Loss = -11560.893718061126
Iteration 12700: Loss = -11560.90584263992
1
Iteration 12800: Loss = -11560.8954710944
2
Iteration 12900: Loss = -11560.92088764712
3
Iteration 13000: Loss = -11560.92065718898
4
Iteration 13100: Loss = -11560.996344072933
5
Iteration 13200: Loss = -11560.89454477674
6
Iteration 13300: Loss = -11560.893935769935
7
Iteration 13400: Loss = -11560.90392113569
8
Iteration 13500: Loss = -11560.901976136664
9
Iteration 13600: Loss = -11560.968848559829
10
Iteration 13700: Loss = -11560.89387175759
11
Iteration 13800: Loss = -11560.895372802706
12
Iteration 13900: Loss = -11560.904482865264
13
Iteration 14000: Loss = -11560.896912742923
14
Iteration 14100: Loss = -11560.941888692725
15
Stopping early at iteration 14100 due to no improvement.
pi: tensor([[0.7809, 0.2191],
        [0.2883, 0.7117]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4101, 0.5899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1934, 0.1082],
         [0.5909, 0.4052]],

        [[0.6832, 0.0961],
         [0.6637, 0.5005]],

        [[0.6278, 0.1082],
         [0.6069, 0.6003]],

        [[0.6610, 0.0986],
         [0.5298, 0.7255]],

        [[0.5945, 0.0979],
         [0.5962, 0.5365]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20135.201647240498
Iteration 100: Loss = -12367.229786639833
Iteration 200: Loss = -12292.457616224063
Iteration 300: Loss = -11697.982469798055
Iteration 400: Loss = -11576.976848885965
Iteration 500: Loss = -11561.889943437893
Iteration 600: Loss = -11561.535692061068
Iteration 700: Loss = -11561.37525218189
Iteration 800: Loss = -11561.27593023459
Iteration 900: Loss = -11561.208771442462
Iteration 1000: Loss = -11561.16066857677
Iteration 1100: Loss = -11561.124709549533
Iteration 1200: Loss = -11561.096992343266
Iteration 1300: Loss = -11561.075047011134
Iteration 1400: Loss = -11561.057332468152
Iteration 1500: Loss = -11561.042761099478
Iteration 1600: Loss = -11561.03059561756
Iteration 1700: Loss = -11561.02031937763
Iteration 1800: Loss = -11561.011589093348
Iteration 1900: Loss = -11561.004023960442
Iteration 2000: Loss = -11560.997348549032
Iteration 2100: Loss = -11560.991295456446
Iteration 2200: Loss = -11560.985355715246
Iteration 2300: Loss = -11560.978808328839
Iteration 2400: Loss = -11560.974321749381
Iteration 2500: Loss = -11560.970687046452
Iteration 2600: Loss = -11560.967722799905
Iteration 2700: Loss = -11560.964387956004
Iteration 2800: Loss = -11560.961415187256
Iteration 2900: Loss = -11560.958246638455
Iteration 3000: Loss = -11560.95581271732
Iteration 3100: Loss = -11560.953611754738
Iteration 3200: Loss = -11560.951882801437
Iteration 3300: Loss = -11560.95033338566
Iteration 3400: Loss = -11560.948874205025
Iteration 3500: Loss = -11560.947532293509
Iteration 3600: Loss = -11560.946449628509
Iteration 3700: Loss = -11560.945326542309
Iteration 3800: Loss = -11560.944174583201
Iteration 3900: Loss = -11560.943588740462
Iteration 4000: Loss = -11560.942640357676
Iteration 4100: Loss = -11560.947516835775
1
Iteration 4200: Loss = -11560.907762351058
Iteration 4300: Loss = -11560.906246180151
Iteration 4400: Loss = -11560.905580683377
Iteration 4500: Loss = -11560.910811106258
1
Iteration 4600: Loss = -11560.90430790425
Iteration 4700: Loss = -11560.906995248732
1
Iteration 4800: Loss = -11560.903077703242
Iteration 4900: Loss = -11560.90200552722
Iteration 5000: Loss = -11560.899891588337
Iteration 5100: Loss = -11560.896362101663
Iteration 5200: Loss = -11560.898295616997
1
Iteration 5300: Loss = -11560.895524206133
Iteration 5400: Loss = -11560.897336530903
1
Iteration 5500: Loss = -11560.894022386876
Iteration 5600: Loss = -11560.891187409321
Iteration 5700: Loss = -11560.897323244819
1
Iteration 5800: Loss = -11560.891757841735
2
Iteration 5900: Loss = -11560.891322787704
3
Iteration 6000: Loss = -11560.890016598012
Iteration 6100: Loss = -11560.893043221555
1
Iteration 6200: Loss = -11560.895164344603
2
Iteration 6300: Loss = -11560.889213395702
Iteration 6400: Loss = -11560.889457693891
1
Iteration 6500: Loss = -11560.886135893947
Iteration 6600: Loss = -11560.88577864539
Iteration 6700: Loss = -11560.899488581967
1
Iteration 6800: Loss = -11560.885494632941
Iteration 6900: Loss = -11560.885325560694
Iteration 7000: Loss = -11560.885479772289
1
Iteration 7100: Loss = -11560.885107438467
Iteration 7200: Loss = -11561.14340979715
1
Iteration 7300: Loss = -11560.8848895741
Iteration 7400: Loss = -11560.88478969838
Iteration 7500: Loss = -11560.886544618605
1
Iteration 7600: Loss = -11560.884605253028
Iteration 7700: Loss = -11560.884675358806
Iteration 7800: Loss = -11560.88476229064
Iteration 7900: Loss = -11560.886153284853
1
Iteration 8000: Loss = -11560.88638473284
2
Iteration 8100: Loss = -11560.884516574402
Iteration 8200: Loss = -11560.884198636104
Iteration 8300: Loss = -11560.884358157133
1
Iteration 8400: Loss = -11560.887316256742
2
Iteration 8500: Loss = -11560.885283782905
3
Iteration 8600: Loss = -11561.085854868255
4
Iteration 8700: Loss = -11560.883956268253
Iteration 8800: Loss = -11560.883879330358
Iteration 8900: Loss = -11560.974649076483
1
Iteration 9000: Loss = -11560.884184477589
2
Iteration 9100: Loss = -11560.883762981799
Iteration 9200: Loss = -11560.886061307796
1
Iteration 9300: Loss = -11560.885376031552
2
Iteration 9400: Loss = -11560.890212660532
3
Iteration 9500: Loss = -11560.959162761634
4
Iteration 9600: Loss = -11560.886914431998
5
Iteration 9700: Loss = -11560.902897444845
6
Iteration 9800: Loss = -11560.937271107605
7
Iteration 9900: Loss = -11560.882629249025
Iteration 10000: Loss = -11560.881846246146
Iteration 10100: Loss = -11560.958332250024
1
Iteration 10200: Loss = -11560.881886668027
Iteration 10300: Loss = -11560.942875207495
1
Iteration 10400: Loss = -11560.888586530657
2
Iteration 10500: Loss = -11560.88913746045
3
Iteration 10600: Loss = -11560.890830986005
4
Iteration 10700: Loss = -11560.89147544231
5
Iteration 10800: Loss = -11560.885073426101
6
Iteration 10900: Loss = -11560.88738611183
7
Iteration 11000: Loss = -11560.899459341523
8
Iteration 11100: Loss = -11560.887053816587
9
Iteration 11200: Loss = -11560.88127151924
Iteration 11300: Loss = -11560.879568287311
Iteration 11400: Loss = -11560.882942510503
1
Iteration 11500: Loss = -11560.88424867769
2
Iteration 11600: Loss = -11560.883659264195
3
Iteration 11700: Loss = -11560.886580896344
4
Iteration 11800: Loss = -11560.879711756863
5
Iteration 11900: Loss = -11560.88398262911
6
Iteration 12000: Loss = -11560.937857794874
7
Iteration 12100: Loss = -11560.913707264155
8
Iteration 12200: Loss = -11560.880024133685
9
Iteration 12300: Loss = -11560.891056736737
10
Iteration 12400: Loss = -11560.883119343383
11
Iteration 12500: Loss = -11560.887684496185
12
Iteration 12600: Loss = -11560.879425495572
Iteration 12700: Loss = -11560.88285503274
1
Iteration 12800: Loss = -11560.879831992399
2
Iteration 12900: Loss = -11560.884688227407
3
Iteration 13000: Loss = -11560.879528311589
4
Iteration 13100: Loss = -11560.879621652659
5
Iteration 13200: Loss = -11560.943892996327
6
Iteration 13300: Loss = -11560.885278334465
7
Iteration 13400: Loss = -11560.892158838922
8
Iteration 13500: Loss = -11560.899723927449
9
Iteration 13600: Loss = -11560.880711866103
10
Iteration 13700: Loss = -11560.975573959917
11
Iteration 13800: Loss = -11560.900140090147
12
Iteration 13900: Loss = -11560.879493144741
Iteration 14000: Loss = -11560.88538490004
1
Iteration 14100: Loss = -11560.924162346184
2
Iteration 14200: Loss = -11560.898814264401
3
Iteration 14300: Loss = -11560.882051022012
4
Iteration 14400: Loss = -11560.880679982387
5
Iteration 14500: Loss = -11560.880621933913
6
Iteration 14600: Loss = -11560.879539933634
Iteration 14700: Loss = -11560.88077379101
1
Iteration 14800: Loss = -11560.881882591095
2
Iteration 14900: Loss = -11560.892890963112
3
Iteration 15000: Loss = -11560.883631174229
4
Iteration 15100: Loss = -11560.883178966362
5
Iteration 15200: Loss = -11560.89727336828
6
Iteration 15300: Loss = -11560.88189893873
7
Iteration 15400: Loss = -11560.898215693023
8
Iteration 15500: Loss = -11560.944926444434
9
Iteration 15600: Loss = -11560.923716608493
10
Iteration 15700: Loss = -11560.890201204184
11
Iteration 15800: Loss = -11560.898081654248
12
Iteration 15900: Loss = -11560.880621261924
13
Iteration 16000: Loss = -11560.879598723228
Iteration 16100: Loss = -11560.879843370045
1
Iteration 16200: Loss = -11560.880929539544
2
Iteration 16300: Loss = -11560.906924538536
3
Iteration 16400: Loss = -11560.886655380396
4
Iteration 16500: Loss = -11560.879373058115
Iteration 16600: Loss = -11560.882961101192
1
Iteration 16700: Loss = -11560.950936311276
2
Iteration 16800: Loss = -11560.895628826105
3
Iteration 16900: Loss = -11560.90227542247
4
Iteration 17000: Loss = -11560.879520084076
5
Iteration 17100: Loss = -11560.882429989806
6
Iteration 17200: Loss = -11560.881399015121
7
Iteration 17300: Loss = -11560.884198187156
8
Iteration 17400: Loss = -11560.88557015499
9
Iteration 17500: Loss = -11560.911667831728
10
Iteration 17600: Loss = -11560.88578143077
11
Iteration 17700: Loss = -11560.88598477629
12
Iteration 17800: Loss = -11560.89108785789
13
Iteration 17900: Loss = -11560.963428977739
14
Iteration 18000: Loss = -11560.87938772004
Iteration 18100: Loss = -11560.879533381432
1
Iteration 18200: Loss = -11560.885419037651
2
Iteration 18300: Loss = -11560.883657693576
3
Iteration 18400: Loss = -11560.879400588765
Iteration 18500: Loss = -11560.882561033457
1
Iteration 18600: Loss = -11560.879758753654
2
Iteration 18700: Loss = -11560.888634687542
3
Iteration 18800: Loss = -11560.884678688028
4
Iteration 18900: Loss = -11560.881684056789
5
Iteration 19000: Loss = -11560.880853360204
6
Iteration 19100: Loss = -11560.880256408649
7
Iteration 19200: Loss = -11560.881328209429
8
Iteration 19300: Loss = -11560.925746672963
9
Iteration 19400: Loss = -11560.888533026235
10
Iteration 19500: Loss = -11560.881521187266
11
Iteration 19600: Loss = -11561.01966687829
12
Iteration 19700: Loss = -11560.88079232614
13
Iteration 19800: Loss = -11560.886534577907
14
Iteration 19900: Loss = -11560.886138724944
15
Stopping early at iteration 19900 due to no improvement.
pi: tensor([[0.7127, 0.2873],
        [0.2195, 0.7805]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5885, 0.4115], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4047, 0.1083],
         [0.5201, 0.1946]],

        [[0.5974, 0.0959],
         [0.5699, 0.5624]],

        [[0.6206, 0.1081],
         [0.5252, 0.7263]],

        [[0.6322, 0.0986],
         [0.6772, 0.6257]],

        [[0.6534, 0.0976],
         [0.7098, 0.6179]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11567.952856942185
[1.0, 1.0] [1.0, 1.0] [11560.941888692725, 11560.886138724944]
-------------------------------------
This iteration is 26
True Objective function: Loss = -11364.878802382842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20989.89068580373
Iteration 100: Loss = -12043.939926745757
Iteration 200: Loss = -11466.403572636606
Iteration 300: Loss = -11361.434512964966
Iteration 400: Loss = -11358.24088716944
Iteration 500: Loss = -11357.983162319419
Iteration 600: Loss = -11357.840920686995
Iteration 700: Loss = -11357.723431570075
Iteration 800: Loss = -11357.666627607723
Iteration 900: Loss = -11357.627631842934
Iteration 1000: Loss = -11357.599061419258
Iteration 1100: Loss = -11357.57739720114
Iteration 1200: Loss = -11357.560492934952
Iteration 1300: Loss = -11357.547088514688
Iteration 1400: Loss = -11357.536115140369
Iteration 1500: Loss = -11357.527081085764
Iteration 1600: Loss = -11357.519525419686
Iteration 1700: Loss = -11357.513152851507
Iteration 1800: Loss = -11357.507696490102
Iteration 1900: Loss = -11357.503154177626
Iteration 2000: Loss = -11357.498938694727
Iteration 2100: Loss = -11357.495398180621
Iteration 2200: Loss = -11357.492602292807
Iteration 2300: Loss = -11357.489486896353
Iteration 2400: Loss = -11357.487098744494
Iteration 2500: Loss = -11357.493699737593
1
Iteration 2600: Loss = -11357.482986494213
Iteration 2700: Loss = -11357.48123185427
Iteration 2800: Loss = -11357.479675257315
Iteration 2900: Loss = -11357.47824623707
Iteration 3000: Loss = -11357.476992419877
Iteration 3100: Loss = -11357.475800777589
Iteration 3200: Loss = -11357.474843619893
Iteration 3300: Loss = -11357.473829254024
Iteration 3400: Loss = -11357.481143913425
1
Iteration 3500: Loss = -11357.472128961046
Iteration 3600: Loss = -11357.47144245458
Iteration 3700: Loss = -11357.47090541568
Iteration 3800: Loss = -11357.476098468911
1
Iteration 3900: Loss = -11357.469501167561
Iteration 4000: Loss = -11357.479254592634
1
Iteration 4100: Loss = -11357.468454022928
Iteration 4200: Loss = -11357.46797711236
Iteration 4300: Loss = -11357.467576613635
Iteration 4400: Loss = -11357.467202612257
Iteration 4500: Loss = -11357.466780362472
Iteration 4600: Loss = -11357.466552863503
Iteration 4700: Loss = -11357.468915470503
1
Iteration 4800: Loss = -11357.465806752418
Iteration 4900: Loss = -11357.46561603615
Iteration 5000: Loss = -11357.466896502441
1
Iteration 5100: Loss = -11357.465035298464
Iteration 5200: Loss = -11357.464853468582
Iteration 5300: Loss = -11357.469052603403
1
Iteration 5400: Loss = -11357.46587016631
2
Iteration 5500: Loss = -11357.465291252573
3
Iteration 5600: Loss = -11357.464035415382
Iteration 5700: Loss = -11357.464138652642
1
Iteration 5800: Loss = -11357.463664361681
Iteration 5900: Loss = -11357.4658080486
1
Iteration 6000: Loss = -11357.46352955769
Iteration 6100: Loss = -11357.47126754245
1
Iteration 6200: Loss = -11357.463019938501
Iteration 6300: Loss = -11357.46302323299
Iteration 6400: Loss = -11357.463967179629
1
Iteration 6500: Loss = -11357.464067035206
2
Iteration 6600: Loss = -11357.463833879267
3
Iteration 6700: Loss = -11357.462499964437
Iteration 6800: Loss = -11357.47502353231
1
Iteration 6900: Loss = -11357.472044204613
2
Iteration 7000: Loss = -11357.467421539919
3
Iteration 7100: Loss = -11357.46402446486
4
Iteration 7200: Loss = -11357.462331168324
Iteration 7300: Loss = -11357.463575715594
1
Iteration 7400: Loss = -11357.46261762937
2
Iteration 7500: Loss = -11357.462224690267
Iteration 7600: Loss = -11357.466669518742
1
Iteration 7700: Loss = -11357.463079680972
2
Iteration 7800: Loss = -11357.475361898316
3
Iteration 7900: Loss = -11357.461864119025
Iteration 8000: Loss = -11357.463301253194
1
Iteration 8100: Loss = -11357.461771492788
Iteration 8200: Loss = -11357.46216499345
1
Iteration 8300: Loss = -11357.461679910468
Iteration 8400: Loss = -11357.463026858659
1
Iteration 8500: Loss = -11357.480404205833
2
Iteration 8600: Loss = -11357.461537735642
Iteration 8700: Loss = -11357.485377796997
1
Iteration 8800: Loss = -11357.46161600263
Iteration 8900: Loss = -11357.464450776768
1
Iteration 9000: Loss = -11357.534322796942
2
Iteration 9100: Loss = -11357.462750777222
3
Iteration 9200: Loss = -11357.46395702565
4
Iteration 9300: Loss = -11357.475636432511
5
Iteration 9400: Loss = -11357.471760668626
6
Iteration 9500: Loss = -11357.46138959065
Iteration 9600: Loss = -11357.461692413573
1
Iteration 9700: Loss = -11357.49083595257
2
Iteration 9800: Loss = -11357.462678129528
3
Iteration 9900: Loss = -11357.50555051526
4
Iteration 10000: Loss = -11357.46316354214
5
Iteration 10100: Loss = -11357.4718938302
6
Iteration 10200: Loss = -11357.483075747361
7
Iteration 10300: Loss = -11357.465394933826
8
Iteration 10400: Loss = -11357.461356740305
Iteration 10500: Loss = -11357.465774990816
1
Iteration 10600: Loss = -11357.46975931506
2
Iteration 10700: Loss = -11357.461678840295
3
Iteration 10800: Loss = -11357.463930537673
4
Iteration 10900: Loss = -11357.463006883941
5
Iteration 11000: Loss = -11357.518973842021
6
Iteration 11100: Loss = -11357.471277330664
7
Iteration 11200: Loss = -11357.47629713487
8
Iteration 11300: Loss = -11357.461264698573
Iteration 11400: Loss = -11357.461676618224
1
Iteration 11500: Loss = -11357.461412809096
2
Iteration 11600: Loss = -11357.464365600446
3
Iteration 11700: Loss = -11357.497851923034
4
Iteration 11800: Loss = -11357.473791671311
5
Iteration 11900: Loss = -11357.469688338033
6
Iteration 12000: Loss = -11357.523740238366
7
Iteration 12100: Loss = -11357.47924655346
8
Iteration 12200: Loss = -11357.463516971626
9
Iteration 12300: Loss = -11357.461332635168
Iteration 12400: Loss = -11357.461365478828
Iteration 12500: Loss = -11357.464604624083
1
Iteration 12600: Loss = -11357.467850978495
2
Iteration 12700: Loss = -11357.461756802339
3
Iteration 12800: Loss = -11357.46902431315
4
Iteration 12900: Loss = -11357.491756464404
5
Iteration 13000: Loss = -11357.461081350815
Iteration 13100: Loss = -11357.462715304837
1
Iteration 13200: Loss = -11357.461509198049
2
Iteration 13300: Loss = -11357.550078792003
3
Iteration 13400: Loss = -11357.462890132723
4
Iteration 13500: Loss = -11357.463049953727
5
Iteration 13600: Loss = -11357.465845529128
6
Iteration 13700: Loss = -11357.461085324047
Iteration 13800: Loss = -11357.461765382102
1
Iteration 13900: Loss = -11357.474799157633
2
Iteration 14000: Loss = -11357.507324092421
3
Iteration 14100: Loss = -11357.527916339917
4
Iteration 14200: Loss = -11357.464007802218
5
Iteration 14300: Loss = -11357.46252375543
6
Iteration 14400: Loss = -11357.461238737002
7
Iteration 14500: Loss = -11357.46667084752
8
Iteration 14600: Loss = -11357.489799230127
9
Iteration 14700: Loss = -11357.461174806873
Iteration 14800: Loss = -11357.462024622988
1
Iteration 14900: Loss = -11357.464476425639
2
Iteration 15000: Loss = -11357.498230292005
3
Iteration 15100: Loss = -11357.462346109081
4
Iteration 15200: Loss = -11357.461392155486
5
Iteration 15300: Loss = -11357.46439878712
6
Iteration 15400: Loss = -11357.542241820469
7
Iteration 15500: Loss = -11357.461004745583
Iteration 15600: Loss = -11357.462210481046
1
Iteration 15700: Loss = -11357.469945527404
2
Iteration 15800: Loss = -11357.464525586876
3
Iteration 15900: Loss = -11357.462089236027
4
Iteration 16000: Loss = -11357.464730607446
5
Iteration 16100: Loss = -11357.461664905295
6
Iteration 16200: Loss = -11357.464591319293
7
Iteration 16300: Loss = -11357.463875741
8
Iteration 16400: Loss = -11357.544897705988
9
Iteration 16500: Loss = -11357.533532156292
10
Iteration 16600: Loss = -11357.463018603286
11
Iteration 16700: Loss = -11357.461756449093
12
Iteration 16800: Loss = -11357.461503764369
13
Iteration 16900: Loss = -11357.461787277389
14
Iteration 17000: Loss = -11357.472508504101
15
Stopping early at iteration 17000 due to no improvement.
pi: tensor([[0.7445, 0.2555],
        [0.2091, 0.7909]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4860, 0.5140], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3932, 0.0943],
         [0.7055, 0.2003]],

        [[0.5120, 0.0983],
         [0.6206, 0.6248]],

        [[0.5482, 0.1024],
         [0.7184, 0.7113]],

        [[0.6252, 0.0970],
         [0.6155, 0.6882]],

        [[0.5843, 0.0990],
         [0.7045, 0.6805]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9839987774932922
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20593.6129618803
Iteration 100: Loss = -12062.241936521548
Iteration 200: Loss = -12053.495222180756
Iteration 300: Loss = -11567.926728971666
Iteration 400: Loss = -11374.984506290504
Iteration 500: Loss = -11365.836883448068
Iteration 600: Loss = -11359.746942587604
Iteration 700: Loss = -11359.610181918497
Iteration 800: Loss = -11359.533720308384
Iteration 900: Loss = -11359.482684127324
Iteration 1000: Loss = -11359.446445179707
Iteration 1100: Loss = -11359.419311208947
Iteration 1200: Loss = -11359.39810640065
Iteration 1300: Loss = -11359.38056850177
Iteration 1400: Loss = -11359.364848322157
Iteration 1500: Loss = -11359.34683006795
Iteration 1600: Loss = -11359.32976376758
Iteration 1700: Loss = -11359.321800868893
Iteration 1800: Loss = -11359.315248751478
Iteration 1900: Loss = -11359.30967039115
Iteration 2000: Loss = -11359.30480347618
Iteration 2100: Loss = -11359.300402867826
Iteration 2200: Loss = -11359.296414654898
Iteration 2300: Loss = -11359.292652335338
Iteration 2400: Loss = -11359.289241224042
Iteration 2500: Loss = -11359.286259801172
Iteration 2600: Loss = -11359.283670327024
Iteration 2700: Loss = -11359.280758273424
Iteration 2800: Loss = -11357.647482668046
Iteration 2900: Loss = -11357.545935683362
Iteration 3000: Loss = -11357.538891650478
Iteration 3100: Loss = -11357.540826032962
1
Iteration 3200: Loss = -11357.536317026957
Iteration 3300: Loss = -11357.534670967862
Iteration 3400: Loss = -11357.533657055767
Iteration 3500: Loss = -11357.532711978572
Iteration 3600: Loss = -11357.531696669927
Iteration 3700: Loss = -11357.53533036215
1
Iteration 3800: Loss = -11357.530010308255
Iteration 3900: Loss = -11357.529280275074
Iteration 4000: Loss = -11357.52827856267
Iteration 4100: Loss = -11357.527462433003
Iteration 4200: Loss = -11357.526830639148
Iteration 4300: Loss = -11357.526252790774
Iteration 4400: Loss = -11357.52588842388
Iteration 4500: Loss = -11357.525316165125
Iteration 4600: Loss = -11357.524967101157
Iteration 4700: Loss = -11357.524481688017
Iteration 4800: Loss = -11357.524504312829
Iteration 4900: Loss = -11357.523788002953
Iteration 5000: Loss = -11357.523534711334
Iteration 5100: Loss = -11357.521079052227
Iteration 5200: Loss = -11357.47304378028
Iteration 5300: Loss = -11357.470582032623
Iteration 5400: Loss = -11357.474432340568
1
Iteration 5500: Loss = -11357.469981374217
Iteration 5600: Loss = -11357.470325384882
1
Iteration 5700: Loss = -11357.469742622856
Iteration 5800: Loss = -11357.46942529608
Iteration 5900: Loss = -11357.469159454819
Iteration 6000: Loss = -11357.469465326492
1
Iteration 6100: Loss = -11357.468784692894
Iteration 6200: Loss = -11357.468637612808
Iteration 6300: Loss = -11357.469801361665
1
Iteration 6400: Loss = -11357.469314024866
2
Iteration 6500: Loss = -11357.467200226942
Iteration 6600: Loss = -11357.465109005174
Iteration 6700: Loss = -11357.464916871233
Iteration 6800: Loss = -11357.46479760794
Iteration 6900: Loss = -11357.464663247765
Iteration 7000: Loss = -11357.464596214628
Iteration 7100: Loss = -11357.465737235596
1
Iteration 7200: Loss = -11357.465046120555
2
Iteration 7300: Loss = -11357.464435536947
Iteration 7400: Loss = -11357.464402549365
Iteration 7500: Loss = -11357.464127061128
Iteration 7600: Loss = -11357.464114086812
Iteration 7700: Loss = -11357.494577758453
1
Iteration 7800: Loss = -11357.476725670333
2
Iteration 7900: Loss = -11357.468240410679
3
Iteration 8000: Loss = -11357.465505267548
4
Iteration 8100: Loss = -11357.463605464269
Iteration 8200: Loss = -11357.467518447214
1
Iteration 8300: Loss = -11357.542094636388
2
Iteration 8400: Loss = -11357.464826931464
3
Iteration 8500: Loss = -11357.49989611063
4
Iteration 8600: Loss = -11357.464673393539
5
Iteration 8700: Loss = -11357.466009135424
6
Iteration 8800: Loss = -11357.463722785109
7
Iteration 8900: Loss = -11357.464665686683
8
Iteration 9000: Loss = -11357.462662079599
Iteration 9100: Loss = -11357.462338027908
Iteration 9200: Loss = -11357.480416723276
1
Iteration 9300: Loss = -11357.46447847192
2
Iteration 9400: Loss = -11357.462373202357
Iteration 9500: Loss = -11357.462815978803
1
Iteration 9600: Loss = -11357.4825037022
2
Iteration 9700: Loss = -11357.464599018866
3
Iteration 9800: Loss = -11357.477533399979
4
Iteration 9900: Loss = -11357.567810410774
5
Iteration 10000: Loss = -11357.462111042807
Iteration 10100: Loss = -11357.463187737689
1
Iteration 10200: Loss = -11357.482273306872
2
Iteration 10300: Loss = -11357.476948260884
3
Iteration 10400: Loss = -11357.462743892509
4
Iteration 10500: Loss = -11357.462378127995
5
Iteration 10600: Loss = -11357.466550942754
6
Iteration 10700: Loss = -11357.461689140882
Iteration 10800: Loss = -11357.466691977426
1
Iteration 10900: Loss = -11357.470675559813
2
Iteration 11000: Loss = -11357.461322406592
Iteration 11100: Loss = -11357.462376180943
1
Iteration 11200: Loss = -11357.465716372613
2
Iteration 11300: Loss = -11357.60722309544
3
Iteration 11400: Loss = -11357.462871261236
4
Iteration 11500: Loss = -11357.463549905324
5
Iteration 11600: Loss = -11357.563155728249
6
Iteration 11700: Loss = -11357.48115371457
7
Iteration 11800: Loss = -11357.461914583979
8
Iteration 11900: Loss = -11357.46125735335
Iteration 12000: Loss = -11357.462412831595
1
Iteration 12100: Loss = -11357.537144035427
2
Iteration 12200: Loss = -11357.463414725833
3
Iteration 12300: Loss = -11357.464381269625
4
Iteration 12400: Loss = -11357.482184982466
5
Iteration 12500: Loss = -11357.46184876223
6
Iteration 12600: Loss = -11357.464352977748
7
Iteration 12700: Loss = -11357.461324637337
Iteration 12800: Loss = -11357.462108463327
1
Iteration 12900: Loss = -11357.48312766327
2
Iteration 13000: Loss = -11357.567102156723
3
Iteration 13100: Loss = -11357.461298957382
Iteration 13200: Loss = -11357.461863926686
1
Iteration 13300: Loss = -11357.465872065284
2
Iteration 13400: Loss = -11357.465473471813
3
Iteration 13500: Loss = -11357.463465630106
4
Iteration 13600: Loss = -11357.461347407896
Iteration 13700: Loss = -11357.463704957543
1
Iteration 13800: Loss = -11357.579265820788
2
Iteration 13900: Loss = -11357.46426966694
3
Iteration 14000: Loss = -11357.461583824692
4
Iteration 14100: Loss = -11357.46290390651
5
Iteration 14200: Loss = -11357.505900860251
6
Iteration 14300: Loss = -11357.461234204071
Iteration 14400: Loss = -11357.46284110099
1
Iteration 14500: Loss = -11357.467899947407
2
Iteration 14600: Loss = -11357.464585316276
3
Iteration 14700: Loss = -11357.463265431443
4
Iteration 14800: Loss = -11357.559365190324
5
Iteration 14900: Loss = -11357.478641669819
6
Iteration 15000: Loss = -11357.462981613771
7
Iteration 15100: Loss = -11357.465259966046
8
Iteration 15200: Loss = -11357.469088879554
9
Iteration 15300: Loss = -11357.476552899947
10
Iteration 15400: Loss = -11357.461928440378
11
Iteration 15500: Loss = -11357.465066507875
12
Iteration 15600: Loss = -11357.462223786979
13
Iteration 15700: Loss = -11357.475509676791
14
Iteration 15800: Loss = -11357.584839688365
15
Stopping early at iteration 15800 due to no improvement.
pi: tensor([[0.7912, 0.2088],
        [0.2553, 0.7447]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5204, 0.4796], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.0954],
         [0.6129, 0.3961]],

        [[0.7219, 0.0994],
         [0.5983, 0.6302]],

        [[0.7075, 0.1036],
         [0.6103, 0.6096]],

        [[0.6663, 0.0973],
         [0.5131, 0.6776]],

        [[0.5725, 0.0986],
         [0.6619, 0.7156]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318395231936
Average Adjusted Rand Index: 0.9839987774932922
11364.878802382842
[0.9840318395231936, 0.9840318395231936] [0.9839987774932922, 0.9839987774932922] [11357.472508504101, 11357.584839688365]
-------------------------------------
This iteration is 27
True Objective function: Loss = -11422.549392962863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22782.38478011012
Iteration 100: Loss = -11765.211369313276
Iteration 200: Loss = -11699.167357667888
Iteration 300: Loss = -11653.28793542512
Iteration 400: Loss = -11607.549904152786
Iteration 500: Loss = -11590.422679564163
Iteration 600: Loss = -11589.527469391098
Iteration 700: Loss = -11589.125179825855
Iteration 800: Loss = -11587.58407856351
Iteration 900: Loss = -11587.437539194683
Iteration 1000: Loss = -11581.21308239731
Iteration 1100: Loss = -11573.397374573458
Iteration 1200: Loss = -11561.53072383075
Iteration 1300: Loss = -11561.156840418673
Iteration 1400: Loss = -11533.66878729283
Iteration 1500: Loss = -11530.959014256177
Iteration 1600: Loss = -11530.9278895629
Iteration 1700: Loss = -11528.965043468406
Iteration 1800: Loss = -11522.621919587313
Iteration 1900: Loss = -11512.68848691591
Iteration 2000: Loss = -11499.34652168198
Iteration 2100: Loss = -11487.99969094152
Iteration 2200: Loss = -11458.682362102316
Iteration 2300: Loss = -11458.648087863476
Iteration 2400: Loss = -11441.881299813818
Iteration 2500: Loss = -11441.852128526976
Iteration 2600: Loss = -11441.841857599642
Iteration 2700: Loss = -11433.79915268036
Iteration 2800: Loss = -11423.022432029938
Iteration 2900: Loss = -11423.004415872669
Iteration 3000: Loss = -11422.949675650521
Iteration 3100: Loss = -11412.416279204317
Iteration 3200: Loss = -11412.41458434665
Iteration 3300: Loss = -11412.413777009504
Iteration 3400: Loss = -11412.41157283078
Iteration 3500: Loss = -11412.410207508185
Iteration 3600: Loss = -11412.409806592603
Iteration 3700: Loss = -11412.408429046774
Iteration 3800: Loss = -11412.407547933073
Iteration 3900: Loss = -11412.409025153567
1
Iteration 4000: Loss = -11412.401919221898
Iteration 4100: Loss = -11412.270740463717
Iteration 4200: Loss = -11412.271118028068
1
Iteration 4300: Loss = -11412.269603455416
Iteration 4400: Loss = -11412.271970402155
1
Iteration 4500: Loss = -11412.2689540116
Iteration 4600: Loss = -11412.277141086946
1
Iteration 4700: Loss = -11412.268753995348
Iteration 4800: Loss = -11412.2711435703
1
Iteration 4900: Loss = -11412.281690384494
2
Iteration 5000: Loss = -11412.268261400637
Iteration 5100: Loss = -11412.267736030339
Iteration 5200: Loss = -11412.2704702288
1
Iteration 5300: Loss = -11412.268215779557
2
Iteration 5400: Loss = -11412.274968614132
3
Iteration 5500: Loss = -11412.267128602743
Iteration 5600: Loss = -11412.27022081955
1
Iteration 5700: Loss = -11412.26808558496
2
Iteration 5800: Loss = -11412.26639958462
Iteration 5900: Loss = -11412.265269232345
Iteration 6000: Loss = -11412.23866517024
Iteration 6100: Loss = -11412.245024060043
1
Iteration 6200: Loss = -11412.238169330265
Iteration 6300: Loss = -11412.238196961194
Iteration 6400: Loss = -11412.237616032444
Iteration 6500: Loss = -11412.242790460432
1
Iteration 6600: Loss = -11412.23886940684
2
Iteration 6700: Loss = -11412.237206478949
Iteration 6800: Loss = -11412.237188115272
Iteration 6900: Loss = -11412.237303409434
1
Iteration 7000: Loss = -11412.238326794437
2
Iteration 7100: Loss = -11412.244276028443
3
Iteration 7200: Loss = -11412.238212132283
4
Iteration 7300: Loss = -11412.237113953533
Iteration 7400: Loss = -11412.236919687502
Iteration 7500: Loss = -11412.23680005426
Iteration 7600: Loss = -11412.236890824934
Iteration 7700: Loss = -11412.24675788916
1
Iteration 7800: Loss = -11412.254085214863
2
Iteration 7900: Loss = -11412.236660693776
Iteration 8000: Loss = -11412.236754305883
Iteration 8100: Loss = -11412.236660493236
Iteration 8200: Loss = -11412.238000457484
1
Iteration 8300: Loss = -11412.236588097412
Iteration 8400: Loss = -11412.246913570223
1
Iteration 8500: Loss = -11412.23654852534
Iteration 8600: Loss = -11412.236533686799
Iteration 8700: Loss = -11412.274282658338
1
Iteration 8800: Loss = -11412.236455129827
Iteration 8900: Loss = -11412.236444385311
Iteration 9000: Loss = -11412.236825409149
1
Iteration 9100: Loss = -11412.240847891886
2
Iteration 9200: Loss = -11412.259051415811
3
Iteration 9300: Loss = -11412.236462507992
Iteration 9400: Loss = -11412.236507294783
Iteration 9500: Loss = -11412.282943312408
1
Iteration 9600: Loss = -11412.239214214442
2
Iteration 9700: Loss = -11412.250287236446
3
Iteration 9800: Loss = -11412.30991745093
4
Iteration 9900: Loss = -11412.237073616236
5
Iteration 10000: Loss = -11412.239665291498
6
Iteration 10100: Loss = -11412.242744598734
7
Iteration 10200: Loss = -11412.244454796859
8
Iteration 10300: Loss = -11412.269627436832
9
Iteration 10400: Loss = -11412.23619370118
Iteration 10500: Loss = -11412.236144821309
Iteration 10600: Loss = -11412.238312686084
1
Iteration 10700: Loss = -11412.240838316182
2
Iteration 10800: Loss = -11412.247655940695
3
Iteration 10900: Loss = -11412.242990444887
4
Iteration 11000: Loss = -11412.24092243352
5
Iteration 11100: Loss = -11412.236661792545
6
Iteration 11200: Loss = -11412.23760562425
7
Iteration 11300: Loss = -11412.237829439524
8
Iteration 11400: Loss = -11412.242864029913
9
Iteration 11500: Loss = -11412.288765023024
10
Iteration 11600: Loss = -11412.261930614639
11
Iteration 11700: Loss = -11412.244949327709
12
Iteration 11800: Loss = -11412.238076887452
13
Iteration 11900: Loss = -11412.236363646622
14
Iteration 12000: Loss = -11412.239924858046
15
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[0.6805, 0.3195],
        [0.2284, 0.7716]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5248, 0.4752], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4058, 0.1001],
         [0.6084, 0.2020]],

        [[0.7147, 0.0954],
         [0.5414, 0.5571]],

        [[0.5016, 0.1049],
         [0.5064, 0.5538]],

        [[0.6240, 0.0973],
         [0.5724, 0.6823]],

        [[0.6944, 0.1002],
         [0.5420, 0.5645]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.976094824644038
Average Adjusted Rand Index: 0.9761611274691304
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20402.562705335393
Iteration 100: Loss = -12114.383019965302
Iteration 200: Loss = -12108.553932580968
Iteration 300: Loss = -12103.953447976532
Iteration 400: Loss = -12102.906874118677
Iteration 500: Loss = -12101.916508192684
Iteration 600: Loss = -11957.92947531759
Iteration 700: Loss = -11732.690069194752
Iteration 800: Loss = -11720.920420906123
Iteration 900: Loss = -11715.288799366577
Iteration 1000: Loss = -11708.949486998268
Iteration 1100: Loss = -11708.760592985598
Iteration 1200: Loss = -11707.461910019065
Iteration 1300: Loss = -11693.368407312571
Iteration 1400: Loss = -11692.72987704487
Iteration 1500: Loss = -11692.606503869565
Iteration 1600: Loss = -11692.594463710971
Iteration 1700: Loss = -11692.583154408956
Iteration 1800: Loss = -11692.534978884774
Iteration 1900: Loss = -11692.560900061168
1
Iteration 2000: Loss = -11692.525343573057
Iteration 2100: Loss = -11692.52115008188
Iteration 2200: Loss = -11692.516943405211
Iteration 2300: Loss = -11692.507275213404
Iteration 2400: Loss = -11692.500987064679
Iteration 2500: Loss = -11692.499135324226
Iteration 2600: Loss = -11692.497500054043
Iteration 2700: Loss = -11692.496123157589
Iteration 2800: Loss = -11692.49482124738
Iteration 2900: Loss = -11692.493644048063
Iteration 3000: Loss = -11692.49254727282
Iteration 3100: Loss = -11692.492885667376
1
Iteration 3200: Loss = -11692.490734229574
Iteration 3300: Loss = -11692.49502789438
1
Iteration 3400: Loss = -11692.48932233865
Iteration 3500: Loss = -11692.489731767693
1
Iteration 3600: Loss = -11692.488091840398
Iteration 3700: Loss = -11692.487451141664
Iteration 3800: Loss = -11692.486228434265
Iteration 3900: Loss = -11692.485007494413
Iteration 4000: Loss = -11692.483300148175
Iteration 4100: Loss = -11692.482207866407
Iteration 4200: Loss = -11692.378523271664
Iteration 4300: Loss = -11692.373982947798
Iteration 4400: Loss = -11689.161679051382
Iteration 4500: Loss = -11689.15184658024
Iteration 4600: Loss = -11689.15020289156
Iteration 4700: Loss = -11689.072332821317
Iteration 4800: Loss = -11689.067528576508
Iteration 4900: Loss = -11689.078852459808
1
Iteration 5000: Loss = -11689.066851522588
Iteration 5100: Loss = -11689.066740745688
Iteration 5200: Loss = -11689.066562623719
Iteration 5300: Loss = -11689.066160689124
Iteration 5400: Loss = -11689.069642837874
1
Iteration 5500: Loss = -11689.065676287739
Iteration 5600: Loss = -11689.074260947009
1
Iteration 5700: Loss = -11689.065390084003
Iteration 5800: Loss = -11689.065208624123
Iteration 5900: Loss = -11689.064675531074
Iteration 6000: Loss = -11686.254163693828
Iteration 6100: Loss = -11676.012323891518
Iteration 6200: Loss = -11675.911816840297
Iteration 6300: Loss = -11675.909540010922
Iteration 6400: Loss = -11675.907422158434
Iteration 6500: Loss = -11675.90001703757
Iteration 6600: Loss = -11675.878140106297
Iteration 6700: Loss = -11675.87598009077
Iteration 6800: Loss = -11675.88040991145
1
Iteration 6900: Loss = -11675.87639380972
2
Iteration 7000: Loss = -11675.876184255296
3
Iteration 7100: Loss = -11675.87241795586
Iteration 7200: Loss = -11675.869569987517
Iteration 7300: Loss = -11675.871111951308
1
Iteration 7400: Loss = -11675.866430359994
Iteration 7500: Loss = -11675.72635312404
Iteration 7600: Loss = -11675.727146977715
1
Iteration 7700: Loss = -11675.726511133857
2
Iteration 7800: Loss = -11675.742678030463
3
Iteration 7900: Loss = -11675.726039328441
Iteration 8000: Loss = -11675.726963109682
1
Iteration 8100: Loss = -11675.725909671526
Iteration 8200: Loss = -11675.725886163953
Iteration 8300: Loss = -11675.74916722496
1
Iteration 8400: Loss = -11675.725899143708
Iteration 8500: Loss = -11675.725552632155
Iteration 8600: Loss = -11675.725306741757
Iteration 8700: Loss = -11675.72599346267
1
Iteration 8800: Loss = -11675.728666256762
2
Iteration 8900: Loss = -11675.707765150231
Iteration 9000: Loss = -11675.987163169459
1
Iteration 9100: Loss = -11675.707531582471
Iteration 9200: Loss = -11675.72687565603
1
Iteration 9300: Loss = -11675.708958620395
2
Iteration 9400: Loss = -11675.707489429471
Iteration 9500: Loss = -11675.70765804821
1
Iteration 9600: Loss = -11675.707633277165
2
Iteration 9700: Loss = -11675.707423697126
Iteration 9800: Loss = -11675.708689401237
1
Iteration 9900: Loss = -11675.708785656303
2
Iteration 10000: Loss = -11675.707881088269
3
Iteration 10100: Loss = -11673.237439623546
Iteration 10200: Loss = -11673.138551842183
Iteration 10300: Loss = -11673.139217339489
1
Iteration 10400: Loss = -11673.380046906106
2
Iteration 10500: Loss = -11673.138057289621
Iteration 10600: Loss = -11673.147508055541
1
Iteration 10700: Loss = -11673.137354725535
Iteration 10800: Loss = -11673.141556680688
1
Iteration 10900: Loss = -11673.092437555706
Iteration 11000: Loss = -11673.104572256994
1
Iteration 11100: Loss = -11673.089781668543
Iteration 11200: Loss = -11673.095595696583
1
Iteration 11300: Loss = -11673.088607150836
Iteration 11400: Loss = -11673.088361649947
Iteration 11500: Loss = -11673.101311613878
1
Iteration 11600: Loss = -11673.088348457199
Iteration 11700: Loss = -11673.089062654044
1
Iteration 11800: Loss = -11673.096687791483
2
Iteration 11900: Loss = -11673.088911342034
3
Iteration 12000: Loss = -11673.098327110756
4
Iteration 12100: Loss = -11673.088670678462
5
Iteration 12200: Loss = -11673.102938902926
6
Iteration 12300: Loss = -11673.0882306094
Iteration 12400: Loss = -11673.090171911204
1
Iteration 12500: Loss = -11673.08817735256
Iteration 12600: Loss = -11673.091084426916
1
Iteration 12700: Loss = -11673.089000694332
2
Iteration 12800: Loss = -11673.092050855978
3
Iteration 12900: Loss = -11673.090282745434
4
Iteration 13000: Loss = -11673.092132741627
5
Iteration 13100: Loss = -11673.100998135513
6
Iteration 13200: Loss = -11673.089686963192
7
Iteration 13300: Loss = -11673.088696669078
8
Iteration 13400: Loss = -11673.092033347788
9
Iteration 13500: Loss = -11673.0881787284
Iteration 13600: Loss = -11673.10972079284
1
Iteration 13700: Loss = -11673.088079787836
Iteration 13800: Loss = -11673.090362298344
1
Iteration 13900: Loss = -11673.08818637566
2
Iteration 14000: Loss = -11673.090100218256
3
Iteration 14100: Loss = -11673.089974769604
4
Iteration 14200: Loss = -11673.090394979261
5
Iteration 14300: Loss = -11673.088217234226
6
Iteration 14400: Loss = -11673.088237648011
7
Iteration 14500: Loss = -11673.094680278175
8
Iteration 14600: Loss = -11673.088110657927
Iteration 14700: Loss = -11673.105101674359
1
Iteration 14800: Loss = -11673.09012253978
2
Iteration 14900: Loss = -11673.093586293853
3
Iteration 15000: Loss = -11673.088092395912
Iteration 15100: Loss = -11673.093074764653
1
Iteration 15200: Loss = -11673.088158844148
Iteration 15300: Loss = -11673.027334940072
Iteration 15400: Loss = -11673.028962048667
1
Iteration 15500: Loss = -11673.021367244168
Iteration 15600: Loss = -11673.022649384471
1
Iteration 15700: Loss = -11673.024241259536
2
Iteration 15800: Loss = -11673.026820166191
3
Iteration 15900: Loss = -11673.022656395462
4
Iteration 16000: Loss = -11673.023856054459
5
Iteration 16100: Loss = -11673.023004325396
6
Iteration 16200: Loss = -11673.021210109224
Iteration 16300: Loss = -11673.024515977153
1
Iteration 16400: Loss = -11673.021473119388
2
Iteration 16500: Loss = -11672.68126362052
Iteration 16600: Loss = -11672.508661708793
Iteration 16700: Loss = -11672.515390109802
1
Iteration 16800: Loss = -11672.510581522658
2
Iteration 16900: Loss = -11672.515470436752
3
Iteration 17000: Loss = -11672.528010800805
4
Iteration 17100: Loss = -11672.50798467813
Iteration 17200: Loss = -11672.531132488833
1
Iteration 17300: Loss = -11672.509266748326
2
Iteration 17400: Loss = -11672.507920685102
Iteration 17500: Loss = -11672.508176473824
1
Iteration 17600: Loss = -11672.521711334979
2
Iteration 17700: Loss = -11672.778448728737
3
Iteration 17800: Loss = -11672.508600632747
4
Iteration 17900: Loss = -11672.508013559544
Iteration 18000: Loss = -11672.508091670148
Iteration 18100: Loss = -11672.50580141265
Iteration 18200: Loss = -11672.465189228928
Iteration 18300: Loss = -11672.485720705708
1
Iteration 18400: Loss = -11672.463657138334
Iteration 18500: Loss = -11672.4633754111
Iteration 18600: Loss = -11672.46531338988
1
Iteration 18700: Loss = -11672.463435332296
Iteration 18800: Loss = -11672.49954716593
1
Iteration 18900: Loss = -11672.548647492365
2
Iteration 19000: Loss = -11672.50104792896
3
Iteration 19100: Loss = -11672.46366502951
4
Iteration 19200: Loss = -11672.465201460247
5
Iteration 19300: Loss = -11672.463249304774
Iteration 19400: Loss = -11672.463601003778
1
Iteration 19500: Loss = -11672.465121584837
2
Iteration 19600: Loss = -11672.474301935163
3
Iteration 19700: Loss = -11672.514422810975
4
Iteration 19800: Loss = -11672.463410123095
5
Iteration 19900: Loss = -11672.46808748167
6
pi: tensor([[0.5172, 0.4828],
        [0.6890, 0.3110]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6180, 0.3820], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2451, 0.0960],
         [0.5553, 0.3533]],

        [[0.5385, 0.0954],
         [0.5005, 0.6763]],

        [[0.7157, 0.1047],
         [0.6469, 0.5770]],

        [[0.5688, 0.0996],
         [0.5215, 0.6352]],

        [[0.6294, 0.1003],
         [0.6390, 0.5660]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026782461542077
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 18
Adjusted Rand Index: 0.4036363636363636
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.08798780026591972
Average Adjusted Rand Index: 0.813247974461295
11422.549392962863
[0.976094824644038, 0.08798780026591972] [0.9761611274691304, 0.813247974461295] [11412.239924858046, 11672.463256863568]
-------------------------------------
This iteration is 28
True Objective function: Loss = -11692.712889377
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21609.366939412517
Iteration 100: Loss = -12502.835890540178
Iteration 200: Loss = -12452.958562781025
Iteration 300: Loss = -12032.003469679985
Iteration 400: Loss = -11891.764548601192
Iteration 500: Loss = -11860.001404400378
Iteration 600: Loss = -11855.062532512526
Iteration 700: Loss = -11854.601713835862
Iteration 800: Loss = -11854.298436449415
Iteration 900: Loss = -11853.93494162439
Iteration 1000: Loss = -11851.397195718188
Iteration 1100: Loss = -11845.46839018644
Iteration 1200: Loss = -11842.724340073033
Iteration 1300: Loss = -11841.052321364365
Iteration 1400: Loss = -11840.88165352609
Iteration 1500: Loss = -11840.64262656892
Iteration 1600: Loss = -11840.56051080205
Iteration 1700: Loss = -11837.418749654878
Iteration 1800: Loss = -11837.377743186038
Iteration 1900: Loss = -11837.351886391
Iteration 2000: Loss = -11837.330700108447
Iteration 2100: Loss = -11837.3123764253
Iteration 2200: Loss = -11837.295042825992
Iteration 2300: Loss = -11837.25486931288
Iteration 2400: Loss = -11837.244887213074
Iteration 2500: Loss = -11837.236966069333
Iteration 2600: Loss = -11837.229928208812
Iteration 2700: Loss = -11837.223612006153
Iteration 2800: Loss = -11837.218018189464
Iteration 2900: Loss = -11837.21291344774
Iteration 3000: Loss = -11837.20808066217
Iteration 3100: Loss = -11837.20358317816
Iteration 3200: Loss = -11837.202737016323
Iteration 3300: Loss = -11837.17761712881
Iteration 3400: Loss = -11837.173163469502
Iteration 3500: Loss = -11837.175431492475
1
Iteration 3600: Loss = -11837.166711365791
Iteration 3700: Loss = -11837.164820913473
Iteration 3800: Loss = -11837.162225107319
Iteration 3900: Loss = -11837.161940208553
Iteration 4000: Loss = -11837.158494426978
Iteration 4100: Loss = -11837.157026913153
Iteration 4200: Loss = -11837.15525482161
Iteration 4300: Loss = -11837.15888326964
1
Iteration 4400: Loss = -11837.152517018876
Iteration 4500: Loss = -11837.15791546066
1
Iteration 4600: Loss = -11837.150133812385
Iteration 4700: Loss = -11837.149178681864
Iteration 4800: Loss = -11837.148128352417
Iteration 4900: Loss = -11837.14724813322
Iteration 5000: Loss = -11837.146433728429
Iteration 5100: Loss = -11837.145653557449
Iteration 5200: Loss = -11837.1452127178
Iteration 5300: Loss = -11837.144202773838
Iteration 5400: Loss = -11837.161162162782
1
Iteration 5500: Loss = -11837.142967724434
Iteration 5600: Loss = -11837.142397069227
Iteration 5700: Loss = -11837.142169660417
Iteration 5800: Loss = -11837.141419789341
Iteration 5900: Loss = -11837.142026424533
1
Iteration 6000: Loss = -11837.140897382897
Iteration 6100: Loss = -11837.145254681072
1
Iteration 6200: Loss = -11837.140972581
Iteration 6300: Loss = -11837.142076794375
1
Iteration 6400: Loss = -11837.138921264455
Iteration 6500: Loss = -11837.138696262207
Iteration 6600: Loss = -11837.138271969094
Iteration 6700: Loss = -11837.138038287138
Iteration 6800: Loss = -11837.138739707016
1
Iteration 6900: Loss = -11837.137642222286
Iteration 7000: Loss = -11837.137270234784
Iteration 7100: Loss = -11837.137032173745
Iteration 7200: Loss = -11837.13697812979
Iteration 7300: Loss = -11837.138682395782
1
Iteration 7400: Loss = -11837.137715313651
2
Iteration 7500: Loss = -11837.136010547
Iteration 7600: Loss = -11837.135890451751
Iteration 7700: Loss = -11837.13724782716
1
Iteration 7800: Loss = -11837.13786679229
2
Iteration 7900: Loss = -11837.13764549379
3
Iteration 8000: Loss = -11837.150968234244
4
Iteration 8100: Loss = -11837.142727093984
5
Iteration 8200: Loss = -11837.135457423588
Iteration 8300: Loss = -11837.13639930738
1
Iteration 8400: Loss = -11837.135200055478
Iteration 8500: Loss = -11837.144994214563
1
Iteration 8600: Loss = -11837.15286042296
2
Iteration 8700: Loss = -11837.134179449005
Iteration 8800: Loss = -11837.136146062596
1
Iteration 8900: Loss = -11837.135574496126
2
Iteration 9000: Loss = -11837.197588836556
3
Iteration 9100: Loss = -11837.15713869706
4
Iteration 9200: Loss = -11837.140820136781
5
Iteration 9300: Loss = -11837.135954021202
6
Iteration 9400: Loss = -11837.13434074608
7
Iteration 9500: Loss = -11837.134098051178
Iteration 9600: Loss = -11837.135164318135
1
Iteration 9700: Loss = -11837.142463635037
2
Iteration 9800: Loss = -11837.137998717206
3
Iteration 9900: Loss = -11837.142571347738
4
Iteration 10000: Loss = -11837.139164811078
5
Iteration 10100: Loss = -11837.134942139088
6
Iteration 10200: Loss = -11837.135422698853
7
Iteration 10300: Loss = -11837.133641495328
Iteration 10400: Loss = -11837.133678022208
Iteration 10500: Loss = -11837.150240382794
1
Iteration 10600: Loss = -11837.133341238354
Iteration 10700: Loss = -11837.133463302704
1
Iteration 10800: Loss = -11837.135645722163
2
Iteration 10900: Loss = -11837.137286480947
3
Iteration 11000: Loss = -11837.14947391067
4
Iteration 11100: Loss = -11837.13990373772
5
Iteration 11200: Loss = -11837.134021329637
6
Iteration 11300: Loss = -11837.133932482773
7
Iteration 11400: Loss = -11837.142664198233
8
Iteration 11500: Loss = -11837.140306878493
9
Iteration 11600: Loss = -11837.139975332224
10
Iteration 11700: Loss = -11837.133054596481
Iteration 11800: Loss = -11837.176296040325
1
Iteration 11900: Loss = -11837.146223977908
2
Iteration 12000: Loss = -11837.133470217626
3
Iteration 12100: Loss = -11837.18560558152
4
Iteration 12200: Loss = -11837.134812275879
5
Iteration 12300: Loss = -11837.138591761464
6
Iteration 12400: Loss = -11837.167985208403
7
Iteration 12500: Loss = -11837.13365046167
8
Iteration 12600: Loss = -11837.134639451875
9
Iteration 12700: Loss = -11837.17690433697
10
Iteration 12800: Loss = -11837.135388203937
11
Iteration 12900: Loss = -11837.134383589791
12
Iteration 13000: Loss = -11837.14047639315
13
Iteration 13100: Loss = -11837.14183272305
14
Iteration 13200: Loss = -11837.136360065786
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[0.7917, 0.2083],
        [0.3387, 0.6613]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1050, 0.8950], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3912, 0.0950],
         [0.7164, 0.2102]],

        [[0.5086, 0.0967],
         [0.5984, 0.5810]],

        [[0.5158, 0.1056],
         [0.5017, 0.5416]],

        [[0.5979, 0.1086],
         [0.6010, 0.6518]],

        [[0.7147, 0.0958],
         [0.6084, 0.5865]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6201871993900662
Average Adjusted Rand Index: 0.7987070707070707
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22577.657734724744
Iteration 100: Loss = -12500.724224740983
Iteration 200: Loss = -12346.144406024612
Iteration 300: Loss = -12087.913063967666
Iteration 400: Loss = -12044.168038669999
Iteration 500: Loss = -12035.073527223378
Iteration 600: Loss = -12021.634372924174
Iteration 700: Loss = -12021.233916226452
Iteration 800: Loss = -11987.588887649525
Iteration 900: Loss = -11981.031598890748
Iteration 1000: Loss = -11970.890645130248
Iteration 1100: Loss = -11961.977819761769
Iteration 1200: Loss = -11961.37881165232
Iteration 1300: Loss = -11961.324303739326
Iteration 1400: Loss = -11961.29658974869
Iteration 1500: Loss = -11959.708457392551
Iteration 1600: Loss = -11951.029350281542
Iteration 1700: Loss = -11950.752142477422
Iteration 1800: Loss = -11950.7223623584
Iteration 1900: Loss = -11950.666769330532
Iteration 2000: Loss = -11950.630796126927
Iteration 2100: Loss = -11950.618936760711
Iteration 2200: Loss = -11950.613857101047
Iteration 2300: Loss = -11950.609963890845
Iteration 2400: Loss = -11950.604421237382
Iteration 2500: Loss = -11950.599003310494
Iteration 2600: Loss = -11950.598570915541
Iteration 2700: Loss = -11950.594531393557
Iteration 2800: Loss = -11950.595662161117
1
Iteration 2900: Loss = -11950.590873388812
Iteration 3000: Loss = -11950.589467047406
Iteration 3100: Loss = -11950.587924055453
Iteration 3200: Loss = -11950.586496224372
Iteration 3300: Loss = -11950.585978222256
Iteration 3400: Loss = -11950.584965641645
Iteration 3500: Loss = -11950.588521144087
1
Iteration 3600: Loss = -11950.582972942753
Iteration 3700: Loss = -11950.584404870278
1
Iteration 3800: Loss = -11950.590957677177
2
Iteration 3900: Loss = -11950.580749671568
Iteration 4000: Loss = -11950.579371852124
Iteration 4100: Loss = -11950.579785375341
1
Iteration 4200: Loss = -11950.58297058931
2
Iteration 4300: Loss = -11950.578856377002
Iteration 4400: Loss = -11950.577334186617
Iteration 4500: Loss = -11950.602190867045
1
Iteration 4600: Loss = -11950.575186081933
Iteration 4700: Loss = -11950.572912110098
Iteration 4800: Loss = -11950.445874424704
Iteration 4900: Loss = -11950.445955517238
Iteration 5000: Loss = -11950.44512673723
Iteration 5100: Loss = -11950.444680096994
Iteration 5200: Loss = -11950.444072660279
Iteration 5300: Loss = -11950.443192494666
Iteration 5400: Loss = -11950.441378882502
Iteration 5500: Loss = -11950.442205087844
1
Iteration 5600: Loss = -11950.44572889444
2
Iteration 5700: Loss = -11950.446359359872
3
Iteration 5800: Loss = -11950.45232478396
4
Iteration 5900: Loss = -11950.44674950057
5
Iteration 6000: Loss = -11950.443362818656
6
Iteration 6100: Loss = -11950.439971539441
Iteration 6200: Loss = -11950.439839138437
Iteration 6300: Loss = -11950.445226487784
1
Iteration 6400: Loss = -11950.43959160002
Iteration 6500: Loss = -11950.449701937889
1
Iteration 6600: Loss = -11950.439343151576
Iteration 6700: Loss = -11950.44589959695
1
Iteration 6800: Loss = -11950.439149775639
Iteration 6900: Loss = -11950.458123679191
1
Iteration 7000: Loss = -11950.438958041617
Iteration 7100: Loss = -11950.439923764654
1
Iteration 7200: Loss = -11950.438741615535
Iteration 7300: Loss = -11950.438864987136
1
Iteration 7400: Loss = -11950.438549334476
Iteration 7500: Loss = -11950.439578539754
1
Iteration 7600: Loss = -11950.438454713447
Iteration 7700: Loss = -11950.438368839325
Iteration 7800: Loss = -11950.438411727344
Iteration 7900: Loss = -11950.43824636629
Iteration 8000: Loss = -11950.437019007888
Iteration 8100: Loss = -11949.250793337285
Iteration 8200: Loss = -11949.250391447382
Iteration 8300: Loss = -11949.250326506408
Iteration 8400: Loss = -11949.260827528802
1
Iteration 8500: Loss = -11949.250226289174
Iteration 8600: Loss = -11949.250209475967
Iteration 8700: Loss = -11949.448218946027
1
Iteration 8800: Loss = -11949.250174421884
Iteration 8900: Loss = -11949.25046339269
1
Iteration 9000: Loss = -11949.251454644198
2
Iteration 9100: Loss = -11949.250401974401
3
Iteration 9200: Loss = -11949.25007799214
Iteration 9300: Loss = -11949.270119938645
1
Iteration 9400: Loss = -11949.250058007374
Iteration 9500: Loss = -11949.25019695217
1
Iteration 9600: Loss = -11949.514804761202
2
Iteration 9700: Loss = -11949.24995424032
Iteration 9800: Loss = -11949.296864336016
1
Iteration 9900: Loss = -11949.249912939315
Iteration 10000: Loss = -11949.249906198538
Iteration 10100: Loss = -11949.249922098661
Iteration 10200: Loss = -11949.250818869119
1
Iteration 10300: Loss = -11949.2588822242
2
Iteration 10400: Loss = -11949.249870728425
Iteration 10500: Loss = -11949.249804019784
Iteration 10600: Loss = -11949.249898598846
Iteration 10700: Loss = -11949.249827193704
Iteration 10800: Loss = -11949.251311140823
1
Iteration 10900: Loss = -11949.249753582917
Iteration 11000: Loss = -11949.26910867614
1
Iteration 11100: Loss = -11949.249845631188
Iteration 11200: Loss = -11949.24977426113
Iteration 11300: Loss = -11949.249930038708
1
Iteration 11400: Loss = -11949.249788300305
Iteration 11500: Loss = -11949.251575503542
1
Iteration 11600: Loss = -11949.249793689634
Iteration 11700: Loss = -11949.29986960134
1
Iteration 11800: Loss = -11949.268905483583
2
Iteration 11900: Loss = -11949.256063236973
3
Iteration 12000: Loss = -11949.249757672387
Iteration 12100: Loss = -11949.24971043966
Iteration 12200: Loss = -11949.251715228816
1
Iteration 12300: Loss = -11949.249728437993
Iteration 12400: Loss = -11949.421267227897
1
Iteration 12500: Loss = -11949.249743633904
Iteration 12600: Loss = -11949.264868229944
1
Iteration 12700: Loss = -11949.249725644333
Iteration 12800: Loss = -11949.27914359545
1
Iteration 12900: Loss = -11949.249723570249
Iteration 13000: Loss = -11949.251441246579
1
Iteration 13100: Loss = -11949.249693935812
Iteration 13200: Loss = -11949.249773347236
Iteration 13300: Loss = -11949.24969919311
Iteration 13400: Loss = -11949.24969104341
Iteration 13500: Loss = -11949.25139178394
1
Iteration 13600: Loss = -11949.249701178729
Iteration 13700: Loss = -11949.251200488
1
Iteration 13800: Loss = -11949.249737871633
Iteration 13900: Loss = -11949.252465055153
1
Iteration 14000: Loss = -11949.249768602427
Iteration 14100: Loss = -11949.24975781434
Iteration 14200: Loss = -11949.24965757235
Iteration 14300: Loss = -11949.24985324464
1
Iteration 14400: Loss = -11949.24965613972
Iteration 14500: Loss = -11949.251406468811
1
Iteration 14600: Loss = -11949.249667215541
Iteration 14700: Loss = -11949.329693939435
1
Iteration 14800: Loss = -11949.24965247362
Iteration 14900: Loss = -11949.249646385939
Iteration 15000: Loss = -11949.253707780099
1
Iteration 15100: Loss = -11949.249642590019
Iteration 15200: Loss = -11949.485458062703
1
Iteration 15300: Loss = -11949.249675361181
Iteration 15400: Loss = -11949.25485386328
1
Iteration 15500: Loss = -11949.250352821966
2
Iteration 15600: Loss = -11949.249752132571
Iteration 15700: Loss = -11949.425156795132
1
Iteration 15800: Loss = -11949.24966664224
Iteration 15900: Loss = -11949.26996993051
1
Iteration 16000: Loss = -11949.249654546527
Iteration 16100: Loss = -11949.260578232506
1
Iteration 16200: Loss = -11949.249666467136
Iteration 16300: Loss = -11949.254205500953
1
Iteration 16400: Loss = -11949.249666251133
Iteration 16500: Loss = -11949.36291058727
1
Iteration 16600: Loss = -11949.249657324053
Iteration 16700: Loss = -11949.287448609999
1
Iteration 16800: Loss = -11949.249678112192
Iteration 16900: Loss = -11949.249667995771
Iteration 17000: Loss = -11949.24987984201
1
Iteration 17100: Loss = -11949.251070482955
2
Iteration 17200: Loss = -11949.282110292053
3
Iteration 17300: Loss = -11949.25619798291
4
Iteration 17400: Loss = -11949.253331859993
5
Iteration 17500: Loss = -11949.249702858158
Iteration 17600: Loss = -11949.249808365508
1
Iteration 17700: Loss = -11949.250954660385
2
Iteration 17800: Loss = -11949.249652752716
Iteration 17900: Loss = -11949.250045836716
1
Iteration 18000: Loss = -11949.249649486856
Iteration 18100: Loss = -11949.254452087349
1
Iteration 18200: Loss = -11949.249643579506
Iteration 18300: Loss = -11949.483973837094
1
Iteration 18400: Loss = -11949.249646375756
Iteration 18500: Loss = -11949.253423133146
1
Iteration 18600: Loss = -11949.24977332517
2
Iteration 18700: Loss = -11949.251657768047
3
Iteration 18800: Loss = -11949.249765878303
4
Iteration 18900: Loss = -11949.261079993654
5
Iteration 19000: Loss = -11949.254310167986
6
Iteration 19100: Loss = -11949.310894415157
7
Iteration 19200: Loss = -11949.249676273388
Iteration 19300: Loss = -11949.559789611252
1
Iteration 19400: Loss = -11949.249630522247
Iteration 19500: Loss = -11949.275184807044
1
Iteration 19600: Loss = -11949.24962209467
Iteration 19700: Loss = -11949.461234965434
1
Iteration 19800: Loss = -11949.249642025774
Iteration 19900: Loss = -11949.249619501546
pi: tensor([[0.3956, 0.6044],
        [0.6333, 0.3667]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6017, 0.3983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2643, 0.0981],
         [0.5019, 0.3527]],

        [[0.5290, 0.0964],
         [0.6329, 0.5329]],

        [[0.5378, 0.1034],
         [0.7224, 0.5466]],

        [[0.5688, 0.1065],
         [0.6698, 0.5154]],

        [[0.6484, 0.0959],
         [0.7061, 0.5317]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 16
Adjusted Rand Index: 0.45696969696969697
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080477173169247
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.07655356572345891
Average Adjusted Rand Index: 0.8450028245846143
11692.712889377
[0.6201871993900662, 0.07655356572345891] [0.7987070707070707, 0.8450028245846143] [11837.136360065786, 11949.284857075481]
-------------------------------------
This iteration is 29
True Objective function: Loss = -11495.100174212863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20766.03065074038
Iteration 100: Loss = -12331.331377608167
Iteration 200: Loss = -12315.917579392148
Iteration 300: Loss = -11597.500260437155
Iteration 400: Loss = -11507.250666770447
Iteration 500: Loss = -11489.707368108087
Iteration 600: Loss = -11489.426344119134
Iteration 700: Loss = -11489.264017435722
Iteration 800: Loss = -11488.538009453969
Iteration 900: Loss = -11488.468981917677
Iteration 1000: Loss = -11488.421786354935
Iteration 1100: Loss = -11488.386487334958
Iteration 1200: Loss = -11488.359231234394
Iteration 1300: Loss = -11488.337647035361
Iteration 1400: Loss = -11488.320205888896
Iteration 1500: Loss = -11488.305812485425
Iteration 1600: Loss = -11488.293846741508
Iteration 1700: Loss = -11488.283749575865
Iteration 1800: Loss = -11488.275036335144
Iteration 1900: Loss = -11488.267536235155
Iteration 2000: Loss = -11488.260938611265
Iteration 2100: Loss = -11488.255118142464
Iteration 2200: Loss = -11488.249730448937
Iteration 2300: Loss = -11488.244678152132
Iteration 2400: Loss = -11488.239413618638
Iteration 2500: Loss = -11488.230870230715
Iteration 2600: Loss = -11488.21170000865
Iteration 2700: Loss = -11488.194222246575
Iteration 2800: Loss = -11488.191466153974
Iteration 2900: Loss = -11488.18912277194
Iteration 3000: Loss = -11488.18705443099
Iteration 3100: Loss = -11488.185158999067
Iteration 3200: Loss = -11488.183440916127
Iteration 3300: Loss = -11488.182055809646
Iteration 3400: Loss = -11488.180465878499
Iteration 3500: Loss = -11488.179183879764
Iteration 3600: Loss = -11488.178155655987
Iteration 3700: Loss = -11488.176874917812
Iteration 3800: Loss = -11488.175895011673
Iteration 3900: Loss = -11488.175773531553
Iteration 4000: Loss = -11488.174094446367
Iteration 4100: Loss = -11488.173265971818
Iteration 4200: Loss = -11488.172536144419
Iteration 4300: Loss = -11488.171862416846
Iteration 4400: Loss = -11488.17183068261
Iteration 4500: Loss = -11488.170583704885
Iteration 4600: Loss = -11488.170317103972
Iteration 4700: Loss = -11488.16952324738
Iteration 4800: Loss = -11488.169018883385
Iteration 4900: Loss = -11488.174060044903
1
Iteration 5000: Loss = -11488.168158580695
Iteration 5100: Loss = -11488.167735740826
Iteration 5200: Loss = -11488.16741226939
Iteration 5300: Loss = -11488.167041664074
Iteration 5400: Loss = -11488.166694522895
Iteration 5500: Loss = -11488.16637607301
Iteration 5600: Loss = -11488.166111087601
Iteration 5700: Loss = -11488.18454405474
1
Iteration 5800: Loss = -11486.549270631627
Iteration 5900: Loss = -11486.548164337748
Iteration 6000: Loss = -11486.565605950502
1
Iteration 6100: Loss = -11486.549443838392
2
Iteration 6200: Loss = -11486.561418644238
3
Iteration 6300: Loss = -11486.423900169328
Iteration 6400: Loss = -11486.422130982342
Iteration 6500: Loss = -11486.422068530566
Iteration 6600: Loss = -11486.42177226738
Iteration 6700: Loss = -11486.438891661137
1
Iteration 6800: Loss = -11486.435347484467
2
Iteration 6900: Loss = -11486.431961244845
3
Iteration 7000: Loss = -11486.42604131407
4
Iteration 7100: Loss = -11486.421853461567
Iteration 7200: Loss = -11486.421250689651
Iteration 7300: Loss = -11486.447310439795
1
Iteration 7400: Loss = -11486.451897053841
2
Iteration 7500: Loss = -11486.495744750813
3
Iteration 7600: Loss = -11486.420619454211
Iteration 7700: Loss = -11486.421185639849
1
Iteration 7800: Loss = -11486.420425620032
Iteration 7900: Loss = -11486.42061823854
1
Iteration 8000: Loss = -11486.42306233916
2
Iteration 8100: Loss = -11486.426798979497
3
Iteration 8200: Loss = -11486.420147279421
Iteration 8300: Loss = -11486.422016581391
1
Iteration 8400: Loss = -11486.420270374294
2
Iteration 8500: Loss = -11486.419966352414
Iteration 8600: Loss = -11486.419887427648
Iteration 8700: Loss = -11486.420816174992
1
Iteration 8800: Loss = -11486.41975477254
Iteration 8900: Loss = -11486.41975741803
Iteration 9000: Loss = -11486.424020143853
1
Iteration 9100: Loss = -11486.419715914795
Iteration 9200: Loss = -11486.419610873305
Iteration 9300: Loss = -11486.420166633538
1
Iteration 9400: Loss = -11486.52021930875
2
Iteration 9500: Loss = -11486.420330592078
3
Iteration 9600: Loss = -11486.419861589406
4
Iteration 9700: Loss = -11486.428192371468
5
Iteration 9800: Loss = -11486.425149199427
6
Iteration 9900: Loss = -11486.485072552352
7
Iteration 10000: Loss = -11486.419450105805
Iteration 10100: Loss = -11486.422315953763
1
Iteration 10200: Loss = -11486.437362397819
2
Iteration 10300: Loss = -11486.421261150103
3
Iteration 10400: Loss = -11486.433554588028
4
Iteration 10500: Loss = -11486.42238199803
5
Iteration 10600: Loss = -11486.420468457547
6
Iteration 10700: Loss = -11486.420674597588
7
Iteration 10800: Loss = -11486.41941811601
Iteration 10900: Loss = -11486.421291417653
1
Iteration 11000: Loss = -11486.465518433104
2
Iteration 11100: Loss = -11486.463988657122
3
Iteration 11200: Loss = -11486.420875783857
4
Iteration 11300: Loss = -11486.45027834039
5
Iteration 11400: Loss = -11486.420694170718
6
Iteration 11500: Loss = -11486.426116731296
7
Iteration 11600: Loss = -11486.526749202216
8
Iteration 11700: Loss = -11486.418794519728
Iteration 11800: Loss = -11486.418807657545
Iteration 11900: Loss = -11486.444826107389
1
Iteration 12000: Loss = -11486.421759804314
2
Iteration 12100: Loss = -11486.423786656225
3
Iteration 12200: Loss = -11486.43375587716
4
Iteration 12300: Loss = -11486.478523865735
5
Iteration 12400: Loss = -11486.419636452008
6
Iteration 12500: Loss = -11486.418840676597
Iteration 12600: Loss = -11486.453652091788
1
Iteration 12700: Loss = -11486.46370409225
2
Iteration 12800: Loss = -11486.420413846412
3
Iteration 12900: Loss = -11486.419824926528
4
Iteration 13000: Loss = -11486.421073910244
5
Iteration 13100: Loss = -11486.491893383656
6
Iteration 13200: Loss = -11486.418787317816
Iteration 13300: Loss = -11486.420493902133
1
Iteration 13400: Loss = -11486.474679852534
2
Iteration 13500: Loss = -11486.51969068129
3
Iteration 13600: Loss = -11486.421986416777
4
Iteration 13700: Loss = -11486.419761500283
5
Iteration 13800: Loss = -11486.420484295999
6
Iteration 13900: Loss = -11486.420332918295
7
Iteration 14000: Loss = -11486.42014719668
8
Iteration 14100: Loss = -11486.420854373018
9
Iteration 14200: Loss = -11486.423379351367
10
Iteration 14300: Loss = -11486.426864924557
11
Iteration 14400: Loss = -11486.426238529246
12
Iteration 14500: Loss = -11486.43140676843
13
Iteration 14600: Loss = -11486.42302418271
14
Iteration 14700: Loss = -11486.418367078217
Iteration 14800: Loss = -11486.419934592108
1
Iteration 14900: Loss = -11486.427795556338
2
Iteration 15000: Loss = -11486.418146976988
Iteration 15100: Loss = -11486.427970254037
1
Iteration 15200: Loss = -11486.46498997982
2
Iteration 15300: Loss = -11486.428274899143
3
Iteration 15400: Loss = -11486.51382078343
4
Iteration 15500: Loss = -11486.418089454442
Iteration 15600: Loss = -11486.418396302235
1
Iteration 15700: Loss = -11486.419079654825
2
Iteration 15800: Loss = -11486.418691899771
3
Iteration 15900: Loss = -11486.429977558983
4
Iteration 16000: Loss = -11486.512670371972
5
Iteration 16100: Loss = -11486.427277036053
6
Iteration 16200: Loss = -11486.418280679527
7
Iteration 16300: Loss = -11486.423328840938
8
Iteration 16400: Loss = -11486.505190345924
9
Iteration 16500: Loss = -11486.41805705674
Iteration 16600: Loss = -11486.418719908224
1
Iteration 16700: Loss = -11486.431016865386
2
Iteration 16800: Loss = -11486.419963814606
3
Iteration 16900: Loss = -11486.418049224592
Iteration 17000: Loss = -11486.418628900266
1
Iteration 17100: Loss = -11486.418887208853
2
Iteration 17200: Loss = -11486.41878255399
3
Iteration 17300: Loss = -11486.430858316015
4
Iteration 17400: Loss = -11486.441092872858
5
Iteration 17500: Loss = -11486.595831150427
6
Iteration 17600: Loss = -11486.418723945553
7
Iteration 17700: Loss = -11486.433969464748
8
Iteration 17800: Loss = -11486.447590177555
9
Iteration 17900: Loss = -11486.418980116845
10
Iteration 18000: Loss = -11486.481972379972
11
Iteration 18100: Loss = -11486.425560406064
12
Iteration 18200: Loss = -11486.441209853592
13
Iteration 18300: Loss = -11486.48116339036
14
Iteration 18400: Loss = -11486.513484960647
15
Stopping early at iteration 18400 due to no improvement.
pi: tensor([[0.7420, 0.2580],
        [0.2690, 0.7310]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5300, 0.4700], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3940, 0.0968],
         [0.6094, 0.1936]],

        [[0.5191, 0.0984],
         [0.6287, 0.6476]],

        [[0.6756, 0.0953],
         [0.6685, 0.7175]],

        [[0.6834, 0.1035],
         [0.5081, 0.6742]],

        [[0.5590, 0.0908],
         [0.5780, 0.5187]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23300.963597510116
Iteration 100: Loss = -12331.123082367545
Iteration 200: Loss = -12315.048636011132
Iteration 300: Loss = -12067.278744609619
Iteration 400: Loss = -11812.885356629122
Iteration 500: Loss = -11796.907727561374
Iteration 600: Loss = -11792.464028519256
Iteration 700: Loss = -11791.726688694922
Iteration 800: Loss = -11788.730980782018
Iteration 900: Loss = -11788.422374587708
Iteration 1000: Loss = -11788.331915099912
Iteration 1100: Loss = -11788.27637868459
Iteration 1200: Loss = -11788.13117595603
Iteration 1300: Loss = -11787.99248749901
Iteration 1400: Loss = -11787.970198553241
Iteration 1500: Loss = -11787.95243716853
Iteration 1600: Loss = -11787.937922780106
Iteration 1700: Loss = -11787.925816459836
Iteration 1800: Loss = -11787.91554771191
Iteration 1900: Loss = -11787.906772314713
Iteration 2000: Loss = -11787.899170604876
Iteration 2100: Loss = -11787.892995588198
Iteration 2200: Loss = -11787.886562736438
Iteration 2300: Loss = -11787.881012356293
Iteration 2400: Loss = -11787.875214094796
Iteration 2500: Loss = -11787.86464391821
Iteration 2600: Loss = -11787.84125315161
Iteration 2700: Loss = -11787.836648545406
Iteration 2800: Loss = -11787.74299834639
Iteration 2900: Loss = -11787.735673531717
Iteration 3000: Loss = -11787.732813033592
Iteration 3100: Loss = -11787.729497405799
Iteration 3200: Loss = -11787.724149034488
Iteration 3300: Loss = -11787.157488395207
Iteration 3400: Loss = -11787.150780140217
Iteration 3500: Loss = -11787.144346579602
Iteration 3600: Loss = -11787.143239251947
Iteration 3700: Loss = -11787.140730353687
Iteration 3800: Loss = -11787.138936198404
Iteration 3900: Loss = -11787.135688848057
Iteration 4000: Loss = -11784.761217437286
Iteration 4100: Loss = -11776.525726392816
Iteration 4200: Loss = -11776.302478159178
Iteration 4300: Loss = -11776.297521626191
Iteration 4400: Loss = -11776.295241198965
Iteration 4500: Loss = -11776.292791812559
Iteration 4600: Loss = -11776.289876927545
Iteration 4700: Loss = -11773.466078512125
Iteration 4800: Loss = -11772.980102967667
Iteration 4900: Loss = -11772.976741449296
Iteration 5000: Loss = -11772.976437526844
Iteration 5100: Loss = -11772.694969014467
Iteration 5200: Loss = -11772.667523524284
Iteration 5300: Loss = -11772.658193723355
Iteration 5400: Loss = -11771.986307250148
Iteration 5500: Loss = -11771.981375436319
Iteration 5600: Loss = -11771.976575733264
Iteration 5700: Loss = -11771.974393514345
Iteration 5800: Loss = -11771.97345040483
Iteration 5900: Loss = -11771.972680619532
Iteration 6000: Loss = -11771.976255398009
1
Iteration 6100: Loss = -11771.971668783399
Iteration 6200: Loss = -11771.970119975891
Iteration 6300: Loss = -11767.681100325148
Iteration 6400: Loss = -11764.989944676263
Iteration 6500: Loss = -11762.055261311085
Iteration 6600: Loss = -11762.050095606068
Iteration 6700: Loss = -11762.043350139686
Iteration 6800: Loss = -11762.0427147972
Iteration 6900: Loss = -11762.04235521302
Iteration 7000: Loss = -11762.048484351184
1
Iteration 7100: Loss = -11762.045813092309
2
Iteration 7200: Loss = -11762.040336831806
Iteration 7300: Loss = -11762.041531951467
1
Iteration 7400: Loss = -11762.040545401605
2
Iteration 7500: Loss = -11762.047680307896
3
Iteration 7600: Loss = -11762.038584302367
Iteration 7700: Loss = -11762.03520986957
Iteration 7800: Loss = -11762.041626946135
1
Iteration 7900: Loss = -11762.088458454646
2
Iteration 8000: Loss = -11762.038465468057
3
Iteration 8100: Loss = -11762.034385263973
Iteration 8200: Loss = -11762.034738691798
1
Iteration 8300: Loss = -11762.036451203507
2
Iteration 8400: Loss = -11762.03378691271
Iteration 8500: Loss = -11762.033112387173
Iteration 8600: Loss = -11762.039603648489
1
Iteration 8700: Loss = -11762.02603833552
Iteration 8800: Loss = -11762.043820989653
1
Iteration 8900: Loss = -11762.025971596317
Iteration 9000: Loss = -11762.027911015586
1
Iteration 9100: Loss = -11762.025863506447
Iteration 9200: Loss = -11762.040329500882
1
Iteration 9300: Loss = -11762.026267690848
2
Iteration 9400: Loss = -11762.034558477659
3
Iteration 9500: Loss = -11762.025744534756
Iteration 9600: Loss = -11762.025754562364
Iteration 9700: Loss = -11762.025820919122
Iteration 9800: Loss = -11762.025621917222
Iteration 9900: Loss = -11762.03587201033
1
Iteration 10000: Loss = -11762.241807207447
2
Iteration 10100: Loss = -11762.02539710608
Iteration 10200: Loss = -11762.02701998513
1
Iteration 10300: Loss = -11762.206338086755
2
Iteration 10400: Loss = -11762.025307847596
Iteration 10500: Loss = -11762.035937445793
1
Iteration 10600: Loss = -11762.040414490559
2
Iteration 10700: Loss = -11762.05461743274
3
Iteration 10800: Loss = -11762.116914681315
4
Iteration 10900: Loss = -11762.025933200894
5
Iteration 11000: Loss = -11762.02528214516
Iteration 11100: Loss = -11762.025451457821
1
Iteration 11200: Loss = -11762.03856100187
2
Iteration 11300: Loss = -11762.02515395686
Iteration 11400: Loss = -11762.03824491054
1
Iteration 11500: Loss = -11762.028284303527
2
Iteration 11600: Loss = -11762.025091567039
Iteration 11700: Loss = -11762.023439384076
Iteration 11800: Loss = -11762.032241270359
1
Iteration 11900: Loss = -11762.055313083405
2
Iteration 12000: Loss = -11762.028628087875
3
Iteration 12100: Loss = -11762.021037350798
Iteration 12200: Loss = -11762.021005126842
Iteration 12300: Loss = -11762.021316280638
1
Iteration 12400: Loss = -11762.024060732756
2
Iteration 12500: Loss = -11762.084595070512
3
Iteration 12600: Loss = -11762.021083960375
Iteration 12700: Loss = -11762.020813889409
Iteration 12800: Loss = -11762.021167665187
1
Iteration 12900: Loss = -11762.022260236394
2
Iteration 13000: Loss = -11762.02065278557
Iteration 13100: Loss = -11762.020628585662
Iteration 13200: Loss = -11762.022620801843
1
Iteration 13300: Loss = -11762.033439865221
2
Iteration 13400: Loss = -11762.020678728988
Iteration 13500: Loss = -11762.021108063742
1
Iteration 13600: Loss = -11762.022592853595
2
Iteration 13700: Loss = -11762.02144902455
3
Iteration 13800: Loss = -11761.95799697534
Iteration 13900: Loss = -11761.980314073333
1
Iteration 14000: Loss = -11761.931908433353
Iteration 14100: Loss = -11761.929129576192
Iteration 14200: Loss = -11761.92733601243
Iteration 14300: Loss = -11761.929724407864
1
Iteration 14400: Loss = -11762.026956123613
2
Iteration 14500: Loss = -11761.940026239437
3
Iteration 14600: Loss = -11761.929103112021
4
Iteration 14700: Loss = -11761.939014008405
5
Iteration 14800: Loss = -11761.927259095724
Iteration 14900: Loss = -11761.929185901592
1
Iteration 15000: Loss = -11761.928563344338
2
Iteration 15100: Loss = -11761.927251181996
Iteration 15200: Loss = -11762.057898238209
1
Iteration 15300: Loss = -11761.92720198047
Iteration 15400: Loss = -11761.927220072912
Iteration 15500: Loss = -11761.93289998348
1
Iteration 15600: Loss = -11761.928640993623
2
Iteration 15700: Loss = -11761.951705014224
3
Iteration 15800: Loss = -11761.927407901043
4
Iteration 15900: Loss = -11761.935271898623
5
Iteration 16000: Loss = -11761.94143073537
6
Iteration 16100: Loss = -11761.927207546896
Iteration 16200: Loss = -11761.927502861196
1
Iteration 16300: Loss = -11761.93139778704
2
Iteration 16400: Loss = -11761.957023343111
3
Iteration 16500: Loss = -11761.942837527798
4
Iteration 16600: Loss = -11761.933494680758
5
Iteration 16700: Loss = -11761.927432126946
6
Iteration 16800: Loss = -11761.951393300924
7
Iteration 16900: Loss = -11762.004406363385
8
Iteration 17000: Loss = -11761.927310698407
9
Iteration 17100: Loss = -11761.927578763843
10
Iteration 17200: Loss = -11761.965022210026
11
Iteration 17300: Loss = -11761.92729860268
Iteration 17400: Loss = -11761.927971338971
1
Iteration 17500: Loss = -11761.932388457575
2
Iteration 17600: Loss = -11761.941985840465
3
Iteration 17700: Loss = -11761.927194115942
Iteration 17800: Loss = -11761.928118552309
1
Iteration 17900: Loss = -11761.927561030929
2
Iteration 18000: Loss = -11761.92717737027
Iteration 18100: Loss = -11761.928140223703
1
Iteration 18200: Loss = -11761.93084079176
2
Iteration 18300: Loss = -11762.166776341597
3
Iteration 18400: Loss = -11761.927193753532
Iteration 18500: Loss = -11761.933925805093
1
Iteration 18600: Loss = -11761.932054484521
2
Iteration 18700: Loss = -11761.927325269
3
Iteration 18800: Loss = -11761.958955902659
4
Iteration 18900: Loss = -11761.969780425703
5
Iteration 19000: Loss = -11761.961441711614
6
Iteration 19100: Loss = -11761.944978494375
7
Iteration 19200: Loss = -11761.927208905488
Iteration 19300: Loss = -11761.92729102007
Iteration 19400: Loss = -11761.93538022366
1
Iteration 19500: Loss = -11761.927128240091
Iteration 19600: Loss = -11761.927356302196
1
Iteration 19700: Loss = -11761.927120660748
Iteration 19800: Loss = -11761.927195340444
Iteration 19900: Loss = -11761.927069234096
pi: tensor([[0.3372, 0.6628],
        [0.5824, 0.4176]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3809, 0.6191], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3466, 0.0973],
         [0.7116, 0.2625]],

        [[0.6049, 0.0962],
         [0.7286, 0.5203]],

        [[0.5901, 0.0942],
         [0.6131, 0.5953]],

        [[0.6291, 0.0960],
         [0.6097, 0.6457]],

        [[0.7045, 0.0898],
         [0.5578, 0.7266]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 92
Adjusted Rand Index: 0.7027008103753108
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 90
Adjusted Rand Index: 0.6363636363636364
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.06783041145412906
Average Adjusted Rand Index: 0.8518117922386423
11495.100174212863
[0.9920000001562724, 0.06783041145412906] [0.992, 0.8518117922386423] [11486.513484960647, 11761.946489874595]
-------------------------------------
This iteration is 30
True Objective function: Loss = -11586.607276960178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21451.635018229
Iteration 100: Loss = -12401.349461352504
Iteration 200: Loss = -12364.232257888121
Iteration 300: Loss = -11937.402544700562
Iteration 400: Loss = -11840.362520749002
Iteration 500: Loss = -11825.000267970157
Iteration 600: Loss = -11787.565736746805
Iteration 700: Loss = -11787.23313434556
Iteration 800: Loss = -11787.067690857912
Iteration 900: Loss = -11786.963155312236
Iteration 1000: Loss = -11786.89087426118
Iteration 1100: Loss = -11786.838010070831
Iteration 1200: Loss = -11786.797739493337
Iteration 1300: Loss = -11786.766000732416
Iteration 1400: Loss = -11786.740283476147
Iteration 1500: Loss = -11786.718246819393
Iteration 1600: Loss = -11786.687048210391
Iteration 1700: Loss = -11785.609356332647
Iteration 1800: Loss = -11785.596590823548
Iteration 1900: Loss = -11785.586074171875
Iteration 2000: Loss = -11785.57696803229
Iteration 2100: Loss = -11785.569029432463
Iteration 2200: Loss = -11785.561991549112
Iteration 2300: Loss = -11785.555709617305
Iteration 2400: Loss = -11785.550082722853
Iteration 2500: Loss = -11785.544909882687
Iteration 2600: Loss = -11785.54022940574
Iteration 2700: Loss = -11785.535667424627
Iteration 2800: Loss = -11785.531466557872
Iteration 2900: Loss = -11785.527632481775
Iteration 3000: Loss = -11785.523115260146
Iteration 3100: Loss = -11785.518817002816
Iteration 3200: Loss = -11785.52544570927
1
Iteration 3300: Loss = -11785.507762832942
Iteration 3400: Loss = -11785.497002305116
Iteration 3500: Loss = -11785.46265135043
Iteration 3600: Loss = -11784.431359971608
Iteration 3700: Loss = -11784.106543273785
Iteration 3800: Loss = -11783.941233481137
Iteration 3900: Loss = -11782.473068259073
Iteration 4000: Loss = -11782.279961698721
Iteration 4100: Loss = -11781.332659406988
Iteration 4200: Loss = -11780.6781223413
Iteration 4300: Loss = -11780.563859489204
Iteration 4400: Loss = -11780.559368538326
Iteration 4500: Loss = -11780.531720471423
Iteration 4600: Loss = -11780.444267850857
Iteration 4700: Loss = -11780.443468092837
Iteration 4800: Loss = -11780.429606805112
Iteration 4900: Loss = -11778.58435929267
Iteration 5000: Loss = -11778.573875294283
Iteration 5100: Loss = -11778.570652601873
Iteration 5200: Loss = -11778.570458516999
Iteration 5300: Loss = -11778.568774856656
Iteration 5400: Loss = -11778.56773168133
Iteration 5500: Loss = -11778.567144610248
Iteration 5600: Loss = -11778.561853723482
Iteration 5700: Loss = -11778.449209587201
Iteration 5800: Loss = -11778.448791369894
Iteration 5900: Loss = -11778.448489443359
Iteration 6000: Loss = -11778.447918319873
Iteration 6100: Loss = -11778.4475728391
Iteration 6200: Loss = -11778.455043177079
1
Iteration 6300: Loss = -11778.445121249244
Iteration 6400: Loss = -11778.444075260206
Iteration 6500: Loss = -11778.443856593249
Iteration 6600: Loss = -11778.445151405087
1
Iteration 6700: Loss = -11778.443399575544
Iteration 6800: Loss = -11778.443306320127
Iteration 6900: Loss = -11778.44477390683
1
Iteration 7000: Loss = -11778.442977756315
Iteration 7100: Loss = -11778.44282734236
Iteration 7200: Loss = -11778.44263269208
Iteration 7300: Loss = -11778.442579149594
Iteration 7400: Loss = -11778.444729602244
1
Iteration 7500: Loss = -11778.50528217819
2
Iteration 7600: Loss = -11778.441851583613
Iteration 7700: Loss = -11778.44353669994
1
Iteration 7800: Loss = -11778.436231759395
Iteration 7900: Loss = -11778.466004632817
1
Iteration 8000: Loss = -11778.443849162431
2
Iteration 8100: Loss = -11778.434937709739
Iteration 8200: Loss = -11778.45053972508
1
Iteration 8300: Loss = -11778.433685078477
Iteration 8400: Loss = -11778.433848436584
1
Iteration 8500: Loss = -11778.398640324702
Iteration 8600: Loss = -11778.413847691127
1
Iteration 8700: Loss = -11778.40268266377
2
Iteration 8800: Loss = -11778.398421808555
Iteration 8900: Loss = -11778.407524049024
1
Iteration 9000: Loss = -11778.398282013413
Iteration 9100: Loss = -11778.399860487893
1
Iteration 9200: Loss = -11778.398318516827
Iteration 9300: Loss = -11778.429035006051
1
Iteration 9400: Loss = -11778.453063186036
2
Iteration 9500: Loss = -11778.029098945146
Iteration 9600: Loss = -11777.982608836743
Iteration 9700: Loss = -11777.980379424724
Iteration 9800: Loss = -11777.980601201249
1
Iteration 9900: Loss = -11777.994229732783
2
Iteration 10000: Loss = -11777.979872007965
Iteration 10100: Loss = -11778.298737555668
1
Iteration 10200: Loss = -11777.97980702332
Iteration 10300: Loss = -11778.012054307075
1
Iteration 10400: Loss = -11777.979824844437
Iteration 10500: Loss = -11777.979689161466
Iteration 10600: Loss = -11777.985765231626
1
Iteration 10700: Loss = -11777.979644710676
Iteration 10800: Loss = -11778.059389347205
1
Iteration 10900: Loss = -11777.97951356459
Iteration 11000: Loss = -11777.976599631176
Iteration 11100: Loss = -11777.967153331994
Iteration 11200: Loss = -11777.988625800348
1
Iteration 11300: Loss = -11777.967127080321
Iteration 11400: Loss = -11777.9675619677
1
Iteration 11500: Loss = -11777.96733550979
2
Iteration 11600: Loss = -11777.967179289182
Iteration 11700: Loss = -11777.967090841903
Iteration 11800: Loss = -11777.96723061624
1
Iteration 11900: Loss = -11777.967036663285
Iteration 12000: Loss = -11777.967346005915
1
Iteration 12100: Loss = -11777.968479661306
2
Iteration 12200: Loss = -11777.967129152426
Iteration 12300: Loss = -11777.976959453998
1
Iteration 12400: Loss = -11777.967327473905
2
Iteration 12500: Loss = -11778.054439494681
3
Iteration 12600: Loss = -11777.967133090555
Iteration 12700: Loss = -11778.044201921373
1
Iteration 12800: Loss = -11777.967042616481
Iteration 12900: Loss = -11777.967218688835
1
Iteration 13000: Loss = -11777.969579687582
2
Iteration 13100: Loss = -11777.967164905609
3
Iteration 13200: Loss = -11777.96798309258
4
Iteration 13300: Loss = -11777.976705093086
5
Iteration 13400: Loss = -11778.025490193977
6
Iteration 13500: Loss = -11777.9767500193
7
Iteration 13600: Loss = -11777.977067725162
8
Iteration 13700: Loss = -11777.966761453872
Iteration 13800: Loss = -11777.97728843865
1
Iteration 13900: Loss = -11777.966725758874
Iteration 14000: Loss = -11777.96668290096
Iteration 14100: Loss = -11777.972187564123
1
Iteration 14200: Loss = -11778.031548346251
2
Iteration 14300: Loss = -11777.968126466816
3
Iteration 14400: Loss = -11777.968218919164
4
Iteration 14500: Loss = -11777.966670270696
Iteration 14600: Loss = -11777.980283166124
1
Iteration 14700: Loss = -11777.967026678874
2
Iteration 14800: Loss = -11777.966690792831
Iteration 14900: Loss = -11777.96841352746
1
Iteration 15000: Loss = -11777.966717986697
Iteration 15100: Loss = -11777.966716832221
Iteration 15200: Loss = -11777.97029307247
1
Iteration 15300: Loss = -11777.966724826665
Iteration 15400: Loss = -11777.967830175212
1
Iteration 15500: Loss = -11777.96753651582
2
Iteration 15600: Loss = -11778.156530603446
3
Iteration 15700: Loss = -11777.966921905469
4
Iteration 15800: Loss = -11777.967979198484
5
Iteration 15900: Loss = -11777.966690154442
Iteration 16000: Loss = -11777.966888501986
1
Iteration 16100: Loss = -11777.966636105119
Iteration 16200: Loss = -11777.967657929104
1
Iteration 16300: Loss = -11777.966651327339
Iteration 16400: Loss = -11777.996483218278
1
Iteration 16500: Loss = -11777.966640142693
Iteration 16600: Loss = -11778.034550827046
1
Iteration 16700: Loss = -11777.966791102386
2
Iteration 16800: Loss = -11777.966948699906
3
Iteration 16900: Loss = -11777.96766116439
4
Iteration 17000: Loss = -11777.988460542581
5
Iteration 17100: Loss = -11777.966676908936
Iteration 17200: Loss = -11777.98786642807
1
Iteration 17300: Loss = -11777.966639014227
Iteration 17400: Loss = -11777.967701382218
1
Iteration 17500: Loss = -11777.966640797096
Iteration 17600: Loss = -11777.969902058117
1
Iteration 17700: Loss = -11777.96663514163
Iteration 17800: Loss = -11777.988174696568
1
Iteration 17900: Loss = -11777.966662251245
Iteration 18000: Loss = -11777.966879815187
1
Iteration 18100: Loss = -11778.147230511468
2
Iteration 18200: Loss = -11777.96669021268
Iteration 18300: Loss = -11777.967982224753
1
Iteration 18400: Loss = -11777.96789479781
2
Iteration 18500: Loss = -11777.975665726373
3
Iteration 18600: Loss = -11777.967001583635
4
Iteration 18700: Loss = -11777.968887500567
5
Iteration 18800: Loss = -11777.968349496436
6
Iteration 18900: Loss = -11777.97467867058
7
Iteration 19000: Loss = -11777.978231397952
8
Iteration 19100: Loss = -11777.969948138629
9
Iteration 19200: Loss = -11777.966755818268
Iteration 19300: Loss = -11777.980769872276
1
Iteration 19400: Loss = -11777.983928165735
2
Iteration 19500: Loss = -11777.973920158467
3
Iteration 19600: Loss = -11777.966587519066
Iteration 19700: Loss = -11777.97915073934
1
Iteration 19800: Loss = -11777.980122143555
2
Iteration 19900: Loss = -11778.009012080596
3
pi: tensor([[0.6431, 0.3569],
        [0.2516, 0.7484]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8825, 0.1175], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2087, 0.0923],
         [0.6099, 0.3910]],

        [[0.6291, 0.0971],
         [0.5831, 0.7094]],

        [[0.5011, 0.1087],
         [0.5109, 0.5945]],

        [[0.6256, 0.1017],
         [0.6430, 0.5769]],

        [[0.6219, 0.1029],
         [0.5049, 0.5119]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 37
Adjusted Rand Index: 0.06298739930430867
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5586257299209246
Average Adjusted Rand Index: 0.8125974798608617
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19163.828070648156
Iteration 100: Loss = -12399.022284929613
Iteration 200: Loss = -11914.54110826221
Iteration 300: Loss = -11654.52027105402
Iteration 400: Loss = -11605.655155650145
Iteration 500: Loss = -11595.40697900094
Iteration 600: Loss = -11582.480516319034
Iteration 700: Loss = -11582.221807712373
Iteration 800: Loss = -11582.115182004542
Iteration 900: Loss = -11582.043585513724
Iteration 1000: Loss = -11581.992317590406
Iteration 1100: Loss = -11581.95397805363
Iteration 1200: Loss = -11581.924276487523
Iteration 1300: Loss = -11581.900820139499
Iteration 1400: Loss = -11581.881929747873
Iteration 1500: Loss = -11581.86641941467
Iteration 1600: Loss = -11581.853472589348
Iteration 1700: Loss = -11581.842592135506
Iteration 1800: Loss = -11581.833385069385
Iteration 1900: Loss = -11581.825405227784
Iteration 2000: Loss = -11581.818530392613
Iteration 2100: Loss = -11581.812455728943
Iteration 2200: Loss = -11581.807129058909
Iteration 2300: Loss = -11581.802350629358
Iteration 2400: Loss = -11581.798196582107
Iteration 2500: Loss = -11581.79435328251
Iteration 2600: Loss = -11581.791053746636
Iteration 2700: Loss = -11581.788836805854
Iteration 2800: Loss = -11581.785447285432
Iteration 2900: Loss = -11581.783060925587
Iteration 3000: Loss = -11581.781085339644
Iteration 3100: Loss = -11581.778967912656
Iteration 3200: Loss = -11581.780305159391
1
Iteration 3300: Loss = -11581.778782554484
Iteration 3400: Loss = -11581.777865885197
Iteration 3500: Loss = -11581.773522256603
Iteration 3600: Loss = -11581.774613046206
1
Iteration 3700: Loss = -11581.770442119137
Iteration 3800: Loss = -11581.773846745402
1
Iteration 3900: Loss = -11581.770494684477
Iteration 4000: Loss = -11581.768232141945
Iteration 4100: Loss = -11581.76739622771
Iteration 4200: Loss = -11581.766576534214
Iteration 4300: Loss = -11581.765600338818
Iteration 4400: Loss = -11581.765591903068
Iteration 4500: Loss = -11581.775327320816
1
Iteration 4600: Loss = -11581.76348187924
Iteration 4700: Loss = -11581.775164949653
1
Iteration 4800: Loss = -11581.764517015647
2
Iteration 4900: Loss = -11581.763349775276
Iteration 5000: Loss = -11581.76172605074
Iteration 5100: Loss = -11581.761129282937
Iteration 5200: Loss = -11581.761010738683
Iteration 5300: Loss = -11581.762069366747
1
Iteration 5400: Loss = -11581.76008752958
Iteration 5500: Loss = -11581.759906343335
Iteration 5600: Loss = -11581.760165883841
1
Iteration 5700: Loss = -11581.760097146012
2
Iteration 5800: Loss = -11581.760545669205
3
Iteration 5900: Loss = -11581.759084894938
Iteration 6000: Loss = -11581.761371684537
1
Iteration 6100: Loss = -11581.76096662245
2
Iteration 6200: Loss = -11581.760515053376
3
Iteration 6300: Loss = -11581.758751899863
Iteration 6400: Loss = -11581.758054838032
Iteration 6500: Loss = -11581.757567060924
Iteration 6600: Loss = -11581.75774725161
1
Iteration 6700: Loss = -11581.764062103479
2
Iteration 6800: Loss = -11581.758534333818
3
Iteration 6900: Loss = -11581.757766027518
4
Iteration 7000: Loss = -11581.76990961682
5
Iteration 7100: Loss = -11581.760900274314
6
Iteration 7200: Loss = -11581.756686144552
Iteration 7300: Loss = -11581.759371020955
1
Iteration 7400: Loss = -11581.756771460028
Iteration 7500: Loss = -11581.75675129588
Iteration 7600: Loss = -11581.762209709252
1
Iteration 7700: Loss = -11581.75660196905
Iteration 7800: Loss = -11581.756613803194
Iteration 7900: Loss = -11581.756107412186
Iteration 8000: Loss = -11581.773725501653
1
Iteration 8100: Loss = -11581.758299304154
2
Iteration 8200: Loss = -11581.767686894167
3
Iteration 8300: Loss = -11581.763461354441
4
Iteration 8400: Loss = -11581.755898428346
Iteration 8500: Loss = -11581.75657759513
1
Iteration 8600: Loss = -11581.756012172325
2
Iteration 8700: Loss = -11581.765524556544
3
Iteration 8800: Loss = -11581.756490408497
4
Iteration 8900: Loss = -11581.756182303287
5
Iteration 9000: Loss = -11581.7577423829
6
Iteration 9100: Loss = -11581.755489910625
Iteration 9200: Loss = -11581.75637946712
1
Iteration 9300: Loss = -11581.755969419142
2
Iteration 9400: Loss = -11581.756969506192
3
Iteration 9500: Loss = -11581.857538932967
4
Iteration 9600: Loss = -11581.75745568036
5
Iteration 9700: Loss = -11581.78493621535
6
Iteration 9800: Loss = -11581.758781375715
7
Iteration 9900: Loss = -11581.76026481907
8
Iteration 10000: Loss = -11581.756999324898
9
Iteration 10100: Loss = -11581.75533062574
Iteration 10200: Loss = -11581.758995997272
1
Iteration 10300: Loss = -11581.798735742164
2
Iteration 10400: Loss = -11581.774370116329
3
Iteration 10500: Loss = -11581.769632890713
4
Iteration 10600: Loss = -11581.76678033387
5
Iteration 10700: Loss = -11581.756564431254
6
Iteration 10800: Loss = -11581.769855556531
7
Iteration 10900: Loss = -11581.80526177364
8
Iteration 11000: Loss = -11581.755707585595
9
Iteration 11100: Loss = -11581.757826951789
10
Iteration 11200: Loss = -11581.75616667959
11
Iteration 11300: Loss = -11581.762885699198
12
Iteration 11400: Loss = -11581.814647055202
13
Iteration 11500: Loss = -11581.754942522017
Iteration 11600: Loss = -11581.757640713005
1
Iteration 11700: Loss = -11581.77461118483
2
Iteration 11800: Loss = -11581.805980815037
3
Iteration 11900: Loss = -11581.758973163625
4
Iteration 12000: Loss = -11581.771763468063
5
Iteration 12100: Loss = -11581.869968930467
6
Iteration 12200: Loss = -11581.759179245537
7
Iteration 12300: Loss = -11581.755287883732
8
Iteration 12400: Loss = -11581.755473894338
9
Iteration 12500: Loss = -11581.771642743643
10
Iteration 12600: Loss = -11581.857587171276
11
Iteration 12700: Loss = -11581.7592841253
12
Iteration 12800: Loss = -11581.755090131488
13
Iteration 12900: Loss = -11581.756069231229
14
Iteration 13000: Loss = -11581.755875396573
15
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.7739, 0.2261],
        [0.2503, 0.7497]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5187, 0.4813], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3947, 0.0932],
         [0.5983, 0.1938]],

        [[0.5380, 0.0971],
         [0.5258, 0.6145]],

        [[0.6920, 0.1087],
         [0.7075, 0.6853]],

        [[0.5661, 0.1019],
         [0.7106, 0.6753]],

        [[0.6354, 0.1030],
         [0.5032, 0.5208]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11586.607276960178
[0.5586257299209246, 1.0] [0.8125974798608617, 1.0] [11778.01897236726, 11581.755875396573]
-------------------------------------
This iteration is 31
True Objective function: Loss = -11577.008541528518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19210.1270155263
Iteration 100: Loss = -11961.075708916604
Iteration 200: Loss = -11758.22807662345
Iteration 300: Loss = -11745.872442046344
Iteration 400: Loss = -11743.425464831158
Iteration 500: Loss = -11743.129863992166
Iteration 600: Loss = -11743.033711403217
Iteration 700: Loss = -11742.974304296777
Iteration 800: Loss = -11742.934063015802
Iteration 900: Loss = -11742.905124480347
Iteration 1000: Loss = -11742.883427567138
Iteration 1100: Loss = -11742.866624882465
Iteration 1200: Loss = -11742.853202469447
Iteration 1300: Loss = -11742.84244877719
Iteration 1400: Loss = -11742.83384433808
Iteration 1500: Loss = -11742.826764242176
Iteration 1600: Loss = -11742.820931032567
Iteration 1700: Loss = -11742.815990193069
Iteration 1800: Loss = -11742.81183555094
Iteration 1900: Loss = -11742.808200375512
Iteration 2000: Loss = -11742.805087224418
Iteration 2100: Loss = -11742.802351054655
Iteration 2200: Loss = -11742.799896012379
Iteration 2300: Loss = -11742.79782476885
Iteration 2400: Loss = -11742.795922234169
Iteration 2500: Loss = -11742.79422311691
Iteration 2600: Loss = -11742.792723048977
Iteration 2700: Loss = -11742.791316542689
Iteration 2800: Loss = -11742.816805812936
1
Iteration 2900: Loss = -11742.788973188457
Iteration 3000: Loss = -11742.78791340849
Iteration 3100: Loss = -11742.78694889736
Iteration 3200: Loss = -11742.786133429565
Iteration 3300: Loss = -11742.785236354932
Iteration 3400: Loss = -11742.784447718635
Iteration 3500: Loss = -11742.78366315545
Iteration 3600: Loss = -11742.782923224722
Iteration 3700: Loss = -11742.782151458428
Iteration 3800: Loss = -11742.78152883977
Iteration 3900: Loss = -11742.780522970867
Iteration 4000: Loss = -11742.779512240511
Iteration 4100: Loss = -11742.778559515773
Iteration 4200: Loss = -11742.776368361783
Iteration 4300: Loss = -11742.776360984348
Iteration 4400: Loss = -11742.765660404062
Iteration 4500: Loss = -11742.753639737495
Iteration 4600: Loss = -11742.748761942263
Iteration 4700: Loss = -11742.745761278082
Iteration 4800: Loss = -11742.743588249561
Iteration 4900: Loss = -11742.742363459081
Iteration 5000: Loss = -11742.741664150584
Iteration 5100: Loss = -11742.740833492959
Iteration 5200: Loss = -11742.740116426598
Iteration 5300: Loss = -11742.73910876039
Iteration 5400: Loss = -11742.737827746341
Iteration 5500: Loss = -11742.73318743793
Iteration 5600: Loss = -11742.674388310956
Iteration 5700: Loss = -11742.671664774813
Iteration 5800: Loss = -11742.676556957984
1
Iteration 5900: Loss = -11742.670566530775
Iteration 6000: Loss = -11742.671682136215
1
Iteration 6100: Loss = -11742.670040547766
Iteration 6200: Loss = -11742.670217660969
1
Iteration 6300: Loss = -11742.67024084813
2
Iteration 6400: Loss = -11742.67247240039
3
Iteration 6500: Loss = -11742.668966459407
Iteration 6600: Loss = -11742.667231055315
Iteration 6700: Loss = -11742.657296711799
Iteration 6800: Loss = -11742.660462474989
1
Iteration 6900: Loss = -11742.656327753897
Iteration 7000: Loss = -11742.224177463042
Iteration 7100: Loss = -11742.204867826595
Iteration 7200: Loss = -11742.20742894974
1
Iteration 7300: Loss = -11742.204475760045
Iteration 7400: Loss = -11742.20501048297
1
Iteration 7500: Loss = -11742.204320285076
Iteration 7600: Loss = -11742.204850976012
1
Iteration 7700: Loss = -11742.204183855287
Iteration 7800: Loss = -11742.209855640087
1
Iteration 7900: Loss = -11742.203980378426
Iteration 8000: Loss = -11742.203918952733
Iteration 8100: Loss = -11742.20387390529
Iteration 8200: Loss = -11742.203859212772
Iteration 8300: Loss = -11742.203837972265
Iteration 8400: Loss = -11742.204696349629
1
Iteration 8500: Loss = -11742.208360308428
2
Iteration 8600: Loss = -11742.203686076113
Iteration 8700: Loss = -11742.351323496294
1
Iteration 8800: Loss = -11742.203568185976
Iteration 8900: Loss = -11742.497506952755
1
Iteration 9000: Loss = -11742.202471720393
Iteration 9100: Loss = -11742.20733905528
1
Iteration 9200: Loss = -11742.20286285486
2
Iteration 9300: Loss = -11742.204024785655
3
Iteration 9400: Loss = -11742.201964134169
Iteration 9500: Loss = -11742.203120594482
1
Iteration 9600: Loss = -11742.182798593838
Iteration 9700: Loss = -11742.183297763111
1
Iteration 9800: Loss = -11742.182388470965
Iteration 9900: Loss = -11742.19784024585
1
Iteration 10000: Loss = -11742.182602530924
2
Iteration 10100: Loss = -11742.183361096024
3
Iteration 10200: Loss = -11742.185226070402
4
Iteration 10300: Loss = -11742.293040640003
5
Iteration 10400: Loss = -11742.182440047369
Iteration 10500: Loss = -11742.183440763525
1
Iteration 10600: Loss = -11742.256965104303
2
Iteration 10700: Loss = -11742.182369199354
Iteration 10800: Loss = -11742.190269970351
1
Iteration 10900: Loss = -11742.182731916982
2
Iteration 11000: Loss = -11742.18578205987
3
Iteration 11100: Loss = -11742.382840066895
4
Iteration 11200: Loss = -11742.184496454756
5
Iteration 11300: Loss = -11742.187835226787
6
Iteration 11400: Loss = -11742.18230091453
Iteration 11500: Loss = -11742.182538044646
1
Iteration 11600: Loss = -11742.185237124164
2
Iteration 11700: Loss = -11742.244318355757
3
Iteration 11800: Loss = -11742.183515982579
4
Iteration 11900: Loss = -11742.184813109154
5
Iteration 12000: Loss = -11742.285220694694
6
Iteration 12100: Loss = -11742.181119020479
Iteration 12200: Loss = -11742.181149474425
Iteration 12300: Loss = -11742.184324650329
1
Iteration 12400: Loss = -11742.192491030382
2
Iteration 12500: Loss = -11742.181970619633
3
Iteration 12600: Loss = -11742.18163219933
4
Iteration 12700: Loss = -11742.181080529524
Iteration 12800: Loss = -11742.277342531634
1
Iteration 12900: Loss = -11742.299740891467
2
Iteration 13000: Loss = -11742.184625220798
3
Iteration 13100: Loss = -11742.181201673198
4
Iteration 13200: Loss = -11742.181121306232
Iteration 13300: Loss = -11742.182173067184
1
Iteration 13400: Loss = -11742.194323139463
2
Iteration 13500: Loss = -11742.182014616059
3
Iteration 13600: Loss = -11742.429743355206
4
Iteration 13700: Loss = -11742.164811408453
Iteration 13800: Loss = -11742.172157407833
1
Iteration 13900: Loss = -11742.1834005456
2
Iteration 14000: Loss = -11742.164599467327
Iteration 14100: Loss = -11742.16905468108
1
Iteration 14200: Loss = -11742.201270185371
2
Iteration 14300: Loss = -11742.168348092331
3
Iteration 14400: Loss = -11742.164412581353
Iteration 14500: Loss = -11742.174047317361
1
Iteration 14600: Loss = -11742.164840817619
2
Iteration 14700: Loss = -11742.165332713636
3
Iteration 14800: Loss = -11742.166222204147
4
Iteration 14900: Loss = -11742.16449771672
Iteration 15000: Loss = -11742.170398568278
1
Iteration 15100: Loss = -11742.203131504
2
Iteration 15200: Loss = -11742.165120355865
3
Iteration 15300: Loss = -11742.171697110301
4
Iteration 15400: Loss = -11742.16440853801
Iteration 15500: Loss = -11742.174395843027
1
Iteration 15600: Loss = -11742.167654664016
2
Iteration 15700: Loss = -11742.16440485181
Iteration 15800: Loss = -11742.164413748407
Iteration 15900: Loss = -11742.169810035863
1
Iteration 16000: Loss = -11742.156385813998
Iteration 16100: Loss = -11742.394906219817
1
Iteration 16200: Loss = -11742.159991941407
2
Iteration 16300: Loss = -11742.167909843049
3
Iteration 16400: Loss = -11742.155988772212
Iteration 16500: Loss = -11742.160693398177
1
Iteration 16600: Loss = -11742.152131850451
Iteration 16700: Loss = -11742.158655552928
1
Iteration 16800: Loss = -11742.152347680269
2
Iteration 16900: Loss = -11742.152547761121
3
Iteration 17000: Loss = -11742.18179609552
4
Iteration 17100: Loss = -11742.282775540443
5
Iteration 17200: Loss = -11742.159331427969
6
Iteration 17300: Loss = -11742.16045281033
7
Iteration 17400: Loss = -11742.152610771334
8
Iteration 17500: Loss = -11742.155149284039
9
Iteration 17600: Loss = -11742.153867639767
10
Iteration 17700: Loss = -11742.151596864192
Iteration 17800: Loss = -11742.232766872354
1
Iteration 17900: Loss = -11742.152019071167
2
Iteration 18000: Loss = -11742.151444724252
Iteration 18100: Loss = -11742.152598991051
1
Iteration 18200: Loss = -11742.163453969732
2
Iteration 18300: Loss = -11742.151418013287
Iteration 18400: Loss = -11742.164295280245
1
Iteration 18500: Loss = -11742.152272223328
2
Iteration 18600: Loss = -11742.175255257778
3
Iteration 18700: Loss = -11742.153255753423
4
Iteration 18800: Loss = -11742.151864625826
5
Iteration 18900: Loss = -11742.156228866937
6
Iteration 19000: Loss = -11742.151310118763
Iteration 19100: Loss = -11742.151328042262
Iteration 19200: Loss = -11742.165933787803
1
Iteration 19300: Loss = -11742.151297405067
Iteration 19400: Loss = -11742.152722582554
1
Iteration 19500: Loss = -11742.15171180166
2
Iteration 19600: Loss = -11742.15155639943
3
Iteration 19700: Loss = -11742.154874627335
4
Iteration 19800: Loss = -11742.151849199161
5
Iteration 19900: Loss = -11742.151519811076
6
pi: tensor([[0.6779, 0.3221],
        [0.2407, 0.7593]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9657, 0.0343], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.1201],
         [0.5510, 0.4033]],

        [[0.6822, 0.1101],
         [0.5982, 0.6427]],

        [[0.5546, 0.1009],
         [0.7310, 0.5642]],

        [[0.5083, 0.0939],
         [0.5063, 0.5578]],

        [[0.6642, 0.1059],
         [0.5291, 0.6572]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6202160704956093
Average Adjusted Rand Index: 0.7841618721313595
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24274.55531607619
Iteration 100: Loss = -12357.186989610695
Iteration 200: Loss = -12235.284402452306
Iteration 300: Loss = -11893.486557605815
Iteration 400: Loss = -11795.20032716085
Iteration 500: Loss = -11787.860084726133
Iteration 600: Loss = -11787.439233584173
Iteration 700: Loss = -11787.292883297425
Iteration 800: Loss = -11787.199548673065
Iteration 900: Loss = -11787.127306774892
Iteration 1000: Loss = -11787.081970597672
Iteration 1100: Loss = -11787.050664573719
Iteration 1200: Loss = -11787.026904553755
Iteration 1300: Loss = -11787.008191751169
Iteration 1400: Loss = -11786.99267719425
Iteration 1500: Loss = -11786.976304446998
Iteration 1600: Loss = -11786.85628104275
Iteration 1700: Loss = -11786.846429007257
Iteration 1800: Loss = -11786.835685502238
Iteration 1900: Loss = -11784.301410976728
Iteration 2000: Loss = -11784.265751458603
Iteration 2100: Loss = -11784.254841573797
Iteration 2200: Loss = -11784.2504863466
Iteration 2300: Loss = -11784.246319431348
Iteration 2400: Loss = -11784.242647996607
Iteration 2500: Loss = -11784.241614078239
Iteration 2600: Loss = -11784.231816732514
Iteration 2700: Loss = -11784.219964002637
Iteration 2800: Loss = -11784.212824784092
Iteration 2900: Loss = -11784.187300692327
Iteration 3000: Loss = -11784.18708988011
Iteration 3100: Loss = -11784.18403729016
Iteration 3200: Loss = -11784.18258522662
Iteration 3300: Loss = -11784.18130214728
Iteration 3400: Loss = -11784.179879255786
Iteration 3500: Loss = -11784.181327380446
1
Iteration 3600: Loss = -11784.176774617383
Iteration 3700: Loss = -11784.175565950853
Iteration 3800: Loss = -11784.174674384409
Iteration 3900: Loss = -11784.17385911041
Iteration 4000: Loss = -11784.176349794941
1
Iteration 4100: Loss = -11784.172555233881
Iteration 4200: Loss = -11784.171990344264
Iteration 4300: Loss = -11784.171528687817
Iteration 4400: Loss = -11784.17095292784
Iteration 4500: Loss = -11784.175829268097
1
Iteration 4600: Loss = -11784.170079129424
Iteration 4700: Loss = -11784.170632971349
1
Iteration 4800: Loss = -11784.169949131154
Iteration 4900: Loss = -11784.206049008262
1
Iteration 5000: Loss = -11784.168586975355
Iteration 5100: Loss = -11784.168431595133
Iteration 5200: Loss = -11784.16798180431
Iteration 5300: Loss = -11784.167748682697
Iteration 5400: Loss = -11784.180623127888
1
Iteration 5500: Loss = -11784.16994117003
2
Iteration 5600: Loss = -11784.167008554165
Iteration 5700: Loss = -11784.166819599977
Iteration 5800: Loss = -11784.166558080351
Iteration 5900: Loss = -11784.16670635248
1
Iteration 6000: Loss = -11784.172366411443
2
Iteration 6100: Loss = -11784.167675109768
3
Iteration 6200: Loss = -11783.755186822627
Iteration 6300: Loss = -11783.732597050415
Iteration 6400: Loss = -11783.732274198146
Iteration 6500: Loss = -11783.73780032055
1
Iteration 6600: Loss = -11783.731011102262
Iteration 6700: Loss = -11783.74166281791
1
Iteration 6800: Loss = -11783.73665678785
2
Iteration 6900: Loss = -11783.74116445526
3
Iteration 7000: Loss = -11783.73052686701
Iteration 7100: Loss = -11783.732013849709
1
Iteration 7200: Loss = -11783.735506837404
2
Iteration 7300: Loss = -11783.730476367178
Iteration 7400: Loss = -11783.73262146953
1
Iteration 7500: Loss = -11783.731067733388
2
Iteration 7600: Loss = -11783.731611361185
3
Iteration 7700: Loss = -11783.727466558721
Iteration 7800: Loss = -11783.611566525904
Iteration 7900: Loss = -11783.610573308653
Iteration 8000: Loss = -11783.614755694281
1
Iteration 8100: Loss = -11783.625455645324
2
Iteration 8200: Loss = -11783.672172322653
3
Iteration 8300: Loss = -11783.610326265438
Iteration 8400: Loss = -11783.610238151563
Iteration 8500: Loss = -11783.612495725867
1
Iteration 8600: Loss = -11783.61008384596
Iteration 8700: Loss = -11783.610242106508
1
Iteration 8800: Loss = -11783.64169077686
2
Iteration 8900: Loss = -11783.610009285636
Iteration 9000: Loss = -11783.610351603334
1
Iteration 9100: Loss = -11783.611014076418
2
Iteration 9200: Loss = -11783.610789110957
3
Iteration 9300: Loss = -11783.60991805369
Iteration 9400: Loss = -11783.613885103243
1
Iteration 9500: Loss = -11783.735247100647
2
Iteration 9600: Loss = -11783.60921321654
Iteration 9700: Loss = -11783.609115127927
Iteration 9800: Loss = -11783.61295929122
1
Iteration 9900: Loss = -11783.615309844947
2
Iteration 10000: Loss = -11783.68305822119
3
Iteration 10100: Loss = -11783.61431810565
4
Iteration 10200: Loss = -11783.608279188345
Iteration 10300: Loss = -11783.608786474424
1
Iteration 10400: Loss = -11783.652210478373
2
Iteration 10500: Loss = -11783.614834319218
3
Iteration 10600: Loss = -11783.611001526882
4
Iteration 10700: Loss = -11783.609098453915
5
Iteration 10800: Loss = -11783.611273940745
6
Iteration 10900: Loss = -11783.62024028703
7
Iteration 11000: Loss = -11783.608727279952
8
Iteration 11100: Loss = -11783.60817047967
Iteration 11200: Loss = -11783.617761964639
1
Iteration 11300: Loss = -11783.611113774898
2
Iteration 11400: Loss = -11783.615298723518
3
Iteration 11500: Loss = -11783.610046964886
4
Iteration 11600: Loss = -11783.609462613489
5
Iteration 11700: Loss = -11783.614554417132
6
Iteration 11800: Loss = -11783.609482937758
7
Iteration 11900: Loss = -11783.619326432941
8
Iteration 12000: Loss = -11783.618132087624
9
Iteration 12100: Loss = -11783.609136279812
10
Iteration 12200: Loss = -11783.607840974337
Iteration 12300: Loss = -11783.619206113266
1
Iteration 12400: Loss = -11783.610916812508
2
Iteration 12500: Loss = -11782.005555410487
Iteration 12600: Loss = -11782.022472342922
1
Iteration 12700: Loss = -11782.007770833243
2
Iteration 12800: Loss = -11782.005725548559
3
Iteration 12900: Loss = -11782.002085561586
Iteration 13000: Loss = -11782.00218211554
Iteration 13100: Loss = -11782.002646063049
1
Iteration 13200: Loss = -11782.025899996075
2
Iteration 13300: Loss = -11781.996315364793
Iteration 13400: Loss = -11781.979703800642
Iteration 13500: Loss = -11781.980141133008
1
Iteration 13600: Loss = -11781.991073136389
2
Iteration 13700: Loss = -11782.024675923973
3
Iteration 13800: Loss = -11781.974104007846
Iteration 13900: Loss = -11781.972881887901
Iteration 14000: Loss = -11781.975860038167
1
Iteration 14100: Loss = -11782.1382883347
2
Iteration 14200: Loss = -11781.973690512556
3
Iteration 14300: Loss = -11781.976804617814
4
Iteration 14400: Loss = -11781.974742320448
5
Iteration 14500: Loss = -11781.97997683361
6
Iteration 14600: Loss = -11781.975148027019
7
Iteration 14700: Loss = -11781.972865785334
Iteration 14800: Loss = -11781.97302314667
1
Iteration 14900: Loss = -11781.983982017837
2
Iteration 15000: Loss = -11781.972616984454
Iteration 15100: Loss = -11781.9727267616
1
Iteration 15200: Loss = -11781.973158367413
2
Iteration 15300: Loss = -11781.974559398324
3
Iteration 15400: Loss = -11782.111677060717
4
Iteration 15500: Loss = -11781.978614087153
5
Iteration 15600: Loss = -11782.039313712865
6
Iteration 15700: Loss = -11781.97445269608
7
Iteration 15800: Loss = -11781.989765834496
8
Iteration 15900: Loss = -11781.97272872672
9
Iteration 16000: Loss = -11781.974303787874
10
Iteration 16100: Loss = -11781.974064330148
11
Iteration 16200: Loss = -11781.985221187108
12
Iteration 16300: Loss = -11781.97729137529
13
Iteration 16400: Loss = -11781.977937787893
14
Iteration 16500: Loss = -11781.972670185316
Iteration 16600: Loss = -11781.974553053258
1
Iteration 16700: Loss = -11781.975035888388
2
Iteration 16800: Loss = -11782.062094382878
3
Iteration 16900: Loss = -11781.973358108244
4
Iteration 17000: Loss = -11781.973819034709
5
Iteration 17100: Loss = -11781.985080807193
6
Iteration 17200: Loss = -11781.976361580913
7
Iteration 17300: Loss = -11781.972641631308
Iteration 17400: Loss = -11781.976993568436
1
Iteration 17500: Loss = -11781.98165659374
2
Iteration 17600: Loss = -11781.97634761634
3
Iteration 17700: Loss = -11781.976964633173
4
Iteration 17800: Loss = -11781.972409424428
Iteration 17900: Loss = -11781.97453075164
1
Iteration 18000: Loss = -11782.01559474364
2
Iteration 18100: Loss = -11781.971407640955
Iteration 18200: Loss = -11781.971314975786
Iteration 18300: Loss = -11781.972117783447
1
Iteration 18400: Loss = -11781.978643379192
2
Iteration 18500: Loss = -11781.971076831502
Iteration 18600: Loss = -11781.975647270137
1
Iteration 18700: Loss = -11781.97740894669
2
Iteration 18800: Loss = -11781.981636295262
3
Iteration 18900: Loss = -11781.985234670025
4
Iteration 19000: Loss = -11781.977988786097
5
Iteration 19100: Loss = -11782.096677168538
6
Iteration 19200: Loss = -11781.971021029696
Iteration 19300: Loss = -11782.050353432787
1
Iteration 19400: Loss = -11781.975035937345
2
Iteration 19500: Loss = -11781.97333491077
3
Iteration 19600: Loss = -11782.053094828812
4
Iteration 19700: Loss = -11782.007526694517
5
Iteration 19800: Loss = -11782.069861883036
6
Iteration 19900: Loss = -11781.975086852739
7
pi: tensor([[0.5851, 0.4149],
        [0.5301, 0.4699]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4987, 0.5013], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2292, 0.0975],
         [0.7169, 0.3844]],

        [[0.6391, 0.1067],
         [0.6040, 0.5545]],

        [[0.5563, 0.0990],
         [0.5119, 0.6148]],

        [[0.6395, 0.0943],
         [0.6936, 0.6763]],

        [[0.5563, 0.1059],
         [0.5453, 0.5142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 20
Adjusted Rand Index: 0.354551002206262
time is 4
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4558962364399039
Average Adjusted Rand Index: 0.8629102004412523
11577.008541528518
[0.6202160704956093, 0.4558962364399039] [0.7841618721313595, 0.8629102004412523] [11742.159229370603, 11781.971665821759]
-------------------------------------
This iteration is 32
True Objective function: Loss = -11443.587745710178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20675.89543484483
Iteration 100: Loss = -12177.834205385347
Iteration 200: Loss = -12149.568947192152
Iteration 300: Loss = -11611.247925056512
Iteration 400: Loss = -11607.852754928579
Iteration 500: Loss = -11607.518211983826
Iteration 600: Loss = -11607.460510660849
Iteration 700: Loss = -11607.40131250568
Iteration 800: Loss = -11607.380617173012
Iteration 900: Loss = -11607.369038842395
Iteration 1000: Loss = -11607.358152733144
Iteration 1100: Loss = -11607.229205795651
Iteration 1200: Loss = -11607.222235139587
Iteration 1300: Loss = -11607.217838925742
Iteration 1400: Loss = -11607.213850609733
Iteration 1500: Loss = -11607.20896943124
Iteration 1600: Loss = -11607.179914809489
Iteration 1700: Loss = -11607.176794706216
Iteration 1800: Loss = -11607.175224860182
Iteration 1900: Loss = -11607.173925436673
Iteration 2000: Loss = -11607.172773700551
Iteration 2100: Loss = -11607.171803879537
Iteration 2200: Loss = -11607.170913442313
Iteration 2300: Loss = -11607.170170647962
Iteration 2400: Loss = -11607.16952038121
Iteration 2500: Loss = -11607.168830138471
Iteration 2600: Loss = -11607.166733659427
Iteration 2700: Loss = -11606.124212173081
Iteration 2800: Loss = -11606.12368555845
Iteration 2900: Loss = -11606.12328034759
Iteration 3000: Loss = -11606.122886383844
Iteration 3100: Loss = -11606.122590641633
Iteration 3200: Loss = -11606.122217102782
Iteration 3300: Loss = -11606.12194675436
Iteration 3400: Loss = -11606.121726952222
Iteration 3500: Loss = -11606.121471962946
Iteration 3600: Loss = -11606.122629118008
1
Iteration 3700: Loss = -11606.121032085886
Iteration 3800: Loss = -11606.12220000331
1
Iteration 3900: Loss = -11606.120698690014
Iteration 4000: Loss = -11606.121648025752
1
Iteration 4100: Loss = -11606.120409358427
Iteration 4200: Loss = -11606.127286135537
1
Iteration 4300: Loss = -11606.120151614496
Iteration 4400: Loss = -11606.120000764666
Iteration 4500: Loss = -11606.11996945871
Iteration 4600: Loss = -11606.119984747638
Iteration 4700: Loss = -11606.119720886214
Iteration 4800: Loss = -11606.119628046976
Iteration 4900: Loss = -11606.127198004458
1
Iteration 5000: Loss = -11606.119431442045
Iteration 5100: Loss = -11606.119362979143
Iteration 5200: Loss = -11606.119251591403
Iteration 5300: Loss = -11606.119324358664
Iteration 5400: Loss = -11606.119395121645
Iteration 5500: Loss = -11606.119016269524
Iteration 5600: Loss = -11606.11892774102
Iteration 5700: Loss = -11606.118885643556
Iteration 5800: Loss = -11606.131402488172
1
Iteration 5900: Loss = -11606.118705941684
Iteration 6000: Loss = -11606.118939615933
1
Iteration 6100: Loss = -11606.120920524854
2
Iteration 6200: Loss = -11606.119682850103
3
Iteration 6300: Loss = -11606.11854001099
Iteration 6400: Loss = -11606.118487895837
Iteration 6500: Loss = -11606.118581211102
Iteration 6600: Loss = -11606.120007220978
1
Iteration 6700: Loss = -11606.121261981481
2
Iteration 6800: Loss = -11606.119913802173
3
Iteration 6900: Loss = -11606.11863685119
Iteration 7000: Loss = -11606.118428794225
Iteration 7100: Loss = -11606.118313173767
Iteration 7200: Loss = -11606.11897865848
1
Iteration 7300: Loss = -11606.121728903267
2
Iteration 7400: Loss = -11606.118279223545
Iteration 7500: Loss = -11606.11823360352
Iteration 7600: Loss = -11606.118245677108
Iteration 7700: Loss = -11606.087942739452
Iteration 7800: Loss = -11606.08535602875
Iteration 7900: Loss = -11606.126584731643
1
Iteration 8000: Loss = -11606.084539043024
Iteration 8100: Loss = -11606.084850133137
1
Iteration 8200: Loss = -11606.084841544542
2
Iteration 8300: Loss = -11606.084203801813
Iteration 8400: Loss = -11606.084097606301
Iteration 8500: Loss = -11606.08430040609
1
Iteration 8600: Loss = -11606.084895255297
2
Iteration 8700: Loss = -11606.0841323822
Iteration 8800: Loss = -11606.084137874046
Iteration 8900: Loss = -11606.084393917088
1
Iteration 9000: Loss = -11606.084199715657
Iteration 9100: Loss = -11606.084100978916
Iteration 9200: Loss = -11606.095453289165
1
Iteration 9300: Loss = -11606.085422482236
2
Iteration 9400: Loss = -11606.085595907605
3
Iteration 9500: Loss = -11606.097376494097
4
Iteration 9600: Loss = -11606.082996398507
Iteration 9700: Loss = -11606.084447865791
1
Iteration 9800: Loss = -11606.15620324226
2
Iteration 9900: Loss = -11606.086572163787
3
Iteration 10000: Loss = -11606.092865542883
4
Iteration 10100: Loss = -11606.087677015566
5
Iteration 10200: Loss = -11606.085994538054
6
Iteration 10300: Loss = -11606.08566804937
7
Iteration 10400: Loss = -11606.093366843164
8
Iteration 10500: Loss = -11606.117102286773
9
Iteration 10600: Loss = -11606.107754945804
10
Iteration 10700: Loss = -11606.090811339773
11
Iteration 10800: Loss = -11606.083415446912
12
Iteration 10900: Loss = -11606.084770437452
13
Iteration 11000: Loss = -11606.086051649232
14
Iteration 11100: Loss = -11606.10226520959
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.6684, 0.3316],
        [0.2810, 0.7190]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9152, 0.0848], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2061, 0.1014],
         [0.6664, 0.3982]],

        [[0.6677, 0.0949],
         [0.6868, 0.5278]],

        [[0.6197, 0.0984],
         [0.6315, 0.5867]],

        [[0.6221, 0.0959],
         [0.7017, 0.5691]],

        [[0.6853, 0.1094],
         [0.7019, 0.5222]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0014658390783536795
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6138549851146129
Average Adjusted Rand Index: 0.7917061739116192
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21774.16497548224
Iteration 100: Loss = -12181.411989040467
Iteration 200: Loss = -12017.473185329369
Iteration 300: Loss = -11574.510348665553
Iteration 400: Loss = -11488.053780094142
Iteration 500: Loss = -11469.100241998052
Iteration 600: Loss = -11448.498598847167
Iteration 700: Loss = -11441.429941587185
Iteration 800: Loss = -11438.160596111335
Iteration 900: Loss = -11438.070974128372
Iteration 1000: Loss = -11438.017991702756
Iteration 1100: Loss = -11437.976987077205
Iteration 1200: Loss = -11437.931252033512
Iteration 1300: Loss = -11437.899876354984
Iteration 1400: Loss = -11437.882082645208
Iteration 1500: Loss = -11437.867492919057
Iteration 1600: Loss = -11437.85539840965
Iteration 1700: Loss = -11437.845192170214
Iteration 1800: Loss = -11437.836309168679
Iteration 1900: Loss = -11437.828183996535
Iteration 2000: Loss = -11437.818642040851
Iteration 2100: Loss = -11437.774037829391
Iteration 2200: Loss = -11437.772219831497
Iteration 2300: Loss = -11437.7602158948
Iteration 2400: Loss = -11437.756438751052
Iteration 2500: Loss = -11437.753154787353
Iteration 2600: Loss = -11437.750255544013
Iteration 2700: Loss = -11437.747619605818
Iteration 2800: Loss = -11437.745293541528
Iteration 2900: Loss = -11437.743170756585
Iteration 3000: Loss = -11437.741254704168
Iteration 3100: Loss = -11437.73950516893
Iteration 3200: Loss = -11437.737924044777
Iteration 3300: Loss = -11437.736511804891
Iteration 3400: Loss = -11437.73568095566
Iteration 3500: Loss = -11437.733958319404
Iteration 3600: Loss = -11437.734351239842
1
Iteration 3700: Loss = -11437.73179240568
Iteration 3800: Loss = -11437.730854922544
Iteration 3900: Loss = -11437.729976174589
Iteration 4000: Loss = -11437.729171444991
Iteration 4100: Loss = -11437.728435625284
Iteration 4200: Loss = -11437.727727255527
Iteration 4300: Loss = -11437.731658410985
1
Iteration 4400: Loss = -11437.72664412723
Iteration 4500: Loss = -11437.725913998916
Iteration 4600: Loss = -11437.725440087115
Iteration 4700: Loss = -11437.725203760234
Iteration 4800: Loss = -11437.724784576953
Iteration 4900: Loss = -11437.728846805494
1
Iteration 5000: Loss = -11437.724715912405
Iteration 5100: Loss = -11437.73202258236
1
Iteration 5200: Loss = -11437.72313278175
Iteration 5300: Loss = -11437.722654694906
Iteration 5400: Loss = -11437.722214377261
Iteration 5500: Loss = -11437.722514273875
1
Iteration 5600: Loss = -11437.723274782998
2
Iteration 5700: Loss = -11437.721403058023
Iteration 5800: Loss = -11437.722838620335
1
Iteration 5900: Loss = -11437.726472054406
2
Iteration 6000: Loss = -11437.720407481134
Iteration 6100: Loss = -11437.719941295863
Iteration 6200: Loss = -11437.719773608167
Iteration 6300: Loss = -11437.719642732343
Iteration 6400: Loss = -11437.720151925041
1
Iteration 6500: Loss = -11437.719238143596
Iteration 6600: Loss = -11437.71911627441
Iteration 6700: Loss = -11437.719005847544
Iteration 6800: Loss = -11437.728475122374
1
Iteration 6900: Loss = -11437.777072462224
2
Iteration 7000: Loss = -11437.71857554402
Iteration 7100: Loss = -11437.718776653632
1
Iteration 7200: Loss = -11437.744356048055
2
Iteration 7300: Loss = -11437.71827855365
Iteration 7400: Loss = -11437.718839091463
1
Iteration 7500: Loss = -11437.72684839232
2
Iteration 7600: Loss = -11437.73059717885
3
Iteration 7700: Loss = -11437.725053031116
4
Iteration 7800: Loss = -11437.7272480459
5
Iteration 7900: Loss = -11437.717817351493
Iteration 8000: Loss = -11437.719157719086
1
Iteration 8100: Loss = -11437.72065174162
2
Iteration 8200: Loss = -11437.727525426693
3
Iteration 8300: Loss = -11437.718569411434
4
Iteration 8400: Loss = -11437.717630682742
Iteration 8500: Loss = -11437.717487585305
Iteration 8600: Loss = -11437.387391836666
Iteration 8700: Loss = -11437.385137118243
Iteration 8800: Loss = -11437.384296781256
Iteration 8900: Loss = -11437.387737004914
1
Iteration 9000: Loss = -11437.384458459614
2
Iteration 9100: Loss = -11437.385389463143
3
Iteration 9200: Loss = -11437.384747839807
4
Iteration 9300: Loss = -11437.384253276598
Iteration 9400: Loss = -11437.384109785913
Iteration 9500: Loss = -11437.384442773044
1
Iteration 9600: Loss = -11437.388420279833
2
Iteration 9700: Loss = -11437.385481283192
3
Iteration 9800: Loss = -11437.391119006184
4
Iteration 9900: Loss = -11437.386697614938
5
Iteration 10000: Loss = -11437.384197288575
Iteration 10100: Loss = -11437.391196098686
1
Iteration 10200: Loss = -11437.385690898189
2
Iteration 10300: Loss = -11437.387364468446
3
Iteration 10400: Loss = -11437.384242232109
Iteration 10500: Loss = -11437.397270163063
1
Iteration 10600: Loss = -11437.408008284678
2
Iteration 10700: Loss = -11437.405708657994
3
Iteration 10800: Loss = -11437.488241018438
4
Iteration 10900: Loss = -11437.384135833236
Iteration 11000: Loss = -11437.383560893646
Iteration 11100: Loss = -11437.38580577638
1
Iteration 11200: Loss = -11437.392621664927
2
Iteration 11300: Loss = -11437.383903117627
3
Iteration 11400: Loss = -11437.38716255132
4
Iteration 11500: Loss = -11437.384044306273
5
Iteration 11600: Loss = -11437.38573913888
6
Iteration 11700: Loss = -11437.384793140409
7
Iteration 11800: Loss = -11437.39160756094
8
Iteration 11900: Loss = -11437.387964449546
9
Iteration 12000: Loss = -11437.416031609497
10
Iteration 12100: Loss = -11437.383709663894
11
Iteration 12200: Loss = -11437.384378299295
12
Iteration 12300: Loss = -11437.38393519292
13
Iteration 12400: Loss = -11437.384384346415
14
Iteration 12500: Loss = -11437.385523447005
15
Stopping early at iteration 12500 due to no improvement.
pi: tensor([[0.7498, 0.2502],
        [0.2307, 0.7693]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4741, 0.5259], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4015, 0.1021],
         [0.5605, 0.1996]],

        [[0.5038, 0.0945],
         [0.6208, 0.6274]],

        [[0.5973, 0.0981],
         [0.6307, 0.6593]],

        [[0.6076, 0.0960],
         [0.6224, 0.6290]],

        [[0.6804, 0.1097],
         [0.6425, 0.6796]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999854008352
Average Adjusted Rand Index: 0.9919993417272899
11443.587745710178
[0.6138549851146129, 0.9919999854008352] [0.7917061739116192, 0.9919993417272899] [11606.10226520959, 11437.385523447005]
-------------------------------------
This iteration is 33
True Objective function: Loss = -11533.162087603168
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22235.205632237103
Iteration 100: Loss = -12408.835572047179
Iteration 200: Loss = -12319.003654850441
Iteration 300: Loss = -11760.97651805697
Iteration 400: Loss = -11569.954303099648
Iteration 500: Loss = -11531.474817455522
Iteration 600: Loss = -11528.126671663356
Iteration 700: Loss = -11527.86424429226
Iteration 800: Loss = -11527.710469600654
Iteration 900: Loss = -11527.608320134339
Iteration 1000: Loss = -11527.535968092756
Iteration 1100: Loss = -11527.482341924078
Iteration 1200: Loss = -11527.44124710711
Iteration 1300: Loss = -11527.408850048048
Iteration 1400: Loss = -11527.382870601665
Iteration 1500: Loss = -11527.361516124321
Iteration 1600: Loss = -11527.343811565113
Iteration 1700: Loss = -11527.328842527211
Iteration 1800: Loss = -11527.31610614237
Iteration 1900: Loss = -11527.305098176945
Iteration 2000: Loss = -11527.295557195357
Iteration 2100: Loss = -11527.287191754112
Iteration 2200: Loss = -11527.27991796995
Iteration 2300: Loss = -11527.273431978982
Iteration 2400: Loss = -11527.267530623205
Iteration 2500: Loss = -11527.261880753204
Iteration 2600: Loss = -11527.253364382173
Iteration 2700: Loss = -11526.369688884459
Iteration 2800: Loss = -11526.363674078977
Iteration 2900: Loss = -11526.360342868902
Iteration 3000: Loss = -11526.357306472553
Iteration 3100: Loss = -11526.359200729888
1
Iteration 3200: Loss = -11526.352125218415
Iteration 3300: Loss = -11526.349903754439
Iteration 3400: Loss = -11526.357401331581
1
Iteration 3500: Loss = -11526.345942642047
Iteration 3600: Loss = -11526.344527599176
Iteration 3700: Loss = -11526.34787268003
1
Iteration 3800: Loss = -11526.356398421654
2
Iteration 3900: Loss = -11526.339801082268
Iteration 4000: Loss = -11526.339816798467
Iteration 4100: Loss = -11526.33737366865
Iteration 4200: Loss = -11526.336326382836
Iteration 4300: Loss = -11526.335625991062
Iteration 4400: Loss = -11526.336064593479
1
Iteration 4500: Loss = -11526.333540397756
Iteration 4600: Loss = -11526.335007477668
1
Iteration 4700: Loss = -11526.33919932158
2
Iteration 4800: Loss = -11526.332672635823
Iteration 4900: Loss = -11526.330729417881
Iteration 5000: Loss = -11526.32993094665
Iteration 5100: Loss = -11526.329927654868
Iteration 5200: Loss = -11526.329679575718
Iteration 5300: Loss = -11526.328422416664
Iteration 5400: Loss = -11526.327987083423
Iteration 5500: Loss = -11526.327492401211
Iteration 5600: Loss = -11526.335207885522
1
Iteration 5700: Loss = -11526.326668708993
Iteration 5800: Loss = -11526.326583985372
Iteration 5900: Loss = -11526.325938755279
Iteration 6000: Loss = -11526.326495372634
1
Iteration 6100: Loss = -11526.325529081847
Iteration 6200: Loss = -11526.325008086787
Iteration 6300: Loss = -11526.32601514365
1
Iteration 6400: Loss = -11526.324620050742
Iteration 6500: Loss = -11526.32568290446
1
Iteration 6600: Loss = -11526.324034398922
Iteration 6700: Loss = -11526.323636283338
Iteration 6800: Loss = -11526.325387010922
1
Iteration 6900: Loss = -11526.321491650737
Iteration 7000: Loss = -11526.388530683515
1
Iteration 7100: Loss = -11526.321373704724
Iteration 7200: Loss = -11526.325144568349
1
Iteration 7300: Loss = -11526.44623205988
2
Iteration 7400: Loss = -11526.320571976803
Iteration 7500: Loss = -11526.320427407587
Iteration 7600: Loss = -11526.321862249268
1
Iteration 7700: Loss = -11526.320160557018
Iteration 7800: Loss = -11526.328163538732
1
Iteration 7900: Loss = -11526.319979341062
Iteration 8000: Loss = -11526.321288315292
1
Iteration 8100: Loss = -11526.320041700399
Iteration 8200: Loss = -11526.321083370134
1
Iteration 8300: Loss = -11526.333384464584
2
Iteration 8400: Loss = -11526.319880152007
Iteration 8500: Loss = -11526.325995649446
1
Iteration 8600: Loss = -11526.319341364608
Iteration 8700: Loss = -11526.320182523603
1
Iteration 8800: Loss = -11526.35305913708
2
Iteration 8900: Loss = -11526.319287427585
Iteration 9000: Loss = -11526.320303338654
1
Iteration 9100: Loss = -11526.319156623895
Iteration 9200: Loss = -11526.320157104894
1
Iteration 9300: Loss = -11526.320353848496
2
Iteration 9400: Loss = -11526.319622638237
3
Iteration 9500: Loss = -11526.319524930888
4
Iteration 9600: Loss = -11526.329285480471
5
Iteration 9700: Loss = -11526.320241666757
6
Iteration 9800: Loss = -11526.322143400885
7
Iteration 9900: Loss = -11526.319665785095
8
Iteration 10000: Loss = -11526.399812384483
9
Iteration 10100: Loss = -11526.319361587488
10
Iteration 10200: Loss = -11526.323603394156
11
Iteration 10300: Loss = -11526.321304782876
12
Iteration 10400: Loss = -11526.350969301477
13
Iteration 10500: Loss = -11526.322852070618
14
Iteration 10600: Loss = -11526.323125683359
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.7740, 0.2260],
        [0.2098, 0.7902]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5064, 0.4936], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.0977],
         [0.5610, 0.4066]],

        [[0.6167, 0.1026],
         [0.6794, 0.6538]],

        [[0.5717, 0.0938],
         [0.6246, 0.6744]],

        [[0.5989, 0.0957],
         [0.6119, 0.5660]],

        [[0.5015, 0.1050],
         [0.5502, 0.6534]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22841.61475163762
Iteration 100: Loss = -12409.150277042178
Iteration 200: Loss = -12302.791842417462
Iteration 300: Loss = -11767.72130030292
Iteration 400: Loss = -11603.359563568567
Iteration 500: Loss = -11547.750171553007
Iteration 600: Loss = -11537.137863185611
Iteration 700: Loss = -11536.831799887508
Iteration 800: Loss = -11536.643797645922
Iteration 900: Loss = -11527.647453608886
Iteration 1000: Loss = -11527.562461627638
Iteration 1100: Loss = -11527.501656646496
Iteration 1200: Loss = -11527.455453549836
Iteration 1300: Loss = -11527.41949348344
Iteration 1400: Loss = -11527.390905702692
Iteration 1500: Loss = -11527.367642125111
Iteration 1600: Loss = -11527.348241342326
Iteration 1700: Loss = -11527.331376634513
Iteration 1800: Loss = -11527.313501465651
Iteration 1900: Loss = -11526.436566547694
Iteration 2000: Loss = -11526.410421903889
Iteration 2100: Loss = -11526.401570942602
Iteration 2200: Loss = -11526.39388213442
Iteration 2300: Loss = -11526.387146316923
Iteration 2400: Loss = -11526.381194232397
Iteration 2500: Loss = -11526.375907115826
Iteration 2600: Loss = -11526.371216322814
Iteration 2700: Loss = -11526.366988156618
Iteration 2800: Loss = -11526.363202895216
Iteration 2900: Loss = -11526.361748910425
Iteration 3000: Loss = -11526.356682153244
Iteration 3100: Loss = -11526.353899329066
Iteration 3200: Loss = -11526.351286381654
Iteration 3300: Loss = -11526.352463162872
1
Iteration 3400: Loss = -11526.346819286708
Iteration 3500: Loss = -11526.344848761237
Iteration 3600: Loss = -11526.343052350736
Iteration 3700: Loss = -11526.341405194482
Iteration 3800: Loss = -11526.339835549948
Iteration 3900: Loss = -11526.33942391455
Iteration 4000: Loss = -11526.337084321418
Iteration 4100: Loss = -11526.336496087155
Iteration 4200: Loss = -11526.33474164613
Iteration 4300: Loss = -11526.333672174265
Iteration 4400: Loss = -11526.333162371855
Iteration 4500: Loss = -11526.331763219523
Iteration 4600: Loss = -11526.330902488335
Iteration 4700: Loss = -11526.330388536002
Iteration 4800: Loss = -11526.32935352189
Iteration 4900: Loss = -11526.328511107566
Iteration 5000: Loss = -11526.328211935355
Iteration 5100: Loss = -11526.326662774094
Iteration 5200: Loss = -11526.326235290791
Iteration 5300: Loss = -11526.32625903529
Iteration 5400: Loss = -11526.323906536407
Iteration 5500: Loss = -11526.329198301055
1
Iteration 5600: Loss = -11526.322965969975
Iteration 5700: Loss = -11526.322600503543
Iteration 5800: Loss = -11526.334101231076
1
Iteration 5900: Loss = -11526.321871921615
Iteration 6000: Loss = -11526.32153123777
Iteration 6100: Loss = -11526.321458927168
Iteration 6200: Loss = -11526.32103024675
Iteration 6300: Loss = -11526.320655265059
Iteration 6400: Loss = -11526.320293251185
Iteration 6500: Loss = -11526.320291200449
Iteration 6600: Loss = -11526.321844040782
1
Iteration 6700: Loss = -11526.319845900169
Iteration 6800: Loss = -11526.320009318652
1
Iteration 6900: Loss = -11526.325641345693
2
Iteration 7000: Loss = -11526.32307341959
3
Iteration 7100: Loss = -11526.319493884606
Iteration 7200: Loss = -11526.31950726578
Iteration 7300: Loss = -11526.318509910456
Iteration 7400: Loss = -11526.318421178623
Iteration 7500: Loss = -11526.319693187887
1
Iteration 7600: Loss = -11526.318262972634
Iteration 7700: Loss = -11526.31830983179
Iteration 7800: Loss = -11526.31816018121
Iteration 7900: Loss = -11526.318175733028
Iteration 8000: Loss = -11526.325563270675
1
Iteration 8100: Loss = -11526.317940875966
Iteration 8200: Loss = -11526.317730384295
Iteration 8300: Loss = -11526.319479312247
1
Iteration 8400: Loss = -11526.317301909095
Iteration 8500: Loss = -11526.317368412861
Iteration 8600: Loss = -11526.322532390257
1
Iteration 8700: Loss = -11526.31876147169
2
Iteration 8800: Loss = -11526.325469859756
3
Iteration 8900: Loss = -11526.319658353494
4
Iteration 9000: Loss = -11526.316902024875
Iteration 9100: Loss = -11526.31871851426
1
Iteration 9200: Loss = -11526.329143577352
2
Iteration 9300: Loss = -11526.318930750858
3
Iteration 9400: Loss = -11526.31684280682
Iteration 9500: Loss = -11526.317375849652
1
Iteration 9600: Loss = -11526.316583900656
Iteration 9700: Loss = -11526.497521572346
1
Iteration 9800: Loss = -11526.316510042521
Iteration 9900: Loss = -11526.349647009265
1
Iteration 10000: Loss = -11526.322044044742
2
Iteration 10100: Loss = -11526.31771388192
3
Iteration 10200: Loss = -11526.316412057096
Iteration 10300: Loss = -11526.318282261582
1
Iteration 10400: Loss = -11526.321937340184
2
Iteration 10500: Loss = -11526.318256483402
3
Iteration 10600: Loss = -11526.325474405734
4
Iteration 10700: Loss = -11526.319674459248
5
Iteration 10800: Loss = -11526.327927701006
6
Iteration 10900: Loss = -11526.456546562842
7
Iteration 11000: Loss = -11526.321991302566
8
Iteration 11100: Loss = -11526.316329907584
Iteration 11200: Loss = -11526.316123186292
Iteration 11300: Loss = -11526.31723873382
1
Iteration 11400: Loss = -11526.326510591076
2
Iteration 11500: Loss = -11526.319296025977
3
Iteration 11600: Loss = -11526.313424711243
Iteration 11700: Loss = -11526.31428281856
1
Iteration 11800: Loss = -11526.327025963255
2
Iteration 11900: Loss = -11526.319535544004
3
Iteration 12000: Loss = -11526.31450706243
4
Iteration 12100: Loss = -11526.335345557796
5
Iteration 12200: Loss = -11526.32007410837
6
Iteration 12300: Loss = -11526.313962245327
7
Iteration 12400: Loss = -11526.32831796153
8
Iteration 12500: Loss = -11526.314595296524
9
Iteration 12600: Loss = -11526.314574989361
10
Iteration 12700: Loss = -11526.32114524691
11
Iteration 12800: Loss = -11526.315387322837
12
Iteration 12900: Loss = -11526.326062049437
13
Iteration 13000: Loss = -11526.338058133395
14
Iteration 13100: Loss = -11526.31331661322
Iteration 13200: Loss = -11526.315189040612
1
Iteration 13300: Loss = -11526.316696313326
2
Iteration 13400: Loss = -11526.32526604202
3
Iteration 13500: Loss = -11526.362054512367
4
Iteration 13600: Loss = -11526.35183909513
5
Iteration 13700: Loss = -11526.382955501518
6
Iteration 13800: Loss = -11526.355456773885
7
Iteration 13900: Loss = -11526.318086880743
8
Iteration 14000: Loss = -11526.321455610761
9
Iteration 14100: Loss = -11526.313294181165
Iteration 14200: Loss = -11526.31537833676
1
Iteration 14300: Loss = -11526.319407958516
2
Iteration 14400: Loss = -11526.326578259075
3
Iteration 14500: Loss = -11526.322359836011
4
Iteration 14600: Loss = -11526.316336496899
5
Iteration 14700: Loss = -11526.322225670605
6
Iteration 14800: Loss = -11526.315363022479
7
Iteration 14900: Loss = -11526.318373933196
8
Iteration 15000: Loss = -11526.315665855105
9
Iteration 15100: Loss = -11526.319121051163
10
Iteration 15200: Loss = -11526.314409610306
11
Iteration 15300: Loss = -11526.313257158587
Iteration 15400: Loss = -11526.320325082814
1
Iteration 15500: Loss = -11526.376002461411
2
Iteration 15600: Loss = -11526.383233451264
3
Iteration 15700: Loss = -11526.313791388877
4
Iteration 15800: Loss = -11526.31515118609
5
Iteration 15900: Loss = -11526.332858681417
6
Iteration 16000: Loss = -11526.316023836764
7
Iteration 16100: Loss = -11526.31670893199
8
Iteration 16200: Loss = -11526.321438008841
9
Iteration 16300: Loss = -11526.3137728717
10
Iteration 16400: Loss = -11526.31334837046
Iteration 16500: Loss = -11526.31346688319
1
Iteration 16600: Loss = -11526.31903253637
2
Iteration 16700: Loss = -11526.314885382137
3
Iteration 16800: Loss = -11526.32975136828
4
Iteration 16900: Loss = -11526.313958744704
5
Iteration 17000: Loss = -11526.313651513943
6
Iteration 17100: Loss = -11526.32230120204
7
Iteration 17200: Loss = -11526.315302345014
8
Iteration 17300: Loss = -11526.314256036077
9
Iteration 17400: Loss = -11526.315724493601
10
Iteration 17500: Loss = -11526.31364560081
11
Iteration 17600: Loss = -11526.314841492105
12
Iteration 17700: Loss = -11526.318152068296
13
Iteration 17800: Loss = -11526.427566073562
14
Iteration 17900: Loss = -11526.315258585033
15
Stopping early at iteration 17900 due to no improvement.
pi: tensor([[0.7898, 0.2102],
        [0.2257, 0.7743]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4889, 0.5111], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4065, 0.0976],
         [0.6593, 0.1987]],

        [[0.6178, 0.1026],
         [0.7180, 0.5288]],

        [[0.5016, 0.0938],
         [0.5585, 0.7112]],

        [[0.7059, 0.0960],
         [0.5737, 0.6082]],

        [[0.5873, 0.1050],
         [0.5005, 0.6966]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.992
11533.162087603168
[0.9919999711388391, 0.9919999711388391] [0.992, 0.992] [11526.323125683359, 11526.315258585033]
-------------------------------------
This iteration is 34
True Objective function: Loss = -11629.412838403518
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20683.71995973161
Iteration 100: Loss = -12484.45992660199
Iteration 200: Loss = -12233.845098608292
Iteration 300: Loss = -11812.519024908606
Iteration 400: Loss = -11798.635968144998
Iteration 500: Loss = -11796.863603896825
Iteration 600: Loss = -11789.946384427105
Iteration 700: Loss = -11788.537416894593
Iteration 800: Loss = -11771.794409407172
Iteration 900: Loss = -11756.787785772382
Iteration 1000: Loss = -11741.724183084407
Iteration 1100: Loss = -11712.029891926866
Iteration 1200: Loss = -11691.153614792189
Iteration 1300: Loss = -11650.359838436836
Iteration 1400: Loss = -11650.237451095678
Iteration 1500: Loss = -11650.184398331343
Iteration 1600: Loss = -11650.146782663187
Iteration 1700: Loss = -11650.093615424921
Iteration 1800: Loss = -11646.104385004692
Iteration 1900: Loss = -11646.082471412261
Iteration 2000: Loss = -11645.980327256679
Iteration 2100: Loss = -11630.39307481459
Iteration 2200: Loss = -11630.378238530566
Iteration 2300: Loss = -11630.357153087014
Iteration 2400: Loss = -11624.120109050682
Iteration 2500: Loss = -11624.09638175658
Iteration 2600: Loss = -11624.088940151356
Iteration 2700: Loss = -11624.082717213463
Iteration 2800: Loss = -11624.073225730412
Iteration 2900: Loss = -11623.184121921367
Iteration 3000: Loss = -11623.180108222337
Iteration 3100: Loss = -11623.176859715213
Iteration 3200: Loss = -11623.174033566314
Iteration 3300: Loss = -11623.171559103992
Iteration 3400: Loss = -11623.169360864347
Iteration 3500: Loss = -11623.167153735676
Iteration 3600: Loss = -11623.16461881718
Iteration 3700: Loss = -11623.157311698982
Iteration 3800: Loss = -11623.154428625288
Iteration 3900: Loss = -11623.153103225226
Iteration 4000: Loss = -11623.151542667447
Iteration 4100: Loss = -11623.150241988475
Iteration 4200: Loss = -11623.148856165939
Iteration 4300: Loss = -11623.148396442359
Iteration 4400: Loss = -11623.146143811877
Iteration 4500: Loss = -11623.145332953874
Iteration 4600: Loss = -11623.145194670913
Iteration 4700: Loss = -11623.143693727547
Iteration 4800: Loss = -11623.155576046262
1
Iteration 4900: Loss = -11623.144360306143
2
Iteration 5000: Loss = -11623.143773873875
Iteration 5100: Loss = -11623.14318360844
Iteration 5200: Loss = -11623.142555485574
Iteration 5300: Loss = -11623.14171589538
Iteration 5400: Loss = -11623.141051527326
Iteration 5500: Loss = -11623.13972258553
Iteration 5600: Loss = -11623.138754216423
Iteration 5700: Loss = -11623.137993926448
Iteration 5800: Loss = -11623.13914766559
1
Iteration 5900: Loss = -11623.139788959747
2
Iteration 6000: Loss = -11623.138244311038
3
Iteration 6100: Loss = -11623.136848764492
Iteration 6200: Loss = -11623.137061180205
1
Iteration 6300: Loss = -11623.136448591289
Iteration 6400: Loss = -11623.136367591906
Iteration 6500: Loss = -11623.136252019558
Iteration 6600: Loss = -11623.136367634263
1
Iteration 6700: Loss = -11623.136503677235
2
Iteration 6800: Loss = -11623.188004311007
3
Iteration 6900: Loss = -11623.134503329378
Iteration 7000: Loss = -11623.137296791603
1
Iteration 7100: Loss = -11623.134180262443
Iteration 7200: Loss = -11623.134077435829
Iteration 7300: Loss = -11623.133946323413
Iteration 7400: Loss = -11623.133781345283
Iteration 7500: Loss = -11623.148084769105
1
Iteration 7600: Loss = -11623.133573674168
Iteration 7700: Loss = -11623.133418964502
Iteration 7800: Loss = -11623.13438234732
1
Iteration 7900: Loss = -11623.13326812163
Iteration 8000: Loss = -11623.133158387885
Iteration 8100: Loss = -11623.133335332857
1
Iteration 8200: Loss = -11623.133010830674
Iteration 8300: Loss = -11623.161731876575
1
Iteration 8400: Loss = -11623.132884920153
Iteration 8500: Loss = -11623.132724783129
Iteration 8600: Loss = -11623.134771083205
1
Iteration 8700: Loss = -11623.131596897058
Iteration 8800: Loss = -11623.146851699224
1
Iteration 8900: Loss = -11623.128958657193
Iteration 9000: Loss = -11623.12888373457
Iteration 9100: Loss = -11623.129004359422
1
Iteration 9200: Loss = -11623.128756776447
Iteration 9300: Loss = -11623.145180280148
1
Iteration 9400: Loss = -11623.129348230661
2
Iteration 9500: Loss = -11623.128654332811
Iteration 9600: Loss = -11623.12875622801
1
Iteration 9700: Loss = -11623.128545339074
Iteration 9800: Loss = -11623.128702467235
1
Iteration 9900: Loss = -11623.16482587055
2
Iteration 10000: Loss = -11623.142044320753
3
Iteration 10100: Loss = -11623.12851936118
Iteration 10200: Loss = -11623.136910471001
1
Iteration 10300: Loss = -11623.138464500908
2
Iteration 10400: Loss = -11623.128541510874
Iteration 10500: Loss = -11623.128956614382
1
Iteration 10600: Loss = -11623.131615980088
2
Iteration 10700: Loss = -11623.12976433324
3
Iteration 10800: Loss = -11623.128256323484
Iteration 10900: Loss = -11623.136532515622
1
Iteration 11000: Loss = -11623.175495063446
2
Iteration 11100: Loss = -11623.135577499192
3
Iteration 11200: Loss = -11623.142178236974
4
Iteration 11300: Loss = -11623.13691364876
5
Iteration 11400: Loss = -11623.128635890876
6
Iteration 11500: Loss = -11623.12822278094
Iteration 11600: Loss = -11623.133303368564
1
Iteration 11700: Loss = -11623.133642302939
2
Iteration 11800: Loss = -11623.131038257063
3
Iteration 11900: Loss = -11623.128093130697
Iteration 12000: Loss = -11623.13094657806
1
Iteration 12100: Loss = -11623.128060314186
Iteration 12200: Loss = -11623.128269594003
1
Iteration 12300: Loss = -11623.128151632587
Iteration 12400: Loss = -11623.12832385236
1
Iteration 12500: Loss = -11623.227584695955
2
Iteration 12600: Loss = -11623.128027141564
Iteration 12700: Loss = -11623.155203414246
1
Iteration 12800: Loss = -11623.15716421226
2
Iteration 12900: Loss = -11623.128782382248
3
Iteration 13000: Loss = -11623.130357629558
4
Iteration 13100: Loss = -11623.131687424697
5
Iteration 13200: Loss = -11623.127977180744
Iteration 13300: Loss = -11623.128401848278
1
Iteration 13400: Loss = -11623.145262214966
2
Iteration 13500: Loss = -11623.128983840026
3
Iteration 13600: Loss = -11623.22952638324
4
Iteration 13700: Loss = -11623.128821872633
5
Iteration 13800: Loss = -11623.127986466712
Iteration 13900: Loss = -11623.1323434089
1
Iteration 14000: Loss = -11623.285041509806
2
Iteration 14100: Loss = -11623.127985762323
Iteration 14200: Loss = -11623.128436470188
1
Iteration 14300: Loss = -11623.14679594859
2
Iteration 14400: Loss = -11623.21709686204
3
Iteration 14500: Loss = -11623.133117072293
4
Iteration 14600: Loss = -11623.129177598987
5
Iteration 14700: Loss = -11623.146850055318
6
Iteration 14800: Loss = -11623.128169370822
7
Iteration 14900: Loss = -11623.132906754872
8
Iteration 15000: Loss = -11623.189386354707
9
Iteration 15100: Loss = -11623.135229103704
10
Iteration 15200: Loss = -11623.128998841288
11
Iteration 15300: Loss = -11623.131952433709
12
Iteration 15400: Loss = -11623.131210129955
13
Iteration 15500: Loss = -11623.149661562204
14
Iteration 15600: Loss = -11623.263313817508
15
Stopping early at iteration 15600 due to no improvement.
pi: tensor([[0.7445, 0.2555],
        [0.2424, 0.7576]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4801, 0.5199], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2015, 0.0990],
         [0.5489, 0.4032]],

        [[0.5168, 0.0890],
         [0.5697, 0.7301]],

        [[0.5304, 0.1057],
         [0.7190, 0.5391]],

        [[0.6349, 0.0938],
         [0.5530, 0.6403]],

        [[0.5659, 0.1111],
         [0.5128, 0.6013]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999944811108
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20657.921631956353
Iteration 100: Loss = -12390.482244589715
Iteration 200: Loss = -11625.349631074745
Iteration 300: Loss = -11623.978868206385
Iteration 400: Loss = -11623.648048494897
Iteration 500: Loss = -11623.491926925855
Iteration 600: Loss = -11623.401134110914
Iteration 700: Loss = -11623.342516643668
Iteration 800: Loss = -11623.30213231836
Iteration 900: Loss = -11623.273007669011
Iteration 1000: Loss = -11623.25097200916
Iteration 1100: Loss = -11623.233965298818
Iteration 1200: Loss = -11623.220512786289
Iteration 1300: Loss = -11623.209616799097
Iteration 1400: Loss = -11623.20067965736
Iteration 1500: Loss = -11623.193214540424
Iteration 1600: Loss = -11623.19284944581
Iteration 1700: Loss = -11623.181673495192
Iteration 1800: Loss = -11623.177074135325
Iteration 1900: Loss = -11623.1731860724
Iteration 2000: Loss = -11623.169755654517
Iteration 2100: Loss = -11623.167637995004
Iteration 2200: Loss = -11623.164111903421
Iteration 2300: Loss = -11623.162478073758
Iteration 2400: Loss = -11623.15967160181
Iteration 2500: Loss = -11623.15781499086
Iteration 2600: Loss = -11623.156867073534
Iteration 2700: Loss = -11623.154679105004
Iteration 2800: Loss = -11623.15334394584
Iteration 2900: Loss = -11623.152124375443
Iteration 3000: Loss = -11623.15102050238
Iteration 3100: Loss = -11623.156443571579
1
Iteration 3200: Loss = -11623.149068176848
Iteration 3300: Loss = -11623.148266795015
Iteration 3400: Loss = -11623.147472917599
Iteration 3500: Loss = -11623.146747052653
Iteration 3600: Loss = -11623.146508742284
Iteration 3700: Loss = -11623.145513528374
Iteration 3800: Loss = -11623.144943549773
Iteration 3900: Loss = -11623.144434776012
Iteration 4000: Loss = -11623.144728986414
1
Iteration 4100: Loss = -11623.147606658906
2
Iteration 4200: Loss = -11623.15351935294
3
Iteration 4300: Loss = -11623.142835711458
Iteration 4400: Loss = -11623.146604151874
1
Iteration 4500: Loss = -11623.142087491791
Iteration 4600: Loss = -11623.141997220093
Iteration 4700: Loss = -11623.14320417391
1
Iteration 4800: Loss = -11623.141876719863
Iteration 4900: Loss = -11623.141048156867
Iteration 5000: Loss = -11623.141242250489
1
Iteration 5100: Loss = -11623.140691358118
Iteration 5200: Loss = -11623.14032836318
Iteration 5300: Loss = -11623.14260378461
1
Iteration 5400: Loss = -11623.14887022587
2
Iteration 5500: Loss = -11623.139814498818
Iteration 5600: Loss = -11623.139723174456
Iteration 5700: Loss = -11623.139953051877
1
Iteration 5800: Loss = -11623.139405101834
Iteration 5900: Loss = -11623.139915206702
1
Iteration 6000: Loss = -11623.143033779494
2
Iteration 6100: Loss = -11623.142223810904
3
Iteration 6200: Loss = -11623.139453292362
Iteration 6300: Loss = -11623.159371721655
1
Iteration 6400: Loss = -11623.138691463204
Iteration 6500: Loss = -11623.138768245735
Iteration 6600: Loss = -11623.138578673483
Iteration 6700: Loss = -11623.138604133484
Iteration 6800: Loss = -11623.138422066686
Iteration 6900: Loss = -11623.138328183635
Iteration 7000: Loss = -11623.14425039977
1
Iteration 7100: Loss = -11623.138185324799
Iteration 7200: Loss = -11623.138117937626
Iteration 7300: Loss = -11623.151307606886
1
Iteration 7400: Loss = -11623.138005158902
Iteration 7500: Loss = -11623.137905832698
Iteration 7600: Loss = -11623.143035636282
1
Iteration 7700: Loss = -11623.137800988798
Iteration 7800: Loss = -11623.137928111393
1
Iteration 7900: Loss = -11623.137817601453
Iteration 8000: Loss = -11623.138442138637
1
Iteration 8100: Loss = -11623.162019623574
2
Iteration 8200: Loss = -11623.137607739569
Iteration 8300: Loss = -11623.14556061574
1
Iteration 8400: Loss = -11623.137380640048
Iteration 8500: Loss = -11623.22335762507
1
Iteration 8600: Loss = -11623.137259634017
Iteration 8700: Loss = -11623.136478376842
Iteration 8800: Loss = -11623.136365053935
Iteration 8900: Loss = -11623.136366738327
Iteration 9000: Loss = -11623.145696108197
1
Iteration 9100: Loss = -11623.139706670892
2
Iteration 9200: Loss = -11623.137794877393
3
Iteration 9300: Loss = -11623.141149730754
4
Iteration 9400: Loss = -11623.136223187885
Iteration 9500: Loss = -11623.137571184276
1
Iteration 9600: Loss = -11623.136512837023
2
Iteration 9700: Loss = -11623.137353130305
3
Iteration 9800: Loss = -11623.136205816178
Iteration 9900: Loss = -11623.136198939312
Iteration 10000: Loss = -11623.136167195738
Iteration 10100: Loss = -11623.136276664465
1
Iteration 10200: Loss = -11623.136203602377
Iteration 10300: Loss = -11623.138043213345
1
Iteration 10400: Loss = -11623.168363076005
2
Iteration 10500: Loss = -11623.137969688569
3
Iteration 10600: Loss = -11623.136447398463
4
Iteration 10700: Loss = -11623.137398362196
5
Iteration 10800: Loss = -11623.145617405511
6
Iteration 10900: Loss = -11623.136296958615
Iteration 11000: Loss = -11623.136200192408
Iteration 11100: Loss = -11623.13810607116
1
Iteration 11200: Loss = -11623.136226724055
Iteration 11300: Loss = -11623.15079913531
1
Iteration 11400: Loss = -11623.144334559389
2
Iteration 11500: Loss = -11623.1381681026
3
Iteration 11600: Loss = -11623.137233344576
4
Iteration 11700: Loss = -11623.138561783271
5
Iteration 11800: Loss = -11623.137066406143
6
Iteration 11900: Loss = -11623.137011208399
7
Iteration 12000: Loss = -11623.15588380737
8
Iteration 12100: Loss = -11623.136523498446
9
Iteration 12200: Loss = -11623.144881406699
10
Iteration 12300: Loss = -11623.152081578637
11
Iteration 12400: Loss = -11623.144518423445
12
Iteration 12500: Loss = -11623.140685436229
13
Iteration 12600: Loss = -11623.147819814018
14
Iteration 12700: Loss = -11623.136043383267
Iteration 12800: Loss = -11623.358922139554
1
Iteration 12900: Loss = -11623.13604033069
Iteration 13000: Loss = -11623.141769369497
1
Iteration 13100: Loss = -11623.136998580576
2
Iteration 13200: Loss = -11623.13121969777
Iteration 13300: Loss = -11623.150262669564
1
Iteration 13400: Loss = -11623.131838887914
2
Iteration 13500: Loss = -11623.132786730668
3
Iteration 13600: Loss = -11623.135831252204
4
Iteration 13700: Loss = -11623.209586317298
5
Iteration 13800: Loss = -11623.133985789202
6
Iteration 13900: Loss = -11623.131066747832
Iteration 14000: Loss = -11623.29214222444
1
Iteration 14100: Loss = -11623.13189861921
2
Iteration 14200: Loss = -11623.131350817232
3
Iteration 14300: Loss = -11623.131866707057
4
Iteration 14400: Loss = -11623.129707221115
Iteration 14500: Loss = -11623.188488256144
1
Iteration 14600: Loss = -11623.129298084694
Iteration 14700: Loss = -11623.129581882113
1
Iteration 14800: Loss = -11623.162407658328
2
Iteration 14900: Loss = -11623.131487952509
3
Iteration 15000: Loss = -11623.133947769833
4
Iteration 15100: Loss = -11623.13074145458
5
Iteration 15200: Loss = -11623.143779288761
6
Iteration 15300: Loss = -11623.135773796705
7
Iteration 15400: Loss = -11623.133485547336
8
Iteration 15500: Loss = -11623.13057741122
9
Iteration 15600: Loss = -11623.135713339618
10
Iteration 15700: Loss = -11623.129493998727
11
Iteration 15800: Loss = -11623.190536737891
12
Iteration 15900: Loss = -11623.133642925772
13
Iteration 16000: Loss = -11623.151357909606
14
Iteration 16100: Loss = -11623.129876795947
15
Stopping early at iteration 16100 due to no improvement.
pi: tensor([[0.7424, 0.2576],
        [0.2387, 0.7613]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4806, 0.5194], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.0996],
         [0.5763, 0.4003]],

        [[0.5754, 0.0881],
         [0.6673, 0.5312]],

        [[0.7280, 0.1048],
         [0.7174, 0.6318]],

        [[0.7116, 0.0938],
         [0.5607, 0.6172]],

        [[0.5072, 0.1101],
         [0.5706, 0.7073]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999944811108
Average Adjusted Rand Index: 0.9919992163297293
11629.412838403518
[0.9919999944811108, 0.9919999944811108] [0.9919992163297293, 0.9919992163297293] [11623.263313817508, 11623.129876795947]
-------------------------------------
This iteration is 35
True Objective function: Loss = -11802.93120837653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22831.44776549039
Iteration 100: Loss = -12568.280601010367
Iteration 200: Loss = -12489.366430878408
Iteration 300: Loss = -12161.9782733465
Iteration 400: Loss = -12098.91358001366
Iteration 500: Loss = -12064.915590148896
Iteration 600: Loss = -12062.922986120815
Iteration 700: Loss = -12062.707927034277
Iteration 800: Loss = -12062.59356728553
Iteration 900: Loss = -12062.52164824294
Iteration 1000: Loss = -12062.473522488293
Iteration 1100: Loss = -12062.118062301233
Iteration 1200: Loss = -12061.66920363671
Iteration 1300: Loss = -12060.325226855686
Iteration 1400: Loss = -12060.062003751153
Iteration 1500: Loss = -12060.04303643483
Iteration 1600: Loss = -12060.025949562798
Iteration 1700: Loss = -12059.949969657828
Iteration 1800: Loss = -12059.932022880776
Iteration 1900: Loss = -12059.925198735307
Iteration 2000: Loss = -12059.919728399354
Iteration 2100: Loss = -12059.915140778601
Iteration 2200: Loss = -12059.912013736415
Iteration 2300: Loss = -12059.907201810262
Iteration 2400: Loss = -12059.90377773092
Iteration 2500: Loss = -12059.900140831533
Iteration 2600: Loss = -12059.895875648028
Iteration 2700: Loss = -12059.893212276776
Iteration 2800: Loss = -12059.890250621573
Iteration 2900: Loss = -12059.883041733074
Iteration 3000: Loss = -12059.880260606073
Iteration 3100: Loss = -12059.878291473653
Iteration 3200: Loss = -12059.8748732009
Iteration 3300: Loss = -12059.869961623044
Iteration 3400: Loss = -12059.869134407294
Iteration 3500: Loss = -12059.867841326379
Iteration 3600: Loss = -12059.872346031523
1
Iteration 3700: Loss = -12059.86618276356
Iteration 3800: Loss = -12059.865546987423
Iteration 3900: Loss = -12059.864731980195
Iteration 4000: Loss = -12059.868217492542
1
Iteration 4100: Loss = -12059.863783800742
Iteration 4200: Loss = -12059.865218096418
1
Iteration 4300: Loss = -12059.874702861232
2
Iteration 4400: Loss = -12059.86105683793
Iteration 4500: Loss = -12059.860886171804
Iteration 4600: Loss = -12059.861591349982
1
Iteration 4700: Loss = -12059.859768987044
Iteration 4800: Loss = -12059.864604838147
1
Iteration 4900: Loss = -12059.861049895666
2
Iteration 5000: Loss = -12059.859113797203
Iteration 5100: Loss = -12059.858801896082
Iteration 5200: Loss = -12059.858359301908
Iteration 5300: Loss = -12059.858213976468
Iteration 5400: Loss = -12059.862692176028
1
Iteration 5500: Loss = -12059.857765434876
Iteration 5600: Loss = -12059.857856498933
Iteration 5700: Loss = -12059.857292654262
Iteration 5800: Loss = -12059.857274703374
Iteration 5900: Loss = -12059.85781161438
1
Iteration 6000: Loss = -12059.862533705167
2
Iteration 6100: Loss = -12059.860295124476
3
Iteration 6200: Loss = -12059.860784773475
4
Iteration 6300: Loss = -12059.856401234192
Iteration 6400: Loss = -12059.85642717414
Iteration 6500: Loss = -12059.857280880324
1
Iteration 6600: Loss = -12059.856104398767
Iteration 6700: Loss = -12059.856239906107
1
Iteration 6800: Loss = -12059.856712414425
2
Iteration 6900: Loss = -12059.8563315581
3
Iteration 7000: Loss = -12059.858016142656
4
Iteration 7100: Loss = -12059.858386745025
5
Iteration 7200: Loss = -12059.855681999246
Iteration 7300: Loss = -12059.85563908671
Iteration 7400: Loss = -12059.856219938147
1
Iteration 7500: Loss = -12059.910817593047
2
Iteration 7600: Loss = -12059.85622635769
3
Iteration 7700: Loss = -12059.855349608046
Iteration 7800: Loss = -12059.862361507146
1
Iteration 7900: Loss = -12059.860895274878
2
Iteration 8000: Loss = -12059.855175552737
Iteration 8100: Loss = -12059.86254305489
1
Iteration 8200: Loss = -12059.855075901347
Iteration 8300: Loss = -12059.855058705863
Iteration 8400: Loss = -12059.855363943781
1
Iteration 8500: Loss = -12059.854995411075
Iteration 8600: Loss = -12059.8549139522
Iteration 8700: Loss = -12059.855265554048
1
Iteration 8800: Loss = -12059.854873554745
Iteration 8900: Loss = -12059.927604599912
1
Iteration 9000: Loss = -12059.855282846358
2
Iteration 9100: Loss = -12059.854892716867
Iteration 9200: Loss = -12059.85740304218
1
Iteration 9300: Loss = -12059.858892942157
2
Iteration 9400: Loss = -12059.854974885215
Iteration 9500: Loss = -12059.86540547544
1
Iteration 9600: Loss = -12059.854631940188
Iteration 9700: Loss = -12059.875654009707
1
Iteration 9800: Loss = -12059.855256828254
2
Iteration 9900: Loss = -12059.854800209725
3
Iteration 10000: Loss = -12059.855885104613
4
Iteration 10100: Loss = -12059.860539056775
5
Iteration 10200: Loss = -12059.854645119301
Iteration 10300: Loss = -12059.856019511353
1
Iteration 10400: Loss = -12059.866681625008
2
Iteration 10500: Loss = -12059.890335696573
3
Iteration 10600: Loss = -12059.854993608258
4
Iteration 10700: Loss = -12059.861405020343
5
Iteration 10800: Loss = -12059.86509663011
6
Iteration 10900: Loss = -12059.854566112459
Iteration 11000: Loss = -12059.85471992504
1
Iteration 11100: Loss = -12059.860209242748
2
Iteration 11200: Loss = -12059.861595074837
3
Iteration 11300: Loss = -12059.93504378605
4
Iteration 11400: Loss = -12059.854702182673
5
Iteration 11500: Loss = -12059.90896364419
6
Iteration 11600: Loss = -12059.854370283887
Iteration 11700: Loss = -12059.854772060895
1
Iteration 11800: Loss = -12059.854415000233
Iteration 11900: Loss = -12059.856246709869
1
Iteration 12000: Loss = -12059.869814105965
2
Iteration 12100: Loss = -12059.854479121548
Iteration 12200: Loss = -12059.858289374492
1
Iteration 12300: Loss = -12059.855765872544
2
Iteration 12400: Loss = -12059.856867724922
3
Iteration 12500: Loss = -12059.857524293426
4
Iteration 12600: Loss = -12059.868328750412
5
Iteration 12700: Loss = -12059.854719609664
6
Iteration 12800: Loss = -12059.855013148075
7
Iteration 12900: Loss = -12059.85701173745
8
Iteration 13000: Loss = -12059.873838097128
9
Iteration 13100: Loss = -12059.856754966057
10
Iteration 13200: Loss = -12059.854368528655
Iteration 13300: Loss = -12059.86398342067
1
Iteration 13400: Loss = -12059.86148764538
2
Iteration 13500: Loss = -12059.85474473246
3
Iteration 13600: Loss = -12059.869667174009
4
Iteration 13700: Loss = -12059.856820552977
5
Iteration 13800: Loss = -12059.871651122563
6
Iteration 13900: Loss = -12059.857353826046
7
Iteration 14000: Loss = -12059.861551154874
8
Iteration 14100: Loss = -12059.855002530889
9
Iteration 14200: Loss = -12059.85452678836
10
Iteration 14300: Loss = -12059.880513174943
11
Iteration 14400: Loss = -12060.059623418063
12
Iteration 14500: Loss = -12059.8581584644
13
Iteration 14600: Loss = -12059.861800770672
14
Iteration 14700: Loss = -12059.874333303409
15
Stopping early at iteration 14700 due to no improvement.
pi: tensor([[0.4307, 0.5693],
        [0.4022, 0.5978]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4844, 0.5156], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3680, 0.1015],
         [0.5718, 0.2624]],

        [[0.7285, 0.0987],
         [0.5598, 0.6934]],

        [[0.7141, 0.1051],
         [0.5376, 0.5758]],

        [[0.5264, 0.1045],
         [0.5169, 0.5269]],

        [[0.5467, 0.1006],
         [0.5446, 0.5723]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 91
Adjusted Rand Index: 0.6677239700512669
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 84
Adjusted Rand Index: 0.4575257538278486
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.0834381882989627
Average Adjusted Rand Index: 0.8092115609374393
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23559.58719622247
Iteration 100: Loss = -12592.287448047417
Iteration 200: Loss = -12550.908040414313
Iteration 300: Loss = -12265.570253315867
Iteration 400: Loss = -11963.639130325282
Iteration 500: Loss = -11923.940540795264
Iteration 600: Loss = -11908.276574624286
Iteration 700: Loss = -11884.786322644699
Iteration 800: Loss = -11873.085191223376
Iteration 900: Loss = -11861.774353448447
Iteration 1000: Loss = -11855.206181820962
Iteration 1100: Loss = -11847.482884237259
Iteration 1200: Loss = -11844.362605941073
Iteration 1300: Loss = -11841.12515882653
Iteration 1400: Loss = -11840.948662351337
Iteration 1500: Loss = -11829.009801049033
Iteration 1600: Loss = -11808.77514463892
Iteration 1700: Loss = -11796.148021689447
Iteration 1800: Loss = -11796.099137910096
Iteration 1900: Loss = -11796.060891587746
Iteration 2000: Loss = -11796.014602247667
Iteration 2100: Loss = -11794.693381813868
Iteration 2200: Loss = -11794.59410227996
Iteration 2300: Loss = -11794.577142104303
Iteration 2400: Loss = -11794.563424136702
Iteration 2500: Loss = -11794.551508278253
Iteration 2600: Loss = -11794.5400680043
Iteration 2700: Loss = -11794.51390505789
Iteration 2800: Loss = -11794.050193624214
Iteration 2900: Loss = -11794.040247179815
Iteration 3000: Loss = -11794.013437466256
Iteration 3100: Loss = -11790.959101005408
Iteration 3200: Loss = -11790.94940395525
Iteration 3300: Loss = -11790.940824032916
Iteration 3400: Loss = -11790.920998013637
Iteration 3500: Loss = -11790.90273971548
Iteration 3600: Loss = -11790.899303123566
Iteration 3700: Loss = -11790.896142428062
Iteration 3800: Loss = -11790.8931887912
Iteration 3900: Loss = -11790.890499989408
Iteration 4000: Loss = -11790.887724945971
Iteration 4100: Loss = -11790.88554432597
Iteration 4200: Loss = -11790.880250364306
Iteration 4300: Loss = -11790.876783095044
Iteration 4400: Loss = -11790.874266870924
Iteration 4500: Loss = -11790.87269640496
Iteration 4600: Loss = -11790.871734490343
Iteration 4700: Loss = -11790.869934679005
Iteration 4800: Loss = -11790.868451231494
Iteration 4900: Loss = -11790.867322485685
Iteration 5000: Loss = -11790.866855317905
Iteration 5100: Loss = -11790.86511187963
Iteration 5200: Loss = -11790.864152211165
Iteration 5300: Loss = -11790.863353983104
Iteration 5400: Loss = -11790.862852952045
Iteration 5500: Loss = -11790.861847493332
Iteration 5600: Loss = -11790.862099836928
1
Iteration 5700: Loss = -11790.860340615112
Iteration 5800: Loss = -11790.85993517871
Iteration 5900: Loss = -11790.860228080044
1
Iteration 6000: Loss = -11790.858498545396
Iteration 6100: Loss = -11790.85809784369
Iteration 6200: Loss = -11790.857914068793
Iteration 6300: Loss = -11790.858114200963
1
Iteration 6400: Loss = -11790.858397832139
2
Iteration 6500: Loss = -11790.856183893926
Iteration 6600: Loss = -11790.855608605034
Iteration 6700: Loss = -11790.855272287612
Iteration 6800: Loss = -11790.857182198988
1
Iteration 6900: Loss = -11790.855180492015
Iteration 7000: Loss = -11790.854253457132
Iteration 7100: Loss = -11790.853879640912
Iteration 7200: Loss = -11790.867657745717
1
Iteration 7300: Loss = -11790.85328715935
Iteration 7400: Loss = -11790.853005916713
Iteration 7500: Loss = -11790.853054637737
Iteration 7600: Loss = -11790.917789082112
1
Iteration 7700: Loss = -11790.857902767762
2
Iteration 7800: Loss = -11790.859218911031
3
Iteration 7900: Loss = -11790.853214311577
4
Iteration 8000: Loss = -11790.852105136228
Iteration 8100: Loss = -11790.851165044478
Iteration 8200: Loss = -11790.850914965584
Iteration 8300: Loss = -11790.850401130205
Iteration 8400: Loss = -11790.8522410007
1
Iteration 8500: Loss = -11790.84986475934
Iteration 8600: Loss = -11790.84988029101
Iteration 8700: Loss = -11790.849556471727
Iteration 8800: Loss = -11790.923771510523
1
Iteration 8900: Loss = -11790.849443795645
Iteration 9000: Loss = -11790.850650043003
1
Iteration 9100: Loss = -11790.850477156702
2
Iteration 9200: Loss = -11790.85148324228
3
Iteration 9300: Loss = -11790.849789804079
4
Iteration 9400: Loss = -11790.850633752361
5
Iteration 9500: Loss = -11790.850787741387
6
Iteration 9600: Loss = -11790.850370837079
7
Iteration 9700: Loss = -11790.859268338618
8
Iteration 9800: Loss = -11790.848625611317
Iteration 9900: Loss = -11790.848258650922
Iteration 10000: Loss = -11790.853960350987
1
Iteration 10100: Loss = -11790.866211493305
2
Iteration 10200: Loss = -11790.859904475597
3
Iteration 10300: Loss = -11790.85751131293
4
Iteration 10400: Loss = -11790.867374000567
5
Iteration 10500: Loss = -11790.847833766797
Iteration 10600: Loss = -11790.848564557426
1
Iteration 10700: Loss = -11790.914156778184
2
Iteration 10800: Loss = -11790.85451451508
3
Iteration 10900: Loss = -11790.85480223586
4
Iteration 11000: Loss = -11790.903697242473
5
Iteration 11100: Loss = -11791.0432915174
6
Iteration 11200: Loss = -11790.849750406718
7
Iteration 11300: Loss = -11790.848022933314
8
Iteration 11400: Loss = -11790.847501128896
Iteration 11500: Loss = -11790.851663940875
1
Iteration 11600: Loss = -11790.867434308915
2
Iteration 11700: Loss = -11790.84822283378
3
Iteration 11800: Loss = -11790.847515774667
Iteration 11900: Loss = -11790.877242871926
1
Iteration 12000: Loss = -11790.904164892507
2
Iteration 12100: Loss = -11790.8566401178
3
Iteration 12200: Loss = -11790.847368674049
Iteration 12300: Loss = -11790.8472742798
Iteration 12400: Loss = -11790.848120165716
1
Iteration 12500: Loss = -11790.852674340289
2
Iteration 12600: Loss = -11790.84793767764
3
Iteration 12700: Loss = -11790.90914431311
4
Iteration 12800: Loss = -11790.903181802161
5
Iteration 12900: Loss = -11790.9635980883
6
Iteration 13000: Loss = -11790.866193201831
7
Iteration 13100: Loss = -11790.850097583168
8
Iteration 13200: Loss = -11790.84954991308
9
Iteration 13300: Loss = -11790.85411023151
10
Iteration 13400: Loss = -11790.872587973
11
Iteration 13500: Loss = -11790.856922832554
12
Iteration 13600: Loss = -11790.95911342504
13
Iteration 13700: Loss = -11790.872417923138
14
Iteration 13800: Loss = -11790.861405341317
15
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[0.7192, 0.2808],
        [0.2610, 0.7390]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5157, 0.4843], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1972, 0.1011],
         [0.6120, 0.4021]],

        [[0.6612, 0.1002],
         [0.6600, 0.7157]],

        [[0.5705, 0.1129],
         [0.5683, 0.6842]],

        [[0.5889, 0.1109],
         [0.5579, 0.7085]],

        [[0.5948, 0.1036],
         [0.6579, 0.7267]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 3
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320578693613
Average Adjusted Rand Index: 0.9839938528092684
11802.93120837653
[0.0834381882989627, 0.9840320578693613] [0.8092115609374393, 0.9839938528092684] [12059.874333303409, 11790.861405341317]
-------------------------------------
This iteration is 36
True Objective function: Loss = -11440.689287686346
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23606.76761110999
Iteration 100: Loss = -12205.878202365848
Iteration 200: Loss = -12190.767680752788
Iteration 300: Loss = -12026.102144426763
Iteration 400: Loss = -11669.078720348247
Iteration 500: Loss = -11618.060599072194
Iteration 600: Loss = -11591.112838845173
Iteration 700: Loss = -11590.737995828864
Iteration 800: Loss = -11590.470420351585
Iteration 900: Loss = -11589.289577094403
Iteration 1000: Loss = -11545.266823962875
Iteration 1100: Loss = -11487.1249052836
Iteration 1200: Loss = -11460.11808881249
Iteration 1300: Loss = -11443.532895745966
Iteration 1400: Loss = -11430.927216480924
Iteration 1500: Loss = -11430.873950892152
Iteration 1600: Loss = -11430.830909359574
Iteration 1700: Loss = -11430.809475077756
Iteration 1800: Loss = -11430.793254189573
Iteration 1900: Loss = -11430.780259107523
Iteration 2000: Loss = -11430.769630884492
Iteration 2100: Loss = -11430.760766514937
Iteration 2200: Loss = -11430.753364437827
Iteration 2300: Loss = -11430.747028642225
Iteration 2400: Loss = -11430.741601836156
Iteration 2500: Loss = -11430.736873688491
Iteration 2600: Loss = -11430.732722299548
Iteration 2700: Loss = -11430.729097167658
Iteration 2800: Loss = -11430.725713750553
Iteration 2900: Loss = -11430.722818047487
Iteration 3000: Loss = -11430.7201916651
Iteration 3100: Loss = -11430.71783084014
Iteration 3200: Loss = -11430.728665483726
1
Iteration 3300: Loss = -11430.713750770748
Iteration 3400: Loss = -11430.712582081565
Iteration 3500: Loss = -11430.710396726885
Iteration 3600: Loss = -11430.708926511967
Iteration 3700: Loss = -11430.707517671011
Iteration 3800: Loss = -11430.706348907845
Iteration 3900: Loss = -11430.705374675697
Iteration 4000: Loss = -11430.70412023668
Iteration 4100: Loss = -11430.703600186476
Iteration 4200: Loss = -11430.702237366251
Iteration 4300: Loss = -11430.701390332253
Iteration 4400: Loss = -11430.702196516635
1
Iteration 4500: Loss = -11430.699890463486
Iteration 4600: Loss = -11430.699237838713
Iteration 4700: Loss = -11430.698627936474
Iteration 4800: Loss = -11430.69802227407
Iteration 4900: Loss = -11430.697497682544
Iteration 5000: Loss = -11430.696997059003
Iteration 5100: Loss = -11430.69649881263
Iteration 5200: Loss = -11430.696045705694
Iteration 5300: Loss = -11430.695718825575
Iteration 5400: Loss = -11430.69522793042
Iteration 5500: Loss = -11430.694868144416
Iteration 5600: Loss = -11430.695058257696
1
Iteration 5700: Loss = -11430.694193242283
Iteration 5800: Loss = -11430.693930925194
Iteration 5900: Loss = -11430.700254978165
1
Iteration 6000: Loss = -11430.693313940908
Iteration 6100: Loss = -11430.693108712267
Iteration 6200: Loss = -11430.712344625903
1
Iteration 6300: Loss = -11430.692656562258
Iteration 6400: Loss = -11430.692411669512
Iteration 6500: Loss = -11430.692198527988
Iteration 6600: Loss = -11430.69222832649
Iteration 6700: Loss = -11430.691812719644
Iteration 6800: Loss = -11430.693968204669
1
Iteration 6900: Loss = -11430.691489715668
Iteration 7000: Loss = -11430.6912430695
Iteration 7100: Loss = -11430.649427224735
Iteration 7200: Loss = -11430.635702930307
Iteration 7300: Loss = -11430.635563950716
Iteration 7400: Loss = -11430.636244586422
1
Iteration 7500: Loss = -11430.701841993457
2
Iteration 7600: Loss = -11430.635986035753
3
Iteration 7700: Loss = -11430.642232525175
4
Iteration 7800: Loss = -11430.635126543495
Iteration 7900: Loss = -11430.638974579517
1
Iteration 8000: Loss = -11430.636832818389
2
Iteration 8100: Loss = -11430.638298167814
3
Iteration 8200: Loss = -11430.774624034872
4
Iteration 8300: Loss = -11430.63419988633
Iteration 8400: Loss = -11430.634172459133
Iteration 8500: Loss = -11430.634121270548
Iteration 8600: Loss = -11430.716696698815
1
Iteration 8700: Loss = -11430.635764995182
2
Iteration 8800: Loss = -11430.636393505716
3
Iteration 8900: Loss = -11430.634256759979
4
Iteration 9000: Loss = -11430.633488440213
Iteration 9100: Loss = -11430.650834864213
1
Iteration 9200: Loss = -11430.642921116223
2
Iteration 9300: Loss = -11430.636532624965
3
Iteration 9400: Loss = -11430.636445202994
4
Iteration 9500: Loss = -11430.63430498686
5
Iteration 9600: Loss = -11430.640444966339
6
Iteration 9700: Loss = -11430.648898371259
7
Iteration 9800: Loss = -11430.638805766856
8
Iteration 9900: Loss = -11430.632817906593
Iteration 10000: Loss = -11430.635572856241
1
Iteration 10100: Loss = -11430.634428803884
2
Iteration 10200: Loss = -11430.637022026964
3
Iteration 10300: Loss = -11430.633119348462
4
Iteration 10400: Loss = -11430.640689438846
5
Iteration 10500: Loss = -11430.63490991316
6
Iteration 10600: Loss = -11430.632504367713
Iteration 10700: Loss = -11430.642597209318
1
Iteration 10800: Loss = -11430.63498184209
2
Iteration 10900: Loss = -11430.633263119802
3
Iteration 11000: Loss = -11430.637271357647
4
Iteration 11100: Loss = -11430.643061934154
5
Iteration 11200: Loss = -11430.6381586282
6
Iteration 11300: Loss = -11430.647493984994
7
Iteration 11400: Loss = -11430.637112039945
8
Iteration 11500: Loss = -11430.649038553147
9
Iteration 11600: Loss = -11430.63293572418
10
Iteration 11700: Loss = -11430.635360297152
11
Iteration 11800: Loss = -11430.650205975626
12
Iteration 11900: Loss = -11430.654581424593
13
Iteration 12000: Loss = -11430.632794032246
14
Iteration 12100: Loss = -11430.662869712261
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.7609, 0.2391],
        [0.2447, 0.7553]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5333, 0.4667], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2125, 0.0945],
         [0.5302, 0.3919]],

        [[0.6057, 0.1009],
         [0.6677, 0.6565]],

        [[0.6222, 0.1041],
         [0.5091, 0.5304]],

        [[0.6111, 0.0828],
         [0.5286, 0.5430]],

        [[0.5589, 0.0948],
         [0.5186, 0.6020]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20670.569065636773
Iteration 100: Loss = -12205.335910318325
Iteration 200: Loss = -12106.010671411335
Iteration 300: Loss = -11799.38288583938
Iteration 400: Loss = -11658.353123319062
Iteration 500: Loss = -11613.29665404926
Iteration 600: Loss = -11602.397586391298
Iteration 700: Loss = -11602.162260819465
Iteration 800: Loss = -11596.431830705424
Iteration 900: Loss = -11596.335707346929
Iteration 1000: Loss = -11596.272550414304
Iteration 1100: Loss = -11596.22572601451
Iteration 1200: Loss = -11596.192133905153
Iteration 1300: Loss = -11596.165765235393
Iteration 1400: Loss = -11596.10495308979
Iteration 1500: Loss = -11590.568923349661
Iteration 1600: Loss = -11590.554473092283
Iteration 1700: Loss = -11590.542666497524
Iteration 1800: Loss = -11590.532721623336
Iteration 1900: Loss = -11590.524255005563
Iteration 2000: Loss = -11590.517008694222
Iteration 2100: Loss = -11590.510694398145
Iteration 2200: Loss = -11590.50512093537
Iteration 2300: Loss = -11590.500269257995
Iteration 2400: Loss = -11590.495939448658
Iteration 2500: Loss = -11590.493926021214
Iteration 2600: Loss = -11590.488413529738
Iteration 2700: Loss = -11590.485013258518
Iteration 2800: Loss = -11590.481796019412
Iteration 2900: Loss = -11590.47850873771
Iteration 3000: Loss = -11590.475593638283
Iteration 3100: Loss = -11590.473336050334
Iteration 3200: Loss = -11590.471449440232
Iteration 3300: Loss = -11590.472303268714
1
Iteration 3400: Loss = -11590.468361346298
Iteration 3500: Loss = -11590.466745097365
Iteration 3600: Loss = -11590.465502196717
Iteration 3700: Loss = -11590.468170665688
1
Iteration 3800: Loss = -11590.463355284166
Iteration 3900: Loss = -11590.464638687157
1
Iteration 4000: Loss = -11590.46131506738
Iteration 4100: Loss = -11590.46046079565
Iteration 4200: Loss = -11590.461073573952
1
Iteration 4300: Loss = -11590.458861246896
Iteration 4400: Loss = -11590.458250984664
Iteration 4500: Loss = -11590.457381995931
Iteration 4600: Loss = -11590.461990420825
1
Iteration 4700: Loss = -11590.456022026034
Iteration 4800: Loss = -11590.456303546674
1
Iteration 4900: Loss = -11590.454504613188
Iteration 5000: Loss = -11590.453816064874
Iteration 5100: Loss = -11590.452330803531
Iteration 5200: Loss = -11590.440397102504
Iteration 5300: Loss = -11590.436719892738
Iteration 5400: Loss = -11590.432999872753
Iteration 5500: Loss = -11590.429388787903
Iteration 5600: Loss = -11590.424516830502
Iteration 5700: Loss = -11590.409756045725
Iteration 5800: Loss = -11590.397071373945
Iteration 5900: Loss = -11590.389024215032
Iteration 6000: Loss = -11590.383385877987
Iteration 6100: Loss = -11590.370661715271
Iteration 6200: Loss = -11590.367716831846
Iteration 6300: Loss = -11590.366123318146
Iteration 6400: Loss = -11590.364275296086
Iteration 6500: Loss = -11590.357828608687
Iteration 6600: Loss = -11590.347824904533
Iteration 6700: Loss = -11590.347091476433
Iteration 6800: Loss = -11590.350894191874
1
Iteration 6900: Loss = -11590.346606547027
Iteration 7000: Loss = -11590.347065206242
1
Iteration 7100: Loss = -11590.3465219246
Iteration 7200: Loss = -11590.345889289918
Iteration 7300: Loss = -11590.346522912969
1
Iteration 7400: Loss = -11590.36434149166
2
Iteration 7500: Loss = -11590.345199791724
Iteration 7600: Loss = -11590.428789924405
1
Iteration 7700: Loss = -11590.34497030282
Iteration 7800: Loss = -11590.352993612
1
Iteration 7900: Loss = -11590.343580034041
Iteration 8000: Loss = -11590.277113085072
Iteration 8100: Loss = -11590.275828579557
Iteration 8200: Loss = -11590.280734887869
1
Iteration 8300: Loss = -11590.274045127582
Iteration 8400: Loss = -11590.280193704766
1
Iteration 8500: Loss = -11590.275545034357
2
Iteration 8600: Loss = -11590.2737654184
Iteration 8700: Loss = -11590.27441860277
1
Iteration 8800: Loss = -11590.273835027328
Iteration 8900: Loss = -11590.256755977894
Iteration 9000: Loss = -11590.256708622746
Iteration 9100: Loss = -11590.25675770161
Iteration 9200: Loss = -11590.331002487093
1
Iteration 9300: Loss = -11590.258003293411
2
Iteration 9400: Loss = -11590.267071863143
3
Iteration 9500: Loss = -11590.262996586702
4
Iteration 9600: Loss = -11590.303615627516
5
Iteration 9700: Loss = -11590.278694096825
6
Iteration 9800: Loss = -11590.291516528969
7
Iteration 9900: Loss = -11590.257762398716
8
Iteration 10000: Loss = -11590.256503685674
Iteration 10100: Loss = -11590.254604196312
Iteration 10200: Loss = -11590.257857109355
1
Iteration 10300: Loss = -11590.266166067708
2
Iteration 10400: Loss = -11590.256934149125
3
Iteration 10500: Loss = -11590.319139974381
4
Iteration 10600: Loss = -11590.253750899345
Iteration 10700: Loss = -11590.306815787113
1
Iteration 10800: Loss = -11590.285825804098
2
Iteration 10900: Loss = -11590.256941356349
3
Iteration 11000: Loss = -11590.258946562797
4
Iteration 11100: Loss = -11590.272854216239
5
Iteration 11200: Loss = -11590.300363625462
6
Iteration 11300: Loss = -11590.262754317222
7
Iteration 11400: Loss = -11590.296351041989
8
Iteration 11500: Loss = -11590.24995883214
Iteration 11600: Loss = -11590.274671374918
1
Iteration 11700: Loss = -11590.247791042468
Iteration 11800: Loss = -11590.249804155332
1
Iteration 11900: Loss = -11590.28925985333
2
Iteration 12000: Loss = -11590.285035888346
3
Iteration 12100: Loss = -11590.284421402977
4
Iteration 12200: Loss = -11590.278597723589
5
Iteration 12300: Loss = -11590.250509996447
6
Iteration 12400: Loss = -11590.256410764296
7
Iteration 12500: Loss = -11590.255686227056
8
Iteration 12600: Loss = -11590.24675174087
Iteration 12700: Loss = -11590.249736613494
1
Iteration 12800: Loss = -11590.251420714492
2
Iteration 12900: Loss = -11590.248564766163
3
Iteration 13000: Loss = -11590.246032770625
Iteration 13100: Loss = -11590.260707925143
1
Iteration 13200: Loss = -11590.253510128123
2
Iteration 13300: Loss = -11590.352116042679
3
Iteration 13400: Loss = -11590.252619974091
4
Iteration 13500: Loss = -11590.250510705038
5
Iteration 13600: Loss = -11590.247539559045
6
Iteration 13700: Loss = -11590.245884146061
Iteration 13800: Loss = -11590.254631563192
1
Iteration 13900: Loss = -11590.246675690689
2
Iteration 14000: Loss = -11590.281853403421
3
Iteration 14100: Loss = -11590.233317915101
Iteration 14200: Loss = -11590.258947199172
1
Iteration 14300: Loss = -11590.299211408383
2
Iteration 14400: Loss = -11590.178136262874
Iteration 14500: Loss = -11590.156776122189
Iteration 14600: Loss = -11590.15730653092
1
Iteration 14700: Loss = -11590.166899134874
2
Iteration 14800: Loss = -11590.159357961365
3
Iteration 14900: Loss = -11590.145307513305
Iteration 15000: Loss = -11590.158579188235
1
Iteration 15100: Loss = -11590.147316872351
2
Iteration 15200: Loss = -11590.148159147053
3
Iteration 15300: Loss = -11590.146537490562
4
Iteration 15400: Loss = -11590.161803652265
5
Iteration 15500: Loss = -11590.192657035854
6
Iteration 15600: Loss = -11590.154142120477
7
Iteration 15700: Loss = -11590.141657859534
Iteration 15800: Loss = -11590.146575761528
1
Iteration 15900: Loss = -11590.141853681716
2
Iteration 16000: Loss = -11590.141387281441
Iteration 16100: Loss = -11590.197553697697
1
Iteration 16200: Loss = -11590.183766374119
2
Iteration 16300: Loss = -11590.227884119706
3
Iteration 16400: Loss = -11590.147114368874
4
Iteration 16500: Loss = -11590.145900885245
5
Iteration 16600: Loss = -11590.14437371928
6
Iteration 16700: Loss = -11590.180111924421
7
Iteration 16800: Loss = -11590.14242852071
8
Iteration 16900: Loss = -11590.160775690352
9
Iteration 17000: Loss = -11590.196668379003
10
Iteration 17100: Loss = -11590.140298374978
Iteration 17200: Loss = -11590.14232207604
1
Iteration 17300: Loss = -11590.149963972563
2
Iteration 17400: Loss = -11590.14147553972
3
Iteration 17500: Loss = -11590.193534139953
4
Iteration 17600: Loss = -11590.160145194217
5
Iteration 17700: Loss = -11590.140393053725
Iteration 17800: Loss = -11590.196384769788
1
Iteration 17900: Loss = -11590.151229519995
2
Iteration 18000: Loss = -11590.250183322014
3
Iteration 18100: Loss = -11590.142437275548
4
Iteration 18200: Loss = -11590.15053181599
5
Iteration 18300: Loss = -11590.151433221514
6
Iteration 18400: Loss = -11590.15198916042
7
Iteration 18500: Loss = -11590.14036146422
Iteration 18600: Loss = -11590.16171977021
1
Iteration 18700: Loss = -11590.156153412154
2
Iteration 18800: Loss = -11590.141751147905
3
Iteration 18900: Loss = -11590.141615825767
4
Iteration 19000: Loss = -11590.165487429436
5
Iteration 19100: Loss = -11590.15457903033
6
Iteration 19200: Loss = -11590.14268241867
7
Iteration 19300: Loss = -11590.141702780513
8
Iteration 19400: Loss = -11590.158030109542
9
Iteration 19500: Loss = -11590.147710519866
10
Iteration 19600: Loss = -11590.140706467928
11
Iteration 19700: Loss = -11590.142699190204
12
Iteration 19800: Loss = -11590.140593632059
13
Iteration 19900: Loss = -11590.151363408404
14
pi: tensor([[0.7463, 0.2537],
        [0.3360, 0.6640]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0248, 0.9752], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3924, 0.1286],
         [0.7100, 0.2029]],

        [[0.5256, 0.1014],
         [0.5561, 0.7308]],

        [[0.7141, 0.1032],
         [0.5225, 0.6512]],

        [[0.6840, 0.0828],
         [0.6456, 0.5472]],

        [[0.6149, 0.0948],
         [0.6855, 0.5714]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6586799312639419
Average Adjusted Rand Index: 0.8
11440.689287686346
[1.0, 0.6586799312639419] [1.0, 0.8] [11430.662869712261, 11590.146712781658]
-------------------------------------
This iteration is 37
True Objective function: Loss = -11516.898169442185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22424.445253489273
Iteration 100: Loss = -12209.2066080397
Iteration 200: Loss = -12205.277000785909
Iteration 300: Loss = -12203.719888136568
Iteration 400: Loss = -11953.857929056247
Iteration 500: Loss = -11905.56468146868
Iteration 600: Loss = -11873.170371552967
Iteration 700: Loss = -11845.750206921146
Iteration 800: Loss = -11809.265084954206
Iteration 900: Loss = -11785.120644142627
Iteration 1000: Loss = -11776.161345462408
Iteration 1100: Loss = -11768.62661540915
Iteration 1200: Loss = -11767.115396052142
Iteration 1300: Loss = -11763.661190428722
Iteration 1400: Loss = -11763.565273330993
Iteration 1500: Loss = -11762.999166998532
Iteration 1600: Loss = -11755.700392757471
Iteration 1700: Loss = -11755.655394822694
Iteration 1800: Loss = -11745.058228894717
Iteration 1900: Loss = -11745.050872731506
Iteration 2000: Loss = -11745.004710055977
Iteration 2100: Loss = -11744.985995743655
Iteration 2200: Loss = -11744.959007564168
Iteration 2300: Loss = -11744.951579762284
Iteration 2400: Loss = -11744.93678942868
Iteration 2500: Loss = -11744.928198884414
Iteration 2600: Loss = -11744.91513750457
Iteration 2700: Loss = -11744.889899549198
Iteration 2800: Loss = -11733.622544058138
Iteration 2900: Loss = -11733.60153674553
Iteration 3000: Loss = -11726.45552964156
Iteration 3100: Loss = -11726.438220489876
Iteration 3200: Loss = -11726.443509812143
1
Iteration 3300: Loss = -11726.430183029028
Iteration 3400: Loss = -11726.409038944546
Iteration 3500: Loss = -11726.407755567096
Iteration 3600: Loss = -11726.406522041596
Iteration 3700: Loss = -11726.405256616345
Iteration 3800: Loss = -11726.403729618714
Iteration 3900: Loss = -11726.40036554219
Iteration 4000: Loss = -11726.382462012702
Iteration 4100: Loss = -11724.08756896424
Iteration 4200: Loss = -11724.080946927537
Iteration 4300: Loss = -11724.055796101045
Iteration 4400: Loss = -11724.060576159385
1
Iteration 4500: Loss = -11716.626570061606
Iteration 4600: Loss = -11716.602837799126
Iteration 4700: Loss = -11716.5340685343
Iteration 4800: Loss = -11716.531678350933
Iteration 4900: Loss = -11716.531165150423
Iteration 5000: Loss = -11716.534919344522
1
Iteration 5100: Loss = -11716.530531939035
Iteration 5200: Loss = -11716.530200236057
Iteration 5300: Loss = -11716.530568314733
1
Iteration 5400: Loss = -11716.529092912235
Iteration 5500: Loss = -11716.528784138054
Iteration 5600: Loss = -11716.528355760593
Iteration 5700: Loss = -11716.418458423575
Iteration 5800: Loss = -11716.418101448502
Iteration 5900: Loss = -11716.41789456902
Iteration 6000: Loss = -11716.417859299789
Iteration 6100: Loss = -11716.417751810855
Iteration 6200: Loss = -11716.419969449405
1
Iteration 6300: Loss = -11716.418197182395
2
Iteration 6400: Loss = -11716.416749243843
Iteration 6500: Loss = -11716.413796430126
Iteration 6600: Loss = -11716.414803306214
1
Iteration 6700: Loss = -11716.413358681235
Iteration 6800: Loss = -11716.413165408114
Iteration 6900: Loss = -11716.426639304982
1
Iteration 7000: Loss = -11716.41284671151
Iteration 7100: Loss = -11716.412415101026
Iteration 7200: Loss = -11716.34602891818
Iteration 7300: Loss = -11716.346806769847
1
Iteration 7400: Loss = -11716.34543624963
Iteration 7500: Loss = -11716.345350362684
Iteration 7600: Loss = -11716.460879508473
1
Iteration 7700: Loss = -11716.345247310344
Iteration 7800: Loss = -11716.345171115872
Iteration 7900: Loss = -11716.381981507444
1
Iteration 8000: Loss = -11716.345103637466
Iteration 8100: Loss = -11716.345084637456
Iteration 8200: Loss = -11716.36476141415
1
Iteration 8300: Loss = -11716.345010695975
Iteration 8400: Loss = -11716.344991923932
Iteration 8500: Loss = -11716.349227881503
1
Iteration 8600: Loss = -11716.344903246012
Iteration 8700: Loss = -11716.344894885915
Iteration 8800: Loss = -11716.351816770693
1
Iteration 8900: Loss = -11716.344861599808
Iteration 9000: Loss = -11716.345048187453
1
Iteration 9100: Loss = -11716.382882275284
2
Iteration 9200: Loss = -11716.34481839328
Iteration 9300: Loss = -11716.376673109708
1
Iteration 9400: Loss = -11716.345716442376
2
Iteration 9500: Loss = -11716.34445523096
Iteration 9600: Loss = -11716.443640068155
1
Iteration 9700: Loss = -11716.344531189483
Iteration 9800: Loss = -11716.34437784618
Iteration 9900: Loss = -11716.344899082827
1
Iteration 10000: Loss = -11716.360343048245
2
Iteration 10100: Loss = -11716.345539853992
3
Iteration 10200: Loss = -11716.674832508053
4
Iteration 10300: Loss = -11716.343864281129
Iteration 10400: Loss = -11716.347129679007
1
Iteration 10500: Loss = -11716.343813644671
Iteration 10600: Loss = -11716.347341709037
1
Iteration 10700: Loss = -11716.350506860148
2
Iteration 10800: Loss = -11716.343559101877
Iteration 10900: Loss = -11716.530122167544
1
Iteration 11000: Loss = -11716.34355939296
Iteration 11100: Loss = -11716.430516848928
1
Iteration 11200: Loss = -11716.343521208697
Iteration 11300: Loss = -11716.343538835068
Iteration 11400: Loss = -11716.34372887446
1
Iteration 11500: Loss = -11716.343683152689
2
Iteration 11600: Loss = -11716.344605024306
3
Iteration 11700: Loss = -11716.390990434265
4
Iteration 11800: Loss = -11716.34772803456
5
Iteration 11900: Loss = -11716.343403294788
Iteration 12000: Loss = -11716.377134238222
1
Iteration 12100: Loss = -11716.342122116537
Iteration 12200: Loss = -11716.353343379691
1
Iteration 12300: Loss = -11716.342117505048
Iteration 12400: Loss = -11716.352031919527
1
Iteration 12500: Loss = -11716.342105812955
Iteration 12600: Loss = -11716.342045911453
Iteration 12700: Loss = -11716.344987003296
1
Iteration 12800: Loss = -11716.341614201294
Iteration 12900: Loss = -11716.34180196362
1
Iteration 13000: Loss = -11716.387036932581
2
Iteration 13100: Loss = -11716.341598360743
Iteration 13200: Loss = -11716.341588935336
Iteration 13300: Loss = -11716.348013410332
1
Iteration 13400: Loss = -11716.373265124746
2
Iteration 13500: Loss = -11716.34186957575
3
Iteration 13600: Loss = -11716.342979658408
4
Iteration 13700: Loss = -11716.350722815037
5
Iteration 13800: Loss = -11716.348871532973
6
Iteration 13900: Loss = -11716.341513654666
Iteration 14000: Loss = -11716.34274880213
1
Iteration 14100: Loss = -11716.341516395903
Iteration 14200: Loss = -11716.342290666718
1
Iteration 14300: Loss = -11716.379668621537
2
Iteration 14400: Loss = -11716.71922610229
3
Iteration 14500: Loss = -11716.341523907777
Iteration 14600: Loss = -11716.341509978141
Iteration 14700: Loss = -11716.343196584945
1
Iteration 14800: Loss = -11716.34150856102
Iteration 14900: Loss = -11716.344926239908
1
Iteration 15000: Loss = -11716.342415063642
2
Iteration 15100: Loss = -11716.359351945044
3
Iteration 15200: Loss = -11716.345787452867
4
Iteration 15300: Loss = -11716.34157737149
Iteration 15400: Loss = -11716.358611947308
1
Iteration 15500: Loss = -11716.341519160915
Iteration 15600: Loss = -11716.342187706548
1
Iteration 15700: Loss = -11716.341524592106
Iteration 15800: Loss = -11716.34317363998
1
Iteration 15900: Loss = -11716.341636217972
2
Iteration 16000: Loss = -11716.36881697813
3
Iteration 16100: Loss = -11716.347419734991
4
Iteration 16200: Loss = -11716.36003376001
5
Iteration 16300: Loss = -11716.341503688287
Iteration 16400: Loss = -11716.343748685842
1
Iteration 16500: Loss = -11716.343370120238
2
Iteration 16600: Loss = -11716.349472838545
3
Iteration 16700: Loss = -11716.353472493518
4
Iteration 16800: Loss = -11716.3414954615
Iteration 16900: Loss = -11716.348416626028
1
Iteration 17000: Loss = -11716.341453429164
Iteration 17100: Loss = -11716.347934874719
1
Iteration 17200: Loss = -11716.342224189117
2
Iteration 17300: Loss = -11716.34121619776
Iteration 17400: Loss = -11716.354643958184
1
Iteration 17500: Loss = -11716.347725181615
2
Iteration 17600: Loss = -11716.341264135228
Iteration 17700: Loss = -11716.342690232786
1
Iteration 17800: Loss = -11716.422844556111
2
Iteration 17900: Loss = -11716.341184943809
Iteration 18000: Loss = -11716.3624829013
1
Iteration 18100: Loss = -11716.34121997428
Iteration 18200: Loss = -11716.3425767453
1
Iteration 18300: Loss = -11716.342413554083
2
Iteration 18400: Loss = -11716.341172987988
Iteration 18500: Loss = -11716.381890619097
1
Iteration 18600: Loss = -11716.339522707145
Iteration 18700: Loss = -11716.341344130751
1
Iteration 18800: Loss = -11716.33954401346
Iteration 18900: Loss = -11716.365275972737
1
Iteration 19000: Loss = -11716.352085580393
2
Iteration 19100: Loss = -11716.33968684811
3
Iteration 19200: Loss = -11716.342927943499
4
Iteration 19300: Loss = -11716.340689868499
5
Iteration 19400: Loss = -11716.359141558707
6
Iteration 19500: Loss = -11716.342803211724
7
Iteration 19600: Loss = -11716.342852046466
8
Iteration 19700: Loss = -11716.339825122055
9
Iteration 19800: Loss = -11716.357130502844
10
Iteration 19900: Loss = -11716.339530951715
pi: tensor([[0.2023, 0.7977],
        [0.6841, 0.3159]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4793, 0.5207], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3241, 0.0981],
         [0.6483, 0.2640]],

        [[0.6604, 0.0890],
         [0.6014, 0.6841]],

        [[0.7281, 0.0987],
         [0.6267, 0.5333]],

        [[0.5731, 0.1057],
         [0.5757, 0.6869]],

        [[0.5333, 0.1055],
         [0.7302, 0.6601]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369696969696969
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9205441326485486
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721116882917585
time is 4
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.05379073602330292
Average Adjusted Rand Index: 0.8700862310511311
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22393.264922584025
Iteration 100: Loss = -12058.455781865649
Iteration 200: Loss = -11903.114260324457
Iteration 300: Loss = -11802.775936187789
Iteration 400: Loss = -11793.876611294247
Iteration 500: Loss = -11785.091064616374
Iteration 600: Loss = -11783.336096044513
Iteration 700: Loss = -11783.17926962362
Iteration 800: Loss = -11783.098145918433
Iteration 900: Loss = -11783.063924858538
Iteration 1000: Loss = -11782.674503497028
Iteration 1100: Loss = -11782.641062399834
Iteration 1200: Loss = -11774.491299845562
Iteration 1300: Loss = -11766.792797499727
Iteration 1400: Loss = -11764.837966379282
Iteration 1500: Loss = -11741.868254863675
Iteration 1600: Loss = -11737.998205072463
Iteration 1700: Loss = -11737.98605816551
Iteration 1800: Loss = -11737.964777020312
Iteration 1900: Loss = -11730.691139616834
Iteration 2000: Loss = -11722.740361575648
Iteration 2100: Loss = -11718.502215863005
Iteration 2200: Loss = -11696.667286021948
Iteration 2300: Loss = -11682.477303313355
Iteration 2400: Loss = -11682.465353875132
Iteration 2500: Loss = -11682.451874950217
Iteration 2600: Loss = -11682.449175230064
Iteration 2700: Loss = -11682.446116388406
Iteration 2800: Loss = -11682.44082928826
Iteration 2900: Loss = -11682.416633812709
Iteration 3000: Loss = -11682.415098457368
Iteration 3100: Loss = -11682.41375567113
Iteration 3200: Loss = -11682.412788571048
Iteration 3300: Loss = -11682.41174619377
Iteration 3400: Loss = -11682.410264590388
Iteration 3500: Loss = -11675.77818336069
Iteration 3600: Loss = -11675.775529426013
Iteration 3700: Loss = -11675.774738465057
Iteration 3800: Loss = -11675.774315104787
Iteration 3900: Loss = -11675.773393644871
Iteration 4000: Loss = -11675.772695512607
Iteration 4100: Loss = -11675.766900201708
Iteration 4200: Loss = -11675.754652767047
Iteration 4300: Loss = -11675.753466738923
Iteration 4400: Loss = -11675.752619613448
Iteration 4500: Loss = -11674.978675071408
Iteration 4600: Loss = -11674.976490715177
Iteration 4700: Loss = -11674.976497219845
Iteration 4800: Loss = -11674.975062141984
Iteration 4900: Loss = -11674.975500640656
1
Iteration 5000: Loss = -11674.9746320734
Iteration 5100: Loss = -11674.974827352158
1
Iteration 5200: Loss = -11674.974391630934
Iteration 5300: Loss = -11674.974330520958
Iteration 5400: Loss = -11674.974106009762
Iteration 5500: Loss = -11674.983965686359
1
Iteration 5600: Loss = -11674.973886438807
Iteration 5700: Loss = -11674.973743912775
Iteration 5800: Loss = -11674.973731115062
Iteration 5900: Loss = -11674.973544993432
Iteration 6000: Loss = -11674.973645128237
1
Iteration 6100: Loss = -11674.973389019402
Iteration 6200: Loss = -11674.976289113854
1
Iteration 6300: Loss = -11674.983268248781
2
Iteration 6400: Loss = -11674.973901263586
3
Iteration 6500: Loss = -11674.974407431178
4
Iteration 6600: Loss = -11674.972097227525
Iteration 6700: Loss = -11674.97111231069
Iteration 6800: Loss = -11674.969106043656
Iteration 6900: Loss = -11674.968998201388
Iteration 7000: Loss = -11674.968837883831
Iteration 7100: Loss = -11674.969309186868
1
Iteration 7200: Loss = -11674.9681516223
Iteration 7300: Loss = -11674.97208131953
1
Iteration 7400: Loss = -11674.969602569923
2
Iteration 7500: Loss = -11674.968392245282
3
Iteration 7600: Loss = -11674.989784747057
4
Iteration 7700: Loss = -11674.958745165899
Iteration 7800: Loss = -11674.958722517014
Iteration 7900: Loss = -11674.962627021114
1
Iteration 8000: Loss = -11674.958611583155
Iteration 8100: Loss = -11674.958534490395
Iteration 8200: Loss = -11674.95874968196
1
Iteration 8300: Loss = -11674.958524230891
Iteration 8400: Loss = -11674.958529861975
Iteration 8500: Loss = -11674.958504316262
Iteration 8600: Loss = -11674.958413572407
Iteration 8700: Loss = -11674.958368560427
Iteration 8800: Loss = -11674.957805269923
Iteration 8900: Loss = -11674.95756806646
Iteration 9000: Loss = -11674.957553586804
Iteration 9100: Loss = -11674.981889432613
1
Iteration 9200: Loss = -11674.957490654697
Iteration 9300: Loss = -11674.95747166406
Iteration 9400: Loss = -11674.957579522856
1
Iteration 9500: Loss = -11674.957425011777
Iteration 9600: Loss = -11674.9573241696
Iteration 9700: Loss = -11674.957300145408
Iteration 9800: Loss = -11674.957237324
Iteration 9900: Loss = -11674.959511775485
1
Iteration 10000: Loss = -11674.957211141498
Iteration 10100: Loss = -11674.961143428149
1
Iteration 10200: Loss = -11674.969643259297
2
Iteration 10300: Loss = -11674.957207578202
Iteration 10400: Loss = -11674.95856264527
1
Iteration 10500: Loss = -11674.957113872717
Iteration 10600: Loss = -11674.957845681487
1
Iteration 10700: Loss = -11675.022953127656
2
Iteration 10800: Loss = -11674.956364893074
Iteration 10900: Loss = -11674.976944021253
1
Iteration 11000: Loss = -11674.956202840047
Iteration 11100: Loss = -11674.95639601707
1
Iteration 11200: Loss = -11674.957947701312
2
Iteration 11300: Loss = -11674.989357994957
3
Iteration 11400: Loss = -11674.826026165569
Iteration 11500: Loss = -11674.82272590854
Iteration 11600: Loss = -11674.823038937426
1
Iteration 11700: Loss = -11674.82262579947
Iteration 11800: Loss = -11674.82355163257
1
Iteration 11900: Loss = -11674.822617376787
Iteration 12000: Loss = -11674.824570210258
1
Iteration 12100: Loss = -11674.82279120446
2
Iteration 12200: Loss = -11674.822776152965
3
Iteration 12300: Loss = -11674.8225929067
Iteration 12400: Loss = -11674.823057322228
1
Iteration 12500: Loss = -11674.822581433617
Iteration 12600: Loss = -11674.828731014284
1
Iteration 12700: Loss = -11674.822591458164
Iteration 12800: Loss = -11674.899293120976
1
Iteration 12900: Loss = -11674.822578764226
Iteration 13000: Loss = -11674.822583052923
Iteration 13100: Loss = -11674.822935166518
1
Iteration 13200: Loss = -11674.82257636263
Iteration 13300: Loss = -11675.279259555527
1
Iteration 13400: Loss = -11674.8226014679
Iteration 13500: Loss = -11674.822572340467
Iteration 13600: Loss = -11674.823649940125
1
Iteration 13700: Loss = -11674.822619546834
Iteration 13800: Loss = -11674.82258470497
Iteration 13900: Loss = -11674.99310527978
1
Iteration 14000: Loss = -11674.822571906629
Iteration 14100: Loss = -11674.822579823105
Iteration 14200: Loss = -11674.828974277249
1
Iteration 14300: Loss = -11674.8238496766
2
Iteration 14400: Loss = -11674.8226248024
Iteration 14500: Loss = -11674.942354448196
1
Iteration 14600: Loss = -11674.822559112192
Iteration 14700: Loss = -11674.839540766472
1
Iteration 14800: Loss = -11674.825819562653
2
Iteration 14900: Loss = -11674.82313816211
3
Iteration 15000: Loss = -11674.83239725431
4
Iteration 15100: Loss = -11674.822585073673
Iteration 15200: Loss = -11674.823551742882
1
Iteration 15300: Loss = -11674.823261726482
2
Iteration 15400: Loss = -11674.822828520239
3
Iteration 15500: Loss = -11674.862928828203
4
Iteration 15600: Loss = -11674.822548640299
Iteration 15700: Loss = -11674.822546078462
Iteration 15800: Loss = -11674.824539864136
1
Iteration 15900: Loss = -11674.8225369397
Iteration 16000: Loss = -11674.836680920764
1
Iteration 16100: Loss = -11674.822560978986
Iteration 16200: Loss = -11674.822534199671
Iteration 16300: Loss = -11674.823441618102
1
Iteration 16400: Loss = -11674.822542025953
Iteration 16500: Loss = -11674.913552781252
1
Iteration 16600: Loss = -11674.822536703396
Iteration 16700: Loss = -11674.822549810908
Iteration 16800: Loss = -11674.82269505704
1
Iteration 16900: Loss = -11674.822533668821
Iteration 17000: Loss = -11674.822661993387
1
Iteration 17100: Loss = -11674.822523616338
Iteration 17200: Loss = -11674.827136226144
1
Iteration 17300: Loss = -11674.822520608512
Iteration 17400: Loss = -11674.82378993991
1
Iteration 17500: Loss = -11674.822540870653
Iteration 17600: Loss = -11674.822519850419
Iteration 17700: Loss = -11674.835920311874
1
Iteration 17800: Loss = -11674.822513241194
Iteration 17900: Loss = -11674.822565033532
Iteration 18000: Loss = -11674.822555629646
Iteration 18100: Loss = -11674.85635750192
1
Iteration 18200: Loss = -11674.822508473888
Iteration 18300: Loss = -11674.845010763935
1
Iteration 18400: Loss = -11674.822533839862
Iteration 18500: Loss = -11674.822854498923
1
Iteration 18600: Loss = -11674.822541781785
Iteration 18700: Loss = -11674.82250995423
Iteration 18800: Loss = -11674.823908849126
1
Iteration 18900: Loss = -11674.822509895852
Iteration 19000: Loss = -11675.302437217146
1
Iteration 19100: Loss = -11674.822516886465
Iteration 19200: Loss = -11674.822505813472
Iteration 19300: Loss = -11674.885849411354
1
Iteration 19400: Loss = -11674.822498209276
Iteration 19500: Loss = -11674.82251353697
Iteration 19600: Loss = -11674.837918480305
1
Iteration 19700: Loss = -11674.82252910713
Iteration 19800: Loss = -11674.823032245424
1
Iteration 19900: Loss = -11674.830071083052
2
pi: tensor([[0.5787, 0.4213],
        [0.5378, 0.4622]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5351, 0.4649], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2247, 0.0990],
         [0.6798, 0.3799]],

        [[0.6938, 0.0965],
         [0.6443, 0.7025]],

        [[0.6369, 0.0989],
         [0.5875, 0.6649]],

        [[0.6187, 0.1069],
         [0.6304, 0.5770]],

        [[0.6660, 0.1053],
         [0.6551, 0.6886]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
Difference count: 19
Adjusted Rand Index: 0.3781818181818182
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9205441326485486
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.43973389605618196
Average Adjusted Rand Index: 0.8517447513296365
11516.898169442185
[0.05379073602330292, 0.43973389605618196] [0.8700862310511311, 0.8517447513296365] [11716.360205085632, 11674.822543058097]
-------------------------------------
This iteration is 38
True Objective function: Loss = -11511.67077404651
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22232.725726964818
Iteration 100: Loss = -11505.270586520484
Iteration 200: Loss = -11503.695594613899
Iteration 300: Loss = -11503.30890190075
Iteration 400: Loss = -11503.138601501914
Iteration 500: Loss = -11503.046140808132
Iteration 600: Loss = -11502.98978260599
Iteration 700: Loss = -11502.952571536407
Iteration 800: Loss = -11502.926589999865
Iteration 900: Loss = -11502.907624568932
Iteration 1000: Loss = -11502.893407361618
Iteration 1100: Loss = -11502.882361187387
Iteration 1200: Loss = -11502.873621571009
Iteration 1300: Loss = -11502.866583532295
Iteration 1400: Loss = -11502.860799678252
Iteration 1500: Loss = -11502.856050399521
Iteration 1600: Loss = -11502.852081674568
Iteration 1700: Loss = -11502.848699300452
Iteration 1800: Loss = -11502.84573777151
Iteration 1900: Loss = -11502.843238548194
Iteration 2000: Loss = -11502.841101181753
Iteration 2100: Loss = -11502.839215660124
Iteration 2200: Loss = -11502.837514259116
Iteration 2300: Loss = -11502.836065951718
Iteration 2400: Loss = -11502.834769848178
Iteration 2500: Loss = -11502.833587938903
Iteration 2600: Loss = -11502.83261786222
Iteration 2700: Loss = -11502.831619248494
Iteration 2800: Loss = -11502.830761308325
Iteration 2900: Loss = -11502.830024097744
Iteration 3000: Loss = -11502.829356411436
Iteration 3100: Loss = -11502.829032978585
Iteration 3200: Loss = -11502.82813555894
Iteration 3300: Loss = -11502.828227360133
Iteration 3400: Loss = -11502.827148666087
Iteration 3500: Loss = -11502.826697647151
Iteration 3600: Loss = -11502.82734376846
1
Iteration 3700: Loss = -11502.828797721388
2
Iteration 3800: Loss = -11502.825665494864
Iteration 3900: Loss = -11502.82544473021
Iteration 4000: Loss = -11502.824974080811
Iteration 4100: Loss = -11502.82699738318
1
Iteration 4200: Loss = -11502.824433694968
Iteration 4300: Loss = -11502.824218619677
Iteration 4400: Loss = -11502.82513264696
1
Iteration 4500: Loss = -11502.823764757113
Iteration 4600: Loss = -11502.823647232033
Iteration 4700: Loss = -11502.824694760311
1
Iteration 4800: Loss = -11502.827908117933
2
Iteration 4900: Loss = -11502.823099116049
Iteration 5000: Loss = -11502.824582749607
1
Iteration 5100: Loss = -11502.82284454296
Iteration 5200: Loss = -11502.83087371932
1
Iteration 5300: Loss = -11502.822594545643
Iteration 5400: Loss = -11502.82268474713
Iteration 5500: Loss = -11502.823126561134
1
Iteration 5600: Loss = -11502.82228646651
Iteration 5700: Loss = -11502.822222396899
Iteration 5800: Loss = -11502.82235705635
1
Iteration 5900: Loss = -11502.822073097468
Iteration 6000: Loss = -11502.822297689128
1
Iteration 6100: Loss = -11502.822138900283
Iteration 6200: Loss = -11502.82344633549
1
Iteration 6300: Loss = -11502.822164512676
Iteration 6400: Loss = -11502.821721301203
Iteration 6500: Loss = -11502.821694784314
Iteration 6600: Loss = -11502.836810814744
1
Iteration 6700: Loss = -11502.821539829998
Iteration 6800: Loss = -11502.836433450577
1
Iteration 6900: Loss = -11502.821473193055
Iteration 7000: Loss = -11502.821434596864
Iteration 7100: Loss = -11502.821873310635
1
Iteration 7200: Loss = -11502.821379819237
Iteration 7300: Loss = -11502.821297678092
Iteration 7400: Loss = -11502.822505759426
1
Iteration 7500: Loss = -11502.821284095373
Iteration 7600: Loss = -11502.821229370047
Iteration 7700: Loss = -11502.82118124809
Iteration 7800: Loss = -11502.821322062164
1
Iteration 7900: Loss = -11502.821165981888
Iteration 8000: Loss = -11502.82109842936
Iteration 8100: Loss = -11502.821251643736
1
Iteration 8200: Loss = -11502.821096429712
Iteration 8300: Loss = -11502.821121874647
Iteration 8400: Loss = -11502.822780782877
1
Iteration 8500: Loss = -11502.821059985834
Iteration 8600: Loss = -11502.833885920529
1
Iteration 8700: Loss = -11502.909273045405
2
Iteration 8800: Loss = -11502.820997684126
Iteration 8900: Loss = -11502.838576755817
1
Iteration 9000: Loss = -11502.820924185868
Iteration 9100: Loss = -11502.824821984017
1
Iteration 9200: Loss = -11502.82625019312
2
Iteration 9300: Loss = -11502.825039518484
3
Iteration 9400: Loss = -11502.833024643402
4
Iteration 9500: Loss = -11502.822957980323
5
Iteration 9600: Loss = -11502.861256288214
6
Iteration 9700: Loss = -11502.911430483642
7
Iteration 9800: Loss = -11502.851124980645
8
Iteration 9900: Loss = -11502.820939143257
Iteration 10000: Loss = -11502.821593145745
1
Iteration 10100: Loss = -11502.861427383768
2
Iteration 10200: Loss = -11502.836789658759
3
Iteration 10300: Loss = -11502.82276912409
4
Iteration 10400: Loss = -11502.955201679906
5
Iteration 10500: Loss = -11502.856860975398
6
Iteration 10600: Loss = -11502.823951297549
7
Iteration 10700: Loss = -11502.833194562269
8
Iteration 10800: Loss = -11502.828391662662
9
Iteration 10900: Loss = -11502.824148162317
10
Iteration 11000: Loss = -11502.821694562666
11
Iteration 11100: Loss = -11502.828536717732
12
Iteration 11200: Loss = -11502.903784519669
13
Iteration 11300: Loss = -11502.837708020123
14
Iteration 11400: Loss = -11502.820803743169
Iteration 11500: Loss = -11502.828520678744
1
Iteration 11600: Loss = -11502.825294966571
2
Iteration 11700: Loss = -11502.881791603791
3
Iteration 11800: Loss = -11502.823614752317
4
Iteration 11900: Loss = -11502.844191816113
5
Iteration 12000: Loss = -11502.822337820882
6
Iteration 12100: Loss = -11502.82297358255
7
Iteration 12200: Loss = -11502.822454434641
8
Iteration 12300: Loss = -11502.836929056026
9
Iteration 12400: Loss = -11502.853510249295
10
Iteration 12500: Loss = -11502.825446375145
11
Iteration 12600: Loss = -11502.826882616584
12
Iteration 12700: Loss = -11502.821911405688
13
Iteration 12800: Loss = -11502.820942144113
14
Iteration 12900: Loss = -11502.828025382114
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.7477, 0.2523],
        [0.2131, 0.7869]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5723, 0.4277], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4014, 0.0959],
         [0.5707, 0.2003]],

        [[0.5019, 0.1052],
         [0.6937, 0.5586]],

        [[0.5807, 0.0882],
         [0.5191, 0.7248]],

        [[0.6589, 0.0987],
         [0.7247, 0.5049]],

        [[0.5200, 0.1055],
         [0.5604, 0.7278]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320089020589
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21675.221486770246
Iteration 100: Loss = -12322.343835969814
Iteration 200: Loss = -12027.189941469698
Iteration 300: Loss = -11526.587770448014
Iteration 400: Loss = -11504.394381573089
Iteration 500: Loss = -11503.818842366749
Iteration 600: Loss = -11503.540722761136
Iteration 700: Loss = -11503.37918511408
Iteration 800: Loss = -11503.275025216464
Iteration 900: Loss = -11503.202659233351
Iteration 1000: Loss = -11503.149826959781
Iteration 1100: Loss = -11503.109809190442
Iteration 1200: Loss = -11503.078675932116
Iteration 1300: Loss = -11503.053833556314
Iteration 1400: Loss = -11503.0360796479
Iteration 1500: Loss = -11503.017156770227
Iteration 1600: Loss = -11503.00332113708
Iteration 1700: Loss = -11502.991627502262
Iteration 1800: Loss = -11502.981633962781
Iteration 1900: Loss = -11502.97338874005
Iteration 2000: Loss = -11502.965627168112
Iteration 2100: Loss = -11502.959106858552
Iteration 2200: Loss = -11502.953381539475
Iteration 2300: Loss = -11502.948365867709
Iteration 2400: Loss = -11502.943920136588
Iteration 2500: Loss = -11502.939948481288
Iteration 2600: Loss = -11502.936322153311
Iteration 2700: Loss = -11502.93311745478
Iteration 2800: Loss = -11502.930543685154
Iteration 2900: Loss = -11502.927727868277
Iteration 3000: Loss = -11502.92519767013
Iteration 3100: Loss = -11502.923037534676
Iteration 3200: Loss = -11502.924600358228
1
Iteration 3300: Loss = -11502.919201779134
Iteration 3400: Loss = -11502.917546347262
Iteration 3500: Loss = -11502.916080590416
Iteration 3600: Loss = -11502.914874870687
Iteration 3700: Loss = -11502.913032677368
Iteration 3800: Loss = -11502.911381336291
Iteration 3900: Loss = -11502.908061476239
Iteration 4000: Loss = -11502.89936682993
Iteration 4100: Loss = -11502.897897069515
Iteration 4200: Loss = -11502.89701723827
Iteration 4300: Loss = -11502.8961310602
Iteration 4400: Loss = -11502.914916539947
1
Iteration 4500: Loss = -11502.89493145356
Iteration 4600: Loss = -11502.894031165744
Iteration 4700: Loss = -11502.893387712567
Iteration 4800: Loss = -11502.898968137148
1
Iteration 4900: Loss = -11502.892923331492
Iteration 5000: Loss = -11502.891835718287
Iteration 5100: Loss = -11502.894030762773
1
Iteration 5200: Loss = -11502.892084578742
2
Iteration 5300: Loss = -11502.890435863894
Iteration 5400: Loss = -11502.890122214438
Iteration 5500: Loss = -11502.890836962404
1
Iteration 5600: Loss = -11502.889404638128
Iteration 5700: Loss = -11502.889056975444
Iteration 5800: Loss = -11502.890770661445
1
Iteration 5900: Loss = -11502.888569898165
Iteration 6000: Loss = -11502.888271528025
Iteration 6100: Loss = -11502.888720477755
1
Iteration 6200: Loss = -11502.888444033253
2
Iteration 6300: Loss = -11502.88750346745
Iteration 6400: Loss = -11502.887297534431
Iteration 6500: Loss = -11502.887306652952
Iteration 6600: Loss = -11502.887394802528
Iteration 6700: Loss = -11502.886717684967
Iteration 6800: Loss = -11502.886502663448
Iteration 6900: Loss = -11502.890402089759
1
Iteration 7000: Loss = -11502.887494315279
2
Iteration 7100: Loss = -11502.886323598706
Iteration 7200: Loss = -11502.88682618982
1
Iteration 7300: Loss = -11503.00461296334
2
Iteration 7400: Loss = -11502.88562160983
Iteration 7500: Loss = -11502.897903062332
1
Iteration 7600: Loss = -11502.838525868076
Iteration 7700: Loss = -11502.863170782406
1
Iteration 7800: Loss = -11502.838328368149
Iteration 7900: Loss = -11502.851575304578
1
Iteration 8000: Loss = -11502.838195299493
Iteration 8100: Loss = -11502.844233744385
1
Iteration 8200: Loss = -11502.839420435765
2
Iteration 8300: Loss = -11502.839506270317
3
Iteration 8400: Loss = -11502.842464319565
4
Iteration 8500: Loss = -11502.840678810662
5
Iteration 8600: Loss = -11502.866388225024
6
Iteration 8700: Loss = -11502.866140134578
7
Iteration 8800: Loss = -11502.837685230212
Iteration 8900: Loss = -11502.8378185194
1
Iteration 9000: Loss = -11502.838942886228
2
Iteration 9100: Loss = -11502.867883997664
3
Iteration 9200: Loss = -11502.84203471167
4
Iteration 9300: Loss = -11502.838375147303
5
Iteration 9400: Loss = -11502.837777134402
Iteration 9500: Loss = -11502.838238266886
1
Iteration 9600: Loss = -11502.839096119009
2
Iteration 9700: Loss = -11502.850161373344
3
Iteration 9800: Loss = -11502.850237145112
4
Iteration 9900: Loss = -11502.857549044758
5
Iteration 10000: Loss = -11502.837272598385
Iteration 10100: Loss = -11502.837469606387
1
Iteration 10200: Loss = -11502.839526694885
2
Iteration 10300: Loss = -11502.885563324018
3
Iteration 10400: Loss = -11502.870951722532
4
Iteration 10500: Loss = -11502.838769637785
5
Iteration 10600: Loss = -11502.838106395368
6
Iteration 10700: Loss = -11502.84994718049
7
Iteration 10800: Loss = -11502.847804020204
8
Iteration 10900: Loss = -11502.840405102868
9
Iteration 11000: Loss = -11502.837625208967
10
Iteration 11100: Loss = -11502.884541408603
11
Iteration 11200: Loss = -11502.851261007272
12
Iteration 11300: Loss = -11502.83779097621
13
Iteration 11400: Loss = -11502.846655027186
14
Iteration 11500: Loss = -11502.850333900917
15
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[0.7875, 0.2125],
        [0.2513, 0.7487]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4300, 0.5700], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2013, 0.0959],
         [0.5086, 0.4010]],

        [[0.6498, 0.1052],
         [0.5219, 0.6949]],

        [[0.6420, 0.0882],
         [0.5611, 0.5423]],

        [[0.6586, 0.0986],
         [0.5801, 0.5450]],

        [[0.6371, 0.1056],
         [0.7076, 0.6946]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320089020589
Average Adjusted Rand Index: 0.9839998119331363
11511.67077404651
[0.9840320089020589, 0.9840320089020589] [0.9839998119331363, 0.9839998119331363] [11502.828025382114, 11502.850333900917]
-------------------------------------
This iteration is 39
True Objective function: Loss = -11167.321792328867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21497.33639505526
Iteration 100: Loss = -11891.64082917147
Iteration 200: Loss = -11882.225015237576
Iteration 300: Loss = -11857.193246858447
Iteration 400: Loss = -11550.480469055703
Iteration 500: Loss = -11489.963821587431
Iteration 600: Loss = -11449.270104027546
Iteration 700: Loss = -11439.672005761377
Iteration 800: Loss = -11409.830066737246
Iteration 900: Loss = -11371.203114162145
Iteration 1000: Loss = -11355.186078349367
Iteration 1100: Loss = -11343.56865101602
Iteration 1200: Loss = -11343.493337248543
Iteration 1300: Loss = -11343.446875884374
Iteration 1400: Loss = -11343.4129183347
Iteration 1500: Loss = -11343.38678907921
Iteration 1600: Loss = -11343.366203024209
Iteration 1700: Loss = -11343.349335457617
Iteration 1800: Loss = -11343.335400582477
Iteration 1900: Loss = -11343.323509958562
Iteration 2000: Loss = -11343.313263056805
Iteration 2100: Loss = -11343.303654466441
Iteration 2200: Loss = -11343.290647873575
Iteration 2300: Loss = -11343.151844035376
Iteration 2400: Loss = -11343.14539337382
Iteration 2500: Loss = -11343.140342649951
Iteration 2600: Loss = -11343.139476072789
Iteration 2700: Loss = -11343.131360615826
Iteration 2800: Loss = -11343.13481866686
1
Iteration 2900: Loss = -11343.123510267586
Iteration 3000: Loss = -11343.119833897885
Iteration 3100: Loss = -11343.115024587465
Iteration 3200: Loss = -11343.111450315613
Iteration 3300: Loss = -11343.108149127434
Iteration 3400: Loss = -11343.107067417235
Iteration 3500: Loss = -11343.10413968934
Iteration 3600: Loss = -11343.108185844916
1
Iteration 3700: Loss = -11343.10091279005
Iteration 3800: Loss = -11343.099555303244
Iteration 3900: Loss = -11343.09810061412
Iteration 4000: Loss = -11343.096810439749
Iteration 4100: Loss = -11343.096202670127
Iteration 4200: Loss = -11343.094529593913
Iteration 4300: Loss = -11343.093512623982
Iteration 4400: Loss = -11343.095174565851
1
Iteration 4500: Loss = -11343.092031781705
Iteration 4600: Loss = -11343.090825461753
Iteration 4700: Loss = -11343.090226902863
Iteration 4800: Loss = -11343.095642903965
1
Iteration 4900: Loss = -11343.089303423572
Iteration 5000: Loss = -11343.08796267736
Iteration 5100: Loss = -11343.087985407305
Iteration 5200: Loss = -11343.08668978805
Iteration 5300: Loss = -11343.087212836734
1
Iteration 5400: Loss = -11343.085482213917
Iteration 5500: Loss = -11343.085561495593
Iteration 5600: Loss = -11343.084671730685
Iteration 5700: Loss = -11343.084112594048
Iteration 5800: Loss = -11343.083696853164
Iteration 5900: Loss = -11343.083820042286
1
Iteration 6000: Loss = -11343.082743100944
Iteration 6100: Loss = -11343.082370531236
Iteration 6200: Loss = -11343.082053053273
Iteration 6300: Loss = -11343.081794411122
Iteration 6400: Loss = -11343.082240156045
1
Iteration 6500: Loss = -11343.083302915269
2
Iteration 6600: Loss = -11343.081096948514
Iteration 6700: Loss = -11343.080898273203
Iteration 6800: Loss = -11343.080685282703
Iteration 6900: Loss = -11343.08048940473
Iteration 7000: Loss = -11343.080393712933
Iteration 7100: Loss = -11343.08016335855
Iteration 7200: Loss = -11343.080008945479
Iteration 7300: Loss = -11343.080239101302
1
Iteration 7400: Loss = -11343.079738192426
Iteration 7500: Loss = -11343.07951001575
Iteration 7600: Loss = -11342.95629571738
Iteration 7700: Loss = -11342.956610522142
1
Iteration 7800: Loss = -11342.955185372386
Iteration 7900: Loss = -11342.95343768815
Iteration 8000: Loss = -11342.953454115905
Iteration 8100: Loss = -11342.992925787084
1
Iteration 8200: Loss = -11342.9520096241
Iteration 8300: Loss = -11342.958656115548
1
Iteration 8400: Loss = -11342.951548172186
Iteration 8500: Loss = -11342.953182972647
1
Iteration 8600: Loss = -11342.951389762266
Iteration 8700: Loss = -11342.951656525172
1
Iteration 8800: Loss = -11342.962551021621
2
Iteration 8900: Loss = -11342.958051697266
3
Iteration 9000: Loss = -11342.950040475522
Iteration 9100: Loss = -11342.950051013348
Iteration 9200: Loss = -11342.949941540577
Iteration 9300: Loss = -11342.975535708065
1
Iteration 9400: Loss = -11342.949822528342
Iteration 9500: Loss = -11342.949833462539
Iteration 9600: Loss = -11342.95011180106
1
Iteration 9700: Loss = -11342.786041593925
Iteration 9800: Loss = -11342.7827510315
Iteration 9900: Loss = -11342.780841697417
Iteration 10000: Loss = -11342.784894299124
1
Iteration 10100: Loss = -11342.780749204117
Iteration 10200: Loss = -11342.782005494786
1
Iteration 10300: Loss = -11342.780713419983
Iteration 10400: Loss = -11342.783050778204
1
Iteration 10500: Loss = -11342.787757076298
2
Iteration 10600: Loss = -11342.879705218851
3
Iteration 10700: Loss = -11342.927104597553
4
Iteration 10800: Loss = -11342.77766960782
Iteration 10900: Loss = -11342.777798644336
1
Iteration 11000: Loss = -11342.797488186809
2
Iteration 11100: Loss = -11342.777585048041
Iteration 11200: Loss = -11342.786018447805
1
Iteration 11300: Loss = -11342.777718052816
2
Iteration 11400: Loss = -11342.777760153062
3
Iteration 11500: Loss = -11342.787379240106
4
Iteration 11600: Loss = -11342.777795940794
5
Iteration 11700: Loss = -11342.777648569623
Iteration 11800: Loss = -11342.77799862197
1
Iteration 11900: Loss = -11342.777959656554
2
Iteration 12000: Loss = -11342.778143825835
3
Iteration 12100: Loss = -11342.777512405408
Iteration 12200: Loss = -11342.790773147597
1
Iteration 12300: Loss = -11342.93276949865
2
Iteration 12400: Loss = -11342.780313575908
3
Iteration 12500: Loss = -11342.777384898882
Iteration 12600: Loss = -11342.777634438977
1
Iteration 12700: Loss = -11342.782356854725
2
Iteration 12800: Loss = -11342.814713290894
3
Iteration 12900: Loss = -11342.777333634542
Iteration 13000: Loss = -11342.777541976446
1
Iteration 13100: Loss = -11342.777581399878
2
Iteration 13200: Loss = -11342.802808659446
3
Iteration 13300: Loss = -11342.788247917286
4
Iteration 13400: Loss = -11342.777369828607
Iteration 13500: Loss = -11342.777394921002
Iteration 13600: Loss = -11342.778781279369
1
Iteration 13700: Loss = -11342.792958730812
2
Iteration 13800: Loss = -11342.777272909603
Iteration 13900: Loss = -11342.777385008963
1
Iteration 14000: Loss = -11342.74433066227
Iteration 14100: Loss = -11342.735547965
Iteration 14200: Loss = -11342.74170005255
1
Iteration 14300: Loss = -11342.966454022082
2
Iteration 14400: Loss = -11342.749334477441
3
Iteration 14500: Loss = -11342.735285538438
Iteration 14600: Loss = -11342.73917855474
1
Iteration 14700: Loss = -11342.734558017513
Iteration 14800: Loss = -11342.733403091383
Iteration 14900: Loss = -11342.73353279989
1
Iteration 15000: Loss = -11342.733528013301
2
Iteration 15100: Loss = -11342.734708397033
3
Iteration 15200: Loss = -11342.735822411167
4
Iteration 15300: Loss = -11342.740583499603
5
Iteration 15400: Loss = -11342.823877864947
6
Iteration 15500: Loss = -11342.866196467705
7
Iteration 15600: Loss = -11342.736150269746
8
Iteration 15700: Loss = -11342.733386129652
Iteration 15800: Loss = -11342.733695101824
1
Iteration 15900: Loss = -11342.741899804718
2
Iteration 16000: Loss = -11342.754135021221
3
Iteration 16100: Loss = -11342.733323277804
Iteration 16200: Loss = -11342.733410523042
Iteration 16300: Loss = -11342.82012557439
1
Iteration 16400: Loss = -11342.735807891328
2
Iteration 16500: Loss = -11342.73731983422
3
Iteration 16600: Loss = -11342.757228815703
4
Iteration 16700: Loss = -11342.741527998165
5
Iteration 16800: Loss = -11342.737089869475
6
Iteration 16900: Loss = -11342.73339098607
Iteration 17000: Loss = -11342.733400767396
Iteration 17100: Loss = -11342.73628420616
1
Iteration 17200: Loss = -11342.734333640106
2
Iteration 17300: Loss = -11342.783870700723
3
Iteration 17400: Loss = -11342.733291230967
Iteration 17500: Loss = -11342.7337623083
1
Iteration 17600: Loss = -11342.73330344034
Iteration 17700: Loss = -11342.733976906526
1
Iteration 17800: Loss = -11342.733500997228
2
Iteration 17900: Loss = -11342.833918077125
3
Iteration 18000: Loss = -11342.853565021382
4
Iteration 18100: Loss = -11342.73670607185
5
Iteration 18200: Loss = -11342.733331117755
Iteration 18300: Loss = -11342.737486269232
1
Iteration 18400: Loss = -11342.7332634811
Iteration 18500: Loss = -11342.734618565155
1
Iteration 18600: Loss = -11342.733493401502
2
Iteration 18700: Loss = -11342.733410163166
3
Iteration 18800: Loss = -11342.738228295853
4
Iteration 18900: Loss = -11342.733234972498
Iteration 19000: Loss = -11342.734791177872
1
Iteration 19100: Loss = -11335.276735252352
Iteration 19200: Loss = -11335.20604143448
Iteration 19300: Loss = -11335.211777278953
1
Iteration 19400: Loss = -11335.21895844228
2
Iteration 19500: Loss = -11335.20816518277
3
Iteration 19600: Loss = -11335.234666950426
4
Iteration 19700: Loss = -11335.152011996337
Iteration 19800: Loss = -11335.1251255734
Iteration 19900: Loss = -11335.121577549764
pi: tensor([[0.4287, 0.5713],
        [0.3838, 0.6162]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4609, 0.5391], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3846, 0.0983],
         [0.6879, 0.2119]],

        [[0.6789, 0.0795],
         [0.5723, 0.6077]],

        [[0.7114, 0.0957],
         [0.6711, 0.6173]],

        [[0.6043, 0.0882],
         [0.5531, 0.7037]],

        [[0.5285, 0.0993],
         [0.5067, 0.6874]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 71
Adjusted Rand Index: 0.17008378432120727
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.5002096259130273
Average Adjusted Rand Index: 0.8180163180278045
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21275.358150059026
Iteration 100: Loss = -11891.445504617239
Iteration 200: Loss = -11881.206979686202
Iteration 300: Loss = -11638.400347443712
Iteration 400: Loss = -11358.76038312374
Iteration 500: Loss = -11241.21710294139
Iteration 600: Loss = -11195.281605397911
Iteration 700: Loss = -11180.97203236571
Iteration 800: Loss = -11180.663259850193
Iteration 900: Loss = -11168.709552140735
Iteration 1000: Loss = -11161.687969550421
Iteration 1100: Loss = -11161.56692782041
Iteration 1200: Loss = -11158.432808036943
Iteration 1300: Loss = -11158.39254073576
Iteration 1400: Loss = -11158.347205152666
Iteration 1500: Loss = -11158.325081544586
Iteration 1600: Loss = -11158.302432750696
Iteration 1700: Loss = -11154.66024410352
Iteration 1800: Loss = -11154.637905589449
Iteration 1900: Loss = -11154.627213307633
Iteration 2000: Loss = -11154.61815129963
Iteration 2100: Loss = -11154.609860765962
Iteration 2200: Loss = -11154.600797522187
Iteration 2300: Loss = -11154.586659233044
Iteration 2400: Loss = -11154.574098602085
Iteration 2500: Loss = -11154.565760585456
Iteration 2600: Loss = -11154.501768616112
Iteration 2700: Loss = -11154.496778749493
Iteration 2800: Loss = -11154.494804959224
Iteration 2900: Loss = -11154.490932775212
Iteration 3000: Loss = -11154.48844969823
Iteration 3100: Loss = -11154.490365578211
1
Iteration 3200: Loss = -11154.486932708238
Iteration 3300: Loss = -11154.48212903286
Iteration 3400: Loss = -11154.486317960056
1
Iteration 3500: Loss = -11154.479003248367
Iteration 3600: Loss = -11154.478712873703
Iteration 3700: Loss = -11154.478019436301
Iteration 3800: Loss = -11154.475820679188
Iteration 3900: Loss = -11154.474239341991
Iteration 4000: Loss = -11154.474932203977
1
Iteration 4100: Loss = -11154.4723353055
Iteration 4200: Loss = -11154.472930010783
1
Iteration 4300: Loss = -11154.470688622472
Iteration 4400: Loss = -11154.469923178829
Iteration 4500: Loss = -11154.469448108355
Iteration 4600: Loss = -11154.468688310422
Iteration 4700: Loss = -11154.468039013336
Iteration 4800: Loss = -11154.4713798302
1
Iteration 4900: Loss = -11154.466978503859
Iteration 5000: Loss = -11154.466913468234
Iteration 5100: Loss = -11154.466570933102
Iteration 5200: Loss = -11154.471044869013
1
Iteration 5300: Loss = -11154.467475326752
2
Iteration 5400: Loss = -11154.47004897383
3
Iteration 5500: Loss = -11154.464389707404
Iteration 5600: Loss = -11154.463928681567
Iteration 5700: Loss = -11154.464031321155
1
Iteration 5800: Loss = -11154.46314106246
Iteration 5900: Loss = -11154.462950276904
Iteration 6000: Loss = -11154.462722399896
Iteration 6100: Loss = -11154.46237177677
Iteration 6200: Loss = -11154.462711909382
1
Iteration 6300: Loss = -11154.46200570308
Iteration 6400: Loss = -11154.463410553159
1
Iteration 6500: Loss = -11154.46197871892
Iteration 6600: Loss = -11154.466034140729
1
Iteration 6700: Loss = -11154.46244159094
2
Iteration 6800: Loss = -11154.462243651713
3
Iteration 6900: Loss = -11154.46671163857
4
Iteration 7000: Loss = -11154.491460197358
5
Iteration 7100: Loss = -11154.541742071673
6
Iteration 7200: Loss = -11154.472006712404
7
Iteration 7300: Loss = -11154.460509245015
Iteration 7400: Loss = -11154.460760244918
1
Iteration 7500: Loss = -11154.460237170812
Iteration 7600: Loss = -11154.460270272271
Iteration 7700: Loss = -11154.461403679568
1
Iteration 7800: Loss = -11154.462987077786
2
Iteration 7900: Loss = -11154.459904447405
Iteration 8000: Loss = -11154.460017225998
1
Iteration 8100: Loss = -11154.46093141813
2
Iteration 8200: Loss = -11154.45997432455
Iteration 8300: Loss = -11154.460587337344
1
Iteration 8400: Loss = -11154.496603612448
2
Iteration 8500: Loss = -11154.458831634327
Iteration 8600: Loss = -11154.461210863465
1
Iteration 8700: Loss = -11154.458717670803
Iteration 8800: Loss = -11154.521924053131
1
Iteration 8900: Loss = -11154.458612841092
Iteration 9000: Loss = -11154.463013949415
1
Iteration 9100: Loss = -11154.459194384308
2
Iteration 9200: Loss = -11154.459055338551
3
Iteration 9300: Loss = -11154.459663986676
4
Iteration 9400: Loss = -11154.481994495896
5
Iteration 9500: Loss = -11154.458495237472
Iteration 9600: Loss = -11154.460067024198
1
Iteration 9700: Loss = -11154.458454960188
Iteration 9800: Loss = -11154.458636515985
1
Iteration 9900: Loss = -11154.461384538428
2
Iteration 10000: Loss = -11154.557635732477
3
Iteration 10100: Loss = -11154.459612461504
4
Iteration 10200: Loss = -11154.459048363318
5
Iteration 10300: Loss = -11154.470623557963
6
Iteration 10400: Loss = -11154.459374416714
7
Iteration 10500: Loss = -11154.463674061544
8
Iteration 10600: Loss = -11154.461854374853
9
Iteration 10700: Loss = -11154.458622811213
10
Iteration 10800: Loss = -11154.459256482456
11
Iteration 10900: Loss = -11154.458519062648
Iteration 11000: Loss = -11154.46245817111
1
Iteration 11100: Loss = -11154.465377792963
2
Iteration 11200: Loss = -11154.45862004074
3
Iteration 11300: Loss = -11154.460870322537
4
Iteration 11400: Loss = -11154.49931823677
5
Iteration 11500: Loss = -11154.461243666947
6
Iteration 11600: Loss = -11154.460852612094
7
Iteration 11700: Loss = -11154.460555185322
8
Iteration 11800: Loss = -11154.460276441005
9
Iteration 11900: Loss = -11154.460361925845
10
Iteration 12000: Loss = -11154.464217662457
11
Iteration 12100: Loss = -11154.459846611619
12
Iteration 12200: Loss = -11154.458317392435
Iteration 12300: Loss = -11154.458317774084
Iteration 12400: Loss = -11154.458566032514
1
Iteration 12500: Loss = -11154.458082641162
Iteration 12600: Loss = -11154.475590048472
1
Iteration 12700: Loss = -11154.466497263367
2
Iteration 12800: Loss = -11154.462485827
3
Iteration 12900: Loss = -11154.456376341346
Iteration 13000: Loss = -11154.45829929527
1
Iteration 13100: Loss = -11154.462610342825
2
Iteration 13200: Loss = -11154.457523900985
3
Iteration 13300: Loss = -11154.461117489016
4
Iteration 13400: Loss = -11154.538954339247
5
Iteration 13500: Loss = -11154.48108460735
6
Iteration 13600: Loss = -11154.46003596811
7
Iteration 13700: Loss = -11154.459883105093
8
Iteration 13800: Loss = -11154.456985518555
9
Iteration 13900: Loss = -11154.456223450023
Iteration 14000: Loss = -11154.45620830141
Iteration 14100: Loss = -11154.455610570529
Iteration 14200: Loss = -11154.45492914074
Iteration 14300: Loss = -11154.458384191654
1
Iteration 14400: Loss = -11154.467212093787
2
Iteration 14500: Loss = -11154.495184575919
3
Iteration 14600: Loss = -11154.458521624592
4
Iteration 14700: Loss = -11154.458017737705
5
Iteration 14800: Loss = -11154.460509694924
6
Iteration 14900: Loss = -11154.496695266234
7
Iteration 15000: Loss = -11154.458084246573
8
Iteration 15100: Loss = -11154.45485887018
Iteration 15200: Loss = -11154.455526253509
1
Iteration 15300: Loss = -11154.455146537484
2
Iteration 15400: Loss = -11154.464687851325
3
Iteration 15500: Loss = -11154.466982381646
4
Iteration 15600: Loss = -11154.455814597972
5
Iteration 15700: Loss = -11154.455207913501
6
Iteration 15800: Loss = -11154.524940907773
7
Iteration 15900: Loss = -11154.460774493898
8
Iteration 16000: Loss = -11154.45683481394
9
Iteration 16100: Loss = -11154.468010388955
10
Iteration 16200: Loss = -11154.47051818201
11
Iteration 16300: Loss = -11154.455114326656
12
Iteration 16400: Loss = -11154.457234738631
13
Iteration 16500: Loss = -11154.456767718833
14
Iteration 16600: Loss = -11154.456761153706
15
Stopping early at iteration 16600 due to no improvement.
pi: tensor([[0.7418, 0.2582],
        [0.3001, 0.6999]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5405, 0.4595], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1924, 0.0985],
         [0.6739, 0.3964]],

        [[0.5225, 0.0873],
         [0.7000, 0.5870]],

        [[0.5457, 0.0953],
         [0.5751, 0.7293]],

        [[0.7115, 0.0884],
         [0.7057, 0.7170]],

        [[0.5765, 0.0993],
         [0.6490, 0.5358]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9919999034306759
Average Adjusted Rand Index: 0.992
11167.321792328867
[0.5002096259130273, 0.9919999034306759] [0.8180163180278045, 0.992] [11335.128192170247, 11154.456761153706]
-------------------------------------
This iteration is 40
True Objective function: Loss = -11477.82594486485
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20643.063859783793
Iteration 100: Loss = -12155.795664509127
Iteration 200: Loss = -12140.088389540473
Iteration 300: Loss = -11928.271045592146
Iteration 400: Loss = -11796.554501959814
Iteration 500: Loss = -11730.19625310902
Iteration 600: Loss = -11720.374200702427
Iteration 700: Loss = -11718.582481598814
Iteration 800: Loss = -11716.722462391766
Iteration 900: Loss = -11716.238102275382
Iteration 1000: Loss = -11716.036136700022
Iteration 1100: Loss = -11715.921671797903
Iteration 1200: Loss = -11715.842515887403
Iteration 1300: Loss = -11715.783863829072
Iteration 1400: Loss = -11715.738808292206
Iteration 1500: Loss = -11715.703360313679
Iteration 1600: Loss = -11715.674953353659
Iteration 1700: Loss = -11715.651814329169
Iteration 1800: Loss = -11715.63273684966
Iteration 1900: Loss = -11715.61675101713
Iteration 2000: Loss = -11715.603219761384
Iteration 2100: Loss = -11715.591625604513
Iteration 2200: Loss = -11715.581640174118
Iteration 2300: Loss = -11715.575271111959
Iteration 2400: Loss = -11715.565179181676
Iteration 2500: Loss = -11715.558253673513
Iteration 2600: Loss = -11715.559573961857
1
Iteration 2700: Loss = -11715.545333042526
Iteration 2800: Loss = -11715.536029501287
Iteration 2900: Loss = -11715.441306742556
Iteration 3000: Loss = -11713.846553106083
Iteration 3100: Loss = -11713.664634330376
Iteration 3200: Loss = -11713.573126660664
Iteration 3300: Loss = -11713.554409808246
Iteration 3400: Loss = -11713.506581515623
Iteration 3500: Loss = -11713.498908737625
Iteration 3600: Loss = -11713.494492851098
Iteration 3700: Loss = -11713.477519109641
Iteration 3800: Loss = -11713.236249077849
Iteration 3900: Loss = -11713.219901545242
Iteration 4000: Loss = -11713.213757327492
Iteration 4100: Loss = -11713.222213764944
1
Iteration 4200: Loss = -11713.204693992458
Iteration 4300: Loss = -11713.201929371036
Iteration 4400: Loss = -11713.19981123597
Iteration 4500: Loss = -11713.19993050367
1
Iteration 4600: Loss = -11713.197142771298
Iteration 4700: Loss = -11713.201494945952
1
Iteration 4800: Loss = -11713.195080353966
Iteration 4900: Loss = -11713.203503286746
1
Iteration 5000: Loss = -11713.19344476474
Iteration 5100: Loss = -11713.192692802
Iteration 5200: Loss = -11713.192081065725
Iteration 5300: Loss = -11713.19140948075
Iteration 5400: Loss = -11713.196979953025
1
Iteration 5500: Loss = -11713.190652052479
Iteration 5600: Loss = -11713.192449196458
1
Iteration 5700: Loss = -11713.189457678936
Iteration 5800: Loss = -11713.190344568617
1
Iteration 5900: Loss = -11713.188472908818
Iteration 6000: Loss = -11713.188844501596
1
Iteration 6100: Loss = -11713.188094786228
Iteration 6200: Loss = -11713.19054526198
1
Iteration 6300: Loss = -11713.187238671402
Iteration 6400: Loss = -11713.187534771683
1
Iteration 6500: Loss = -11713.199344269913
2
Iteration 6600: Loss = -11713.186212816569
Iteration 6700: Loss = -11713.186013913513
Iteration 6800: Loss = -11713.185707206509
Iteration 6900: Loss = -11713.185498083183
Iteration 7000: Loss = -11713.185644700166
1
Iteration 7100: Loss = -11713.187645805607
2
Iteration 7200: Loss = -11713.191529881098
3
Iteration 7300: Loss = -11713.18450485334
Iteration 7400: Loss = -11713.184447595067
Iteration 7500: Loss = -11713.184245662349
Iteration 7600: Loss = -11713.184124951886
Iteration 7700: Loss = -11713.184034670323
Iteration 7800: Loss = -11713.183851683636
Iteration 7900: Loss = -11713.183786570573
Iteration 8000: Loss = -11713.183614533134
Iteration 8100: Loss = -11713.190794654025
1
Iteration 8200: Loss = -11713.186138210298
2
Iteration 8300: Loss = -11713.189859304955
3
Iteration 8400: Loss = -11713.226142093848
4
Iteration 8500: Loss = -11713.183176980203
Iteration 8600: Loss = -11713.183587135507
1
Iteration 8700: Loss = -11713.18305421129
Iteration 8800: Loss = -11713.183391462082
1
Iteration 8900: Loss = -11713.182952784784
Iteration 9000: Loss = -11713.183300653021
1
Iteration 9100: Loss = -11713.183605483271
2
Iteration 9200: Loss = -11713.215042955277
3
Iteration 9300: Loss = -11713.302507355682
4
Iteration 9400: Loss = -11713.18304115004
Iteration 9500: Loss = -11713.182702388476
Iteration 9600: Loss = -11713.202444411292
1
Iteration 9700: Loss = -11713.190369175649
2
Iteration 9800: Loss = -11713.183494959769
3
Iteration 9900: Loss = -11713.184512313399
4
Iteration 10000: Loss = -11713.187655173206
5
Iteration 10100: Loss = -11713.220112038543
6
Iteration 10200: Loss = -11713.18781919291
7
Iteration 10300: Loss = -11713.182632681704
Iteration 10400: Loss = -11713.18255072666
Iteration 10500: Loss = -11713.18373617065
1
Iteration 10600: Loss = -11713.192323915422
2
Iteration 10700: Loss = -11713.184204186737
3
Iteration 10800: Loss = -11713.183995317966
4
Iteration 10900: Loss = -11713.191409927194
5
Iteration 11000: Loss = -11713.185168984668
6
Iteration 11100: Loss = -11713.182380075054
Iteration 11200: Loss = -11713.184074976145
1
Iteration 11300: Loss = -11713.253889528052
2
Iteration 11400: Loss = -11713.183403506455
3
Iteration 11500: Loss = -11713.182440039409
Iteration 11600: Loss = -11713.18792450103
1
Iteration 11700: Loss = -11713.257974340717
2
Iteration 11800: Loss = -11713.185672324153
3
Iteration 11900: Loss = -11713.185052210296
4
Iteration 12000: Loss = -11713.189181483707
5
Iteration 12100: Loss = -11713.185272189647
6
Iteration 12200: Loss = -11713.200655561901
7
Iteration 12300: Loss = -11713.23330740373
8
Iteration 12400: Loss = -11713.190000616574
9
Iteration 12500: Loss = -11713.185546341236
10
Iteration 12600: Loss = -11713.182703027584
11
Iteration 12700: Loss = -11713.183331288777
12
Iteration 12800: Loss = -11713.249688903248
13
Iteration 12900: Loss = -11713.23089986386
14
Iteration 13000: Loss = -11713.185969339434
15
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[0.6939, 0.3061],
        [0.3640, 0.6360]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9917, 0.0083], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.3194],
         [0.7032, 0.4055]],

        [[0.5208, 0.0913],
         [0.6370, 0.7215]],

        [[0.6164, 0.1063],
         [0.5322, 0.6590]],

        [[0.6114, 0.1042],
         [0.5854, 0.7108]],

        [[0.6931, 0.1025],
         [0.7127, 0.5466]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 30
Adjusted Rand Index: 0.15307250954025728
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.2811727804927671
Average Adjusted Rand Index: 0.6225995544112322
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21431.543136881217
Iteration 100: Loss = -12154.619172789877
Iteration 200: Loss = -12130.58336556821
Iteration 300: Loss = -11889.432370090637
Iteration 400: Loss = -11719.658374120132
Iteration 500: Loss = -11540.76780940726
Iteration 600: Loss = -11497.951756603867
Iteration 700: Loss = -11493.221984557
Iteration 800: Loss = -11492.879909528383
Iteration 900: Loss = -11473.56211140178
Iteration 1000: Loss = -11473.374460269833
Iteration 1100: Loss = -11473.275524311302
Iteration 1200: Loss = -11473.205501312072
Iteration 1300: Loss = -11473.153267019863
Iteration 1400: Loss = -11473.112829295867
Iteration 1500: Loss = -11473.080779931932
Iteration 1600: Loss = -11473.054783755586
Iteration 1700: Loss = -11473.033360185214
Iteration 1800: Loss = -11473.015442621407
Iteration 1900: Loss = -11473.000305131482
Iteration 2000: Loss = -11472.987358591785
Iteration 2100: Loss = -11472.976161782692
Iteration 2200: Loss = -11472.966473683608
Iteration 2300: Loss = -11472.957929495244
Iteration 2400: Loss = -11472.95061428761
Iteration 2500: Loss = -11472.943883150567
Iteration 2600: Loss = -11472.93792743993
Iteration 2700: Loss = -11472.93751465108
Iteration 2800: Loss = -11472.927922798497
Iteration 2900: Loss = -11472.923656625297
Iteration 3000: Loss = -11472.92063010413
Iteration 3100: Loss = -11472.916241291907
Iteration 3200: Loss = -11472.913841285614
Iteration 3300: Loss = -11472.910152120963
Iteration 3400: Loss = -11472.908182029063
Iteration 3500: Loss = -11472.905498248127
Iteration 3600: Loss = -11472.902933394584
Iteration 3700: Loss = -11472.905642187854
1
Iteration 3800: Loss = -11472.899070465115
Iteration 3900: Loss = -11472.91284189116
1
Iteration 4000: Loss = -11472.895742160283
Iteration 4100: Loss = -11472.894525957981
Iteration 4200: Loss = -11472.892880459372
Iteration 4300: Loss = -11472.89175646977
Iteration 4400: Loss = -11472.89080091793
Iteration 4500: Loss = -11472.88984448003
Iteration 4600: Loss = -11472.888234981372
Iteration 4700: Loss = -11472.887325059619
Iteration 4800: Loss = -11472.88631462913
Iteration 4900: Loss = -11472.885536973408
Iteration 5000: Loss = -11472.886108807064
1
Iteration 5100: Loss = -11472.883774755212
Iteration 5200: Loss = -11472.88792425458
1
Iteration 5300: Loss = -11472.882351877071
Iteration 5400: Loss = -11472.881650172287
Iteration 5500: Loss = -11472.881223609791
Iteration 5600: Loss = -11472.880447041507
Iteration 5700: Loss = -11472.882529102908
1
Iteration 5800: Loss = -11472.879426256635
Iteration 5900: Loss = -11472.879109166062
Iteration 6000: Loss = -11472.878640861785
Iteration 6100: Loss = -11472.88828495029
1
Iteration 6200: Loss = -11472.883744642364
2
Iteration 6300: Loss = -11472.88638542419
3
Iteration 6400: Loss = -11472.877561409665
Iteration 6500: Loss = -11472.877231086022
Iteration 6600: Loss = -11472.877591692408
1
Iteration 6700: Loss = -11472.88515389316
2
Iteration 6800: Loss = -11472.876200847395
Iteration 6900: Loss = -11472.878592411209
1
Iteration 7000: Loss = -11472.880400674396
2
Iteration 7100: Loss = -11472.883427268502
3
Iteration 7200: Loss = -11472.87537682598
Iteration 7300: Loss = -11472.894156188313
1
Iteration 7400: Loss = -11472.875020779113
Iteration 7500: Loss = -11472.875127497377
1
Iteration 7600: Loss = -11472.912831088817
2
Iteration 7700: Loss = -11472.876002395582
3
Iteration 7800: Loss = -11472.877872540841
4
Iteration 7900: Loss = -11472.874505546963
Iteration 8000: Loss = -11472.874950827814
1
Iteration 8100: Loss = -11472.890330348973
2
Iteration 8200: Loss = -11472.874269343196
Iteration 8300: Loss = -11472.87422778353
Iteration 8400: Loss = -11472.87399073074
Iteration 8500: Loss = -11472.877633172655
1
Iteration 8600: Loss = -11472.879524259779
2
Iteration 8700: Loss = -11472.87358006595
Iteration 8800: Loss = -11472.874276830566
1
Iteration 8900: Loss = -11472.981067972669
2
Iteration 9000: Loss = -11472.873259385717
Iteration 9100: Loss = -11472.896144334893
1
Iteration 9200: Loss = -11472.873101770725
Iteration 9300: Loss = -11472.881199274734
1
Iteration 9400: Loss = -11472.904774495995
2
Iteration 9500: Loss = -11472.87727932854
3
Iteration 9600: Loss = -11472.88714952214
4
Iteration 9700: Loss = -11472.904937863012
5
Iteration 9800: Loss = -11472.882722857896
6
Iteration 9900: Loss = -11472.896745416583
7
Iteration 10000: Loss = -11472.887997041982
8
Iteration 10100: Loss = -11472.880698691431
9
Iteration 10200: Loss = -11472.910539053106
10
Iteration 10300: Loss = -11472.873837593204
11
Iteration 10400: Loss = -11472.876881642733
12
Iteration 10500: Loss = -11472.87835730686
13
Iteration 10600: Loss = -11472.876343418855
14
Iteration 10700: Loss = -11472.87445275833
15
Stopping early at iteration 10700 due to no improvement.
pi: tensor([[0.7680, 0.2320],
        [0.2647, 0.7353]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5391, 0.4609], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1959, 0.1079],
         [0.6982, 0.4027]],

        [[0.5378, 0.0981],
         [0.6886, 0.7281]],

        [[0.7048, 0.1059],
         [0.6207, 0.5972]],

        [[0.5675, 0.1040],
         [0.5779, 0.6152]],

        [[0.5530, 0.1026],
         [0.5500, 0.5480]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599252625159036
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.9919850525031807
11477.82594486485
[0.2811727804927671, 0.9919998213107827] [0.6225995544112322, 0.9919850525031807] [11713.185969339434, 11472.87445275833]
-------------------------------------
This iteration is 41
True Objective function: Loss = -11762.238136335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22386.172326871656
Iteration 100: Loss = -12680.182069943156
Iteration 200: Loss = -12189.49208421126
Iteration 300: Loss = -12030.228127170882
Iteration 400: Loss = -12019.053615558454
Iteration 500: Loss = -12001.795282986744
Iteration 600: Loss = -12000.925276775326
Iteration 700: Loss = -11997.818291970509
Iteration 800: Loss = -11993.856849955802
Iteration 900: Loss = -11971.865045926908
Iteration 1000: Loss = -11934.63486329387
Iteration 1100: Loss = -11900.875542533773
Iteration 1200: Loss = -11850.552082980119
Iteration 1300: Loss = -11820.505479772964
Iteration 1400: Loss = -11820.346818970958
Iteration 1500: Loss = -11790.152235093756
Iteration 1600: Loss = -11790.094878163949
Iteration 1700: Loss = -11767.55135079133
Iteration 1800: Loss = -11751.192517405145
Iteration 1900: Loss = -11751.178793774205
Iteration 2000: Loss = -11751.151029036551
Iteration 2100: Loss = -11751.138995228803
Iteration 2200: Loss = -11751.129401076178
Iteration 2300: Loss = -11751.12129505275
Iteration 2400: Loss = -11751.11451322205
Iteration 2500: Loss = -11751.10901147706
Iteration 2600: Loss = -11751.103700553758
Iteration 2700: Loss = -11751.099304577914
Iteration 2800: Loss = -11751.096448882408
Iteration 2900: Loss = -11751.09204963772
Iteration 3000: Loss = -11751.088976335508
Iteration 3100: Loss = -11751.095623469962
1
Iteration 3200: Loss = -11751.083764197117
Iteration 3300: Loss = -11751.081558085581
Iteration 3400: Loss = -11751.079579289104
Iteration 3500: Loss = -11751.078524874572
Iteration 3600: Loss = -11751.076093694388
Iteration 3700: Loss = -11751.074888480718
Iteration 3800: Loss = -11751.073181292277
Iteration 3900: Loss = -11751.083657615132
1
Iteration 4000: Loss = -11751.070762960864
Iteration 4100: Loss = -11751.06970446495
Iteration 4200: Loss = -11751.069333167405
Iteration 4300: Loss = -11751.068545077525
Iteration 4400: Loss = -11751.06737009354
Iteration 4500: Loss = -11751.066040800879
Iteration 4600: Loss = -11751.065387345532
Iteration 4700: Loss = -11751.064785180875
Iteration 4800: Loss = -11751.064036298836
Iteration 4900: Loss = -11751.063504677904
Iteration 5000: Loss = -11751.062821605627
Iteration 5100: Loss = -11751.062292900671
Iteration 5200: Loss = -11751.06212898847
Iteration 5300: Loss = -11751.061393397567
Iteration 5400: Loss = -11751.060907627138
Iteration 5500: Loss = -11751.06055395572
Iteration 5600: Loss = -11751.062840948236
1
Iteration 5700: Loss = -11751.064454213703
2
Iteration 5800: Loss = -11751.059621976523
Iteration 5900: Loss = -11751.05927672609
Iteration 6000: Loss = -11751.0601675393
1
Iteration 6100: Loss = -11751.058639812793
Iteration 6200: Loss = -11751.062261034745
1
Iteration 6300: Loss = -11751.058124592408
Iteration 6400: Loss = -11751.057935493323
Iteration 6500: Loss = -11751.058502503021
1
Iteration 6600: Loss = -11751.057412483451
Iteration 6700: Loss = -11751.05866588535
1
Iteration 6800: Loss = -11751.057019238886
Iteration 6900: Loss = -11751.074757975639
1
Iteration 7000: Loss = -11751.062603342005
2
Iteration 7100: Loss = -11751.061529668976
3
Iteration 7200: Loss = -11751.078008265811
4
Iteration 7300: Loss = -11751.06174902517
5
Iteration 7400: Loss = -11751.056228543332
Iteration 7500: Loss = -11751.06489994832
1
Iteration 7600: Loss = -11751.056172143157
Iteration 7700: Loss = -11751.06263157839
1
Iteration 7800: Loss = -11751.213557578632
2
Iteration 7900: Loss = -11751.060802221658
3
Iteration 8000: Loss = -11751.111409549067
4
Iteration 8100: Loss = -11751.055469301453
Iteration 8200: Loss = -11751.090119440925
1
Iteration 8300: Loss = -11751.057415887859
2
Iteration 8400: Loss = -11751.055247645478
Iteration 8500: Loss = -11751.117085932765
1
Iteration 8600: Loss = -11751.05576835952
2
Iteration 8700: Loss = -11751.069567219667
3
Iteration 8800: Loss = -11751.054887732684
Iteration 8900: Loss = -11751.054987115334
Iteration 9000: Loss = -11751.062785928081
1
Iteration 9100: Loss = -11751.055568765976
2
Iteration 9200: Loss = -11751.05467523884
Iteration 9300: Loss = -11751.055054226195
1
Iteration 9400: Loss = -11751.091630806124
2
Iteration 9500: Loss = -11751.057692962308
3
Iteration 9600: Loss = -11751.054762805634
Iteration 9700: Loss = -11751.083597362996
1
Iteration 9800: Loss = -11751.072667495508
2
Iteration 9900: Loss = -11751.095481743072
3
Iteration 10000: Loss = -11751.07138377319
4
Iteration 10100: Loss = -11751.085841384172
5
Iteration 10200: Loss = -11751.057488315544
6
Iteration 10300: Loss = -11751.055806303295
7
Iteration 10400: Loss = -11751.058509853074
8
Iteration 10500: Loss = -11751.214327719059
9
Iteration 10600: Loss = -11751.062782259096
10
Iteration 10700: Loss = -11751.054429790218
Iteration 10800: Loss = -11751.08565891011
1
Iteration 10900: Loss = -11751.054175000834
Iteration 11000: Loss = -11751.055823620572
1
Iteration 11100: Loss = -11751.05491010783
2
Iteration 11200: Loss = -11751.05559430223
3
Iteration 11300: Loss = -11751.05819773959
4
Iteration 11400: Loss = -11751.054471336776
5
Iteration 11500: Loss = -11751.063543333787
6
Iteration 11600: Loss = -11751.080766187544
7
Iteration 11700: Loss = -11751.071962723125
8
Iteration 11800: Loss = -11751.066380651562
9
Iteration 11900: Loss = -11751.056132059344
10
Iteration 12000: Loss = -11751.062687392105
11
Iteration 12100: Loss = -11751.067645479305
12
Iteration 12200: Loss = -11751.09416077856
13
Iteration 12300: Loss = -11751.054484180666
14
Iteration 12400: Loss = -11751.054071980381
Iteration 12500: Loss = -11751.054761157671
1
Iteration 12600: Loss = -11751.060533872436
2
Iteration 12700: Loss = -11751.05618243727
3
Iteration 12800: Loss = -11751.103420707479
4
Iteration 12900: Loss = -11751.07398572719
5
Iteration 13000: Loss = -11751.07616243002
6
Iteration 13100: Loss = -11751.05760033439
7
Iteration 13200: Loss = -11751.06846549963
8
Iteration 13300: Loss = -11751.073969432606
9
Iteration 13400: Loss = -11751.059110961765
10
Iteration 13500: Loss = -11751.057048557599
11
Iteration 13600: Loss = -11751.053955070785
Iteration 13700: Loss = -11751.054961688336
1
Iteration 13800: Loss = -11751.054450733658
2
Iteration 13900: Loss = -11751.061303587961
3
Iteration 14000: Loss = -11751.054304292404
4
Iteration 14100: Loss = -11751.061189459344
5
Iteration 14200: Loss = -11751.069036639434
6
Iteration 14300: Loss = -11751.074220791368
7
Iteration 14400: Loss = -11751.145064570546
8
Iteration 14500: Loss = -11751.08259756108
9
Iteration 14600: Loss = -11751.060881488589
10
Iteration 14700: Loss = -11751.057990749781
11
Iteration 14800: Loss = -11751.064926189074
12
Iteration 14900: Loss = -11751.054240957708
13
Iteration 15000: Loss = -11751.057051619704
14
Iteration 15100: Loss = -11751.084822445217
15
Stopping early at iteration 15100 due to no improvement.
pi: tensor([[0.7885, 0.2115],
        [0.2606, 0.7394]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5500, 0.4500], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3995, 0.0812],
         [0.5459, 0.2049]],

        [[0.6379, 0.1019],
         [0.7125, 0.6375]],

        [[0.5375, 0.1036],
         [0.6903, 0.5808]],

        [[0.6241, 0.1128],
         [0.7026, 0.7000]],

        [[0.5990, 0.0948],
         [0.5374, 0.6145]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9919997667277575
Average Adjusted Rand Index: 0.9919971467023199
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19135.135316838347
Iteration 100: Loss = -12688.969656115822
Iteration 200: Loss = -12418.01386822738
Iteration 300: Loss = -12015.697263295393
Iteration 400: Loss = -11931.40526182129
Iteration 500: Loss = -11861.94453819852
Iteration 600: Loss = -11794.168936104548
Iteration 700: Loss = -11751.641520747282
Iteration 800: Loss = -11751.380109176022
Iteration 900: Loss = -11751.311080544896
Iteration 1000: Loss = -11751.265471124554
Iteration 1100: Loss = -11751.232916699513
Iteration 1200: Loss = -11751.208483051862
Iteration 1300: Loss = -11751.189466320662
Iteration 1400: Loss = -11751.174312291378
Iteration 1500: Loss = -11751.162020594527
Iteration 1600: Loss = -11751.151832311241
Iteration 1700: Loss = -11751.143278207983
Iteration 1800: Loss = -11751.13599740891
Iteration 1900: Loss = -11751.129709517045
Iteration 2000: Loss = -11751.12428956705
Iteration 2100: Loss = -11751.119510012551
Iteration 2200: Loss = -11751.115293734263
Iteration 2300: Loss = -11751.111519751874
Iteration 2400: Loss = -11751.108083120771
Iteration 2500: Loss = -11751.104939369625
Iteration 2600: Loss = -11751.102156885767
Iteration 2700: Loss = -11751.09976924007
Iteration 2800: Loss = -11751.09759118889
Iteration 2900: Loss = -11751.095547377647
Iteration 3000: Loss = -11751.093454217202
Iteration 3100: Loss = -11751.092410440038
Iteration 3200: Loss = -11751.08934924951
Iteration 3300: Loss = -11751.087971198607
Iteration 3400: Loss = -11751.08675988078
Iteration 3500: Loss = -11751.085706238755
Iteration 3600: Loss = -11751.093029008265
1
Iteration 3700: Loss = -11751.083623125529
Iteration 3800: Loss = -11751.082736155404
Iteration 3900: Loss = -11751.0817961792
Iteration 4000: Loss = -11751.081777199423
Iteration 4100: Loss = -11751.082425694703
1
Iteration 4200: Loss = -11751.078914262718
Iteration 4300: Loss = -11751.080698092226
1
Iteration 4400: Loss = -11751.07881283971
Iteration 4500: Loss = -11751.07759776189
Iteration 4600: Loss = -11751.076995559242
Iteration 4700: Loss = -11751.07620456468
Iteration 4800: Loss = -11751.07574601309
Iteration 4900: Loss = -11751.075271214366
Iteration 5000: Loss = -11751.075058864984
Iteration 5100: Loss = -11751.07429121777
Iteration 5200: Loss = -11751.079913093581
1
Iteration 5300: Loss = -11751.073544629257
Iteration 5400: Loss = -11751.074089124922
1
Iteration 5500: Loss = -11751.073012506895
Iteration 5600: Loss = -11751.073005938553
Iteration 5700: Loss = -11751.07253450927
Iteration 5800: Loss = -11751.077954492448
1
Iteration 5900: Loss = -11751.074020386763
2
Iteration 6000: Loss = -11751.071913350755
Iteration 6100: Loss = -11751.072546445828
1
Iteration 6200: Loss = -11751.076002868644
2
Iteration 6300: Loss = -11751.075113490342
3
Iteration 6400: Loss = -11751.071218740588
Iteration 6500: Loss = -11751.071557769772
1
Iteration 6600: Loss = -11751.073589994738
2
Iteration 6700: Loss = -11751.071335232968
3
Iteration 6800: Loss = -11751.071770459803
4
Iteration 6900: Loss = -11751.058015572791
Iteration 7000: Loss = -11751.058947462974
1
Iteration 7100: Loss = -11751.07393618609
2
Iteration 7200: Loss = -11751.057032773422
Iteration 7300: Loss = -11751.064249967234
1
Iteration 7400: Loss = -11751.064447531151
2
Iteration 7500: Loss = -11751.05677836912
Iteration 7600: Loss = -11751.085860446055
1
Iteration 7700: Loss = -11751.064177374728
2
Iteration 7800: Loss = -11751.057133880431
3
Iteration 7900: Loss = -11751.05729719653
4
Iteration 8000: Loss = -11751.119372747515
5
Iteration 8100: Loss = -11751.064494309652
6
Iteration 8200: Loss = -11751.05636568823
Iteration 8300: Loss = -11751.056637951631
1
Iteration 8400: Loss = -11751.057001867834
2
Iteration 8500: Loss = -11751.07935698485
3
Iteration 8600: Loss = -11751.057646939405
4
Iteration 8700: Loss = -11751.056763246035
5
Iteration 8800: Loss = -11751.056067601483
Iteration 8900: Loss = -11751.07548642337
1
Iteration 9000: Loss = -11751.056720955778
2
Iteration 9100: Loss = -11751.062479543123
3
Iteration 9200: Loss = -11751.060920883172
4
Iteration 9300: Loss = -11751.11867937678
5
Iteration 9400: Loss = -11751.056306145907
6
Iteration 9500: Loss = -11751.055952423289
Iteration 9600: Loss = -11751.056410537694
1
Iteration 9700: Loss = -11751.06308580356
2
Iteration 9800: Loss = -11751.05573813452
Iteration 9900: Loss = -11751.081666338452
1
Iteration 10000: Loss = -11751.055761392961
Iteration 10100: Loss = -11751.056032648148
1
Iteration 10200: Loss = -11751.151536729676
2
Iteration 10300: Loss = -11751.069963960883
3
Iteration 10400: Loss = -11751.055562217503
Iteration 10500: Loss = -11751.05494028113
Iteration 10600: Loss = -11751.055502426698
1
Iteration 10700: Loss = -11751.055481924293
2
Iteration 10800: Loss = -11751.057059879286
3
Iteration 10900: Loss = -11751.06193064353
4
Iteration 11000: Loss = -11751.139332727143
5
Iteration 11100: Loss = -11751.05957967809
6
Iteration 11200: Loss = -11751.05489733055
Iteration 11300: Loss = -11751.095069321382
1
Iteration 11400: Loss = -11751.178130045295
2
Iteration 11500: Loss = -11751.059144285158
3
Iteration 11600: Loss = -11751.057313395362
4
Iteration 11700: Loss = -11751.071633646616
5
Iteration 11800: Loss = -11751.075044429057
6
Iteration 11900: Loss = -11751.059681239843
7
Iteration 12000: Loss = -11751.062104082139
8
Iteration 12100: Loss = -11751.059029803373
9
Iteration 12200: Loss = -11751.061621078743
10
Iteration 12300: Loss = -11751.06483633341
11
Iteration 12400: Loss = -11751.068573216737
12
Iteration 12500: Loss = -11751.073987555572
13
Iteration 12600: Loss = -11751.080859234982
14
Iteration 12700: Loss = -11751.054639858927
Iteration 12800: Loss = -11751.055048019607
1
Iteration 12900: Loss = -11751.057788557828
2
Iteration 13000: Loss = -11751.057954524129
3
Iteration 13100: Loss = -11751.057877864807
4
Iteration 13200: Loss = -11751.054237460083
Iteration 13300: Loss = -11751.058385962093
1
Iteration 13400: Loss = -11751.054791766725
2
Iteration 13500: Loss = -11751.060917433508
3
Iteration 13600: Loss = -11751.087315813811
4
Iteration 13700: Loss = -11751.081525041967
5
Iteration 13800: Loss = -11751.073124907201
6
Iteration 13900: Loss = -11751.054654329233
7
Iteration 14000: Loss = -11751.054782390165
8
Iteration 14100: Loss = -11751.053886079697
Iteration 14200: Loss = -11751.05414563092
1
Iteration 14300: Loss = -11751.067105408803
2
Iteration 14400: Loss = -11751.060312773847
3
Iteration 14500: Loss = -11751.068579436936
4
Iteration 14600: Loss = -11751.054198828135
5
Iteration 14700: Loss = -11751.05636877941
6
Iteration 14800: Loss = -11751.060272172148
7
Iteration 14900: Loss = -11751.053938190818
Iteration 15000: Loss = -11751.053949561228
Iteration 15100: Loss = -11751.060211679302
1
Iteration 15200: Loss = -11751.068937349299
2
Iteration 15300: Loss = -11751.054143509042
3
Iteration 15400: Loss = -11751.07530672682
4
Iteration 15500: Loss = -11751.053830501627
Iteration 15600: Loss = -11751.05491757235
1
Iteration 15700: Loss = -11751.055848166801
2
Iteration 15800: Loss = -11751.079210068452
3
Iteration 15900: Loss = -11751.108160771968
4
Iteration 16000: Loss = -11751.05416403175
5
Iteration 16100: Loss = -11751.053953822444
6
Iteration 16200: Loss = -11751.055704987655
7
Iteration 16300: Loss = -11751.241464627963
8
Iteration 16400: Loss = -11751.055196072111
9
Iteration 16500: Loss = -11751.055838608983
10
Iteration 16600: Loss = -11751.05739874162
11
Iteration 16700: Loss = -11751.084057990507
12
Iteration 16800: Loss = -11751.116803286504
13
Iteration 16900: Loss = -11751.055216628096
14
Iteration 17000: Loss = -11751.133591413087
15
Stopping early at iteration 17000 due to no improvement.
pi: tensor([[0.7427, 0.2573],
        [0.2135, 0.7865]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4500, 0.5500], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2034, 0.0819],
         [0.5292, 0.4020]],

        [[0.5626, 0.1027],
         [0.6085, 0.6556]],

        [[0.6497, 0.1051],
         [0.6864, 0.6805]],

        [[0.5960, 0.1129],
         [0.5341, 0.5417]],

        [[0.5048, 0.0955],
         [0.5945, 0.5304]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.9919997667277575
Average Adjusted Rand Index: 0.9919971467023199
11762.238136335178
[0.9919997667277575, 0.9919997667277575] [0.9919971467023199, 0.9919971467023199] [11751.084822445217, 11751.133591413087]
-------------------------------------
This iteration is 42
True Objective function: Loss = -11715.842300141367
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23535.01302941668
Iteration 100: Loss = -12531.630372270434
Iteration 200: Loss = -12526.431598817866
Iteration 300: Loss = -12512.240248461743
Iteration 400: Loss = -12509.280989353787
Iteration 500: Loss = -12509.110987074975
Iteration 600: Loss = -12509.024528180153
Iteration 700: Loss = -12508.971443072624
Iteration 800: Loss = -12508.936102260472
Iteration 900: Loss = -12508.911010717395
Iteration 1000: Loss = -12508.89235998344
Iteration 1100: Loss = -12508.87810385185
Iteration 1200: Loss = -12508.866826019905
Iteration 1300: Loss = -12508.857778406717
Iteration 1400: Loss = -12508.85033884175
Iteration 1500: Loss = -12508.84424560922
Iteration 1600: Loss = -12508.839094671548
Iteration 1700: Loss = -12508.834704517583
Iteration 1800: Loss = -12508.830978541126
Iteration 1900: Loss = -12508.8276653507
Iteration 2000: Loss = -12508.824856482071
Iteration 2100: Loss = -12508.8223437079
Iteration 2200: Loss = -12508.820173691718
Iteration 2300: Loss = -12508.818210095575
Iteration 2400: Loss = -12508.816453397569
Iteration 2500: Loss = -12508.814868425221
Iteration 2600: Loss = -12508.813493699072
Iteration 2700: Loss = -12508.812216693237
Iteration 2800: Loss = -12508.811015705816
Iteration 2900: Loss = -12508.810024904094
Iteration 3000: Loss = -12508.809022477022
Iteration 3100: Loss = -12508.808195560216
Iteration 3200: Loss = -12508.807408214845
Iteration 3300: Loss = -12508.806645461153
Iteration 3400: Loss = -12508.805924962631
Iteration 3500: Loss = -12508.805319947829
Iteration 3600: Loss = -12508.804713002963
Iteration 3700: Loss = -12508.804209282152
Iteration 3800: Loss = -12508.803724502384
Iteration 3900: Loss = -12508.803210141403
Iteration 4000: Loss = -12508.80280170327
Iteration 4100: Loss = -12508.802480587692
Iteration 4200: Loss = -12508.802006074864
Iteration 4300: Loss = -12508.80164378389
Iteration 4400: Loss = -12508.80136142042
Iteration 4500: Loss = -12508.801058977744
Iteration 4600: Loss = -12508.800753280793
Iteration 4700: Loss = -12508.800571288004
Iteration 4800: Loss = -12508.800209379746
Iteration 4900: Loss = -12508.799991118116
Iteration 5000: Loss = -12508.799967287256
Iteration 5100: Loss = -12508.799565100668
Iteration 5200: Loss = -12508.799370870875
Iteration 5300: Loss = -12508.799196853937
Iteration 5400: Loss = -12508.799034883237
Iteration 5500: Loss = -12508.798849722738
Iteration 5600: Loss = -12508.801822226422
1
Iteration 5700: Loss = -12508.798504038452
Iteration 5800: Loss = -12508.801449657292
1
Iteration 5900: Loss = -12508.798277929896
Iteration 6000: Loss = -12508.807226416588
1
Iteration 6100: Loss = -12508.79803116096
Iteration 6200: Loss = -12508.797914175466
Iteration 6300: Loss = -12508.797887496525
Iteration 6400: Loss = -12508.79772422165
Iteration 6500: Loss = -12508.797650059238
Iteration 6600: Loss = -12508.798822271194
1
Iteration 6700: Loss = -12508.797449400947
Iteration 6800: Loss = -12508.797328095767
Iteration 6900: Loss = -12508.797430666398
1
Iteration 7000: Loss = -12508.797456249651
2
Iteration 7100: Loss = -12508.806917916232
3
Iteration 7200: Loss = -12508.797067872347
Iteration 7300: Loss = -12508.797972292106
1
Iteration 7400: Loss = -12508.796942889496
Iteration 7500: Loss = -12508.797451307755
1
Iteration 7600: Loss = -12508.796879309684
Iteration 7700: Loss = -12508.797235388029
1
Iteration 7800: Loss = -12508.796940159575
Iteration 7900: Loss = -12508.79675836766
Iteration 8000: Loss = -12508.909777426874
1
Iteration 8100: Loss = -12508.796637256102
Iteration 8200: Loss = -12508.823562343878
1
Iteration 8300: Loss = -12508.796580426964
Iteration 8400: Loss = -12508.813936903609
1
Iteration 8500: Loss = -12508.796470914745
Iteration 8600: Loss = -12508.796481764894
Iteration 8700: Loss = -12508.796954220303
1
Iteration 8800: Loss = -12508.796353414127
Iteration 8900: Loss = -12508.796517606646
1
Iteration 9000: Loss = -12508.797355452092
2
Iteration 9100: Loss = -12508.796358201547
Iteration 9200: Loss = -12509.101013687976
1
Iteration 9300: Loss = -12508.79628286977
Iteration 9400: Loss = -12508.796284887923
Iteration 9500: Loss = -12508.798978734432
1
Iteration 9600: Loss = -12508.796203898122
Iteration 9700: Loss = -12508.797923393346
1
Iteration 9800: Loss = -12508.818554835481
2
Iteration 9900: Loss = -12508.796677251683
3
Iteration 10000: Loss = -12508.796186313622
Iteration 10100: Loss = -12508.851728453323
1
Iteration 10200: Loss = -12508.796133625397
Iteration 10300: Loss = -12508.796118067406
Iteration 10400: Loss = -12508.797215343424
1
Iteration 10500: Loss = -12508.79608915652
Iteration 10600: Loss = -12508.796162754146
Iteration 10700: Loss = -12508.796120112676
Iteration 10800: Loss = -12508.796211075365
Iteration 10900: Loss = -12508.806082012328
1
Iteration 11000: Loss = -12508.806042753087
2
Iteration 11100: Loss = -12508.796026483773
Iteration 11200: Loss = -12508.997447756257
1
Iteration 11300: Loss = -12508.79630901572
2
Iteration 11400: Loss = -12508.795992066156
Iteration 11500: Loss = -12508.796586466435
1
Iteration 11600: Loss = -12508.79601087564
Iteration 11700: Loss = -12509.150610193004
1
Iteration 11800: Loss = -12508.795971725012
Iteration 11900: Loss = -12508.799508488557
1
Iteration 12000: Loss = -12508.796015837857
Iteration 12100: Loss = -12508.79593751453
Iteration 12200: Loss = -12508.807101061539
1
Iteration 12300: Loss = -12508.795940840644
Iteration 12400: Loss = -12508.795978762832
Iteration 12500: Loss = -12508.819189501588
1
Iteration 12600: Loss = -12508.79593711848
Iteration 12700: Loss = -12508.836149727924
1
Iteration 12800: Loss = -12508.795913494692
Iteration 12900: Loss = -12508.795927748919
Iteration 13000: Loss = -12508.802522323032
1
Iteration 13100: Loss = -12508.795924192711
Iteration 13200: Loss = -12508.8013903417
1
Iteration 13300: Loss = -12508.795906954527
Iteration 13400: Loss = -12508.798301671954
1
Iteration 13500: Loss = -12508.796657624887
2
Iteration 13600: Loss = -12508.795967647182
Iteration 13700: Loss = -12508.7988091948
1
Iteration 13800: Loss = -12508.796118761506
2
Iteration 13900: Loss = -12508.80348048555
3
Iteration 14000: Loss = -12508.795874891524
Iteration 14100: Loss = -12508.801936987042
1
Iteration 14200: Loss = -12508.795939601505
Iteration 14300: Loss = -12508.79738064767
1
Iteration 14400: Loss = -12508.796272558186
2
Iteration 14500: Loss = -12508.808857629925
3
Iteration 14600: Loss = -12508.79607054823
4
Iteration 14700: Loss = -12508.796258265505
5
Iteration 14800: Loss = -12508.920245125091
6
Iteration 14900: Loss = -12508.803893557677
7
Iteration 15000: Loss = -12508.799219283661
8
Iteration 15100: Loss = -12508.796399572711
9
Iteration 15200: Loss = -12508.796025998701
Iteration 15300: Loss = -12508.849759600858
1
Iteration 15400: Loss = -12508.796348185399
2
Iteration 15500: Loss = -12508.79708728079
3
Iteration 15600: Loss = -12508.800769395239
4
Iteration 15700: Loss = -12509.094553406992
5
Iteration 15800: Loss = -12508.795898564553
Iteration 15900: Loss = -12508.845942794502
1
Iteration 16000: Loss = -12508.79767441678
2
Iteration 16100: Loss = -12508.796711480456
3
Iteration 16200: Loss = -12508.870151056757
4
Iteration 16300: Loss = -12508.797817127028
5
Iteration 16400: Loss = -12508.798667764624
6
Iteration 16500: Loss = -12508.800481320279
7
Iteration 16600: Loss = -12508.796053386242
8
Iteration 16700: Loss = -12509.255098436011
9
Iteration 16800: Loss = -12508.796920686957
10
Iteration 16900: Loss = -12508.795999531912
11
Iteration 17000: Loss = -12508.796512039271
12
Iteration 17100: Loss = -12508.892329157794
13
Iteration 17200: Loss = -12508.796025507114
14
Iteration 17300: Loss = -12508.801202256966
15
Stopping early at iteration 17300 due to no improvement.
pi: tensor([[9.6652e-01, 3.3478e-02],
        [1.0000e+00, 1.7736e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6825, 0.3175], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2097, 0.1013],
         [0.6200, 0.2334]],

        [[0.6772, 0.2708],
         [0.6824, 0.6516]],

        [[0.5074, 0.3055],
         [0.6895, 0.7251]],

        [[0.7131, 0.3056],
         [0.7116, 0.6282]],

        [[0.5434, 0.2820],
         [0.7148, 0.6440]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.770789833373946
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.022670269761576805
Average Adjusted Rand Index: 0.15448447160470255
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23178.802123866586
Iteration 100: Loss = -12515.569643733254
Iteration 200: Loss = -12233.060964726878
Iteration 300: Loss = -11758.026974168926
Iteration 400: Loss = -11735.885277603404
Iteration 500: Loss = -11735.002405041088
Iteration 600: Loss = -11720.852759702791
Iteration 700: Loss = -11720.661393350763
Iteration 800: Loss = -11720.539434855687
Iteration 900: Loss = -11720.454568322444
Iteration 1000: Loss = -11720.387059398901
Iteration 1100: Loss = -11716.923669083097
Iteration 1200: Loss = -11715.416902035073
Iteration 1300: Loss = -11706.4585525381
Iteration 1400: Loss = -11706.430513296977
Iteration 1500: Loss = -11706.407791449423
Iteration 1600: Loss = -11706.387933962704
Iteration 1700: Loss = -11706.361391018443
Iteration 1800: Loss = -11704.517429716114
Iteration 1900: Loss = -11704.484264811417
Iteration 2000: Loss = -11704.369721701925
Iteration 2100: Loss = -11704.331593735738
Iteration 2200: Loss = -11704.32526240783
Iteration 2300: Loss = -11704.318570212914
Iteration 2400: Loss = -11704.312846873434
Iteration 2500: Loss = -11704.307439479046
Iteration 2600: Loss = -11704.318329675802
1
Iteration 2700: Loss = -11704.298680309887
Iteration 2800: Loss = -11704.295097122627
Iteration 2900: Loss = -11704.293550893679
Iteration 3000: Loss = -11704.288736803794
Iteration 3100: Loss = -11704.286184005554
Iteration 3200: Loss = -11704.283753583935
Iteration 3300: Loss = -11704.281736020004
Iteration 3400: Loss = -11704.279870574319
Iteration 3500: Loss = -11704.278200579938
Iteration 3600: Loss = -11704.279601346181
1
Iteration 3700: Loss = -11704.275224905665
Iteration 3800: Loss = -11704.273895681377
Iteration 3900: Loss = -11704.272706098025
Iteration 4000: Loss = -11704.273732650123
1
Iteration 4100: Loss = -11704.27982763836
2
Iteration 4200: Loss = -11704.27023623705
Iteration 4300: Loss = -11704.268842873535
Iteration 4400: Loss = -11704.28623421693
1
Iteration 4500: Loss = -11704.267188183789
Iteration 4600: Loss = -11704.266446525922
Iteration 4700: Loss = -11704.268089686571
1
Iteration 4800: Loss = -11704.26792393277
2
Iteration 4900: Loss = -11704.264606382576
Iteration 5000: Loss = -11704.264038407844
Iteration 5100: Loss = -11704.263541015083
Iteration 5200: Loss = -11704.26313408601
Iteration 5300: Loss = -11704.262638135475
Iteration 5400: Loss = -11704.262201076754
Iteration 5500: Loss = -11704.261796732022
Iteration 5600: Loss = -11704.26236944188
1
Iteration 5700: Loss = -11704.261097674651
Iteration 5800: Loss = -11704.260814134004
Iteration 5900: Loss = -11704.261690361544
1
Iteration 6000: Loss = -11704.260157752491
Iteration 6100: Loss = -11704.264307581836
1
Iteration 6200: Loss = -11704.259616778556
Iteration 6300: Loss = -11704.26023915489
1
Iteration 6400: Loss = -11704.259236814969
Iteration 6500: Loss = -11704.262921540703
1
Iteration 6600: Loss = -11704.258699903166
Iteration 6700: Loss = -11704.2606382211
1
Iteration 6800: Loss = -11704.25912973224
2
Iteration 6900: Loss = -11704.258349463162
Iteration 7000: Loss = -11704.259539573963
1
Iteration 7100: Loss = -11704.25808068967
Iteration 7200: Loss = -11704.288042705919
1
Iteration 7300: Loss = -11704.257581058433
Iteration 7400: Loss = -11704.290904592604
1
Iteration 7500: Loss = -11704.257118738475
Iteration 7600: Loss = -11704.259193245773
1
Iteration 7700: Loss = -11704.256900819191
Iteration 7800: Loss = -11704.256790064357
Iteration 7900: Loss = -11704.258536144109
1
Iteration 8000: Loss = -11704.256540892842
Iteration 8100: Loss = -11704.268885108993
1
Iteration 8200: Loss = -11704.264717120355
2
Iteration 8300: Loss = -11704.260770205468
3
Iteration 8400: Loss = -11704.274702812589
4
Iteration 8500: Loss = -11704.257691108844
5
Iteration 8600: Loss = -11704.25725624358
6
Iteration 8700: Loss = -11704.255748000485
Iteration 8800: Loss = -11704.255732112906
Iteration 8900: Loss = -11704.282652553953
1
Iteration 9000: Loss = -11704.255714322966
Iteration 9100: Loss = -11704.257967557027
1
Iteration 9200: Loss = -11704.272110410777
2
Iteration 9300: Loss = -11704.254394591375
Iteration 9400: Loss = -11704.25770205757
1
Iteration 9500: Loss = -11704.258343314876
2
Iteration 9600: Loss = -11704.253837270982
Iteration 9700: Loss = -11704.267272989982
1
Iteration 9800: Loss = -11704.253716089306
Iteration 9900: Loss = -11704.2553986391
1
Iteration 10000: Loss = -11704.253642928854
Iteration 10100: Loss = -11704.258048706028
1
Iteration 10200: Loss = -11704.25646272541
2
Iteration 10300: Loss = -11704.281627362921
3
Iteration 10400: Loss = -11704.275355742066
4
Iteration 10500: Loss = -11704.25680639164
5
Iteration 10600: Loss = -11704.255235069437
6
Iteration 10700: Loss = -11704.284050481125
7
Iteration 10800: Loss = -11704.2651228057
8
Iteration 10900: Loss = -11704.265986367196
9
Iteration 11000: Loss = -11704.253740303036
Iteration 11100: Loss = -11704.253446582266
Iteration 11200: Loss = -11704.254781395415
1
Iteration 11300: Loss = -11704.253360676443
Iteration 11400: Loss = -11704.253579314556
1
Iteration 11500: Loss = -11704.406027687723
2
Iteration 11600: Loss = -11704.256780620137
3
Iteration 11700: Loss = -11704.275238478558
4
Iteration 11800: Loss = -11704.25419631887
5
Iteration 11900: Loss = -11704.253524977996
6
Iteration 12000: Loss = -11704.253598285082
7
Iteration 12100: Loss = -11704.255006747071
8
Iteration 12200: Loss = -11704.257140951178
9
Iteration 12300: Loss = -11704.317069092474
10
Iteration 12400: Loss = -11704.253220476076
Iteration 12500: Loss = -11704.255571465625
1
Iteration 12600: Loss = -11704.26644710476
2
Iteration 12700: Loss = -11704.253385372054
3
Iteration 12800: Loss = -11704.259523058901
4
Iteration 12900: Loss = -11704.256872556274
5
Iteration 13000: Loss = -11704.25833048742
6
Iteration 13100: Loss = -11704.262860567693
7
Iteration 13200: Loss = -11704.253419826411
8
Iteration 13300: Loss = -11704.255020580247
9
Iteration 13400: Loss = -11704.259354543821
10
Iteration 13500: Loss = -11704.254225222649
11
Iteration 13600: Loss = -11704.255051482802
12
Iteration 13700: Loss = -11704.255513544407
13
Iteration 13800: Loss = -11704.259318764456
14
Iteration 13900: Loss = -11704.266349036967
15
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[0.6949, 0.3051],
        [0.2733, 0.7267]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6200, 0.3800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3971, 0.1023],
         [0.5257, 0.2014]],

        [[0.6949, 0.0945],
         [0.5776, 0.7017]],

        [[0.6180, 0.1017],
         [0.5991, 0.7037]],

        [[0.5394, 0.1053],
         [0.5914, 0.7025]],

        [[0.5291, 0.0954],
         [0.7080, 0.6244]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.976096191770377
Average Adjusted Rand Index: 0.9759987148239894
11715.842300141367
[0.022670269761576805, 0.976096191770377] [0.15448447160470255, 0.9759987148239894] [12508.801202256966, 11704.266349036967]
-------------------------------------
This iteration is 43
True Objective function: Loss = -11665.912128650833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22507.79400187806
Iteration 100: Loss = -12507.024298239085
Iteration 200: Loss = -12344.817284897006
Iteration 300: Loss = -11986.516601934045
Iteration 400: Loss = -11872.143992123109
Iteration 500: Loss = -11850.467488930286
Iteration 600: Loss = -11842.148264570407
Iteration 700: Loss = -11841.91456117164
Iteration 800: Loss = -11841.809840302618
Iteration 900: Loss = -11841.742382734634
Iteration 1000: Loss = -11841.695075281432
Iteration 1100: Loss = -11841.660005796348
Iteration 1200: Loss = -11841.632568645098
Iteration 1300: Loss = -11841.60952284626
Iteration 1400: Loss = -11841.591067984933
Iteration 1500: Loss = -11841.576615386899
Iteration 1600: Loss = -11841.564478307519
Iteration 1700: Loss = -11841.553858229801
Iteration 1800: Loss = -11841.545037981294
Iteration 1900: Loss = -11841.538005102066
Iteration 2000: Loss = -11841.53203379897
Iteration 2100: Loss = -11841.526794464356
Iteration 2200: Loss = -11841.522160127364
Iteration 2300: Loss = -11841.517837623976
Iteration 2400: Loss = -11841.513506592819
Iteration 2500: Loss = -11841.50751443272
Iteration 2600: Loss = -11841.506357020331
Iteration 2700: Loss = -11841.499783253716
Iteration 2800: Loss = -11841.497326217586
Iteration 2900: Loss = -11841.495041747812
Iteration 3000: Loss = -11841.492876314749
Iteration 3100: Loss = -11841.489672843998
Iteration 3200: Loss = -11841.35303104125
Iteration 3300: Loss = -11841.352401607022
Iteration 3400: Loss = -11841.349210298615
Iteration 3500: Loss = -11841.366649617405
1
Iteration 3600: Loss = -11841.346564713138
Iteration 3700: Loss = -11841.345446300236
Iteration 3800: Loss = -11841.344424379799
Iteration 3900: Loss = -11841.343431444788
Iteration 4000: Loss = -11841.34314648516
Iteration 4100: Loss = -11841.34161787582
Iteration 4200: Loss = -11841.342091580987
1
Iteration 4300: Loss = -11841.339796435172
Iteration 4400: Loss = -11841.33903884591
Iteration 4500: Loss = -11841.339904437375
1
Iteration 4600: Loss = -11841.338407710291
Iteration 4700: Loss = -11841.34026205191
1
Iteration 4800: Loss = -11841.336971443665
Iteration 4900: Loss = -11841.338532801587
1
Iteration 5000: Loss = -11841.336148831364
Iteration 5100: Loss = -11841.335494333487
Iteration 5200: Loss = -11841.342983174472
1
Iteration 5300: Loss = -11841.334441624256
Iteration 5400: Loss = -11841.334045722597
Iteration 5500: Loss = -11841.334222798561
1
Iteration 5600: Loss = -11841.333438936503
Iteration 5700: Loss = -11841.333601015043
1
Iteration 5800: Loss = -11841.335327159919
2
Iteration 5900: Loss = -11841.33239867296
Iteration 6000: Loss = -11841.3321098092
Iteration 6100: Loss = -11841.331695064437
Iteration 6200: Loss = -11841.33155752506
Iteration 6300: Loss = -11841.336093976299
1
Iteration 6400: Loss = -11841.331163530294
Iteration 6500: Loss = -11841.331353464333
1
Iteration 6600: Loss = -11841.330655412856
Iteration 6700: Loss = -11841.331387737455
1
Iteration 6800: Loss = -11841.330279383532
Iteration 6900: Loss = -11841.33016785159
Iteration 7000: Loss = -11841.329978222926
Iteration 7100: Loss = -11841.329848271796
Iteration 7200: Loss = -11841.32951120538
Iteration 7300: Loss = -11841.233615113008
Iteration 7400: Loss = -11841.227192940105
Iteration 7500: Loss = -11841.22735190973
1
Iteration 7600: Loss = -11841.227008523503
Iteration 7700: Loss = -11841.264771223261
1
Iteration 7800: Loss = -11841.229652567861
2
Iteration 7900: Loss = -11841.248065401142
3
Iteration 8000: Loss = -11841.2400101046
4
Iteration 8100: Loss = -11841.259264206848
5
Iteration 8200: Loss = -11841.232590125719
6
Iteration 8300: Loss = -11841.228118419036
7
Iteration 8400: Loss = -11841.23058230543
8
Iteration 8500: Loss = -11841.2265809927
Iteration 8600: Loss = -11841.226593588963
Iteration 8700: Loss = -11841.2263073882
Iteration 8800: Loss = -11841.226685982696
1
Iteration 8900: Loss = -11841.226230763179
Iteration 9000: Loss = -11841.226170817023
Iteration 9100: Loss = -11841.250024812483
1
Iteration 9200: Loss = -11841.226078409214
Iteration 9300: Loss = -11841.226158685502
Iteration 9400: Loss = -11841.230900063256
1
Iteration 9500: Loss = -11841.226050579904
Iteration 9600: Loss = -11841.226348926302
1
Iteration 9700: Loss = -11841.246021123328
2
Iteration 9800: Loss = -11841.22629890738
3
Iteration 9900: Loss = -11841.226854524492
4
Iteration 10000: Loss = -11841.226254230533
5
Iteration 10100: Loss = -11841.226840031446
6
Iteration 10200: Loss = -11841.26469699297
7
Iteration 10300: Loss = -11841.229225267893
8
Iteration 10400: Loss = -11841.221527126847
Iteration 10500: Loss = -11841.235381296492
1
Iteration 10600: Loss = -11841.221041723116
Iteration 10700: Loss = -11841.221144199235
1
Iteration 10800: Loss = -11841.220862059881
Iteration 10900: Loss = -11841.223549665241
1
Iteration 11000: Loss = -11841.222278086474
2
Iteration 11100: Loss = -11841.23033820818
3
Iteration 11200: Loss = -11841.319077792503
4
Iteration 11300: Loss = -11841.234647394596
5
Iteration 11400: Loss = -11841.222782917694
6
Iteration 11500: Loss = -11841.279947527595
7
Iteration 11600: Loss = -11841.22917534623
8
Iteration 11700: Loss = -11841.220661599427
Iteration 11800: Loss = -11841.228416303396
1
Iteration 11900: Loss = -11841.255481730734
2
Iteration 12000: Loss = -11841.239509163353
3
Iteration 12100: Loss = -11841.221088327697
4
Iteration 12200: Loss = -11841.220970184762
5
Iteration 12300: Loss = -11841.256705997175
6
Iteration 12400: Loss = -11841.329813053111
7
Iteration 12500: Loss = -11841.220602533816
Iteration 12600: Loss = -11841.221186786213
1
Iteration 12700: Loss = -11841.260028972689
2
Iteration 12800: Loss = -11841.269330168858
3
Iteration 12900: Loss = -11841.222404290147
4
Iteration 13000: Loss = -11841.256504013289
5
Iteration 13100: Loss = -11841.232042668176
6
Iteration 13200: Loss = -11841.22063461763
Iteration 13300: Loss = -11841.221426648024
1
Iteration 13400: Loss = -11841.288772444637
2
Iteration 13500: Loss = -11841.23150741237
3
Iteration 13600: Loss = -11841.259239115889
4
Iteration 13700: Loss = -11841.221989865971
5
Iteration 13800: Loss = -11841.221927871025
6
Iteration 13900: Loss = -11841.225091429022
7
Iteration 14000: Loss = -11841.220577911841
Iteration 14100: Loss = -11841.220756660408
1
Iteration 14200: Loss = -11841.298811170196
2
Iteration 14300: Loss = -11841.230244891847
3
Iteration 14400: Loss = -11841.22053819458
Iteration 14500: Loss = -11841.220688531623
1
Iteration 14600: Loss = -11841.225304319638
2
Iteration 14700: Loss = -11841.221836531236
3
Iteration 14800: Loss = -11841.23481771895
4
Iteration 14900: Loss = -11841.21973416449
Iteration 15000: Loss = -11841.219784025203
Iteration 15100: Loss = -11841.223359985228
1
Iteration 15200: Loss = -11841.22631604716
2
Iteration 15300: Loss = -11841.228061613227
3
Iteration 15400: Loss = -11841.221941004515
4
Iteration 15500: Loss = -11841.21966118613
Iteration 15600: Loss = -11841.221794615125
1
Iteration 15700: Loss = -11841.222033716325
2
Iteration 15800: Loss = -11841.227145477867
3
Iteration 15900: Loss = -11841.305248700392
4
Iteration 16000: Loss = -11841.237539678617
5
Iteration 16100: Loss = -11841.219630934142
Iteration 16200: Loss = -11841.221650970961
1
Iteration 16300: Loss = -11841.278401844947
2
Iteration 16400: Loss = -11841.262965855205
3
Iteration 16500: Loss = -11841.22231840423
4
Iteration 16600: Loss = -11841.224520585294
5
Iteration 16700: Loss = -11841.21972191639
Iteration 16800: Loss = -11841.22052615933
1
Iteration 16900: Loss = -11841.230521300327
2
Iteration 17000: Loss = -11841.230105212439
3
Iteration 17100: Loss = -11841.21998140828
4
Iteration 17200: Loss = -11841.219730038625
Iteration 17300: Loss = -11841.233034496634
1
Iteration 17400: Loss = -11841.219980313033
2
Iteration 17500: Loss = -11841.219612382181
Iteration 17600: Loss = -11841.225383911975
1
Iteration 17700: Loss = -11841.226039634372
2
Iteration 17800: Loss = -11841.278494998185
3
Iteration 17900: Loss = -11841.23032449419
4
Iteration 18000: Loss = -11841.219678784346
Iteration 18100: Loss = -11841.219788691267
1
Iteration 18200: Loss = -11841.227806224206
2
Iteration 18300: Loss = -11841.230791558779
3
Iteration 18400: Loss = -11841.299198582223
4
Iteration 18500: Loss = -11841.227223558832
5
Iteration 18600: Loss = -11841.21961723886
Iteration 18700: Loss = -11841.220180174529
1
Iteration 18800: Loss = -11841.223284611671
2
Iteration 18900: Loss = -11841.220997707636
3
Iteration 19000: Loss = -11841.230605626784
4
Iteration 19100: Loss = -11841.219669721904
Iteration 19200: Loss = -11841.242416681105
1
Iteration 19300: Loss = -11841.222799970026
2
Iteration 19400: Loss = -11841.23945878647
3
Iteration 19500: Loss = -11841.222373854347
4
Iteration 19600: Loss = -11841.241138013806
5
Iteration 19700: Loss = -11841.21958160059
Iteration 19800: Loss = -11841.21959422479
Iteration 19900: Loss = -11841.222306258056
1
pi: tensor([[0.7886, 0.2114],
        [0.3289, 0.6711]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0258, 0.9742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4088, 0.1036],
         [0.5143, 0.2053]],

        [[0.5890, 0.0979],
         [0.6788, 0.6131]],

        [[0.5635, 0.1085],
         [0.5713, 0.7203]],

        [[0.6117, 0.0976],
         [0.5049, 0.5186]],

        [[0.5600, 0.1068],
         [0.5709, 0.6017]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6329072572696726
Average Adjusted Rand Index: 0.8
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21839.240464278108
Iteration 100: Loss = -12515.096878607774
Iteration 200: Loss = -12413.410952861876
Iteration 300: Loss = -12124.712775641403
Iteration 400: Loss = -11929.374165186044
Iteration 500: Loss = -11893.988769610616
Iteration 600: Loss = -11866.32274573499
Iteration 700: Loss = -11858.800940321164
Iteration 800: Loss = -11852.232683896744
Iteration 900: Loss = -11852.051622488605
Iteration 1000: Loss = -11851.916512067954
Iteration 1100: Loss = -11851.715969457784
Iteration 1200: Loss = -11850.485802290965
Iteration 1300: Loss = -11849.951897708816
Iteration 1400: Loss = -11841.880608344129
Iteration 1500: Loss = -11841.749400596766
Iteration 1600: Loss = -11841.69388250913
Iteration 1700: Loss = -11841.657220721265
Iteration 1800: Loss = -11841.629503913728
Iteration 1900: Loss = -11841.605370976662
Iteration 2000: Loss = -11841.556336068123
Iteration 2100: Loss = -11841.443000454972
Iteration 2200: Loss = -11841.429643812968
Iteration 2300: Loss = -11841.41909958353
Iteration 2400: Loss = -11841.410227905772
Iteration 2500: Loss = -11841.402615598463
Iteration 2600: Loss = -11841.395975942663
Iteration 2700: Loss = -11841.39011754514
Iteration 2800: Loss = -11841.384934957552
Iteration 2900: Loss = -11841.381323793443
Iteration 3000: Loss = -11841.376164330615
Iteration 3100: Loss = -11841.372434302764
Iteration 3200: Loss = -11841.369038867584
Iteration 3300: Loss = -11841.365991342285
Iteration 3400: Loss = -11841.36371469765
Iteration 3500: Loss = -11841.360599850588
Iteration 3600: Loss = -11841.364323120019
1
Iteration 3700: Loss = -11841.35604406302
Iteration 3800: Loss = -11841.354328402602
Iteration 3900: Loss = -11841.351971782846
Iteration 4000: Loss = -11841.364457702739
1
Iteration 4100: Loss = -11841.347961745643
Iteration 4200: Loss = -11841.345183199617
Iteration 4300: Loss = -11841.334535519447
Iteration 4400: Loss = -11841.242974683457
Iteration 4500: Loss = -11841.240699664047
Iteration 4600: Loss = -11841.239741267516
Iteration 4700: Loss = -11841.238132911523
Iteration 4800: Loss = -11841.2370838578
Iteration 4900: Loss = -11841.236171546907
Iteration 5000: Loss = -11841.235341541395
Iteration 5100: Loss = -11841.252395680269
1
Iteration 5200: Loss = -11841.233816818894
Iteration 5300: Loss = -11841.233142070174
Iteration 5400: Loss = -11841.232783739053
Iteration 5500: Loss = -11841.231899335471
Iteration 5600: Loss = -11841.231364101941
Iteration 5700: Loss = -11841.231155872463
Iteration 5800: Loss = -11841.230378892858
Iteration 5900: Loss = -11841.241669724119
1
Iteration 6000: Loss = -11841.22950869257
Iteration 6100: Loss = -11841.229151583208
Iteration 6200: Loss = -11841.228647997534
Iteration 6300: Loss = -11841.228273799914
Iteration 6400: Loss = -11841.227917796541
Iteration 6500: Loss = -11841.227536859582
Iteration 6600: Loss = -11841.227097419918
Iteration 6700: Loss = -11841.228273185485
1
Iteration 6800: Loss = -11841.226258428294
Iteration 6900: Loss = -11841.226664705538
1
Iteration 7000: Loss = -11841.234397252494
2
Iteration 7100: Loss = -11841.225495500585
Iteration 7200: Loss = -11841.225543210703
Iteration 7300: Loss = -11841.225030266278
Iteration 7400: Loss = -11841.228817382213
1
Iteration 7500: Loss = -11841.226510043596
2
Iteration 7600: Loss = -11841.224610958048
Iteration 7700: Loss = -11841.22437411811
Iteration 7800: Loss = -11841.233197256712
1
Iteration 7900: Loss = -11841.238071058082
2
Iteration 8000: Loss = -11841.22388216292
Iteration 8100: Loss = -11841.222788965159
Iteration 8200: Loss = -11841.222731611522
Iteration 8300: Loss = -11841.226894107676
1
Iteration 8400: Loss = -11841.221736444566
Iteration 8500: Loss = -11841.221779596372
Iteration 8600: Loss = -11841.22154126008
Iteration 8700: Loss = -11841.22155524496
Iteration 8800: Loss = -11841.222105119225
1
Iteration 8900: Loss = -11841.22305148066
2
Iteration 9000: Loss = -11841.225838678063
3
Iteration 9100: Loss = -11841.22367797905
4
Iteration 9200: Loss = -11841.224520302014
5
Iteration 9300: Loss = -11841.279192386251
6
Iteration 9400: Loss = -11841.222477776742
7
Iteration 9500: Loss = -11841.226962119388
8
Iteration 9600: Loss = -11841.247322727098
9
Iteration 9700: Loss = -11841.226198090566
10
Iteration 9800: Loss = -11841.220586335683
Iteration 9900: Loss = -11841.232024820349
1
Iteration 10000: Loss = -11841.222324966588
2
Iteration 10100: Loss = -11841.22448743173
3
Iteration 10200: Loss = -11841.22471769136
4
Iteration 10300: Loss = -11841.222072910003
5
Iteration 10400: Loss = -11841.220763120657
6
Iteration 10500: Loss = -11841.220338323505
Iteration 10600: Loss = -11841.222911148838
1
Iteration 10700: Loss = -11841.223593966435
2
Iteration 10800: Loss = -11841.221133493049
3
Iteration 10900: Loss = -11841.227619435454
4
Iteration 11000: Loss = -11841.227949040225
5
Iteration 11100: Loss = -11841.231299834024
6
Iteration 11200: Loss = -11841.254240414657
7
Iteration 11300: Loss = -11841.220322105997
Iteration 11400: Loss = -11841.22363526138
1
Iteration 11500: Loss = -11841.224831798954
2
Iteration 11600: Loss = -11841.220487617747
3
Iteration 11700: Loss = -11841.220059876445
Iteration 11800: Loss = -11841.223160471605
1
Iteration 11900: Loss = -11841.222509514491
2
Iteration 12000: Loss = -11841.222473862736
3
Iteration 12100: Loss = -11841.220649298346
4
Iteration 12200: Loss = -11841.220435786146
5
Iteration 12300: Loss = -11841.220447353153
6
Iteration 12400: Loss = -11841.222541401874
7
Iteration 12500: Loss = -11841.225821115884
8
Iteration 12600: Loss = -11841.219856768877
Iteration 12700: Loss = -11841.243672262062
1
Iteration 12800: Loss = -11841.362150699848
2
Iteration 12900: Loss = -11841.219679495938
Iteration 13000: Loss = -11841.220192278002
1
Iteration 13100: Loss = -11841.219626602195
Iteration 13200: Loss = -11841.221445920602
1
Iteration 13300: Loss = -11841.236646783083
2
Iteration 13400: Loss = -11841.222996983906
3
Iteration 13500: Loss = -11841.221864127547
4
Iteration 13600: Loss = -11841.221436039918
5
Iteration 13700: Loss = -11841.231583333292
6
Iteration 13800: Loss = -11841.223113476935
7
Iteration 13900: Loss = -11841.219546531847
Iteration 14000: Loss = -11841.221326577097
1
Iteration 14100: Loss = -11841.221389346316
2
Iteration 14200: Loss = -11841.234978347009
3
Iteration 14300: Loss = -11841.220551071578
4
Iteration 14400: Loss = -11841.220206404292
5
Iteration 14500: Loss = -11841.261703672122
6
Iteration 14600: Loss = -11841.229203147439
7
Iteration 14700: Loss = -11841.339580763319
8
Iteration 14800: Loss = -11841.219520585935
Iteration 14900: Loss = -11841.223170749723
1
Iteration 15000: Loss = -11841.243511795783
2
Iteration 15100: Loss = -11841.221735607807
3
Iteration 15200: Loss = -11841.249820918987
4
Iteration 15300: Loss = -11841.257209540552
5
Iteration 15400: Loss = -11841.220256419121
6
Iteration 15500: Loss = -11841.219769580995
7
Iteration 15600: Loss = -11841.220206294309
8
Iteration 15700: Loss = -11841.222856372646
9
Iteration 15800: Loss = -11841.259086783106
10
Iteration 15900: Loss = -11841.219598023408
Iteration 16000: Loss = -11841.223755292554
1
Iteration 16100: Loss = -11841.223932822239
2
Iteration 16200: Loss = -11841.23006353811
3
Iteration 16300: Loss = -11841.228393985517
4
Iteration 16400: Loss = -11841.221120826434
5
Iteration 16500: Loss = -11841.266768236257
6
Iteration 16600: Loss = -11841.22499709536
7
Iteration 16700: Loss = -11841.228144076522
8
Iteration 16800: Loss = -11841.228988241659
9
Iteration 16900: Loss = -11841.240596962609
10
Iteration 17000: Loss = -11841.223690799925
11
Iteration 17100: Loss = -11841.227252223347
12
Iteration 17200: Loss = -11841.236409425961
13
Iteration 17300: Loss = -11841.242446080143
14
Iteration 17400: Loss = -11841.220670610586
15
Stopping early at iteration 17400 due to no improvement.
pi: tensor([[0.7874, 0.2126],
        [0.3312, 0.6688]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0259, 0.9741], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4086, 0.1039],
         [0.7078, 0.2058]],

        [[0.6138, 0.0979],
         [0.5365, 0.6113]],

        [[0.6911, 0.1083],
         [0.6110, 0.7118]],

        [[0.5188, 0.0974],
         [0.5040, 0.5039]],

        [[0.5518, 0.1068],
         [0.6717, 0.6272]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6329072572696726
Average Adjusted Rand Index: 0.8
11665.912128650833
[0.6329072572696726, 0.6329072572696726] [0.8, 0.8] [11841.225338983866, 11841.220670610586]
-------------------------------------
This iteration is 44
True Objective function: Loss = -11625.693091317185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21439.94564361524
Iteration 100: Loss = -12181.133578045663
Iteration 200: Loss = -11623.829931259406
Iteration 300: Loss = -11622.40703725714
Iteration 400: Loss = -11622.022711018553
Iteration 500: Loss = -11621.834944757573
Iteration 600: Loss = -11621.724942108218
Iteration 700: Loss = -11621.65387594199
Iteration 800: Loss = -11621.604548761481
Iteration 900: Loss = -11621.569136083812
Iteration 1000: Loss = -11621.542973433186
Iteration 1100: Loss = -11621.522717614553
Iteration 1200: Loss = -11621.506585854824
Iteration 1300: Loss = -11621.493553115932
Iteration 1400: Loss = -11621.482887023341
Iteration 1500: Loss = -11621.474132163397
Iteration 1600: Loss = -11621.466885304179
Iteration 1700: Loss = -11621.460709985502
Iteration 1800: Loss = -11621.45549472687
Iteration 1900: Loss = -11621.450941846828
Iteration 2000: Loss = -11621.447023983445
Iteration 2100: Loss = -11621.443651148336
Iteration 2200: Loss = -11621.440567343396
Iteration 2300: Loss = -11621.437939321166
Iteration 2400: Loss = -11621.435566321612
Iteration 2500: Loss = -11621.433482339142
Iteration 2600: Loss = -11621.431618670644
Iteration 2700: Loss = -11621.42990974394
Iteration 2800: Loss = -11621.428404803495
Iteration 2900: Loss = -11621.426980768107
Iteration 3000: Loss = -11621.425768301322
Iteration 3100: Loss = -11621.424625654608
Iteration 3200: Loss = -11621.42368368496
Iteration 3300: Loss = -11621.42266299474
Iteration 3400: Loss = -11621.421766085617
Iteration 3500: Loss = -11621.427578112147
1
Iteration 3600: Loss = -11621.420228970168
Iteration 3700: Loss = -11621.419602661463
Iteration 3800: Loss = -11621.418991338569
Iteration 3900: Loss = -11621.418416260389
Iteration 4000: Loss = -11621.417876996553
Iteration 4100: Loss = -11621.42061200409
1
Iteration 4200: Loss = -11621.420873124489
2
Iteration 4300: Loss = -11621.416495568314
Iteration 4400: Loss = -11621.416177996889
Iteration 4500: Loss = -11621.430841517658
1
Iteration 4600: Loss = -11621.415788640688
Iteration 4700: Loss = -11621.41505122288
Iteration 4800: Loss = -11621.4223745491
1
Iteration 4900: Loss = -11621.414486794329
Iteration 5000: Loss = -11621.4150611939
1
Iteration 5100: Loss = -11621.413966267039
Iteration 5200: Loss = -11621.414160550317
1
Iteration 5300: Loss = -11621.413538011782
Iteration 5400: Loss = -11621.423277507081
1
Iteration 5500: Loss = -11621.413162402348
Iteration 5600: Loss = -11621.412965094763
Iteration 5700: Loss = -11621.413136614527
1
Iteration 5800: Loss = -11621.412645033108
Iteration 5900: Loss = -11621.429866227383
1
Iteration 6000: Loss = -11621.412327003003
Iteration 6100: Loss = -11621.416702118531
1
Iteration 6200: Loss = -11621.412999676137
2
Iteration 6300: Loss = -11621.41201553631
Iteration 6400: Loss = -11621.425160412507
1
Iteration 6500: Loss = -11621.411696792191
Iteration 6600: Loss = -11621.422853195143
1
Iteration 6700: Loss = -11621.41028905366
Iteration 6800: Loss = -11621.407451855202
Iteration 6900: Loss = -11621.414077665426
1
Iteration 7000: Loss = -11621.40725583068
Iteration 7100: Loss = -11621.408025828508
1
Iteration 7200: Loss = -11621.472791765687
2
Iteration 7300: Loss = -11621.40730960958
Iteration 7400: Loss = -11621.40859344542
1
Iteration 7500: Loss = -11621.407225913175
Iteration 7600: Loss = -11621.417438673921
1
Iteration 7700: Loss = -11621.42781472514
2
Iteration 7800: Loss = -11621.40913615372
3
Iteration 7900: Loss = -11621.409532346408
4
Iteration 8000: Loss = -11621.406640272913
Iteration 8100: Loss = -11621.40675590016
1
Iteration 8200: Loss = -11621.438397236514
2
Iteration 8300: Loss = -11621.406510169958
Iteration 8400: Loss = -11621.406945210752
1
Iteration 8500: Loss = -11621.420755251602
2
Iteration 8600: Loss = -11621.406445263274
Iteration 8700: Loss = -11621.425243991593
1
Iteration 8800: Loss = -11621.415383784075
2
Iteration 8900: Loss = -11621.409792243432
3
Iteration 9000: Loss = -11621.41131116491
4
Iteration 9100: Loss = -11621.415427662296
5
Iteration 9200: Loss = -11621.439167139992
6
Iteration 9300: Loss = -11621.42195111464
7
Iteration 9400: Loss = -11621.455992976722
8
Iteration 9500: Loss = -11621.406620383485
9
Iteration 9600: Loss = -11621.41280468216
10
Iteration 9700: Loss = -11621.412697232
11
Iteration 9800: Loss = -11621.40981255753
12
Iteration 9900: Loss = -11621.409179136153
13
Iteration 10000: Loss = -11621.423841398508
14
Iteration 10100: Loss = -11621.41007230171
15
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.7442, 0.2558],
        [0.2499, 0.7501]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5505, 0.4495], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2063, 0.1068],
         [0.6410, 0.4007]],

        [[0.6275, 0.1039],
         [0.5815, 0.5065]],

        [[0.5560, 0.0919],
         [0.7222, 0.6605]],

        [[0.6439, 0.1059],
         [0.5400, 0.7081]],

        [[0.5636, 0.0947],
         [0.6311, 0.5057]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21756.963830699875
Iteration 100: Loss = -12396.181195835785
Iteration 200: Loss = -12336.592708938888
Iteration 300: Loss = -11776.358432327297
Iteration 400: Loss = -11741.587312851834
Iteration 500: Loss = -11729.13734314243
Iteration 600: Loss = -11719.79772488117
Iteration 700: Loss = -11719.669037104468
Iteration 800: Loss = -11719.604299985784
Iteration 900: Loss = -11719.560006250269
Iteration 1000: Loss = -11719.526903737238
Iteration 1100: Loss = -11719.498629617157
Iteration 1200: Loss = -11719.478273164546
Iteration 1300: Loss = -11719.459945500526
Iteration 1400: Loss = -11710.167574729476
Iteration 1500: Loss = -11710.155270535497
Iteration 1600: Loss = -11710.146220528317
Iteration 1700: Loss = -11710.122427593411
Iteration 1800: Loss = -11700.464418792348
Iteration 1900: Loss = -11700.45754996894
Iteration 2000: Loss = -11688.464239409403
Iteration 2100: Loss = -11688.4538954413
Iteration 2200: Loss = -11688.447690568866
Iteration 2300: Loss = -11688.445019700332
Iteration 2400: Loss = -11672.930787310337
Iteration 2500: Loss = -11672.912124845614
Iteration 2600: Loss = -11672.902594416188
Iteration 2700: Loss = -11672.900738097687
Iteration 2800: Loss = -11672.898122924511
Iteration 2900: Loss = -11672.89629884932
Iteration 3000: Loss = -11672.896455640068
1
Iteration 3100: Loss = -11672.892387019197
Iteration 3200: Loss = -11669.642641662524
Iteration 3300: Loss = -11668.648899515754
Iteration 3400: Loss = -11668.647467077528
Iteration 3500: Loss = -11659.531719856912
Iteration 3600: Loss = -11659.465318322857
Iteration 3700: Loss = -11659.464207653118
Iteration 3800: Loss = -11659.463322787531
Iteration 3900: Loss = -11659.462551905113
Iteration 4000: Loss = -11659.461879272789
Iteration 4100: Loss = -11659.464607636402
1
Iteration 4200: Loss = -11659.460702435643
Iteration 4300: Loss = -11659.461134963572
1
Iteration 4400: Loss = -11659.459725793113
Iteration 4500: Loss = -11659.459324828806
Iteration 4600: Loss = -11659.459049718555
Iteration 4700: Loss = -11659.458519957108
Iteration 4800: Loss = -11659.458193733315
Iteration 4900: Loss = -11659.457975003712
Iteration 5000: Loss = -11659.457587815476
Iteration 5100: Loss = -11659.457307461764
Iteration 5200: Loss = -11659.457056491938
Iteration 5300: Loss = -11659.456843414831
Iteration 5400: Loss = -11659.456599668576
Iteration 5500: Loss = -11659.456443882598
Iteration 5600: Loss = -11659.456321720829
Iteration 5700: Loss = -11659.456028560455
Iteration 5800: Loss = -11659.455953107734
Iteration 5900: Loss = -11659.45574276515
Iteration 6000: Loss = -11659.456245497506
1
Iteration 6100: Loss = -11659.464687309066
2
Iteration 6200: Loss = -11659.455296609054
Iteration 6300: Loss = -11659.455382422884
Iteration 6400: Loss = -11659.455196428195
Iteration 6500: Loss = -11659.45496201814
Iteration 6600: Loss = -11659.454993695457
Iteration 6700: Loss = -11659.454843912607
Iteration 6800: Loss = -11659.454716291875
Iteration 6900: Loss = -11659.455391321613
1
Iteration 7000: Loss = -11659.458990144916
2
Iteration 7100: Loss = -11659.454436976252
Iteration 7200: Loss = -11659.454345067408
Iteration 7300: Loss = -11659.459618771456
1
Iteration 7400: Loss = -11659.454200012036
Iteration 7500: Loss = -11659.457999235217
1
Iteration 7600: Loss = -11659.457542774462
2
Iteration 7700: Loss = -11659.461647183278
3
Iteration 7800: Loss = -11659.454025370966
Iteration 7900: Loss = -11659.454115474251
Iteration 8000: Loss = -11659.454532871066
1
Iteration 8100: Loss = -11659.490041247704
2
Iteration 8200: Loss = -11659.44505005694
Iteration 8300: Loss = -11659.439696978
Iteration 8400: Loss = -11659.439775302915
Iteration 8500: Loss = -11659.444470061535
1
Iteration 8600: Loss = -11659.481048888774
2
Iteration 8700: Loss = -11659.441831063868
3
Iteration 8800: Loss = -11659.442874075812
4
Iteration 8900: Loss = -11659.439478365508
Iteration 9000: Loss = -11659.44885904709
1
Iteration 9100: Loss = -11659.440405333531
2
Iteration 9200: Loss = -11659.441462747569
3
Iteration 9300: Loss = -11659.439363066354
Iteration 9400: Loss = -11646.777640245757
Iteration 9500: Loss = -11646.76910854374
Iteration 9600: Loss = -11646.771468858718
1
Iteration 9700: Loss = -11646.770484876877
2
Iteration 9800: Loss = -11646.773660732491
3
Iteration 9900: Loss = -11640.833180839814
Iteration 10000: Loss = -11640.868646911977
1
Iteration 10100: Loss = -11640.847644193007
2
Iteration 10200: Loss = -11640.840460922984
3
Iteration 10300: Loss = -11640.832134822946
Iteration 10400: Loss = -11640.845346950633
1
Iteration 10500: Loss = -11640.830771107974
Iteration 10600: Loss = -11626.162335715557
Iteration 10700: Loss = -11626.239774598756
1
Iteration 10800: Loss = -11626.153374074796
Iteration 10900: Loss = -11626.151624672624
Iteration 11000: Loss = -11626.17298806996
1
Iteration 11100: Loss = -11626.169373068295
2
Iteration 11200: Loss = -11626.146703338069
Iteration 11300: Loss = -11626.148096169365
1
Iteration 11400: Loss = -11626.150044237416
2
Iteration 11500: Loss = -11626.253142884214
3
Iteration 11600: Loss = -11626.148334970314
4
Iteration 11700: Loss = -11626.146133406792
Iteration 11800: Loss = -11626.153252107462
1
Iteration 11900: Loss = -11626.144758756147
Iteration 12000: Loss = -11626.171409379524
1
Iteration 12100: Loss = -11626.146309673546
2
Iteration 12200: Loss = -11626.14533326232
3
Iteration 12300: Loss = -11626.188552223291
4
Iteration 12400: Loss = -11626.151904259526
5
Iteration 12500: Loss = -11626.147035852306
6
Iteration 12600: Loss = -11626.146854433333
7
Iteration 12700: Loss = -11626.204185780576
8
Iteration 12800: Loss = -11626.169272466897
9
Iteration 12900: Loss = -11626.148531501001
10
Iteration 13000: Loss = -11626.16398414594
11
Iteration 13100: Loss = -11626.14837819565
12
Iteration 13200: Loss = -11626.158797520822
13
Iteration 13300: Loss = -11626.22046376401
14
Iteration 13400: Loss = -11626.182550894408
15
Stopping early at iteration 13400 due to no improvement.
pi: tensor([[0.7420, 0.2580],
        [0.2603, 0.7397]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4473, 0.5527], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4032, 0.1064],
         [0.5909, 0.2065]],

        [[0.7174, 0.1072],
         [0.5967, 0.5297]],

        [[0.5066, 0.0924],
         [0.5251, 0.6975]],

        [[0.6772, 0.1061],
         [0.7051, 0.6259]],

        [[0.5594, 0.0949],
         [0.6008, 0.6081]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919998119331364
11625.693091317185
[1.0, 0.9919999775871758] [1.0, 0.9919998119331364] [11621.41007230171, 11626.182550894408]
-------------------------------------
This iteration is 45
True Objective function: Loss = -11651.939308210178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22329.50314716113
Iteration 100: Loss = -12311.376780556257
Iteration 200: Loss = -12070.24642660383
Iteration 300: Loss = -11647.897764517742
Iteration 400: Loss = -11647.092254259258
Iteration 500: Loss = -11646.842204938066
Iteration 600: Loss = -11646.715495658687
Iteration 700: Loss = -11646.638571593037
Iteration 800: Loss = -11646.587259605298
Iteration 900: Loss = -11646.551132999135
Iteration 1000: Loss = -11646.524679100141
Iteration 1100: Loss = -11646.504487547445
Iteration 1200: Loss = -11646.488561599206
Iteration 1300: Loss = -11646.475788664446
Iteration 1400: Loss = -11646.465354681182
Iteration 1500: Loss = -11646.456707227837
Iteration 1600: Loss = -11646.449502654697
Iteration 1700: Loss = -11646.443427185168
Iteration 1800: Loss = -11646.438196962603
Iteration 1900: Loss = -11646.433721951775
Iteration 2000: Loss = -11646.429853836438
Iteration 2100: Loss = -11646.42636440638
Iteration 2200: Loss = -11646.423413074011
Iteration 2300: Loss = -11646.420734624866
Iteration 2400: Loss = -11646.418389110291
Iteration 2500: Loss = -11646.416294551462
Iteration 2600: Loss = -11646.414410533736
Iteration 2700: Loss = -11646.412722812252
Iteration 2800: Loss = -11646.41389421235
1
Iteration 2900: Loss = -11646.409822626792
Iteration 3000: Loss = -11646.408594329938
Iteration 3100: Loss = -11646.407467535235
Iteration 3200: Loss = -11646.406433907776
Iteration 3300: Loss = -11646.405434587383
Iteration 3400: Loss = -11646.404581857765
Iteration 3500: Loss = -11646.404017283485
Iteration 3600: Loss = -11646.403010920209
Iteration 3700: Loss = -11646.402395652507
Iteration 3800: Loss = -11646.40178052462
Iteration 3900: Loss = -11646.401204871185
Iteration 4000: Loss = -11646.4006387901
Iteration 4100: Loss = -11646.402676250003
1
Iteration 4200: Loss = -11646.400777000354
2
Iteration 4300: Loss = -11646.399293793143
Iteration 4400: Loss = -11646.404771224392
1
Iteration 4500: Loss = -11646.39862150772
Iteration 4600: Loss = -11646.398200437674
Iteration 4700: Loss = -11646.39800374839
Iteration 4800: Loss = -11646.397570760686
Iteration 4900: Loss = -11646.397309516788
Iteration 5000: Loss = -11646.398244835942
1
Iteration 5100: Loss = -11646.396795963916
Iteration 5200: Loss = -11646.396629909945
Iteration 5300: Loss = -11646.396311489316
Iteration 5400: Loss = -11646.396166213692
Iteration 5500: Loss = -11646.395927402673
Iteration 5600: Loss = -11646.39697227006
1
Iteration 5700: Loss = -11646.39557302182
Iteration 5800: Loss = -11646.410524253637
1
Iteration 5900: Loss = -11646.395338278717
Iteration 6000: Loss = -11646.400977177787
1
Iteration 6100: Loss = -11646.39509005319
Iteration 6200: Loss = -11646.395147619507
Iteration 6300: Loss = -11646.39544554344
1
Iteration 6400: Loss = -11646.394710005094
Iteration 6500: Loss = -11646.395541925387
1
Iteration 6600: Loss = -11646.395074384956
2
Iteration 6700: Loss = -11646.394505869634
Iteration 6800: Loss = -11646.394560013834
Iteration 6900: Loss = -11646.394368310936
Iteration 7000: Loss = -11646.394375829615
Iteration 7100: Loss = -11646.394207083133
Iteration 7200: Loss = -11646.394101585312
Iteration 7300: Loss = -11646.396347843112
1
Iteration 7400: Loss = -11646.396954247084
2
Iteration 7500: Loss = -11646.394566493675
3
Iteration 7600: Loss = -11646.393947827615
Iteration 7700: Loss = -11646.396546615568
1
Iteration 7800: Loss = -11646.394287639918
2
Iteration 7900: Loss = -11646.393702125059
Iteration 8000: Loss = -11646.397311276314
1
Iteration 8100: Loss = -11646.393827043652
2
Iteration 8200: Loss = -11646.39513678121
3
Iteration 8300: Loss = -11646.39372967802
Iteration 8400: Loss = -11646.399643173952
1
Iteration 8500: Loss = -11646.394023691193
2
Iteration 8600: Loss = -11646.393883045046
3
Iteration 8700: Loss = -11646.39440462058
4
Iteration 8800: Loss = -11646.39380236145
Iteration 8900: Loss = -11646.400723694156
1
Iteration 9000: Loss = -11646.393753346221
Iteration 9100: Loss = -11646.399668582018
1
Iteration 9200: Loss = -11646.416138459088
2
Iteration 9300: Loss = -11646.403497919526
3
Iteration 9400: Loss = -11646.395075912487
4
Iteration 9500: Loss = -11646.393589593341
Iteration 9600: Loss = -11646.394265074086
1
Iteration 9700: Loss = -11646.393407039091
Iteration 9800: Loss = -11646.398242041008
1
Iteration 9900: Loss = -11646.393292558556
Iteration 10000: Loss = -11646.39338368017
Iteration 10100: Loss = -11646.394743351193
1
Iteration 10200: Loss = -11646.394319977655
2
Iteration 10300: Loss = -11646.409854495647
3
Iteration 10400: Loss = -11646.403949411295
4
Iteration 10500: Loss = -11646.413892448145
5
Iteration 10600: Loss = -11646.39509639471
6
Iteration 10700: Loss = -11646.403288348774
7
Iteration 10800: Loss = -11646.401524936166
8
Iteration 10900: Loss = -11646.39623541669
9
Iteration 11000: Loss = -11646.399507167065
10
Iteration 11100: Loss = -11646.395660926857
11
Iteration 11200: Loss = -11646.395571300518
12
Iteration 11300: Loss = -11646.414459937554
13
Iteration 11400: Loss = -11646.39848461573
14
Iteration 11500: Loss = -11646.409448158269
15
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[0.7938, 0.2062],
        [0.2710, 0.7290]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5000, 0.5000], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3916, 0.1006],
         [0.6824, 0.1927]],

        [[0.6583, 0.0996],
         [0.6285, 0.7176]],

        [[0.6032, 0.1033],
         [0.5444, 0.6341]],

        [[0.5737, 0.0917],
         [0.5939, 0.6110]],

        [[0.5558, 0.0925],
         [0.6602, 0.6936]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22588.560039453037
Iteration 100: Loss = -12515.4547033349
Iteration 200: Loss = -12027.507208775105
Iteration 300: Loss = -11828.831434012793
Iteration 400: Loss = -11823.699937621857
Iteration 500: Loss = -11823.083598736697
Iteration 600: Loss = -11822.78405180544
Iteration 700: Loss = -11822.597640546692
Iteration 800: Loss = -11822.44745809632
Iteration 900: Loss = -11822.142485153985
Iteration 1000: Loss = -11814.55596352716
Iteration 1100: Loss = -11798.230830236582
Iteration 1200: Loss = -11763.84717804845
Iteration 1300: Loss = -11762.185367153299
Iteration 1400: Loss = -11761.785831092724
Iteration 1500: Loss = -11745.06239010176
Iteration 1600: Loss = -11745.017008805873
Iteration 1700: Loss = -11744.957769696575
Iteration 1800: Loss = -11734.892034976903
Iteration 1900: Loss = -11696.346164731363
Iteration 2000: Loss = -11682.121346135345
Iteration 2100: Loss = -11662.922364077744
Iteration 2200: Loss = -11662.88041897144
Iteration 2300: Loss = -11646.53348665488
Iteration 2400: Loss = -11646.515565200005
Iteration 2500: Loss = -11646.504935935622
Iteration 2600: Loss = -11646.496805656538
Iteration 2700: Loss = -11646.490432450926
Iteration 2800: Loss = -11646.485076969755
Iteration 2900: Loss = -11646.480589708113
Iteration 3000: Loss = -11646.477632181311
Iteration 3100: Loss = -11646.476545298665
Iteration 3200: Loss = -11646.470038060901
Iteration 3300: Loss = -11646.467202356254
Iteration 3400: Loss = -11646.464281037706
Iteration 3500: Loss = -11646.461691112607
Iteration 3600: Loss = -11646.460180544373
Iteration 3700: Loss = -11646.457370012122
Iteration 3800: Loss = -11646.456281394092
Iteration 3900: Loss = -11646.452791010432
Iteration 4000: Loss = -11646.453064716656
1
Iteration 4100: Loss = -11646.427092625985
Iteration 4200: Loss = -11646.416811514555
Iteration 4300: Loss = -11646.413117817268
Iteration 4400: Loss = -11646.40912171065
Iteration 4500: Loss = -11646.407467377798
Iteration 4600: Loss = -11646.408966141948
1
Iteration 4700: Loss = -11646.405182069331
Iteration 4800: Loss = -11646.403861382665
Iteration 4900: Loss = -11646.403653474803
Iteration 5000: Loss = -11646.401967493657
Iteration 5100: Loss = -11646.405192869546
1
Iteration 5200: Loss = -11646.400617229194
Iteration 5300: Loss = -11646.400157474916
Iteration 5400: Loss = -11646.405373107198
1
Iteration 5500: Loss = -11646.39931254277
Iteration 5600: Loss = -11646.398964449721
Iteration 5700: Loss = -11646.39859739627
Iteration 5800: Loss = -11646.398502502796
Iteration 5900: Loss = -11646.397966857217
Iteration 6000: Loss = -11646.401943213194
1
Iteration 6100: Loss = -11646.397394290148
Iteration 6200: Loss = -11646.40660936441
1
Iteration 6300: Loss = -11646.396924734849
Iteration 6400: Loss = -11646.39667118183
Iteration 6500: Loss = -11646.409750159568
1
Iteration 6600: Loss = -11646.399339469335
2
Iteration 6700: Loss = -11646.396532523297
Iteration 6800: Loss = -11646.396311968801
Iteration 6900: Loss = -11646.396230134013
Iteration 7000: Loss = -11646.399384918945
1
Iteration 7100: Loss = -11646.395581528514
Iteration 7200: Loss = -11646.402680581488
1
Iteration 7300: Loss = -11646.397550156911
2
Iteration 7400: Loss = -11646.39535453708
Iteration 7500: Loss = -11646.39529041682
Iteration 7600: Loss = -11646.396702293809
1
Iteration 7700: Loss = -11646.43103709159
2
Iteration 7800: Loss = -11646.39468814325
Iteration 7900: Loss = -11646.395537535409
1
Iteration 8000: Loss = -11646.39449508888
Iteration 8100: Loss = -11646.398299926468
1
Iteration 8200: Loss = -11646.39438002187
Iteration 8300: Loss = -11646.39908220993
1
Iteration 8400: Loss = -11646.394211770232
Iteration 8500: Loss = -11646.394810950047
1
Iteration 8600: Loss = -11646.3941270293
Iteration 8700: Loss = -11646.395085726137
1
Iteration 8800: Loss = -11646.394024509003
Iteration 8900: Loss = -11646.39468905645
1
Iteration 9000: Loss = -11646.394016941065
Iteration 9100: Loss = -11646.395128732183
1
Iteration 9200: Loss = -11646.393833052169
Iteration 9300: Loss = -11646.397455693354
1
Iteration 9400: Loss = -11646.393747763683
Iteration 9500: Loss = -11646.394201662526
1
Iteration 9600: Loss = -11646.408151971755
2
Iteration 9700: Loss = -11646.393605166666
Iteration 9800: Loss = -11646.40748436122
1
Iteration 9900: Loss = -11646.393576239083
Iteration 10000: Loss = -11646.412381226957
1
Iteration 10100: Loss = -11646.400451765174
2
Iteration 10200: Loss = -11646.428557019632
3
Iteration 10300: Loss = -11646.393372519346
Iteration 10400: Loss = -11646.395379375532
1
Iteration 10500: Loss = -11646.412761017176
2
Iteration 10600: Loss = -11646.508046381912
3
Iteration 10700: Loss = -11646.393370052858
Iteration 10800: Loss = -11646.39451092091
1
Iteration 10900: Loss = -11646.393309114646
Iteration 11000: Loss = -11646.40050449156
1
Iteration 11100: Loss = -11646.409395756242
2
Iteration 11200: Loss = -11646.398512636972
3
Iteration 11300: Loss = -11646.39576261298
4
Iteration 11400: Loss = -11646.405722075267
5
Iteration 11500: Loss = -11646.397736236737
6
Iteration 11600: Loss = -11646.3955413816
7
Iteration 11700: Loss = -11646.39378127917
8
Iteration 11800: Loss = -11646.423341420208
9
Iteration 11900: Loss = -11646.39525081811
10
Iteration 12000: Loss = -11646.48137370562
11
Iteration 12100: Loss = -11646.395371518754
12
Iteration 12200: Loss = -11646.431618096673
13
Iteration 12300: Loss = -11646.423974382713
14
Iteration 12400: Loss = -11646.399366303769
15
Stopping early at iteration 12400 due to no improvement.
pi: tensor([[0.7946, 0.2054],
        [0.2729, 0.7271]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5002, 0.4998], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3922, 0.1004],
         [0.6093, 0.1930]],

        [[0.6463, 0.0997],
         [0.6607, 0.6912]],

        [[0.5378, 0.1027],
         [0.7228, 0.5873]],

        [[0.6013, 0.0920],
         [0.5060, 0.7249]],

        [[0.7186, 0.0920],
         [0.6141, 0.6384]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11651.939308210178
[1.0, 1.0] [1.0, 1.0] [11646.409448158269, 11646.399366303769]
-------------------------------------
This iteration is 46
True Objective function: Loss = -11482.738136335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20767.09599555594
Iteration 100: Loss = -12278.645754744419
Iteration 200: Loss = -12275.517973637352
Iteration 300: Loss = -12091.437439801719
Iteration 400: Loss = -11592.687504919537
Iteration 500: Loss = -11496.219028596368
Iteration 600: Loss = -11478.882871972806
Iteration 700: Loss = -11473.160675646903
Iteration 800: Loss = -11473.02323035662
Iteration 900: Loss = -11472.938123528025
Iteration 1000: Loss = -11472.880008060027
Iteration 1100: Loss = -11472.838524699899
Iteration 1200: Loss = -11472.805947595236
Iteration 1300: Loss = -11472.781054658084
Iteration 1400: Loss = -11472.762291179628
Iteration 1500: Loss = -11472.745057357555
Iteration 1600: Loss = -11472.731593444585
Iteration 1700: Loss = -11472.720395010198
Iteration 1800: Loss = -11472.71090179302
Iteration 1900: Loss = -11472.702765317033
Iteration 2000: Loss = -11472.695843341407
Iteration 2100: Loss = -11472.689947057881
Iteration 2200: Loss = -11472.684470744141
Iteration 2300: Loss = -11472.679817599925
Iteration 2400: Loss = -11472.675706075885
Iteration 2500: Loss = -11472.67200790647
Iteration 2600: Loss = -11472.66873316284
Iteration 2700: Loss = -11472.679895189629
1
Iteration 2800: Loss = -11472.6630863092
Iteration 2900: Loss = -11472.660912417774
Iteration 3000: Loss = -11472.658719620358
Iteration 3100: Loss = -11472.656362735674
Iteration 3200: Loss = -11472.65446396456
Iteration 3300: Loss = -11472.652804760692
Iteration 3400: Loss = -11472.651072546934
Iteration 3500: Loss = -11472.650754001832
Iteration 3600: Loss = -11472.648350569498
Iteration 3700: Loss = -11472.647506071033
Iteration 3800: Loss = -11472.65280110113
1
Iteration 3900: Loss = -11472.644835978068
Iteration 4000: Loss = -11472.64498988213
1
Iteration 4100: Loss = -11472.642841803401
Iteration 4200: Loss = -11472.643895957048
1
Iteration 4300: Loss = -11472.641359409588
Iteration 4400: Loss = -11472.644113682054
1
Iteration 4500: Loss = -11472.639999820167
Iteration 4600: Loss = -11472.639405975338
Iteration 4700: Loss = -11472.638647580714
Iteration 4800: Loss = -11472.638289680282
Iteration 4900: Loss = -11472.637768193299
Iteration 5000: Loss = -11472.63723705437
Iteration 5100: Loss = -11472.637040698897
Iteration 5200: Loss = -11472.638152906533
1
Iteration 5300: Loss = -11472.636116421498
Iteration 5400: Loss = -11472.635722863844
Iteration 5500: Loss = -11472.642065193102
1
Iteration 5600: Loss = -11472.634913268512
Iteration 5700: Loss = -11472.635674272407
1
Iteration 5800: Loss = -11472.634289376669
Iteration 5900: Loss = -11472.635629651943
1
Iteration 6000: Loss = -11472.633753810758
Iteration 6100: Loss = -11472.633710571083
Iteration 6200: Loss = -11472.633482029683
Iteration 6300: Loss = -11472.633113296342
Iteration 6400: Loss = -11472.634354433576
1
Iteration 6500: Loss = -11472.637842129323
2
Iteration 6600: Loss = -11472.632586132187
Iteration 6700: Loss = -11472.632443272512
Iteration 6800: Loss = -11472.632802299346
1
Iteration 6900: Loss = -11472.631987525647
Iteration 7000: Loss = -11472.669247367838
1
Iteration 7100: Loss = -11472.631678387017
Iteration 7200: Loss = -11472.632759868384
1
Iteration 7300: Loss = -11472.631453485415
Iteration 7400: Loss = -11472.647005358058
1
Iteration 7500: Loss = -11472.63119119482
Iteration 7600: Loss = -11472.631147488832
Iteration 7700: Loss = -11472.632294664932
1
Iteration 7800: Loss = -11472.630986036585
Iteration 7900: Loss = -11472.630902487783
Iteration 8000: Loss = -11472.750727060786
1
Iteration 8100: Loss = -11472.630764033882
Iteration 8200: Loss = -11472.630703340734
Iteration 8300: Loss = -11472.649432434371
1
Iteration 8400: Loss = -11472.635363552517
2
Iteration 8500: Loss = -11472.630523810978
Iteration 8600: Loss = -11472.630726277164
1
Iteration 8700: Loss = -11472.630459805505
Iteration 8800: Loss = -11472.630429214369
Iteration 8900: Loss = -11472.630352197692
Iteration 9000: Loss = -11472.630368715432
Iteration 9100: Loss = -11472.630231278476
Iteration 9200: Loss = -11472.63036774268
1
Iteration 9300: Loss = -11472.64967552811
2
Iteration 9400: Loss = -11472.65468744736
3
Iteration 9500: Loss = -11472.633765693385
4
Iteration 9600: Loss = -11472.631670428751
5
Iteration 9700: Loss = -11472.631708559627
6
Iteration 9800: Loss = -11472.630632398206
7
Iteration 9900: Loss = -11472.630512576148
8
Iteration 10000: Loss = -11472.630266284461
Iteration 10100: Loss = -11472.63130606995
1
Iteration 10200: Loss = -11472.632996156433
2
Iteration 10300: Loss = -11472.631358099983
3
Iteration 10400: Loss = -11472.633340479488
4
Iteration 10500: Loss = -11472.630400667891
5
Iteration 10600: Loss = -11472.629944334332
Iteration 10700: Loss = -11472.630288590797
1
Iteration 10800: Loss = -11472.631921854756
2
Iteration 10900: Loss = -11472.638581396248
3
Iteration 11000: Loss = -11472.629828546887
Iteration 11100: Loss = -11472.630989578096
1
Iteration 11200: Loss = -11472.63340114595
2
Iteration 11300: Loss = -11472.634858726704
3
Iteration 11400: Loss = -11472.635751364842
4
Iteration 11500: Loss = -11472.638194677
5
Iteration 11600: Loss = -11472.631628081324
6
Iteration 11700: Loss = -11472.67258237697
7
Iteration 11800: Loss = -11472.632119031861
8
Iteration 11900: Loss = -11472.632253184343
9
Iteration 12000: Loss = -11472.63360781462
10
Iteration 12100: Loss = -11472.631678099935
11
Iteration 12200: Loss = -11472.632213307306
12
Iteration 12300: Loss = -11472.630224518422
13
Iteration 12400: Loss = -11472.63276274758
14
Iteration 12500: Loss = -11472.629716136793
Iteration 12600: Loss = -11472.629392080422
Iteration 12700: Loss = -11472.630410561193
1
Iteration 12800: Loss = -11472.632903759973
2
Iteration 12900: Loss = -11472.761855713678
3
Iteration 13000: Loss = -11472.629554857387
4
Iteration 13100: Loss = -11472.629438735396
Iteration 13200: Loss = -11472.629609422382
1
Iteration 13300: Loss = -11472.630557629911
2
Iteration 13400: Loss = -11472.629500147246
Iteration 13500: Loss = -11472.629452369696
Iteration 13600: Loss = -11472.63199365197
1
Iteration 13700: Loss = -11472.632639628193
2
Iteration 13800: Loss = -11472.62968177098
3
Iteration 13900: Loss = -11472.635419645403
4
Iteration 14000: Loss = -11472.685723846394
5
Iteration 14100: Loss = -11472.629311540775
Iteration 14200: Loss = -11472.630215864026
1
Iteration 14300: Loss = -11472.641818524167
2
Iteration 14400: Loss = -11472.629272813572
Iteration 14500: Loss = -11472.685967441372
1
Iteration 14600: Loss = -11472.635234197953
2
Iteration 14700: Loss = -11472.640636722872
3
Iteration 14800: Loss = -11472.678769533133
4
Iteration 14900: Loss = -11472.644493522852
5
Iteration 15000: Loss = -11472.736388531386
6
Iteration 15100: Loss = -11472.701808234757
7
Iteration 15200: Loss = -11472.647489768513
8
Iteration 15300: Loss = -11472.642332643376
9
Iteration 15400: Loss = -11472.651338120997
10
Iteration 15500: Loss = -11472.726612663393
11
Iteration 15600: Loss = -11472.64777045146
12
Iteration 15700: Loss = -11472.626317809487
Iteration 15800: Loss = -11472.627967182465
1
Iteration 15900: Loss = -11472.626947007722
2
Iteration 16000: Loss = -11472.62679428497
3
Iteration 16100: Loss = -11472.628685135756
4
Iteration 16200: Loss = -11472.627270346364
5
Iteration 16300: Loss = -11472.630673760625
6
Iteration 16400: Loss = -11472.628681619257
7
Iteration 16500: Loss = -11472.637706465855
8
Iteration 16600: Loss = -11472.627937205487
9
Iteration 16700: Loss = -11472.625962691249
Iteration 16800: Loss = -11472.628120750082
1
Iteration 16900: Loss = -11472.629479875708
2
Iteration 17000: Loss = -11472.628267487327
3
Iteration 17100: Loss = -11472.625785038033
Iteration 17200: Loss = -11472.62625689987
1
Iteration 17300: Loss = -11472.627453838
2
Iteration 17400: Loss = -11472.627031718084
3
Iteration 17500: Loss = -11472.66641681759
4
Iteration 17600: Loss = -11472.626143926516
5
Iteration 17700: Loss = -11472.625936750117
6
Iteration 17800: Loss = -11472.626409928029
7
Iteration 17900: Loss = -11472.625795963415
Iteration 18000: Loss = -11472.629458639605
1
Iteration 18100: Loss = -11472.679861638038
2
Iteration 18200: Loss = -11472.71364869807
3
Iteration 18300: Loss = -11472.626269424693
4
Iteration 18400: Loss = -11472.627057153793
5
Iteration 18500: Loss = -11472.626638927693
6
Iteration 18600: Loss = -11472.683626128073
7
Iteration 18700: Loss = -11472.625979971393
8
Iteration 18800: Loss = -11472.632912694336
9
Iteration 18900: Loss = -11472.631438769738
10
Iteration 19000: Loss = -11472.626812162942
11
Iteration 19100: Loss = -11472.625799157951
Iteration 19200: Loss = -11472.630856046737
1
Iteration 19300: Loss = -11472.807940579078
2
Iteration 19400: Loss = -11472.625818569937
Iteration 19500: Loss = -11472.653352598178
1
Iteration 19600: Loss = -11472.660235857482
2
Iteration 19700: Loss = -11472.636450743521
3
Iteration 19800: Loss = -11472.636633092014
4
Iteration 19900: Loss = -11472.627263095577
5
pi: tensor([[0.7739, 0.2261],
        [0.2511, 0.7489]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5483, 0.4517], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1966, 0.0995],
         [0.6522, 0.4188]],

        [[0.7126, 0.1103],
         [0.7155, 0.7188]],

        [[0.6774, 0.1066],
         [0.5036, 0.6737]],

        [[0.5791, 0.0939],
         [0.6774, 0.6273]],

        [[0.5848, 0.1018],
         [0.6131, 0.6922]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840319537411472
Average Adjusted Rand Index: 0.984
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20350.633938708455
Iteration 100: Loss = -12276.968967275425
Iteration 200: Loss = -12268.358263277845
Iteration 300: Loss = -11664.8390847109
Iteration 400: Loss = -11497.246196402497
 47%|████▋     | 47/100 [13:01:39<14:54:45, 1012.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 48%|████▊     | 48/100 [13:14:46<13:39:04, 945.08s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 49%|████▉     | 49/100 [13:26:47<12:26:00, 877.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 50%|█████     | 50/100 [13:48:23<13:56:02, 1003.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 51%|█████     | 51/100 [14:00:26<12:30:40, 919.20s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 52%|█████▏    | 52/100 [14:17:57<12:46:58, 958.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 53%|█████▎    | 53/100 [14:36:35<13:08:21, 1006.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 54%|█████▍    | 54/100 [14:50:45<12:15:38, 959.52s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 55%|█████▌    | 55/100 [15:07:11<12:05:39, 967.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 56%|█████▌    | 56/100 [15:24:08<12:00:25, 982.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 57%|█████▋    | 57/100 [15:45:32<12:48:57, 1072.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 58%|█████▊    | 58/100 [15:59:37<11:43:07, 1004.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 59%|█████▉    | 59/100 [16:16:21<11:26:13, 1004.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 60%|██████    | 60/100 [16:29:55<10:31:31, 947.28s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 61%|██████    | 61/100 [16:45:04<10:08:14, 935.75s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 62%|██████▏   | 62/100 [16:58:14<9:25:00, 892.12s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 63%|██████▎   | 63/100 [17:14:52<9:29:42, 923.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 64%|██████▍   | 64/100 [17:29:39<9:07:36, 912.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 65%|██████▌   | 65/100 [17:49:22<9:39:49, 993.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 66%|██████▌   | 66/100 [18:06:39<9:30:25, 1006.63s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 67%|██████▋   | 67/100 [18:21:09<8:51:07, 965.68s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 68%|██████▊   | 68/100 [18:32:02<7:45:03, 871.98s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 69%|██████▉   | 69/100 [18:49:37<7:58:51, 926.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 70%|███████   | 70/100 [19:07:04<8:01:32, 963.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 71%|███████   | 71/100 [19:25:41<8:07:47, 1009.21s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 72%|███████▏  | 72/100 [19:44:40<8:09:07, 1048.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 73%|███████▎  | 73/100 [19:57:26<7:13:31, 963.41s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 74%|███████▍  | 74/100 [20:12:08<6:46:53, 938.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 75%|███████▌  | 75/100 [20:28:27<6:36:11, 950.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 76%|███████▌  | 76/100 [20:46:27<6:35:50, 989.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 77%|███████▋  | 77/100 [21:02:21<6:15:19, 979.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 78%|███████▊  | 78/100 [21:15:21<5:37:08, 919.47s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 79%|███████▉  | 79/100 [21:32:25<5:32:42, 950.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 80%|████████  | 80/100 [21:44:53<4:56:38, 889.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 81%|████████  | 81/100 [21:57:57<4:31:44, 858.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 82%|████████▏ | 82/100 [22:14:44<4:30:52, 902.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 83%|████████▎ | 83/100 [22:27:41<4:05:06, 865.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 84%|████████▍ | 84/100 [22:43:19<3:56:31, 886.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 85%|████████▌ | 85/100 [22:58:34<3:43:49, 895.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 86%|████████▌ | 86/100 [23:16:53<3:43:09, 956.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 87%|████████▋ | 87/100 [23:30:03<3:16:23, 906.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 88%|████████▊ | 88/100 [23:44:12<2:57:50, 889.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 89%|████████▉ | 89/100 [23:57:30<2:37:59, 861.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 90%|█████████ | 90/100 [24:14:51<2:32:35, 915.57s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 91%|█████████ | 91/100 [24:33:51<2:27:26, 982.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 92%|█████████▏| 92/100 [24:51:29<2:14:02, 1005.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 93%|█████████▎| 93/100 [25:05:49<1:52:13, 961.99s/it] /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 94%|█████████▍| 94/100 [25:18:35<1:30:17, 902.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 500: Loss = -11491.578683882888
Iteration 600: Loss = -11481.564327743188
Iteration 700: Loss = -11481.375078988443
Iteration 800: Loss = -11473.214093536579
Iteration 900: Loss = -11473.105029652797
Iteration 1000: Loss = -11473.05358880775
Iteration 1100: Loss = -11473.015843860361
Iteration 1200: Loss = -11472.98711774282
Iteration 1300: Loss = -11472.964604021765
Iteration 1400: Loss = -11472.946552404457
Iteration 1500: Loss = -11472.93190499151
Iteration 1600: Loss = -11472.919715382008
Iteration 1700: Loss = -11472.909508019691
Iteration 1800: Loss = -11472.900854093506
Iteration 1900: Loss = -11472.89341669967
Iteration 2000: Loss = -11472.886987863205
Iteration 2100: Loss = -11472.881381502426
Iteration 2200: Loss = -11472.87639708284
Iteration 2300: Loss = -11472.871978765741
Iteration 2400: Loss = -11472.867984958513
Iteration 2500: Loss = -11472.864331289778
Iteration 2600: Loss = -11472.861018302143
Iteration 2700: Loss = -11472.858288726684
Iteration 2800: Loss = -11472.855587147767
Iteration 2900: Loss = -11472.853253062818
Iteration 3000: Loss = -11472.851106228434
Iteration 3100: Loss = -11472.849165436946
Iteration 3200: Loss = -11472.847667543923
Iteration 3300: Loss = -11472.845913390956
Iteration 3400: Loss = -11472.84456583699
Iteration 3500: Loss = -11472.848435470338
1
Iteration 3600: Loss = -11472.842069469465
Iteration 3700: Loss = -11472.841891861251
Iteration 3800: Loss = -11472.846309943312
1
Iteration 3900: Loss = -11472.839480652347
Iteration 4000: Loss = -11472.838530957615
Iteration 4100: Loss = -11472.837671100655
Iteration 4200: Loss = -11472.83923266059
1
Iteration 4300: Loss = -11472.836027068997
Iteration 4400: Loss = -11472.835817189605
Iteration 4500: Loss = -11472.834835874515
Iteration 4600: Loss = -11472.83428684571
Iteration 4700: Loss = -11472.840344822027
1
Iteration 4800: Loss = -11472.833270202791
Iteration 4900: Loss = -11472.834831678008
1
Iteration 5000: Loss = -11472.839409197639
2
Iteration 5100: Loss = -11472.832057063886
Iteration 5200: Loss = -11472.831661318465
Iteration 5300: Loss = -11472.834308655216
1
Iteration 5400: Loss = -11472.852090229984
2
Iteration 5500: Loss = -11472.84067849974
3
Iteration 5600: Loss = -11472.830441536891
Iteration 5700: Loss = -11472.831498045294
1
Iteration 5800: Loss = -11472.829956367941
Iteration 5900: Loss = -11472.829740172745
Iteration 6000: Loss = -11472.829566042048
Iteration 6100: Loss = -11472.829787705621
1
Iteration 6200: Loss = -11472.831086086915
2
Iteration 6300: Loss = -11472.8290016236
Iteration 6400: Loss = -11472.828916441656
Iteration 6500: Loss = -11472.829047989677
1
Iteration 6600: Loss = -11472.828389452767
Iteration 6700: Loss = -11472.828200848564
Iteration 6800: Loss = -11472.829122682966
1
Iteration 6900: Loss = -11472.827879827777
Iteration 7000: Loss = -11472.856210589902
1
Iteration 7100: Loss = -11472.845653127048
2
Iteration 7200: Loss = -11472.826903283409
Iteration 7300: Loss = -11472.6375356739
Iteration 7400: Loss = -11472.65220398628
1
Iteration 7500: Loss = -11472.632202702753
Iteration 7600: Loss = -11472.641014997802
1
Iteration 7700: Loss = -11472.632065252494
Iteration 7800: Loss = -11472.631993936917
Iteration 7900: Loss = -11472.632144198764
1
Iteration 8000: Loss = -11472.631857252454
Iteration 8100: Loss = -11472.634315907617
1
Iteration 8200: Loss = -11472.63172151974
Iteration 8300: Loss = -11472.63165450967
Iteration 8400: Loss = -11472.632247749361
1
Iteration 8500: Loss = -11472.631542475134
Iteration 8600: Loss = -11472.631523874996
Iteration 8700: Loss = -11472.63162112773
Iteration 8800: Loss = -11472.63139345171
Iteration 8900: Loss = -11472.63269857408
1
Iteration 9000: Loss = -11472.631709420286
2
Iteration 9100: Loss = -11472.65604927285
3
Iteration 9200: Loss = -11472.631261738223
Iteration 9300: Loss = -11472.631310574863
Iteration 9400: Loss = -11472.635159372552
1
Iteration 9500: Loss = -11472.63173948067
2
Iteration 9600: Loss = -11472.631701372096
3
Iteration 9700: Loss = -11472.68825820055
4
Iteration 9800: Loss = -11472.636087891957
5
Iteration 9900: Loss = -11472.632142466695
6
Iteration 10000: Loss = -11472.637336227304
7
Iteration 10100: Loss = -11472.632327106268
8
Iteration 10200: Loss = -11472.63158334953
9
Iteration 10300: Loss = -11472.630192193938
Iteration 10400: Loss = -11472.632341836848
1
Iteration 10500: Loss = -11472.641734899853
2
Iteration 10600: Loss = -11472.641086006388
3
Iteration 10700: Loss = -11472.722229692978
4
Iteration 10800: Loss = -11472.645507230141
5
Iteration 10900: Loss = -11472.634661559106
6
Iteration 11000: Loss = -11472.633336429504
7
Iteration 11100: Loss = -11472.631582412232
8
Iteration 11200: Loss = -11472.636711405472
9
Iteration 11300: Loss = -11472.632251552774
10
Iteration 11400: Loss = -11472.636353160353
11
Iteration 11500: Loss = -11472.63018738315
Iteration 11600: Loss = -11472.630295295507
1
Iteration 11700: Loss = -11472.632106085734
2
Iteration 11800: Loss = -11472.651959847022
3
Iteration 11900: Loss = -11472.63313095568
4
Iteration 12000: Loss = -11472.630015983916
Iteration 12100: Loss = -11472.630807898437
1
Iteration 12200: Loss = -11472.63573674926
2
Iteration 12300: Loss = -11472.642608051781
3
Iteration 12400: Loss = -11472.63741631764
4
Iteration 12500: Loss = -11472.637161879
5
Iteration 12600: Loss = -11472.630365376115
6
Iteration 12700: Loss = -11472.652918502383
7
Iteration 12800: Loss = -11472.635518995663
8
Iteration 12900: Loss = -11472.63052426206
9
Iteration 13000: Loss = -11472.630618339119
10
Iteration 13100: Loss = -11472.632641202486
11
Iteration 13200: Loss = -11472.635358659974
12
Iteration 13300: Loss = -11472.629877816924
Iteration 13400: Loss = -11472.632305140545
1
Iteration 13500: Loss = -11472.631861766224
2
Iteration 13600: Loss = -11472.642539903742
3
Iteration 13700: Loss = -11472.631157094413
4
Iteration 13800: Loss = -11472.633056744198
5
Iteration 13900: Loss = -11472.634364203223
6
Iteration 14000: Loss = -11472.628844954634
Iteration 14100: Loss = -11472.626622348287
Iteration 14200: Loss = -11472.62793300889
1
Iteration 14300: Loss = -11472.701581490706
2
Iteration 14400: Loss = -11472.648405433303
3
Iteration 14500: Loss = -11472.650321735122
4
Iteration 14600: Loss = -11472.643828011238
5
Iteration 14700: Loss = -11472.646961639723
6
Iteration 14800: Loss = -11472.646296731902
7
Iteration 14900: Loss = -11472.62765430072
8
Iteration 15000: Loss = -11472.626667120277
Iteration 15100: Loss = -11472.629794543285
1
Iteration 15200: Loss = -11472.680896471864
2
Iteration 15300: Loss = -11472.653235759135
3
Iteration 15400: Loss = -11472.728121034253
4
Iteration 15500: Loss = -11472.634370744043
5
Iteration 15600: Loss = -11472.745028621093
6
Iteration 15700: Loss = -11472.628261379155
7
Iteration 15800: Loss = -11472.627176895408
8
Iteration 15900: Loss = -11472.626941014676
9
Iteration 16000: Loss = -11472.626804597345
10
Iteration 16100: Loss = -11472.630180890143
11
Iteration 16200: Loss = -11472.730328662361
12
Iteration 16300: Loss = -11472.62836415015
13
Iteration 16400: Loss = -11472.626239925317
Iteration 16500: Loss = -11472.630638576533
1
Iteration 16600: Loss = -11472.638049966878
2
Iteration 16700: Loss = -11472.667957705886
3
Iteration 16800: Loss = -11472.645868188689
4
Iteration 16900: Loss = -11472.666884780683
5
Iteration 17000: Loss = -11472.63759516087
6
Iteration 17100: Loss = -11472.652346424344
7
Iteration 17200: Loss = -11472.628333346123
8
Iteration 17300: Loss = -11472.650611302917
9
Iteration 17400: Loss = -11472.653060333576
10
Iteration 17500: Loss = -11472.62722305111
11
Iteration 17600: Loss = -11472.629508584834
12
Iteration 17700: Loss = -11472.627828380768
13
Iteration 17800: Loss = -11472.62639217637
14
Iteration 17900: Loss = -11472.629565331083
15
Stopping early at iteration 17900 due to no improvement.
pi: tensor([[0.7490, 0.2510],
        [0.2260, 0.7740]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4513, 0.5487], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4188, 0.0995],
         [0.5332, 0.1966]],

        [[0.6774, 0.1097],
         [0.7274, 0.7173]],

        [[0.7130, 0.1065],
         [0.6921, 0.5717]],

        [[0.5795, 0.0939],
         [0.6369, 0.5055]],

        [[0.5430, 0.1011],
         [0.6965, 0.6529]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840319537411472
Average Adjusted Rand Index: 0.984
11482.738136335178
[0.9840319537411472, 0.9840319537411472] [0.984, 0.984] [11472.632941204254, 11472.629565331083]
-------------------------------------
This iteration is 47
True Objective function: Loss = -11462.113608383312
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22563.82460296281
Iteration 100: Loss = -12318.701275204023
Iteration 200: Loss = -12178.029150537932
Iteration 300: Loss = -12114.253263331253
Iteration 400: Loss = -11570.675484826223
Iteration 500: Loss = -11456.281824775897
Iteration 600: Loss = -11455.82032472779
Iteration 700: Loss = -11455.609550483567
Iteration 800: Loss = -11455.500898045284
Iteration 900: Loss = -11455.431274828396
Iteration 1000: Loss = -11455.382873834513
Iteration 1100: Loss = -11455.347354841262
Iteration 1200: Loss = -11455.320100253357
Iteration 1300: Loss = -11455.29835314598
Iteration 1400: Loss = -11455.276115717059
Iteration 1500: Loss = -11449.874770717746
Iteration 1600: Loss = -11449.861843675119
Iteration 1700: Loss = -11449.851921833231
Iteration 1800: Loss = -11449.8436136442
Iteration 1900: Loss = -11449.836484005215
Iteration 2000: Loss = -11449.830383105573
Iteration 2100: Loss = -11449.825040004336
Iteration 2200: Loss = -11449.82038405183
Iteration 2300: Loss = -11449.816272214766
Iteration 2400: Loss = -11449.812594915491
Iteration 2500: Loss = -11449.809381222789
Iteration 2600: Loss = -11449.806339494404
Iteration 2700: Loss = -11449.803464457402
Iteration 2800: Loss = -11449.797786830602
Iteration 2900: Loss = -11449.431487589922
Iteration 3000: Loss = -11449.429570323022
Iteration 3100: Loss = -11449.42779022055
Iteration 3200: Loss = -11449.426222423253
Iteration 3300: Loss = -11449.424747436848
Iteration 3400: Loss = -11449.423423560771
Iteration 3500: Loss = -11449.42231020977
Iteration 3600: Loss = -11449.421062570993
Iteration 3700: Loss = -11449.419998465011
Iteration 3800: Loss = -11449.419011571115
Iteration 3900: Loss = -11449.41821359394
Iteration 4000: Loss = -11449.417346082926
Iteration 4100: Loss = -11449.417097918691
Iteration 4200: Loss = -11449.418439157065
1
Iteration 4300: Loss = -11449.415265252876
Iteration 4400: Loss = -11449.41462485457
Iteration 4500: Loss = -11449.414065787123
Iteration 4600: Loss = -11449.413553884508
Iteration 4700: Loss = -11449.413249643938
Iteration 4800: Loss = -11449.413189896815
Iteration 4900: Loss = -11449.41216499333
Iteration 5000: Loss = -11449.411756171627
Iteration 5100: Loss = -11449.411336217034
Iteration 5200: Loss = -11449.410908444288
Iteration 5300: Loss = -11449.41059003905
Iteration 5400: Loss = -11449.410147877956
Iteration 5500: Loss = -11449.413747512248
1
Iteration 5600: Loss = -11449.408218474924
Iteration 5700: Loss = -11449.413461367194
1
Iteration 5800: Loss = -11449.405423468175
Iteration 5900: Loss = -11449.40655142441
1
Iteration 6000: Loss = -11449.406601199875
2
Iteration 6100: Loss = -11449.403721128474
Iteration 6200: Loss = -11449.414430314646
1
Iteration 6300: Loss = -11449.403278317788
Iteration 6400: Loss = -11449.40368079162
1
Iteration 6500: Loss = -11449.40336341011
Iteration 6600: Loss = -11449.403736816495
1
Iteration 6700: Loss = -11449.402844864579
Iteration 6800: Loss = -11449.405900463844
1
Iteration 6900: Loss = -11449.402583065545
Iteration 7000: Loss = -11449.402317399346
Iteration 7100: Loss = -11449.40237827253
Iteration 7200: Loss = -11449.402683823731
1
Iteration 7300: Loss = -11449.403704940238
2
Iteration 7400: Loss = -11449.402224555455
Iteration 7500: Loss = -11449.403122787298
1
Iteration 7600: Loss = -11449.402234494448
Iteration 7700: Loss = -11449.402616137484
1
Iteration 7800: Loss = -11449.401592460796
Iteration 7900: Loss = -11449.401575747135
Iteration 8000: Loss = -11449.40156716454
Iteration 8100: Loss = -11449.401425063925
Iteration 8200: Loss = -11449.401392713527
Iteration 8300: Loss = -11449.432166095032
1
Iteration 8400: Loss = -11449.401520972147
2
Iteration 8500: Loss = -11449.417170628414
3
Iteration 8600: Loss = -11449.401142111225
Iteration 8700: Loss = -11449.411033694669
1
Iteration 8800: Loss = -11449.417512010106
2
Iteration 8900: Loss = -11449.402340552522
3
Iteration 9000: Loss = -11449.404455559446
4
Iteration 9100: Loss = -11449.408255095705
5
Iteration 9200: Loss = -11449.406243639676
6
Iteration 9300: Loss = -11449.40503720844
7
Iteration 9400: Loss = -11449.400860287926
Iteration 9500: Loss = -11449.402235899308
1
Iteration 9600: Loss = -11449.59460646537
2
Iteration 9700: Loss = -11449.400752196387
Iteration 9800: Loss = -11449.43921517786
1
Iteration 9900: Loss = -11449.400704581349
Iteration 10000: Loss = -11449.495686614102
1
Iteration 10100: Loss = -11449.402253045811
2
Iteration 10200: Loss = -11449.40316536036
3
Iteration 10300: Loss = -11449.407266079046
4
Iteration 10400: Loss = -11449.40155245105
5
Iteration 10500: Loss = -11449.405375601305
6
Iteration 10600: Loss = -11449.40979662227
7
Iteration 10700: Loss = -11449.400889133483
8
Iteration 10800: Loss = -11449.405290995324
9
Iteration 10900: Loss = -11449.407601339952
10
Iteration 11000: Loss = -11449.412422090638
11
Iteration 11100: Loss = -11449.402527588341
12
Iteration 11200: Loss = -11449.400700280481
Iteration 11300: Loss = -11449.402647249974
1
Iteration 11400: Loss = -11449.40070872414
Iteration 11500: Loss = -11449.401335095452
1
Iteration 11600: Loss = -11449.437529619625
2
Iteration 11700: Loss = -11449.442581375108
3
Iteration 11800: Loss = -11449.403116877986
4
Iteration 11900: Loss = -11449.415123279185
5
Iteration 12000: Loss = -11449.405113580922
6
Iteration 12100: Loss = -11449.417125673446
7
Iteration 12200: Loss = -11449.403576440867
8
Iteration 12300: Loss = -11449.468507119354
9
Iteration 12400: Loss = -11449.41916402453
10
Iteration 12500: Loss = -11449.413694633902
11
Iteration 12600: Loss = -11449.401009031277
12
Iteration 12700: Loss = -11449.40145949113
13
Iteration 12800: Loss = -11449.405670312723
14
Iteration 12900: Loss = -11449.408581717771
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.8318, 0.1682],
        [0.1941, 0.8059]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4821, 0.5179], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4027, 0.1008],
         [0.5352, 0.1999]],

        [[0.5663, 0.1020],
         [0.5304, 0.7287]],

        [[0.5667, 0.0873],
         [0.6483, 0.5288]],

        [[0.5021, 0.1010],
         [0.5149, 0.6030]],

        [[0.6629, 0.0972],
         [0.6837, 0.6645]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9760961002009502
Average Adjusted Rand Index: 0.9759993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21135.43319817219
Iteration 100: Loss = -12325.278589256817
Iteration 200: Loss = -11988.536943031551
Iteration 300: Loss = -11653.205653440267
Iteration 400: Loss = -11552.027112831853
Iteration 500: Loss = -11526.236264918653
Iteration 600: Loss = -11514.256177224428
Iteration 700: Loss = -11470.881980884114
Iteration 800: Loss = -11470.22178614011
Iteration 900: Loss = -11460.433815701468
Iteration 1000: Loss = -11460.341185693236
Iteration 1100: Loss = -11460.261864420534
Iteration 1200: Loss = -11452.673383914458
Iteration 1300: Loss = -11452.63651902731
Iteration 1400: Loss = -11452.604174838803
Iteration 1500: Loss = -11452.08201455913
Iteration 1600: Loss = -11449.512868698821
Iteration 1700: Loss = -11449.48074489204
Iteration 1800: Loss = -11449.461113886986
Iteration 1900: Loss = -11449.451602127203
Iteration 2000: Loss = -11449.443596147714
Iteration 2100: Loss = -11449.436711014361
Iteration 2200: Loss = -11449.430778546064
Iteration 2300: Loss = -11449.42556868684
Iteration 2400: Loss = -11449.420973745002
Iteration 2500: Loss = -11449.416923355326
Iteration 2600: Loss = -11449.413333490698
Iteration 2700: Loss = -11449.410074868088
Iteration 2800: Loss = -11449.407229636354
Iteration 2900: Loss = -11449.404570911627
Iteration 3000: Loss = -11449.402188299317
Iteration 3100: Loss = -11449.400024364324
Iteration 3200: Loss = -11449.398109364845
Iteration 3300: Loss = -11449.397462264562
Iteration 3400: Loss = -11449.394633225049
Iteration 3500: Loss = -11449.393124250528
Iteration 3600: Loss = -11449.395095406302
1
Iteration 3700: Loss = -11449.390503307568
Iteration 3800: Loss = -11449.389350465051
Iteration 3900: Loss = -11449.388354685318
Iteration 4000: Loss = -11449.387387181763
Iteration 4100: Loss = -11449.386457891671
Iteration 4200: Loss = -11449.386114437004
Iteration 4300: Loss = -11449.386172874
Iteration 4400: Loss = -11449.384147538967
Iteration 4500: Loss = -11449.38348430736
Iteration 4600: Loss = -11449.38287659459
Iteration 4700: Loss = -11449.382290098998
Iteration 4800: Loss = -11449.390061464437
1
Iteration 4900: Loss = -11449.381284995712
Iteration 5000: Loss = -11449.380801858257
Iteration 5100: Loss = -11449.383708254663
1
Iteration 5200: Loss = -11449.379953070675
Iteration 5300: Loss = -11449.393730125985
1
Iteration 5400: Loss = -11449.379275509904
Iteration 5500: Loss = -11449.378875210732
Iteration 5600: Loss = -11449.37959199436
1
Iteration 5700: Loss = -11449.378286679212
Iteration 5800: Loss = -11449.37803045127
Iteration 5900: Loss = -11449.377825097943
Iteration 6000: Loss = -11449.37752144472
Iteration 6100: Loss = -11449.384808706993
1
Iteration 6200: Loss = -11449.377099211752
Iteration 6300: Loss = -11449.381005214756
1
Iteration 6400: Loss = -11449.383004663761
2
Iteration 6500: Loss = -11449.37687975714
Iteration 6600: Loss = -11449.37633439975
Iteration 6700: Loss = -11449.3830679826
1
Iteration 6800: Loss = -11449.376059429016
Iteration 6900: Loss = -11449.379303278116
1
Iteration 7000: Loss = -11449.381486126074
2
Iteration 7100: Loss = -11449.375665726007
Iteration 7200: Loss = -11449.375529149305
Iteration 7300: Loss = -11449.375418296479
Iteration 7400: Loss = -11449.375576676539
1
Iteration 7500: Loss = -11449.376155323678
2
Iteration 7600: Loss = -11449.376649306185
3
Iteration 7700: Loss = -11449.374202908932
Iteration 7800: Loss = -11449.374154371762
Iteration 7900: Loss = -11449.374102387379
Iteration 8000: Loss = -11449.374923811745
1
Iteration 8100: Loss = -11449.379212391505
2
Iteration 8200: Loss = -11449.37385297859
Iteration 8300: Loss = -11449.38425702565
1
Iteration 8400: Loss = -11449.373671764446
Iteration 8500: Loss = -11449.37361896796
Iteration 8600: Loss = -11449.375706008666
1
Iteration 8700: Loss = -11449.373522700706
Iteration 8800: Loss = -11449.375309922025
1
Iteration 8900: Loss = -11449.376801262079
2
Iteration 9000: Loss = -11449.378507952495
3
Iteration 9100: Loss = -11449.373585094052
Iteration 9200: Loss = -11449.397995633164
1
Iteration 9300: Loss = -11449.379024847074
2
Iteration 9400: Loss = -11449.37603818627
3
Iteration 9500: Loss = -11449.375661502732
4
Iteration 9600: Loss = -11449.373547399027
Iteration 9700: Loss = -11449.373579985573
Iteration 9800: Loss = -11449.373280219936
Iteration 9900: Loss = -11449.39482871409
1
Iteration 10000: Loss = -11449.377734098456
2
Iteration 10100: Loss = -11449.373890240982
3
Iteration 10200: Loss = -11449.374004806077
4
Iteration 10300: Loss = -11449.373838317308
5
Iteration 10400: Loss = -11449.598320646426
6
Iteration 10500: Loss = -11449.374284003976
7
Iteration 10600: Loss = -11449.384449180188
8
Iteration 10700: Loss = -11449.420832295467
9
Iteration 10800: Loss = -11449.388074074159
10
Iteration 10900: Loss = -11449.39095458562
11
Iteration 11000: Loss = -11449.389541863624
12
Iteration 11100: Loss = -11449.377418864608
13
Iteration 11200: Loss = -11449.38254869474
14
Iteration 11300: Loss = -11449.42899508979
15
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[0.8068, 0.1932],
        [0.1665, 0.8335]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5213, 0.4787], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2002, 0.1009],
         [0.5053, 0.4007]],

        [[0.6045, 0.1017],
         [0.5176, 0.5727]],

        [[0.6642, 0.0868],
         [0.6420, 0.5481]],

        [[0.5900, 0.1011],
         [0.5632, 0.7023]],

        [[0.5580, 0.0965],
         [0.5296, 0.5972]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9760961002009502
Average Adjusted Rand Index: 0.9759993417272899
11462.113608383312
[0.9760961002009502, 0.9760961002009502] [0.9759993417272899, 0.9759993417272899] [11449.408581717771, 11449.42899508979]
-------------------------------------
This iteration is 48
True Objective function: Loss = -11816.202754323027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20543.305158275838
Iteration 100: Loss = -12651.678649445988
Iteration 200: Loss = -12323.814669922956
Iteration 300: Loss = -11912.869398511299
Iteration 400: Loss = -11845.16630232748
Iteration 500: Loss = -11827.551506701697
Iteration 600: Loss = -11816.780973775962
Iteration 700: Loss = -11816.636203357391
Iteration 800: Loss = -11816.552512531594
Iteration 900: Loss = -11816.49546905391
Iteration 1000: Loss = -11816.447553633438
Iteration 1100: Loss = -11808.335069288576
Iteration 1200: Loss = -11802.593605144442
Iteration 1300: Loss = -11802.479355895548
Iteration 1400: Loss = -11802.463061129189
Iteration 1500: Loss = -11802.44996759174
Iteration 1600: Loss = -11802.43905417212
Iteration 1700: Loss = -11802.4295116153
Iteration 1800: Loss = -11802.41929251363
Iteration 1900: Loss = -11802.397110399648
Iteration 2000: Loss = -11802.389903873096
Iteration 2100: Loss = -11802.384757216905
Iteration 2200: Loss = -11802.391779571988
1
Iteration 2300: Loss = -11802.37525176652
Iteration 2400: Loss = -11802.365670408413
Iteration 2500: Loss = -11802.358759145454
Iteration 2600: Loss = -11802.356056522445
Iteration 2700: Loss = -11802.353681552466
Iteration 2800: Loss = -11802.351584243435
Iteration 2900: Loss = -11802.349689421048
Iteration 3000: Loss = -11802.34794721575
Iteration 3100: Loss = -11802.346367018174
Iteration 3200: Loss = -11802.345159767629
Iteration 3300: Loss = -11802.343578269669
Iteration 3400: Loss = -11802.34332055805
Iteration 3500: Loss = -11802.343682551102
1
Iteration 3600: Loss = -11802.249508764333
Iteration 3700: Loss = -11802.248480026183
Iteration 3800: Loss = -11802.256868688948
1
Iteration 3900: Loss = -11802.246801530695
Iteration 4000: Loss = -11802.246091899886
Iteration 4100: Loss = -11802.24551042947
Iteration 4200: Loss = -11802.24487375284
Iteration 4300: Loss = -11802.244290843662
Iteration 4400: Loss = -11802.247742198213
1
Iteration 4500: Loss = -11802.243147068697
Iteration 4600: Loss = -11802.242754920011
Iteration 4700: Loss = -11802.24216770081
Iteration 4800: Loss = -11802.244189484423
1
Iteration 4900: Loss = -11802.241116791749
Iteration 5000: Loss = -11802.240474562313
Iteration 5100: Loss = -11802.242454286847
1
Iteration 5200: Loss = -11802.238212204515
Iteration 5300: Loss = -11802.241178966084
1
Iteration 5400: Loss = -11802.2372598779
Iteration 5500: Loss = -11802.24760664759
1
Iteration 5600: Loss = -11802.227119436395
Iteration 5700: Loss = -11802.226457889386
Iteration 5800: Loss = -11802.225754236968
Iteration 5900: Loss = -11802.227700248473
1
Iteration 6000: Loss = -11802.225145670589
Iteration 6100: Loss = -11802.226841219943
1
Iteration 6200: Loss = -11802.224629135339
Iteration 6300: Loss = -11802.229035910495
1
Iteration 6400: Loss = -11802.228357097947
2
Iteration 6500: Loss = -11802.245620191368
3
Iteration 6600: Loss = -11802.227326436125
4
Iteration 6700: Loss = -11802.226472056076
5
Iteration 6800: Loss = -11802.24871290728
6
Iteration 6900: Loss = -11802.224757327447
7
Iteration 7000: Loss = -11802.227682631263
8
Iteration 7100: Loss = -11802.223578621946
Iteration 7200: Loss = -11802.224859536756
1
Iteration 7300: Loss = -11802.237318783235
2
Iteration 7400: Loss = -11802.225297007955
3
Iteration 7500: Loss = -11802.22279198906
Iteration 7600: Loss = -11802.221807003654
Iteration 7700: Loss = -11802.221143643636
Iteration 7800: Loss = -11802.221028837545
Iteration 7900: Loss = -11802.221053979718
Iteration 8000: Loss = -11802.310116484221
1
Iteration 8100: Loss = -11802.217901196269
Iteration 8200: Loss = -11802.217884644384
Iteration 8300: Loss = -11802.235613606894
1
Iteration 8400: Loss = -11802.217686337492
Iteration 8500: Loss = -11802.21851429058
1
Iteration 8600: Loss = -11802.441249538482
2
Iteration 8700: Loss = -11802.217436414912
Iteration 8800: Loss = -11802.217420370753
Iteration 8900: Loss = -11802.217711194267
1
Iteration 9000: Loss = -11802.219202106753
2
Iteration 9100: Loss = -11802.223629655131
3
Iteration 9200: Loss = -11802.217241670389
Iteration 9300: Loss = -11802.217328456269
Iteration 9400: Loss = -11802.217961091195
1
Iteration 9500: Loss = -11802.233146659994
2
Iteration 9600: Loss = -11802.22433967123
3
Iteration 9700: Loss = -11802.218533862046
4
Iteration 9800: Loss = -11802.218657995081
5
Iteration 9900: Loss = -11802.218284086786
6
Iteration 10000: Loss = -11802.226738029924
7
Iteration 10100: Loss = -11802.220766725814
8
Iteration 10200: Loss = -11802.222170488578
9
Iteration 10300: Loss = -11802.222676546742
10
Iteration 10400: Loss = -11802.222389030338
11
Iteration 10500: Loss = -11802.232868986666
12
Iteration 10600: Loss = -11802.228854995423
13
Iteration 10700: Loss = -11802.222303286144
14
Iteration 10800: Loss = -11802.218261716089
15
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[0.6936, 0.3064],
        [0.2372, 0.7628]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4713, 0.5287], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1916, 0.1115],
         [0.6054, 0.3967]],

        [[0.7174, 0.1028],
         [0.6154, 0.5661]],

        [[0.5677, 0.1063],
         [0.7273, 0.5402]],

        [[0.5392, 0.0938],
         [0.5933, 0.5775]],

        [[0.7203, 0.0954],
         [0.6562, 0.5101]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.976094824644038
Average Adjusted Rand Index: 0.9761609394022667
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21848.598756057847
Iteration 100: Loss = -12667.357357746117
Iteration 200: Loss = -12580.36556578127
Iteration 300: Loss = -12155.141905729288
Iteration 400: Loss = -11934.876407950122
Iteration 500: Loss = -11860.535275138553
Iteration 600: Loss = -11841.936292239978
Iteration 700: Loss = -11830.991440692636
Iteration 800: Loss = -11830.702136087397
Iteration 900: Loss = -11816.889040982214
Iteration 1000: Loss = -11811.036130834464
Iteration 1100: Loss = -11810.930382777758
Iteration 1200: Loss = -11810.858887769518
Iteration 1300: Loss = -11810.797672550076
Iteration 1400: Loss = -11810.71570589696
Iteration 1500: Loss = -11810.602613427905
Iteration 1600: Loss = -11810.570647467324
Iteration 1700: Loss = -11810.532606645496
Iteration 1800: Loss = -11810.496577151147
Iteration 1900: Loss = -11802.403391653263
Iteration 2000: Loss = -11802.376668474964
Iteration 2100: Loss = -11802.363482015411
Iteration 2200: Loss = -11802.351716828904
Iteration 2300: Loss = -11802.341384688783
Iteration 2400: Loss = -11802.332421639445
Iteration 2500: Loss = -11802.324251126216
Iteration 2600: Loss = -11802.316637501952
Iteration 2700: Loss = -11802.30880353419
Iteration 2800: Loss = -11802.29907818226
Iteration 2900: Loss = -11802.290963002908
Iteration 3000: Loss = -11802.29450346894
1
Iteration 3100: Loss = -11802.282184703101
Iteration 3200: Loss = -11802.278614742012
Iteration 3300: Loss = -11802.288629575856
1
Iteration 3400: Loss = -11802.272541420965
Iteration 3500: Loss = -11802.269889250998
Iteration 3600: Loss = -11802.273894911434
1
Iteration 3700: Loss = -11802.265189844737
Iteration 3800: Loss = -11802.264695320167
Iteration 3900: Loss = -11802.262358362355
Iteration 4000: Loss = -11802.259593223185
Iteration 4100: Loss = -11802.257827774705
Iteration 4200: Loss = -11802.262462308505
1
Iteration 4300: Loss = -11802.254899453312
Iteration 4400: Loss = -11802.253676385042
Iteration 4500: Loss = -11802.252387872277
Iteration 4600: Loss = -11802.25123633867
Iteration 4700: Loss = -11802.250298073037
Iteration 4800: Loss = -11802.249214310134
Iteration 4900: Loss = -11802.248346990502
Iteration 5000: Loss = -11802.247406059822
Iteration 5100: Loss = -11802.251356040731
1
Iteration 5200: Loss = -11802.245812817791
Iteration 5300: Loss = -11802.246115429882
1
Iteration 5400: Loss = -11802.244390818
Iteration 5500: Loss = -11802.25504348351
1
Iteration 5600: Loss = -11802.242976007858
Iteration 5700: Loss = -11802.244929430322
1
Iteration 5800: Loss = -11802.241167570428
Iteration 5900: Loss = -11802.24036655045
Iteration 6000: Loss = -11802.239839423886
Iteration 6100: Loss = -11802.239398061292
Iteration 6200: Loss = -11802.239621261775
1
Iteration 6300: Loss = -11802.238593645541
Iteration 6400: Loss = -11802.240187439193
1
Iteration 6500: Loss = -11802.247367179043
2
Iteration 6600: Loss = -11802.241506157621
3
Iteration 6700: Loss = -11802.237311318273
Iteration 6800: Loss = -11802.237113498824
Iteration 6900: Loss = -11802.236853185525
Iteration 7000: Loss = -11802.237178343225
1
Iteration 7100: Loss = -11802.236482718612
Iteration 7200: Loss = -11802.236404323921
Iteration 7300: Loss = -11802.237836295548
1
Iteration 7400: Loss = -11802.237355315889
2
Iteration 7500: Loss = -11802.236275065716
Iteration 7600: Loss = -11802.246894606635
1
Iteration 7700: Loss = -11802.235238037742
Iteration 7800: Loss = -11802.235253654278
Iteration 7900: Loss = -11802.25047740031
1
Iteration 8000: Loss = -11802.23469465662
Iteration 8100: Loss = -11802.235750829865
1
Iteration 8200: Loss = -11802.234423882534
Iteration 8300: Loss = -11802.2346132317
1
Iteration 8400: Loss = -11802.234201243575
Iteration 8500: Loss = -11802.234094742951
Iteration 8600: Loss = -11802.233916280056
Iteration 8700: Loss = -11802.233891965325
Iteration 8800: Loss = -11802.233747536271
Iteration 8900: Loss = -11802.234902585431
1
Iteration 9000: Loss = -11802.234229732401
2
Iteration 9100: Loss = -11802.244473238306
3
Iteration 9200: Loss = -11802.234508499818
4
Iteration 9300: Loss = -11802.235613124994
5
Iteration 9400: Loss = -11802.254296505691
6
Iteration 9500: Loss = -11802.23364796703
Iteration 9600: Loss = -11802.233095785985
Iteration 9700: Loss = -11802.233152152898
Iteration 9800: Loss = -11802.233048754955
Iteration 9900: Loss = -11802.254294948158
1
Iteration 10000: Loss = -11802.353430042469
2
Iteration 10100: Loss = -11802.259966324582
3
Iteration 10200: Loss = -11802.233883665282
4
Iteration 10300: Loss = -11802.25434323534
5
Iteration 10400: Loss = -11802.234530209162
6
Iteration 10500: Loss = -11802.286752510418
7
Iteration 10600: Loss = -11802.234063628777
8
Iteration 10700: Loss = -11802.23713411319
9
Iteration 10800: Loss = -11802.23628500269
10
Iteration 10900: Loss = -11802.233776033961
11
Iteration 11000: Loss = -11802.233609904848
12
Iteration 11100: Loss = -11802.234604147116
13
Iteration 11200: Loss = -11802.236939031702
14
Iteration 11300: Loss = -11802.246234887067
15
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[0.7623, 0.2377],
        [0.3071, 0.6929]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5335, 0.4665], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3960, 0.1114],
         [0.6796, 0.1912]],

        [[0.6984, 0.1027],
         [0.5107, 0.5244]],

        [[0.5100, 0.1057],
         [0.5462, 0.5383]],

        [[0.5566, 0.0941],
         [0.6763, 0.5330]],

        [[0.7059, 0.0955],
         [0.5394, 0.5304]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.976094824644038
Average Adjusted Rand Index: 0.9761609394022667
11816.202754323027
[0.976094824644038, 0.976094824644038] [0.9761609394022667, 0.9761609394022667] [11802.218261716089, 11802.246234887067]
-------------------------------------
This iteration is 49
True Objective function: Loss = -11636.287005507842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21077.809541542207
Iteration 100: Loss = -12484.194413233947
Iteration 200: Loss = -12275.374223303415
Iteration 300: Loss = -11659.448928472588
Iteration 400: Loss = -11641.820818953529
Iteration 500: Loss = -11638.673254666019
Iteration 600: Loss = -11627.690627683234
Iteration 700: Loss = -11627.527891660005
Iteration 800: Loss = -11627.388583251168
Iteration 900: Loss = -11627.312915233462
Iteration 1000: Loss = -11627.262059874573
Iteration 1100: Loss = -11627.223824280849
Iteration 1200: Loss = -11627.194077247452
Iteration 1300: Loss = -11627.17041796182
Iteration 1400: Loss = -11627.151165735098
Iteration 1500: Loss = -11627.1349543844
Iteration 1600: Loss = -11627.1204201967
Iteration 1700: Loss = -11627.104359127603
Iteration 1800: Loss = -11627.088107884234
Iteration 1900: Loss = -11627.079128499532
Iteration 2000: Loss = -11627.071980402734
Iteration 2100: Loss = -11627.065827473303
Iteration 2200: Loss = -11627.060489814328
Iteration 2300: Loss = -11627.055818681896
Iteration 2400: Loss = -11627.051585747959
Iteration 2500: Loss = -11627.0478702355
Iteration 2600: Loss = -11627.045178820115
Iteration 2700: Loss = -11627.041550224156
Iteration 2800: Loss = -11627.038826262826
Iteration 2900: Loss = -11627.036353484673
Iteration 3000: Loss = -11627.034067956642
Iteration 3100: Loss = -11627.031962294715
Iteration 3200: Loss = -11627.030075706512
Iteration 3300: Loss = -11627.030197107753
1
Iteration 3400: Loss = -11627.026682727
Iteration 3500: Loss = -11627.0252555062
Iteration 3600: Loss = -11627.023949460487
Iteration 3700: Loss = -11627.022512914828
Iteration 3800: Loss = -11627.02541240583
1
Iteration 3900: Loss = -11627.019791001874
Iteration 4000: Loss = -11627.018694085707
Iteration 4100: Loss = -11627.022619146695
1
Iteration 4200: Loss = -11627.013827670862
Iteration 4300: Loss = -11627.015499490657
1
Iteration 4400: Loss = -11627.012284593819
Iteration 4500: Loss = -11627.011811735756
Iteration 4600: Loss = -11627.010999837845
Iteration 4700: Loss = -11627.010607370092
Iteration 4800: Loss = -11627.010833564158
1
Iteration 4900: Loss = -11627.012737344636
2
Iteration 5000: Loss = -11627.009012144625
Iteration 5100: Loss = -11627.008736469583
Iteration 5200: Loss = -11627.008362501289
Iteration 5300: Loss = -11627.017515699365
1
Iteration 5400: Loss = -11627.007590505242
Iteration 5500: Loss = -11627.007171539499
Iteration 5600: Loss = -11627.006973056334
Iteration 5700: Loss = -11627.009174839372
1
Iteration 5800: Loss = -11627.006575730493
Iteration 5900: Loss = -11627.006810715935
1
Iteration 6000: Loss = -11627.006091191428
Iteration 6100: Loss = -11627.007610661003
1
Iteration 6200: Loss = -11627.006412417384
2
Iteration 6300: Loss = -11627.005170199218
Iteration 6400: Loss = -11627.005035209339
Iteration 6500: Loss = -11627.006359072442
1
Iteration 6600: Loss = -11627.00462333235
Iteration 6700: Loss = -11627.004515623345
Iteration 6800: Loss = -11627.005055466789
1
Iteration 6900: Loss = -11627.026067328687
2
Iteration 7000: Loss = -11627.004122183671
Iteration 7100: Loss = -11627.007347320761
1
Iteration 7200: Loss = -11627.003809974487
Iteration 7300: Loss = -11627.0046586551
1
Iteration 7400: Loss = -11627.045437776203
2
Iteration 7500: Loss = -11627.003245819427
Iteration 7600: Loss = -11627.024919214975
1
Iteration 7700: Loss = -11627.002695945725
Iteration 7800: Loss = -11627.008732163124
1
Iteration 7900: Loss = -11627.003954084636
2
Iteration 8000: Loss = -11627.022388588728
3
Iteration 8100: Loss = -11627.008530389794
4
Iteration 8200: Loss = -11627.002942183371
5
Iteration 8300: Loss = -11627.010217855386
6
Iteration 8400: Loss = -11627.00786496632
7
Iteration 8500: Loss = -11627.001850406554
Iteration 8600: Loss = -11627.087444095023
1
Iteration 8700: Loss = -11627.002033418385
2
Iteration 8800: Loss = -11627.002732789213
3
Iteration 8900: Loss = -11627.001563477737
Iteration 9000: Loss = -11627.026181741332
1
Iteration 9100: Loss = -11627.001911846019
2
Iteration 9200: Loss = -11627.001786670104
3
Iteration 9300: Loss = -11627.001771916703
4
Iteration 9400: Loss = -11627.035757935355
5
Iteration 9500: Loss = -11627.001286522387
Iteration 9600: Loss = -11627.001562871643
1
Iteration 9700: Loss = -11627.043486899636
2
Iteration 9800: Loss = -11627.013304918552
3
Iteration 9900: Loss = -11627.011394919502
4
Iteration 10000: Loss = -11627.025377689411
5
Iteration 10100: Loss = -11627.03988786509
6
Iteration 10200: Loss = -11627.036286552528
7
Iteration 10300: Loss = -11627.002755483347
8
Iteration 10400: Loss = -11627.001700214012
9
Iteration 10500: Loss = -11627.000693920632
Iteration 10600: Loss = -11627.004716403333
1
Iteration 10700: Loss = -11627.002116977932
2
Iteration 10800: Loss = -11627.001717455394
3
Iteration 10900: Loss = -11627.001240361977
4
Iteration 11000: Loss = -11627.00060205154
Iteration 11100: Loss = -11627.007029419226
1
Iteration 11200: Loss = -11627.001457558086
2
Iteration 11300: Loss = -11627.000539292398
Iteration 11400: Loss = -11627.000487541074
Iteration 11500: Loss = -11627.001293461344
1
Iteration 11600: Loss = -11627.004537327919
2
Iteration 11700: Loss = -11627.007282006816
3
Iteration 11800: Loss = -11627.000151627106
Iteration 11900: Loss = -11627.000222203646
Iteration 12000: Loss = -11627.002338051536
1
Iteration 12100: Loss = -11627.004179322335
2
Iteration 12200: Loss = -11627.007652422362
3
Iteration 12300: Loss = -11627.031619150153
4
Iteration 12400: Loss = -11627.002421016463
5
Iteration 12500: Loss = -11627.002734898822
6
Iteration 12600: Loss = -11627.011048287222
7
Iteration 12700: Loss = -11627.003927861186
8
Iteration 12800: Loss = -11627.030702110682
9
Iteration 12900: Loss = -11627.002643033342
10
Iteration 13000: Loss = -11627.004247288622
11
Iteration 13100: Loss = -11627.000500968436
12
Iteration 13200: Loss = -11627.000632086558
13
Iteration 13300: Loss = -11627.000169477153
Iteration 13400: Loss = -11627.00143925466
1
Iteration 13500: Loss = -11627.004648355747
2
Iteration 13600: Loss = -11627.093911690521
3
Iteration 13700: Loss = -11627.00484862495
4
Iteration 13800: Loss = -11627.02643010122
5
Iteration 13900: Loss = -11627.008398321546
6
Iteration 14000: Loss = -11627.058816026298
7
Iteration 14100: Loss = -11627.065258455988
8
Iteration 14200: Loss = -11627.017222652965
9
Iteration 14300: Loss = -11627.01031954199
10
Iteration 14400: Loss = -11627.002228665224
11
Iteration 14500: Loss = -11627.000132012647
Iteration 14600: Loss = -11627.000669386873
1
Iteration 14700: Loss = -11627.001462652323
2
Iteration 14800: Loss = -11627.007533148066
3
Iteration 14900: Loss = -11627.015352322762
4
Iteration 15000: Loss = -11627.006872861026
5
Iteration 15100: Loss = -11627.016695234435
6
Iteration 15200: Loss = -11626.999981862406
Iteration 15300: Loss = -11627.000677147471
1
Iteration 15400: Loss = -11627.000651893979
2
Iteration 15500: Loss = -11627.0014595979
3
Iteration 15600: Loss = -11627.009568471634
4
Iteration 15700: Loss = -11627.041773486282
5
Iteration 15800: Loss = -11627.001115317677
6
Iteration 15900: Loss = -11627.002269412234
7
Iteration 16000: Loss = -11627.001750702631
8
Iteration 16100: Loss = -11627.01618828831
9
Iteration 16200: Loss = -11627.055708336891
10
Iteration 16300: Loss = -11627.068947562677
11
Iteration 16400: Loss = -11627.004617294568
12
Iteration 16500: Loss = -11627.00172070466
13
Iteration 16600: Loss = -11626.99996407813
Iteration 16700: Loss = -11627.000892894497
1
Iteration 16800: Loss = -11627.12606695797
2
Iteration 16900: Loss = -11627.002391252938
3
Iteration 17000: Loss = -11627.001604553921
4
Iteration 17100: Loss = -11627.002071515279
5
Iteration 17200: Loss = -11626.99995873785
Iteration 17300: Loss = -11627.001192623546
1
Iteration 17400: Loss = -11627.000461758405
2
Iteration 17500: Loss = -11627.001592135479
3
Iteration 17600: Loss = -11627.005226206773
4
Iteration 17700: Loss = -11627.075325248583
5
Iteration 17800: Loss = -11627.013332961042
6
Iteration 17900: Loss = -11627.02537243268
7
Iteration 18000: Loss = -11627.02576852646
8
Iteration 18100: Loss = -11627.066169960352
9
Iteration 18200: Loss = -11627.004882554755
10
Iteration 18300: Loss = -11626.999878652165
Iteration 18400: Loss = -11627.002113612365
1
Iteration 18500: Loss = -11627.030284528639
2
Iteration 18600: Loss = -11627.003864485732
3
Iteration 18700: Loss = -11627.000386466347
4
Iteration 18800: Loss = -11627.004921482541
5
Iteration 18900: Loss = -11627.004396805754
6
Iteration 19000: Loss = -11627.079443450326
7
Iteration 19100: Loss = -11627.1184739096
8
Iteration 19200: Loss = -11627.011379337322
9
Iteration 19300: Loss = -11627.000234541067
10
Iteration 19400: Loss = -11627.000038106084
11
Iteration 19500: Loss = -11627.000817757276
12
Iteration 19600: Loss = -11627.023790166357
13
Iteration 19700: Loss = -11627.054555290099
14
Iteration 19800: Loss = -11627.000673560646
15
Stopping early at iteration 19800 due to no improvement.
pi: tensor([[0.7685, 0.2315],
        [0.2312, 0.7688]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4870, 0.5130], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1918, 0.1055],
         [0.5472, 0.4061]],

        [[0.6254, 0.1155],
         [0.6385, 0.7091]],

        [[0.5395, 0.0962],
         [0.5224, 0.6487]],

        [[0.6741, 0.0977],
         [0.6373, 0.6744]],

        [[0.6628, 0.1017],
         [0.6820, 0.5142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22422.50645289022
Iteration 100: Loss = -12482.560504634199
Iteration 200: Loss = -12454.413088714868
Iteration 300: Loss = -12143.999460036448
Iteration 400: Loss = -11884.956162074817
Iteration 500: Loss = -11846.950729762413
Iteration 600: Loss = -11823.07953499709
Iteration 700: Loss = -11814.863176501229
Iteration 800: Loss = -11814.324124745546
Iteration 900: Loss = -11814.172494455279
Iteration 1000: Loss = -11814.071221435006
Iteration 1100: Loss = -11813.99877191587
Iteration 1200: Loss = -11813.944520387582
Iteration 1300: Loss = -11813.902603295404
Iteration 1400: Loss = -11813.869363393489
Iteration 1500: Loss = -11813.84246146466
Iteration 1600: Loss = -11813.820383743423
Iteration 1700: Loss = -11813.80191776019
Iteration 1800: Loss = -11813.786395745894
Iteration 1900: Loss = -11813.77329372063
Iteration 2000: Loss = -11813.761667145089
Iteration 2100: Loss = -11813.751761760666
Iteration 2200: Loss = -11813.74303937241
Iteration 2300: Loss = -11813.735472635759
Iteration 2400: Loss = -11813.728602195457
Iteration 2500: Loss = -11813.722606202326
Iteration 2600: Loss = -11813.717576871633
Iteration 2700: Loss = -11813.71253250021
Iteration 2800: Loss = -11813.708213115762
Iteration 2900: Loss = -11813.708373638206
1
Iteration 3000: Loss = -11813.70093486104
Iteration 3100: Loss = -11813.697952574294
Iteration 3200: Loss = -11813.694940109905
Iteration 3300: Loss = -11813.692363922768
Iteration 3400: Loss = -11813.69003564522
Iteration 3500: Loss = -11813.687739185467
Iteration 3600: Loss = -11813.687290556132
Iteration 3700: Loss = -11813.683833255338
Iteration 3800: Loss = -11813.682453967127
Iteration 3900: Loss = -11813.69641709337
1
Iteration 4000: Loss = -11813.68318307059
2
Iteration 4100: Loss = -11813.677459781862
Iteration 4200: Loss = -11813.677488236537
Iteration 4300: Loss = -11813.674725223278
Iteration 4400: Loss = -11813.67359543434
Iteration 4500: Loss = -11813.672102759321
Iteration 4600: Loss = -11813.670788957057
Iteration 4700: Loss = -11813.669348078625
Iteration 4800: Loss = -11813.668122873954
Iteration 4900: Loss = -11813.664955127457
Iteration 5000: Loss = -11813.662074944894
Iteration 5100: Loss = -11813.660030700437
Iteration 5200: Loss = -11813.65917600366
Iteration 5300: Loss = -11813.659097894679
Iteration 5400: Loss = -11813.661399130242
1
Iteration 5500: Loss = -11813.657398917425
Iteration 5600: Loss = -11813.65697154171
Iteration 5700: Loss = -11813.661087962111
1
Iteration 5800: Loss = -11813.656029100983
Iteration 5900: Loss = -11813.656094237227
Iteration 6000: Loss = -11813.658558418343
1
Iteration 6100: Loss = -11813.654886890743
Iteration 6200: Loss = -11813.654928445996
Iteration 6300: Loss = -11813.654273574735
Iteration 6400: Loss = -11813.653950260596
Iteration 6500: Loss = -11813.65383260503
Iteration 6600: Loss = -11813.654146833866
1
Iteration 6700: Loss = -11813.65829420209
2
Iteration 6800: Loss = -11813.652874565118
Iteration 6900: Loss = -11813.654289418171
1
Iteration 7000: Loss = -11813.654727124036
2
Iteration 7100: Loss = -11813.652441619426
Iteration 7200: Loss = -11813.657683244983
1
Iteration 7300: Loss = -11813.651922099714
Iteration 7400: Loss = -11813.65173036288
Iteration 7500: Loss = -11813.66301760189
1
Iteration 7600: Loss = -11813.652194661883
2
Iteration 7700: Loss = -11813.651313692078
Iteration 7800: Loss = -11813.651268530693
Iteration 7900: Loss = -11813.66418314688
1
Iteration 8000: Loss = -11813.650985913791
Iteration 8100: Loss = -11813.663019801226
1
Iteration 8200: Loss = -11813.650776499348
Iteration 8300: Loss = -11813.650676532485
Iteration 8400: Loss = -11813.65084992874
1
Iteration 8500: Loss = -11813.650468817352
Iteration 8600: Loss = -11813.772834069609
1
Iteration 8700: Loss = -11813.650339988286
Iteration 8800: Loss = -11813.650276741406
Iteration 8900: Loss = -11813.650553686366
1
Iteration 9000: Loss = -11813.650125270799
Iteration 9100: Loss = -11813.650651598404
1
Iteration 9200: Loss = -11813.650033694952
Iteration 9300: Loss = -11813.650280723554
1
Iteration 9400: Loss = -11813.649912409543
Iteration 9500: Loss = -11813.650007001857
Iteration 9600: Loss = -11813.649816517676
Iteration 9700: Loss = -11813.65755966205
1
Iteration 9800: Loss = -11813.649730808535
Iteration 9900: Loss = -11813.649821783509
Iteration 10000: Loss = -11813.64971563657
Iteration 10100: Loss = -11813.6496960639
Iteration 10200: Loss = -11813.649718720335
Iteration 10300: Loss = -11813.652429690694
1
Iteration 10400: Loss = -11813.650202314635
2
Iteration 10500: Loss = -11813.650079807006
3
Iteration 10600: Loss = -11813.649679359782
Iteration 10700: Loss = -11813.649701737486
Iteration 10800: Loss = -11813.65185687068
1
Iteration 10900: Loss = -11813.709034865618
2
Iteration 11000: Loss = -11813.65860373078
3
Iteration 11100: Loss = -11813.649389447812
Iteration 11200: Loss = -11813.66423143453
1
Iteration 11300: Loss = -11813.660447245946
2
Iteration 11400: Loss = -11813.703915741044
3
Iteration 11500: Loss = -11813.652631908886
4
Iteration 11600: Loss = -11813.650773894506
5
Iteration 11700: Loss = -11813.651086030484
6
Iteration 11800: Loss = -11813.6493088959
Iteration 11900: Loss = -11813.649693657057
1
Iteration 12000: Loss = -11813.64929720109
Iteration 12100: Loss = -11813.649261913863
Iteration 12200: Loss = -11813.649204511306
Iteration 12300: Loss = -11813.653055056373
1
Iteration 12400: Loss = -11813.649195315787
Iteration 12500: Loss = -11813.649177045856
Iteration 12600: Loss = -11813.650253250848
1
Iteration 12700: Loss = -11813.64914983663
Iteration 12800: Loss = -11813.680485736464
1
Iteration 12900: Loss = -11813.649142421453
Iteration 13000: Loss = -11813.650250642597
1
Iteration 13100: Loss = -11813.64952812008
2
Iteration 13200: Loss = -11813.650537285916
3
Iteration 13300: Loss = -11813.65944672532
4
Iteration 13400: Loss = -11813.651123514957
5
Iteration 13500: Loss = -11813.649145334251
Iteration 13600: Loss = -11813.649303218204
1
Iteration 13700: Loss = -11813.651406951492
2
Iteration 13800: Loss = -11813.68719018263
3
Iteration 13900: Loss = -11813.649119014126
Iteration 14000: Loss = -11813.671714635535
1
Iteration 14100: Loss = -11813.649086172203
Iteration 14200: Loss = -11813.728979190255
1
Iteration 14300: Loss = -11813.64911433882
Iteration 14400: Loss = -11813.649079586277
Iteration 14500: Loss = -11813.652865773465
1
Iteration 14600: Loss = -11813.64908944973
Iteration 14700: Loss = -11813.649091216468
Iteration 14800: Loss = -11813.6493329806
1
Iteration 14900: Loss = -11813.64905152535
Iteration 15000: Loss = -11813.751273704826
1
Iteration 15100: Loss = -11813.649079424116
Iteration 15200: Loss = -11813.649285078718
1
Iteration 15300: Loss = -11813.649134420135
Iteration 15400: Loss = -11813.684512320808
1
Iteration 15500: Loss = -11813.64989120924
2
Iteration 15600: Loss = -11813.697916056442
3
Iteration 15700: Loss = -11813.651986128543
4
Iteration 15800: Loss = -11813.649734722896
5
Iteration 15900: Loss = -11813.659412147526
6
Iteration 16000: Loss = -11813.648978480705
Iteration 16100: Loss = -11813.650056388431
1
Iteration 16200: Loss = -11813.648980560458
Iteration 16300: Loss = -11813.651037160402
1
Iteration 16400: Loss = -11813.648969018956
Iteration 16500: Loss = -11813.667963005224
1
Iteration 16600: Loss = -11813.648967344769
Iteration 16700: Loss = -11813.649029764192
Iteration 16800: Loss = -11813.649022165335
Iteration 16900: Loss = -11813.648953322352
Iteration 17000: Loss = -11813.649486409344
1
Iteration 17100: Loss = -11813.865357014094
2
Iteration 17200: Loss = -11813.64929406861
3
Iteration 17300: Loss = -11813.650246689092
4
Iteration 17400: Loss = -11813.656809215187
5
Iteration 17500: Loss = -11813.976044791905
6
Iteration 17600: Loss = -11813.648964671978
Iteration 17700: Loss = -11813.650269052727
1
Iteration 17800: Loss = -11813.649027957537
Iteration 17900: Loss = -11813.64898386386
Iteration 18000: Loss = -11813.649558867013
1
Iteration 18100: Loss = -11813.648974268295
Iteration 18200: Loss = -11813.655374439948
1
Iteration 18300: Loss = -11813.649357351771
2
Iteration 18400: Loss = -11813.649048985357
Iteration 18500: Loss = -11813.657019261573
1
Iteration 18600: Loss = -11813.649027594258
Iteration 18700: Loss = -11813.649077476908
Iteration 18800: Loss = -11813.666467111994
1
Iteration 18900: Loss = -11813.65393519081
2
Iteration 19000: Loss = -11813.664932096146
3
Iteration 19100: Loss = -11813.650418463
4
Iteration 19200: Loss = -11813.666437402184
5
Iteration 19300: Loss = -11813.65632700272
6
Iteration 19400: Loss = -11813.67728639849
7
Iteration 19500: Loss = -11813.653001969795
8
Iteration 19600: Loss = -11813.659211820055
9
Iteration 19700: Loss = -11813.662026516258
10
Iteration 19800: Loss = -11813.65511169228
11
Iteration 19900: Loss = -11813.659449956727
12
pi: tensor([[0.7514, 0.2486],
        [0.4047, 0.5953]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4873, 0.5127], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2068, 0.1056],
         [0.6036, 0.4045]],

        [[0.5652, 0.1155],
         [0.5837, 0.6817]],

        [[0.6177, 0.0964],
         [0.7117, 0.5511]],

        [[0.7192, 0.0976],
         [0.5171, 0.6753]],

        [[0.6077, 0.0986],
         [0.6253, 0.5096]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 37
Adjusted Rand Index: 0.06204045798734316
Global Adjusted Rand Index: 0.552648026349287
Average Adjusted Rand Index: 0.8044076527610317
11636.287005507842
[0.99199998169963, 0.552648026349287] [0.9919995611635631, 0.8044076527610317] [11627.000673560646, 11813.654518301813]
-------------------------------------
This iteration is 50
True Objective function: Loss = -11906.313270076182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20860.642296876074
Iteration 100: Loss = -12418.959521496852
Iteration 200: Loss = -12115.056572812015
Iteration 300: Loss = -12102.450392751163
Iteration 400: Loss = -12100.863434114706
Iteration 500: Loss = -12100.693703500445
Iteration 600: Loss = -12100.58244066848
Iteration 700: Loss = -12100.425309460734
Iteration 800: Loss = -12100.391931976992
Iteration 900: Loss = -12100.368171982642
Iteration 1000: Loss = -12100.350555107249
Iteration 1100: Loss = -12100.337154196963
Iteration 1200: Loss = -12100.326657339816
Iteration 1300: Loss = -12100.318239301643
Iteration 1400: Loss = -12100.311381455465
Iteration 1500: Loss = -12100.305664013724
Iteration 1600: Loss = -12100.30089949917
Iteration 1700: Loss = -12100.296875205466
Iteration 1800: Loss = -12100.293398283104
Iteration 1900: Loss = -12100.290392149258
Iteration 2000: Loss = -12100.287828015533
Iteration 2100: Loss = -12100.285583523762
Iteration 2200: Loss = -12100.283579915822
Iteration 2300: Loss = -12100.281823113237
Iteration 2400: Loss = -12100.280220046687
Iteration 2500: Loss = -12100.278825917143
Iteration 2600: Loss = -12100.277608033584
Iteration 2700: Loss = -12100.276465604342
Iteration 2800: Loss = -12100.275444655374
Iteration 2900: Loss = -12100.274549710583
Iteration 3000: Loss = -12100.273722012922
Iteration 3100: Loss = -12100.272942862986
Iteration 3200: Loss = -12100.272571926565
Iteration 3300: Loss = -12100.271650902092
Iteration 3400: Loss = -12100.2710716595
Iteration 3500: Loss = -12100.270531915223
Iteration 3600: Loss = -12100.270035203608
Iteration 3700: Loss = -12100.269565285354
Iteration 3800: Loss = -12100.26939099872
Iteration 3900: Loss = -12100.268812137254
Iteration 4000: Loss = -12100.271523030484
1
Iteration 4100: Loss = -12100.268197167934
Iteration 4200: Loss = -12100.267823655337
Iteration 4300: Loss = -12100.267756053621
Iteration 4400: Loss = -12100.267203884427
Iteration 4500: Loss = -12100.267178964175
Iteration 4600: Loss = -12100.266733184162
Iteration 4700: Loss = -12100.266450154426
Iteration 4800: Loss = -12100.268051378827
1
Iteration 4900: Loss = -12100.262421618714
Iteration 5000: Loss = -12100.269403945043
1
Iteration 5100: Loss = -12100.261229518082
Iteration 5200: Loss = -12100.261717674632
1
Iteration 5300: Loss = -12100.263038986792
2
Iteration 5400: Loss = -12100.263657367528
3
Iteration 5500: Loss = -12100.261815977858
4
Iteration 5600: Loss = -12100.260561111483
Iteration 5700: Loss = -12100.260438786703
Iteration 5800: Loss = -12100.26223115211
1
Iteration 5900: Loss = -12100.260118311136
Iteration 6000: Loss = -12100.290732387204
1
Iteration 6100: Loss = -12100.262817555644
2
Iteration 6200: Loss = -12100.28573902543
3
Iteration 6300: Loss = -12100.259762148804
Iteration 6400: Loss = -12100.262304447766
1
Iteration 6500: Loss = -12100.25963122252
Iteration 6600: Loss = -12100.260585346883
1
Iteration 6700: Loss = -12100.261270433943
2
Iteration 6800: Loss = -12100.25954209101
Iteration 6900: Loss = -12100.263554711042
1
Iteration 7000: Loss = -12100.262715561703
2
Iteration 7100: Loss = -12100.274693563248
3
Iteration 7200: Loss = -12100.260673090259
4
Iteration 7300: Loss = -12100.259303495039
Iteration 7400: Loss = -12100.259214114221
Iteration 7500: Loss = -12100.260341671525
1
Iteration 7600: Loss = -12100.260107060078
2
Iteration 7700: Loss = -12100.274567341987
3
Iteration 7800: Loss = -12100.259060528084
Iteration 7900: Loss = -12100.259557116266
1
Iteration 8000: Loss = -12100.258984138654
Iteration 8100: Loss = -12100.260158494139
1
Iteration 8200: Loss = -12100.2592561011
2
Iteration 8300: Loss = -12100.258909652046
Iteration 8400: Loss = -12100.263045930176
1
Iteration 8500: Loss = -12100.259222764495
2
Iteration 8600: Loss = -12100.25969519187
3
Iteration 8700: Loss = -12100.258795670328
Iteration 8800: Loss = -12100.260237096127
1
Iteration 8900: Loss = -12100.261643858945
2
Iteration 9000: Loss = -12100.261263264247
3
Iteration 9100: Loss = -12100.259500991075
4
Iteration 9200: Loss = -12100.259560568
5
Iteration 9300: Loss = -12100.263552501341
6
Iteration 9400: Loss = -12100.266620673507
7
Iteration 9500: Loss = -12100.26088998559
8
Iteration 9600: Loss = -12100.259450330856
9
Iteration 9700: Loss = -12100.261982209404
10
Iteration 9800: Loss = -12100.263921580896
11
Iteration 9900: Loss = -12100.279785870458
12
Iteration 10000: Loss = -12100.268836006684
13
Iteration 10100: Loss = -12100.266706152655
14
Iteration 10200: Loss = -12100.25992818206
15
Stopping early at iteration 10200 due to no improvement.
pi: tensor([[0.5312, 0.4688],
        [0.3966, 0.6034]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5821, 0.4179], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3978, 0.1062],
         [0.6921, 0.2354]],

        [[0.5430, 0.1057],
         [0.5366, 0.7191]],

        [[0.5718, 0.1026],
         [0.6340, 0.6901]],

        [[0.7084, 0.1051],
         [0.6316, 0.5401]],

        [[0.5383, 0.1026],
         [0.6470, 0.7073]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 81
Adjusted Rand Index: 0.3781818181818182
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.45048693373721027
Average Adjusted Rand Index: 0.8676363636363635
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21063.632448611814
Iteration 100: Loss = -12722.877639130676
Iteration 200: Loss = -12298.50911368577
Iteration 300: Loss = -11967.416255603443
Iteration 400: Loss = -11922.180568766342
Iteration 500: Loss = -11912.884294613737
Iteration 600: Loss = -11912.617989624925
Iteration 700: Loss = -11912.464757497028
Iteration 800: Loss = -11912.364374572107
Iteration 900: Loss = -11912.27204584172
Iteration 1000: Loss = -11911.06662296089
Iteration 1100: Loss = -11911.028198942242
Iteration 1200: Loss = -11910.99813860175
Iteration 1300: Loss = -11910.973494450525
Iteration 1400: Loss = -11910.949106885222
Iteration 1500: Loss = -11910.16691590952
Iteration 1600: Loss = -11910.152271622916
Iteration 1700: Loss = -11910.13968541213
Iteration 1800: Loss = -11910.122108879319
Iteration 1900: Loss = -11910.09141569087
Iteration 2000: Loss = -11910.083860048193
Iteration 2100: Loss = -11910.077588130993
Iteration 2200: Loss = -11910.072166333068
Iteration 2300: Loss = -11910.0674620024
Iteration 2400: Loss = -11910.063222457906
Iteration 2500: Loss = -11910.05944996069
Iteration 2600: Loss = -11910.056138551334
Iteration 2700: Loss = -11910.05317956388
Iteration 2800: Loss = -11910.050414303674
Iteration 2900: Loss = -11910.04795239319
Iteration 3000: Loss = -11910.04578156692
Iteration 3100: Loss = -11910.043676746513
Iteration 3200: Loss = -11910.041736381649
Iteration 3300: Loss = -11910.078070551914
1
Iteration 3400: Loss = -11910.038153492344
Iteration 3500: Loss = -11910.036349372722
Iteration 3600: Loss = -11910.034417296083
Iteration 3700: Loss = -11910.033567505521
Iteration 3800: Loss = -11910.031148190137
Iteration 3900: Loss = -11910.029472182563
Iteration 4000: Loss = -11910.027724931622
Iteration 4100: Loss = -11910.0266394593
Iteration 4200: Loss = -11910.025621540284
Iteration 4300: Loss = -11910.02480605741
Iteration 4400: Loss = -11910.024075835941
Iteration 4500: Loss = -11910.02374867791
Iteration 4600: Loss = -11910.024531670406
1
Iteration 4700: Loss = -11910.035220267117
2
Iteration 4800: Loss = -11910.021702545026
Iteration 4900: Loss = -11910.021666731087
Iteration 5000: Loss = -11910.020773727852
Iteration 5100: Loss = -11910.022239247468
1
Iteration 5200: Loss = -11910.019815317975
Iteration 5300: Loss = -11910.019459300405
Iteration 5400: Loss = -11910.018453414814
Iteration 5500: Loss = -11910.02089898826
1
Iteration 5600: Loss = -11910.016898000187
Iteration 5700: Loss = -11910.016610140015
Iteration 5800: Loss = -11910.017396686917
1
Iteration 5900: Loss = -11910.016257884865
Iteration 6000: Loss = -11910.022029946596
1
Iteration 6100: Loss = -11910.015611647756
Iteration 6200: Loss = -11910.021412991255
1
Iteration 6300: Loss = -11910.015188726491
Iteration 6400: Loss = -11910.015876805497
1
Iteration 6500: Loss = -11910.024300458843
2
Iteration 6600: Loss = -11910.014629039577
Iteration 6700: Loss = -11910.014657641581
Iteration 6800: Loss = -11910.05782883236
1
Iteration 6900: Loss = -11910.137406960534
2
Iteration 7000: Loss = -11910.015910424267
3
Iteration 7100: Loss = -11910.015676917637
4
Iteration 7200: Loss = -11910.017129887105
5
Iteration 7300: Loss = -11910.013566849897
Iteration 7400: Loss = -11910.013910551888
1
Iteration 7500: Loss = -11910.013287008549
Iteration 7600: Loss = -11910.01320433342
Iteration 7700: Loss = -11910.017282876568
1
Iteration 7800: Loss = -11910.012877221125
Iteration 7900: Loss = -11910.012299206062
Iteration 8000: Loss = -11910.023777310364
1
Iteration 8100: Loss = -11910.011221226923
Iteration 8200: Loss = -11910.011210162931
Iteration 8300: Loss = -11910.114919203786
1
Iteration 8400: Loss = -11910.010807291237
Iteration 8500: Loss = -11910.011006985309
1
Iteration 8600: Loss = -11910.142701654975
2
Iteration 8700: Loss = -11910.01359533832
3
Iteration 8800: Loss = -11910.012031603788
4
Iteration 8900: Loss = -11910.025346995213
5
Iteration 9000: Loss = -11910.01055648742
Iteration 9100: Loss = -11910.010759751976
1
Iteration 9200: Loss = -11910.116444091345
2
Iteration 9300: Loss = -11910.014461419754
3
Iteration 9400: Loss = -11910.010485602877
Iteration 9500: Loss = -11910.013019738506
1
Iteration 9600: Loss = -11910.012022728202
2
Iteration 9700: Loss = -11910.010429282705
Iteration 9800: Loss = -11901.580272293575
Iteration 9900: Loss = -11901.549605790491
Iteration 10000: Loss = -11901.569729186882
1
Iteration 10100: Loss = -11901.54807433789
Iteration 10200: Loss = -11901.55029564199
1
Iteration 10300: Loss = -11901.57254849861
2
Iteration 10400: Loss = -11901.548989031007
3
Iteration 10500: Loss = -11901.548179889482
4
Iteration 10600: Loss = -11901.547659138918
Iteration 10700: Loss = -11901.549199543348
1
Iteration 10800: Loss = -11901.558770052365
2
Iteration 10900: Loss = -11901.549146334006
3
Iteration 11000: Loss = -11901.551450946099
4
Iteration 11100: Loss = -11901.573217924375
5
Iteration 11200: Loss = -11901.559879076005
6
Iteration 11300: Loss = -11901.55245499619
7
Iteration 11400: Loss = -11901.595102342566
8
Iteration 11500: Loss = -11901.554573631673
9
Iteration 11600: Loss = -11901.551504832307
10
Iteration 11700: Loss = -11901.559997467426
11
Iteration 11800: Loss = -11901.551189893287
12
Iteration 11900: Loss = -11901.628083793355
13
Iteration 12000: Loss = -11901.583874436996
14
Iteration 12100: Loss = -11901.549078979662
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.7591, 0.2409],
        [0.2579, 0.7421]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5804, 0.4196], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4053, 0.1063],
         [0.7179, 0.2019]],

        [[0.7298, 0.1061],
         [0.6854, 0.7194]],

        [[0.5167, 0.1039],
         [0.5135, 0.5703]],

        [[0.6621, 0.1053],
         [0.6688, 0.7157]],

        [[0.7205, 0.1025],
         [0.5001, 0.6725]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919998213107827
Average Adjusted Rand Index: 0.992
11906.313270076182
[0.45048693373721027, 0.9919998213107827] [0.8676363636363635, 0.992] [12100.25992818206, 11901.549078979662]
-------------------------------------
This iteration is 51
True Objective function: Loss = -11731.177692079336
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20179.859859725457
Iteration 100: Loss = -12486.821292684237
Iteration 200: Loss = -12473.558990277377
Iteration 300: Loss = -11995.298834431902
Iteration 400: Loss = -11861.250254316152
Iteration 500: Loss = -11858.38701596657
Iteration 600: Loss = -11857.640512745942
Iteration 700: Loss = -11857.414286634496
Iteration 800: Loss = -11857.288291238343
Iteration 900: Loss = -11857.20190129432
Iteration 1000: Loss = -11857.106683114382
Iteration 1100: Loss = -11856.837494027222
Iteration 1200: Loss = -11856.672825744312
Iteration 1300: Loss = -11856.627866037737
Iteration 1400: Loss = -11856.582973605517
Iteration 1500: Loss = -11856.562981575062
Iteration 1600: Loss = -11856.548454707496
Iteration 1700: Loss = -11856.536682073096
Iteration 1800: Loss = -11856.52678875971
Iteration 1900: Loss = -11856.518359082453
Iteration 2000: Loss = -11856.511005165012
Iteration 2100: Loss = -11856.504629053
Iteration 2200: Loss = -11856.498982936922
Iteration 2300: Loss = -11856.498038910815
Iteration 2400: Loss = -11856.489788230465
Iteration 2500: Loss = -11856.485943626685
Iteration 2600: Loss = -11856.482537420707
Iteration 2700: Loss = -11856.48407867279
1
Iteration 2800: Loss = -11856.476834663006
Iteration 2900: Loss = -11856.474815228345
Iteration 3000: Loss = -11856.473611412117
Iteration 3100: Loss = -11856.470157458145
Iteration 3200: Loss = -11856.46944680595
Iteration 3300: Loss = -11856.466601445414
Iteration 3400: Loss = -11856.465053753669
Iteration 3500: Loss = -11856.463597617232
Iteration 3600: Loss = -11856.462256126666
Iteration 3700: Loss = -11856.4611662062
Iteration 3800: Loss = -11856.459670082384
Iteration 3900: Loss = -11856.458332027754
Iteration 4000: Loss = -11856.457071349221
Iteration 4100: Loss = -11856.454127956958
Iteration 4200: Loss = -11856.450555496373
Iteration 4300: Loss = -11856.447886161912
Iteration 4400: Loss = -11856.448586470808
1
Iteration 4500: Loss = -11856.446030980045
Iteration 4600: Loss = -11856.445912713574
Iteration 4700: Loss = -11856.444668180862
Iteration 4800: Loss = -11856.44691603027
1
Iteration 4900: Loss = -11856.443598617541
Iteration 5000: Loss = -11856.443795930843
1
Iteration 5100: Loss = -11856.447087645585
2
Iteration 5200: Loss = -11856.442336628847
Iteration 5300: Loss = -11856.441987459173
Iteration 5400: Loss = -11856.442172957453
1
Iteration 5500: Loss = -11856.441319655121
Iteration 5600: Loss = -11856.441268296454
Iteration 5700: Loss = -11856.448237570023
1
Iteration 5800: Loss = -11856.440469534575
Iteration 5900: Loss = -11856.440219103839
Iteration 6000: Loss = -11856.439994661392
Iteration 6100: Loss = -11856.439805055104
Iteration 6200: Loss = -11856.439543376613
Iteration 6300: Loss = -11856.440367287281
1
Iteration 6400: Loss = -11856.442594575336
2
Iteration 6500: Loss = -11856.439080934535
Iteration 6600: Loss = -11856.439550715315
1
Iteration 6700: Loss = -11856.439120847052
Iteration 6800: Loss = -11856.438628906124
Iteration 6900: Loss = -11856.584505065768
1
Iteration 7000: Loss = -11856.438277771158
Iteration 7100: Loss = -11856.445132825456
1
Iteration 7200: Loss = -11856.43808046456
Iteration 7300: Loss = -11856.443311671055
1
Iteration 7400: Loss = -11856.438149472218
Iteration 7500: Loss = -11856.549689717292
1
Iteration 7600: Loss = -11856.437656629321
Iteration 7700: Loss = -11856.438182401202
1
Iteration 7800: Loss = -11856.437475574365
Iteration 7900: Loss = -11856.438312017342
1
Iteration 8000: Loss = -11856.438713253416
2
Iteration 8100: Loss = -11856.43762399013
3
Iteration 8200: Loss = -11856.437270048953
Iteration 8300: Loss = -11856.437263882959
Iteration 8400: Loss = -11856.440092367453
1
Iteration 8500: Loss = -11856.437085724116
Iteration 8600: Loss = -11856.437228439303
1
Iteration 8700: Loss = -11856.486678748754
2
Iteration 8800: Loss = -11856.436988594876
Iteration 8900: Loss = -11856.442649878569
1
Iteration 9000: Loss = -11856.437378706694
2
Iteration 9100: Loss = -11856.456425451457
3
Iteration 9200: Loss = -11856.465214407832
4
Iteration 9300: Loss = -11856.439564153126
5
Iteration 9400: Loss = -11856.436754879995
Iteration 9500: Loss = -11856.453032163661
1
Iteration 9600: Loss = -11856.438760236715
2
Iteration 9700: Loss = -11856.569559886
3
Iteration 9800: Loss = -11856.440949049842
4
Iteration 9900: Loss = -11856.437270723656
5
Iteration 10000: Loss = -11856.454459548639
6
Iteration 10100: Loss = -11856.438480220892
7
Iteration 10200: Loss = -11856.438348373349
8
Iteration 10300: Loss = -11856.440570023622
9
Iteration 10400: Loss = -11856.439681419106
10
Iteration 10500: Loss = -11856.442608372754
11
Iteration 10600: Loss = -11856.452911559578
12
Iteration 10700: Loss = -11856.436972494996
13
Iteration 10800: Loss = -11856.436658651432
Iteration 10900: Loss = -11856.441206556769
1
Iteration 11000: Loss = -11856.440700600291
2
Iteration 11100: Loss = -11856.44106194267
3
Iteration 11200: Loss = -11856.467898932815
4
Iteration 11300: Loss = -11856.44584166595
5
Iteration 11400: Loss = -11856.438358954618
6
Iteration 11500: Loss = -11856.43731865558
7
Iteration 11600: Loss = -11856.452100296749
8
Iteration 11700: Loss = -11856.462293820252
9
Iteration 11800: Loss = -11856.438734342031
10
Iteration 11900: Loss = -11856.44032575695
11
Iteration 12000: Loss = -11856.468732487665
12
Iteration 12100: Loss = -11856.443671205243
13
Iteration 12200: Loss = -11856.436407084479
Iteration 12300: Loss = -11856.437709204747
1
Iteration 12400: Loss = -11856.454999846534
2
Iteration 12500: Loss = -11856.437386354584
3
Iteration 12600: Loss = -11856.43616659768
Iteration 12700: Loss = -11856.445685244476
1
Iteration 12800: Loss = -11856.460457800506
2
Iteration 12900: Loss = -11856.616890066485
3
Iteration 13000: Loss = -11856.436147464281
Iteration 13100: Loss = -11856.436723745288
1
Iteration 13200: Loss = -11856.437890353944
2
Iteration 13300: Loss = -11856.436845855818
3
Iteration 13400: Loss = -11856.453982252124
4
Iteration 13500: Loss = -11856.483463350949
5
Iteration 13600: Loss = -11856.449998818314
6
Iteration 13700: Loss = -11856.444429845484
7
Iteration 13800: Loss = -11856.447802428362
8
Iteration 13900: Loss = -11856.44503638846
9
Iteration 14000: Loss = -11856.564233387577
10
Iteration 14100: Loss = -11856.436297758479
11
Iteration 14200: Loss = -11856.43647287834
12
Iteration 14300: Loss = -11856.441966589166
13
Iteration 14400: Loss = -11856.447353678985
14
Iteration 14500: Loss = -11856.438341735357
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[0.7635, 0.2365],
        [0.3420, 0.6580]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1329, 0.8671], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3921, 0.0975],
         [0.6118, 0.2127]],

        [[0.6168, 0.1040],
         [0.6420, 0.5051]],

        [[0.7083, 0.1024],
         [0.5415, 0.7254]],

        [[0.7075, 0.1075],
         [0.7116, 0.5477]],

        [[0.6812, 0.1041],
         [0.5296, 0.7062]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.033695242120345935
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5767598873144058
Average Adjusted Rand Index: 0.8067390484240692
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21598.27461996833
Iteration 100: Loss = -12485.426008418319
Iteration 200: Loss = -12395.229238438249
Iteration 300: Loss = -12057.99793954855
Iteration 400: Loss = -12047.828039143262
Iteration 500: Loss = -12046.543426277793
Iteration 600: Loss = -12038.921013986523
Iteration 700: Loss = -12038.510454619944
Iteration 800: Loss = -12028.36229763189
Iteration 900: Loss = -12010.669586179714
Iteration 1000: Loss = -12004.81905595365
Iteration 1100: Loss = -12003.579355286964
Iteration 1200: Loss = -12002.69007273495
Iteration 1300: Loss = -11995.060075592903
Iteration 1400: Loss = -11989.949926335354
Iteration 1500: Loss = -11989.224384532285
Iteration 1600: Loss = -11987.867196285153
Iteration 1700: Loss = -11987.784694732434
Iteration 1800: Loss = -11987.768156901351
Iteration 1900: Loss = -11987.752735523613
Iteration 2000: Loss = -11987.746398233887
Iteration 2100: Loss = -11987.741965807452
Iteration 2200: Loss = -11987.738437606271
Iteration 2300: Loss = -11987.748686962277
1
Iteration 2400: Loss = -11987.733257873042
Iteration 2500: Loss = -11987.731586334685
Iteration 2600: Loss = -11987.745689298858
1
Iteration 2700: Loss = -11987.73199824843
2
Iteration 2800: Loss = -11987.726540191217
Iteration 2900: Loss = -11987.725016114693
Iteration 3000: Loss = -11987.724197244796
Iteration 3100: Loss = -11987.722846502958
Iteration 3200: Loss = -11987.722210246491
Iteration 3300: Loss = -11987.721215245168
Iteration 3400: Loss = -11987.720843829926
Iteration 3500: Loss = -11987.719640091424
Iteration 3600: Loss = -11987.718120856023
Iteration 3700: Loss = -11987.711726313608
Iteration 3800: Loss = -11987.710913745195
Iteration 3900: Loss = -11987.7093588656
Iteration 4000: Loss = -11987.709134940795
Iteration 4100: Loss = -11987.70831979381
Iteration 4200: Loss = -11987.707895174612
Iteration 4300: Loss = -11987.70735187669
Iteration 4400: Loss = -11987.571176482821
Iteration 4500: Loss = -11987.58470650523
1
Iteration 4600: Loss = -11987.56856262709
Iteration 4700: Loss = -11987.568225073743
Iteration 4800: Loss = -11987.314653014562
Iteration 4900: Loss = -11987.27289604449
Iteration 5000: Loss = -11987.268979051487
Iteration 5100: Loss = -11987.272459122361
1
Iteration 5200: Loss = -11987.277178299677
2
Iteration 5300: Loss = -11987.268767244577
Iteration 5400: Loss = -11987.268360203256
Iteration 5500: Loss = -11987.269195802908
1
Iteration 5600: Loss = -11987.268159752075
Iteration 5700: Loss = -11987.267869725394
Iteration 5800: Loss = -11987.26953005567
1
Iteration 5900: Loss = -11987.26879346941
2
Iteration 6000: Loss = -11987.267723745119
Iteration 6100: Loss = -11987.267524863431
Iteration 6200: Loss = -11987.268811833015
1
Iteration 6300: Loss = -11987.268171936897
2
Iteration 6400: Loss = -11987.267330416524
Iteration 6500: Loss = -11987.173167187933
Iteration 6600: Loss = -11987.168990543156
Iteration 6700: Loss = -11987.173242525683
1
Iteration 6800: Loss = -11987.168127918843
Iteration 6900: Loss = -11987.166367146923
Iteration 7000: Loss = -11987.171083326552
1
Iteration 7100: Loss = -11987.166238236356
Iteration 7200: Loss = -11987.168052167235
1
Iteration 7300: Loss = -11987.166138684923
Iteration 7400: Loss = -11987.296566763303
1
Iteration 7500: Loss = -11987.166026266435
Iteration 7600: Loss = -11987.16599534852
Iteration 7700: Loss = -11987.175772230126
1
Iteration 7800: Loss = -11987.165908033508
Iteration 7900: Loss = -11987.165872178717
Iteration 8000: Loss = -11987.172886056333
1
Iteration 8100: Loss = -11987.165836021753
Iteration 8200: Loss = -11987.165804579643
Iteration 8300: Loss = -11987.165876648995
Iteration 8400: Loss = -11987.165739000164
Iteration 8500: Loss = -11987.165728080923
Iteration 8600: Loss = -11987.165770275247
Iteration 8700: Loss = -11987.165663498503
Iteration 8800: Loss = -11987.165991181904
1
Iteration 8900: Loss = -11987.165627828203
Iteration 9000: Loss = -11987.165615593867
Iteration 9100: Loss = -11987.165575340265
Iteration 9200: Loss = -11987.165723869211
1
Iteration 9300: Loss = -11987.165636904328
Iteration 9400: Loss = -11987.165561644
Iteration 9500: Loss = -11987.171222350205
1
Iteration 9600: Loss = -11987.165502306732
Iteration 9700: Loss = -11987.165650456109
1
Iteration 9800: Loss = -11987.166300340998
2
Iteration 9900: Loss = -11987.29212794349
3
Iteration 10000: Loss = -11987.173727110248
4
Iteration 10100: Loss = -11987.16561964149
5
Iteration 10200: Loss = -11987.1664166192
6
Iteration 10300: Loss = -11987.162293183075
Iteration 10400: Loss = -11987.16247827438
1
Iteration 10500: Loss = -11987.203784315287
2
Iteration 10600: Loss = -11987.16213317479
Iteration 10700: Loss = -11987.172387826391
1
Iteration 10800: Loss = -11987.162188893702
Iteration 10900: Loss = -11987.162267329093
Iteration 11000: Loss = -11987.192368983637
1
Iteration 11100: Loss = -11987.16211549629
Iteration 11200: Loss = -11987.191350585352
1
Iteration 11300: Loss = -11987.16214054776
Iteration 11400: Loss = -11987.16587045681
1
Iteration 11500: Loss = -11987.165694663694
2
Iteration 11600: Loss = -11987.165054347623
3
Iteration 11700: Loss = -11987.161771019877
Iteration 11800: Loss = -11987.139593418793
Iteration 11900: Loss = -11987.139449330622
Iteration 12000: Loss = -11983.063117587999
Iteration 12100: Loss = -11983.065669991305
1
Iteration 12200: Loss = -11983.067672102135
2
Iteration 12300: Loss = -11983.103283682798
3
Iteration 12400: Loss = -11983.047074955484
Iteration 12500: Loss = -11983.03960650451
Iteration 12600: Loss = -11983.063119986115
1
Iteration 12700: Loss = -11983.034322564252
Iteration 12800: Loss = -11983.055713197677
1
Iteration 12900: Loss = -11983.034885545872
2
Iteration 13000: Loss = -11983.016023125234
Iteration 13100: Loss = -11983.019735827567
1
Iteration 13200: Loss = -11983.113675424527
2
Iteration 13300: Loss = -11983.018651146456
3
Iteration 13400: Loss = -11983.017884334837
4
Iteration 13500: Loss = -11983.02613364518
5
Iteration 13600: Loss = -11983.011767479748
Iteration 13700: Loss = -11983.057385788034
1
Iteration 13800: Loss = -11983.012906222984
2
Iteration 13900: Loss = -11983.011541363621
Iteration 14000: Loss = -11983.215679844543
1
Iteration 14100: Loss = -11983.021478656772
2
Iteration 14200: Loss = -11983.024239538567
3
Iteration 14300: Loss = -11983.004600349259
Iteration 14400: Loss = -11983.019399198254
1
Iteration 14500: Loss = -11983.007720572314
2
Iteration 14600: Loss = -11983.05005653269
3
Iteration 14700: Loss = -11983.007229262506
4
Iteration 14800: Loss = -11983.004766493023
5
Iteration 14900: Loss = -11983.013045828577
6
Iteration 15000: Loss = -11983.004654680599
Iteration 15100: Loss = -11983.00611190719
1
Iteration 15200: Loss = -11983.017032003278
2
Iteration 15300: Loss = -11983.007525307805
3
Iteration 15400: Loss = -11983.010466000807
4
Iteration 15500: Loss = -11983.004350497911
Iteration 15600: Loss = -11983.018755158937
1
Iteration 15700: Loss = -11983.046710844104
2
Iteration 15800: Loss = -11983.00544371422
3
Iteration 15900: Loss = -11983.004453617628
4
Iteration 16000: Loss = -11983.003836361535
Iteration 16100: Loss = -11983.007312223957
1
Iteration 16200: Loss = -11983.008239343888
2
Iteration 16300: Loss = -11983.00908358661
3
Iteration 16400: Loss = -11983.004598556972
4
Iteration 16500: Loss = -11983.11064493119
5
Iteration 16600: Loss = -11983.003594003285
Iteration 16700: Loss = -11983.012257734099
1
Iteration 16800: Loss = -11983.02078961271
2
Iteration 16900: Loss = -11983.0056433426
3
Iteration 17000: Loss = -11983.006313680968
4
Iteration 17100: Loss = -11983.026873555056
5
Iteration 17200: Loss = -11983.005338532486
6
Iteration 17300: Loss = -11983.130011781586
7
Iteration 17400: Loss = -11983.004131124735
8
Iteration 17500: Loss = -11983.20218069103
9
Iteration 17600: Loss = -11983.00934203843
10
Iteration 17700: Loss = -11983.00971120123
11
Iteration 17800: Loss = -11983.02043393958
12
Iteration 17900: Loss = -11983.005333763056
13
Iteration 18000: Loss = -11983.015314709219
14
Iteration 18100: Loss = -11983.006950540122
15
Stopping early at iteration 18100 due to no improvement.
pi: tensor([[0.3334, 0.6666],
        [0.5880, 0.4120]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3186, 0.6814], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3626, 0.1026],
         [0.7087, 0.2504]],

        [[0.7147, 0.1038],
         [0.6553, 0.6400]],

        [[0.5748, 0.1002],
         [0.6025, 0.6765]],

        [[0.5791, 0.1074],
         [0.5055, 0.6490]],

        [[0.6146, 0.1037],
         [0.7255, 0.6798]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 78
Adjusted Rand Index: 0.30727461959311375
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 86
Adjusted Rand Index: 0.5139202893869905
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.11658245570352326
Average Adjusted Rand Index: 0.7642389817960209
11731.177692079336
[0.5767598873144058, 0.11658245570352326] [0.8067390484240692, 0.7642389817960209] [11856.438341735357, 11983.006950540122]
-------------------------------------
This iteration is 52
True Objective function: Loss = -11533.567988698027
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23603.237376219375
Iteration 100: Loss = -11531.47921798684
Iteration 200: Loss = -11529.23803643376
Iteration 300: Loss = -11528.74965816005
Iteration 400: Loss = -11528.535886925882
Iteration 500: Loss = -11528.421626792424
Iteration 600: Loss = -11528.352215215047
Iteration 700: Loss = -11528.30643452109
Iteration 800: Loss = -11528.27449253474
Iteration 900: Loss = -11528.251217828747
Iteration 1000: Loss = -11528.23367229906
Iteration 1100: Loss = -11528.220068382612
Iteration 1200: Loss = -11528.2092989688
Iteration 1300: Loss = -11528.200606377652
Iteration 1400: Loss = -11528.193514997825
Iteration 1500: Loss = -11528.187625260192
Iteration 1600: Loss = -11528.182646899511
Iteration 1700: Loss = -11528.178447763446
Iteration 1800: Loss = -11528.174832956462
Iteration 1900: Loss = -11528.171744311396
Iteration 2000: Loss = -11528.169036203777
Iteration 2100: Loss = -11528.166686864924
Iteration 2200: Loss = -11528.164610826338
Iteration 2300: Loss = -11528.162750963522
Iteration 2400: Loss = -11528.161126593412
Iteration 2500: Loss = -11528.159651827676
Iteration 2600: Loss = -11528.158403187676
Iteration 2700: Loss = -11528.15722905607
Iteration 2800: Loss = -11528.15615161171
Iteration 2900: Loss = -11528.155844214732
Iteration 3000: Loss = -11528.154325168847
Iteration 3100: Loss = -11528.15352706052
Iteration 3200: Loss = -11528.152874136951
Iteration 3300: Loss = -11528.152141528582
Iteration 3400: Loss = -11528.15154265371
Iteration 3500: Loss = -11528.151034429866
Iteration 3600: Loss = -11528.150519034241
Iteration 3700: Loss = -11528.15021464398
Iteration 3800: Loss = -11528.149709473639
Iteration 3900: Loss = -11528.149254130807
Iteration 4000: Loss = -11528.149938012803
1
Iteration 4100: Loss = -11528.14848811664
Iteration 4200: Loss = -11528.14816804969
Iteration 4300: Loss = -11528.147891755545
Iteration 4400: Loss = -11528.147611790533
Iteration 4500: Loss = -11528.147339761394
Iteration 4600: Loss = -11528.147272503993
Iteration 4700: Loss = -11528.147450044185
1
Iteration 4800: Loss = -11528.14666532754
Iteration 4900: Loss = -11528.146724834478
Iteration 5000: Loss = -11528.147501389478
1
Iteration 5100: Loss = -11528.146317634271
Iteration 5200: Loss = -11528.14597451105
Iteration 5300: Loss = -11528.145862171656
Iteration 5400: Loss = -11528.145704850906
Iteration 5500: Loss = -11528.145624214703
Iteration 5600: Loss = -11528.147264222835
1
Iteration 5700: Loss = -11528.14533420945
Iteration 5800: Loss = -11528.145434805096
1
Iteration 5900: Loss = -11528.1530927536
2
Iteration 6000: Loss = -11528.144995944884
Iteration 6100: Loss = -11528.145204602268
1
Iteration 6200: Loss = -11528.144838946759
Iteration 6300: Loss = -11528.144920306902
Iteration 6400: Loss = -11528.144717558422
Iteration 6500: Loss = -11528.144691608704
Iteration 6600: Loss = -11528.144697502907
Iteration 6700: Loss = -11528.144653445836
Iteration 6800: Loss = -11528.14443887081
Iteration 6900: Loss = -11528.146414765055
1
Iteration 7000: Loss = -11528.145278809005
2
Iteration 7100: Loss = -11528.144973281273
3
Iteration 7200: Loss = -11528.14494627419
4
Iteration 7300: Loss = -11528.14716472659
5
Iteration 7400: Loss = -11528.149795935029
6
Iteration 7500: Loss = -11528.144070619062
Iteration 7600: Loss = -11528.14410326575
Iteration 7700: Loss = -11528.14431460307
1
Iteration 7800: Loss = -11528.143993362026
Iteration 7900: Loss = -11528.1441148022
1
Iteration 8000: Loss = -11528.143917749021
Iteration 8100: Loss = -11528.171662772844
1
Iteration 8200: Loss = -11528.148650983225
2
Iteration 8300: Loss = -11528.143848556874
Iteration 8400: Loss = -11528.143882375083
Iteration 8500: Loss = -11528.144085094109
1
Iteration 8600: Loss = -11528.14383424936
Iteration 8700: Loss = -11528.14378791342
Iteration 8800: Loss = -11528.14377522589
Iteration 8900: Loss = -11528.147482993396
1
Iteration 9000: Loss = -11528.143759412256
Iteration 9100: Loss = -11528.143695291556
Iteration 9200: Loss = -11528.145056025158
1
Iteration 9300: Loss = -11528.228442754673
2
Iteration 9400: Loss = -11528.143654920006
Iteration 9500: Loss = -11528.144649684042
1
Iteration 9600: Loss = -11528.144155361892
2
Iteration 9700: Loss = -11528.145364550908
3
Iteration 9800: Loss = -11528.160321926129
4
Iteration 9900: Loss = -11528.143604321045
Iteration 10000: Loss = -11528.143862290119
1
Iteration 10100: Loss = -11528.150608324435
2
Iteration 10200: Loss = -11528.172724108492
3
Iteration 10300: Loss = -11528.23597082616
4
Iteration 10400: Loss = -11528.144604925496
5
Iteration 10500: Loss = -11528.143745962805
6
Iteration 10600: Loss = -11528.16438911618
7
Iteration 10700: Loss = -11528.147433278431
8
Iteration 10800: Loss = -11528.144808796458
9
Iteration 10900: Loss = -11528.170905787847
10
Iteration 11000: Loss = -11528.153552407392
11
Iteration 11100: Loss = -11528.144263121023
12
Iteration 11200: Loss = -11528.143555000437
Iteration 11300: Loss = -11528.145020847713
1
Iteration 11400: Loss = -11528.144750408463
2
Iteration 11500: Loss = -11528.171963324683
3
Iteration 11600: Loss = -11528.154345874535
4
Iteration 11700: Loss = -11528.14547018474
5
Iteration 11800: Loss = -11528.143723304333
6
Iteration 11900: Loss = -11528.14873904059
7
Iteration 12000: Loss = -11528.143694692766
8
Iteration 12100: Loss = -11528.14502402192
9
Iteration 12200: Loss = -11528.14347236202
Iteration 12300: Loss = -11528.144486785182
1
Iteration 12400: Loss = -11528.143459924762
Iteration 12500: Loss = -11528.143676137695
1
Iteration 12600: Loss = -11528.143439822956
Iteration 12700: Loss = -11528.152342703488
1
Iteration 12800: Loss = -11528.144335041441
2
Iteration 12900: Loss = -11528.382488679752
3
Iteration 13000: Loss = -11528.143499158958
Iteration 13100: Loss = -11528.162430264634
1
Iteration 13200: Loss = -11528.158436051206
2
Iteration 13300: Loss = -11528.222897738218
3
Iteration 13400: Loss = -11528.14402404754
4
Iteration 13500: Loss = -11528.146587414607
5
Iteration 13600: Loss = -11528.143620621166
6
Iteration 13700: Loss = -11528.144753046177
7
Iteration 13800: Loss = -11528.144446887738
8
Iteration 13900: Loss = -11528.17659781494
9
Iteration 14000: Loss = -11528.218953950509
10
Iteration 14100: Loss = -11528.157688891526
11
Iteration 14200: Loss = -11528.205187383652
12
Iteration 14300: Loss = -11528.153316691823
13
Iteration 14400: Loss = -11528.156780043571
14
Iteration 14500: Loss = -11528.199508769445
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[0.7295, 0.2705],
        [0.2648, 0.7352]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5400, 0.4600], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3965, 0.0902],
         [0.6787, 0.2033]],

        [[0.7268, 0.0942],
         [0.7105, 0.5275]],

        [[0.5341, 0.1054],
         [0.6212, 0.6418]],

        [[0.6038, 0.0960],
         [0.6337, 0.5534]],

        [[0.6361, 0.1009],
         [0.5157, 0.5721]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20055.542913781585
Iteration 100: Loss = -12353.01694182854
Iteration 200: Loss = -12045.213901972318
Iteration 300: Loss = -11547.937239782064
Iteration 400: Loss = -11531.287147136676
Iteration 500: Loss = -11530.762661983681
Iteration 600: Loss = -11530.506519405086
Iteration 700: Loss = -11530.349254591052
Iteration 800: Loss = -11530.154050812205
Iteration 900: Loss = -11528.456884196896
Iteration 1000: Loss = -11528.405034457153
Iteration 1100: Loss = -11528.365930314296
Iteration 1200: Loss = -11528.334226847497
Iteration 1300: Loss = -11528.304611284748
Iteration 1400: Loss = -11528.27917269536
Iteration 1500: Loss = -11528.262437761186
Iteration 1600: Loss = -11528.248867349586
Iteration 1700: Loss = -11528.237488962115
Iteration 1800: Loss = -11528.227775190104
Iteration 1900: Loss = -11528.219491113752
Iteration 2000: Loss = -11528.212317647787
Iteration 2100: Loss = -11528.206397257689
Iteration 2200: Loss = -11528.200642611213
Iteration 2300: Loss = -11528.195830941575
Iteration 2400: Loss = -11528.19158259539
Iteration 2500: Loss = -11528.187768872105
Iteration 2600: Loss = -11528.184256140246
Iteration 2700: Loss = -11528.18133257558
Iteration 2800: Loss = -11528.177958775008
Iteration 2900: Loss = -11528.174947535825
Iteration 3000: Loss = -11528.172534689937
Iteration 3100: Loss = -11528.170521597242
Iteration 3200: Loss = -11528.168350374654
Iteration 3300: Loss = -11528.170107073016
1
Iteration 3400: Loss = -11528.16502202157
Iteration 3500: Loss = -11528.16365149185
Iteration 3600: Loss = -11528.164903888119
1
Iteration 3700: Loss = -11528.161117125816
Iteration 3800: Loss = -11528.159997749348
Iteration 3900: Loss = -11528.159069668385
Iteration 4000: Loss = -11528.158093603748
Iteration 4100: Loss = -11528.157213169243
Iteration 4200: Loss = -11528.159080011515
1
Iteration 4300: Loss = -11528.155558457109
Iteration 4400: Loss = -11528.155644745131
Iteration 4500: Loss = -11528.161272013554
1
Iteration 4600: Loss = -11528.155036169639
Iteration 4700: Loss = -11528.153138188802
Iteration 4800: Loss = -11528.15435058477
1
Iteration 4900: Loss = -11528.152029114997
Iteration 5000: Loss = -11528.15156840026
Iteration 5100: Loss = -11528.152212649518
1
Iteration 5200: Loss = -11528.156985014888
2
Iteration 5300: Loss = -11528.15117616438
Iteration 5400: Loss = -11528.14992836136
Iteration 5500: Loss = -11528.149718188413
Iteration 5600: Loss = -11528.149290067317
Iteration 5700: Loss = -11528.149333751046
Iteration 5800: Loss = -11528.157383510294
1
Iteration 5900: Loss = -11528.148959760567
Iteration 6000: Loss = -11528.148113804602
Iteration 6100: Loss = -11528.150902974608
1
Iteration 6200: Loss = -11528.147676922383
Iteration 6300: Loss = -11528.147329470057
Iteration 6400: Loss = -11528.147225558583
Iteration 6500: Loss = -11528.165149102571
1
Iteration 6600: Loss = -11528.150133981419
2
Iteration 6700: Loss = -11528.146788123708
Iteration 6800: Loss = -11528.146886437204
Iteration 6900: Loss = -11528.151407007335
1
Iteration 7000: Loss = -11528.197022074213
2
Iteration 7100: Loss = -11528.146023473766
Iteration 7200: Loss = -11528.147706117477
1
Iteration 7300: Loss = -11528.14580496015
Iteration 7400: Loss = -11528.146138005697
1
Iteration 7500: Loss = -11528.1455667325
Iteration 7600: Loss = -11528.154105345873
1
Iteration 7700: Loss = -11528.145418040327
Iteration 7800: Loss = -11528.145353999615
Iteration 7900: Loss = -11528.145537114262
1
Iteration 8000: Loss = -11528.145118676735
Iteration 8100: Loss = -11528.145085736207
Iteration 8200: Loss = -11528.145162243745
Iteration 8300: Loss = -11528.144950707889
Iteration 8400: Loss = -11528.14486360801
Iteration 8500: Loss = -11528.1474471698
1
Iteration 8600: Loss = -11528.144758248842
Iteration 8700: Loss = -11528.144690428864
Iteration 8800: Loss = -11528.14482052338
1
Iteration 8900: Loss = -11528.144612549413
Iteration 9000: Loss = -11528.14443193437
Iteration 9100: Loss = -11528.147293397986
1
Iteration 9200: Loss = -11528.163779461707
2
Iteration 9300: Loss = -11528.168111602743
3
Iteration 9400: Loss = -11528.155795358733
4
Iteration 9500: Loss = -11528.14420934358
Iteration 9600: Loss = -11528.146541894514
1
Iteration 9700: Loss = -11528.16812486138
2
Iteration 9800: Loss = -11528.14497065547
3
Iteration 9900: Loss = -11528.144202103338
Iteration 10000: Loss = -11528.144504749263
1
Iteration 10100: Loss = -11528.144427077828
2
Iteration 10200: Loss = -11528.2331809586
3
Iteration 10300: Loss = -11528.144065044435
Iteration 10400: Loss = -11528.145905537674
1
Iteration 10500: Loss = -11528.145120192146
2
Iteration 10600: Loss = -11528.1454009207
3
Iteration 10700: Loss = -11528.176471556633
4
Iteration 10800: Loss = -11528.145580939725
5
Iteration 10900: Loss = -11528.144092638722
Iteration 11000: Loss = -11528.249383348058
1
Iteration 11100: Loss = -11528.145587544837
2
Iteration 11200: Loss = -11528.143764559349
Iteration 11300: Loss = -11528.147005188805
1
Iteration 11400: Loss = -11528.144116876165
2
Iteration 11500: Loss = -11528.146626896794
3
Iteration 11600: Loss = -11528.155147252317
4
Iteration 11700: Loss = -11528.155464566515
5
Iteration 11800: Loss = -11528.209110419933
6
Iteration 11900: Loss = -11528.165702638142
7
Iteration 12000: Loss = -11528.144524562838
8
Iteration 12100: Loss = -11528.143753032733
Iteration 12200: Loss = -11528.145918229828
1
Iteration 12300: Loss = -11528.160081695423
2
Iteration 12400: Loss = -11528.143679002536
Iteration 12500: Loss = -11528.143799363472
1
Iteration 12600: Loss = -11528.14520961682
2
Iteration 12700: Loss = -11528.144030709533
3
Iteration 12800: Loss = -11528.148414964895
4
Iteration 12900: Loss = -11528.144093098785
5
Iteration 13000: Loss = -11528.144620038753
6
Iteration 13100: Loss = -11528.163459535535
7
Iteration 13200: Loss = -11528.200606641218
8
Iteration 13300: Loss = -11528.143620971106
Iteration 13400: Loss = -11528.143735377418
1
Iteration 13500: Loss = -11528.241995137018
2
Iteration 13600: Loss = -11528.143610906267
Iteration 13700: Loss = -11528.144930156628
1
Iteration 13800: Loss = -11528.24679532602
2
Iteration 13900: Loss = -11528.158175279044
3
Iteration 14000: Loss = -11528.266138299112
4
Iteration 14100: Loss = -11528.163709191638
5
Iteration 14200: Loss = -11528.143670912457
Iteration 14300: Loss = -11528.147257635748
1
Iteration 14400: Loss = -11528.15011487143
2
Iteration 14500: Loss = -11528.1466942765
3
Iteration 14600: Loss = -11528.148235072113
4
Iteration 14700: Loss = -11528.215420073326
5
Iteration 14800: Loss = -11528.242180227628
6
Iteration 14900: Loss = -11528.144734431373
7
Iteration 15000: Loss = -11528.144153325815
8
Iteration 15100: Loss = -11528.159808132887
9
Iteration 15200: Loss = -11528.14362886895
Iteration 15300: Loss = -11528.149787278278
1
Iteration 15400: Loss = -11528.143712866968
Iteration 15500: Loss = -11528.144886208214
1
Iteration 15600: Loss = -11528.159169482524
2
Iteration 15700: Loss = -11528.144891599237
3
Iteration 15800: Loss = -11528.144581700344
4
Iteration 15900: Loss = -11528.148145583846
5
Iteration 16000: Loss = -11528.143743654688
Iteration 16100: Loss = -11528.143937596622
1
Iteration 16200: Loss = -11528.154431711977
2
Iteration 16300: Loss = -11528.143622030451
Iteration 16400: Loss = -11528.147263283094
1
Iteration 16500: Loss = -11528.143867135286
2
Iteration 16600: Loss = -11528.143665982883
Iteration 16700: Loss = -11528.147091637264
1
Iteration 16800: Loss = -11528.144885156478
2
Iteration 16900: Loss = -11528.229320500679
3
Iteration 17000: Loss = -11528.156193679273
4
Iteration 17100: Loss = -11528.230084439885
5
Iteration 17200: Loss = -11528.227086250934
6
Iteration 17300: Loss = -11528.188739758003
7
Iteration 17400: Loss = -11528.15568516254
8
Iteration 17500: Loss = -11528.144958743369
9
Iteration 17600: Loss = -11528.14463040775
10
Iteration 17700: Loss = -11528.146941147943
11
Iteration 17800: Loss = -11528.144040370686
12
Iteration 17900: Loss = -11528.143554420698
Iteration 18000: Loss = -11528.146064125636
1
Iteration 18100: Loss = -11528.146991430744
2
Iteration 18200: Loss = -11528.167612058709
3
Iteration 18300: Loss = -11528.143623410742
Iteration 18400: Loss = -11528.143631298288
Iteration 18500: Loss = -11528.144998476342
1
Iteration 18600: Loss = -11528.162542353668
2
Iteration 18700: Loss = -11528.143508144622
Iteration 18800: Loss = -11528.152613960896
1
Iteration 18900: Loss = -11528.144310577114
2
Iteration 19000: Loss = -11528.144003914991
3
Iteration 19100: Loss = -11528.159855782773
4
Iteration 19200: Loss = -11528.18998323069
5
Iteration 19300: Loss = -11528.16515979885
6
Iteration 19400: Loss = -11528.150165833411
7
Iteration 19500: Loss = -11528.148808607839
8
Iteration 19600: Loss = -11528.143618154825
9
Iteration 19700: Loss = -11528.146074264856
10
Iteration 19800: Loss = -11528.156469084264
11
Iteration 19900: Loss = -11528.143864128102
12
pi: tensor([[0.7290, 0.2710],
        [0.2655, 0.7345]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5402, 0.4598], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3980, 0.0891],
         [0.6607, 0.2030]],

        [[0.5821, 0.0949],
         [0.6097, 0.6968]],

        [[0.7172, 0.1052],
         [0.7100, 0.5600]],

        [[0.5107, 0.0961],
         [0.6974, 0.6040]],

        [[0.5478, 0.1003],
         [0.7059, 0.5460]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999711388391
Average Adjusted Rand Index: 0.9919992163297293
11533.567988698027
[0.9919999711388391, 0.9919999711388391] [0.9919992163297293, 0.9919992163297293] [11528.199508769445, 11528.168488993153]
-------------------------------------
This iteration is 53
True Objective function: Loss = -11415.882359727699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19694.26418160413
Iteration 100: Loss = -12152.466645178962
Iteration 200: Loss = -12123.740665786756
Iteration 300: Loss = -11698.96857897314
Iteration 400: Loss = -11539.279080583496
Iteration 500: Loss = -11465.224195024126
Iteration 600: Loss = -11439.689924835735
Iteration 700: Loss = -11436.489939843657
Iteration 800: Loss = -11415.868550253123
Iteration 900: Loss = -11415.716630242223
Iteration 1000: Loss = -11415.626127709555
Iteration 1100: Loss = -11415.568029084094
Iteration 1200: Loss = -11415.525829631319
Iteration 1300: Loss = -11415.493027730896
Iteration 1400: Loss = -11415.465825003665
Iteration 1500: Loss = -11415.440626889835
Iteration 1600: Loss = -11415.408615497316
Iteration 1700: Loss = -11411.837906875975
Iteration 1800: Loss = -11410.788794166816
Iteration 1900: Loss = -11410.777235134285
Iteration 2000: Loss = -11410.767682440666
Iteration 2100: Loss = -11410.759539355984
Iteration 2200: Loss = -11410.752568852035
Iteration 2300: Loss = -11410.746702471735
Iteration 2400: Loss = -11410.741696814766
Iteration 2500: Loss = -11410.737668146374
Iteration 2600: Loss = -11410.73368304985
Iteration 2700: Loss = -11410.7303955008
Iteration 2800: Loss = -11410.758403163421
1
Iteration 2900: Loss = -11410.724804159143
Iteration 3000: Loss = -11410.722375547557
Iteration 3100: Loss = -11410.720269206362
Iteration 3200: Loss = -11410.718269947176
Iteration 3300: Loss = -11410.716505723678
Iteration 3400: Loss = -11410.714909029917
Iteration 3500: Loss = -11410.713455595174
Iteration 3600: Loss = -11410.712602465746
Iteration 3700: Loss = -11410.710823252128
Iteration 3800: Loss = -11410.709900280744
Iteration 3900: Loss = -11410.708638139367
Iteration 4000: Loss = -11410.70971344694
1
Iteration 4100: Loss = -11410.706765003884
Iteration 4200: Loss = -11410.706526896785
Iteration 4300: Loss = -11410.705848428252
Iteration 4400: Loss = -11410.70452145492
Iteration 4500: Loss = -11410.705352246385
1
Iteration 4600: Loss = -11410.703205406438
Iteration 4700: Loss = -11410.70276478227
Iteration 4800: Loss = -11410.702117927378
Iteration 4900: Loss = -11410.706561171035
1
Iteration 5000: Loss = -11410.706946013597
2
Iteration 5100: Loss = -11410.700735997405
Iteration 5200: Loss = -11410.700323561794
Iteration 5300: Loss = -11410.702191468128
1
Iteration 5400: Loss = -11410.699613837394
Iteration 5500: Loss = -11410.70291235672
1
Iteration 5600: Loss = -11410.710145768026
2
Iteration 5700: Loss = -11410.698740118434
Iteration 5800: Loss = -11410.701015046707
1
Iteration 5900: Loss = -11410.699511074414
2
Iteration 6000: Loss = -11410.699562771786
3
Iteration 6100: Loss = -11410.70565721018
4
Iteration 6200: Loss = -11410.69748678527
Iteration 6300: Loss = -11410.70449690202
1
Iteration 6400: Loss = -11410.697032257101
Iteration 6500: Loss = -11410.697137025818
1
Iteration 6600: Loss = -11410.696707925576
Iteration 6700: Loss = -11410.702693303698
1
Iteration 6800: Loss = -11410.696396824518
Iteration 6900: Loss = -11410.79324695579
1
Iteration 7000: Loss = -11410.696122028787
Iteration 7100: Loss = -11410.733203162152
1
Iteration 7200: Loss = -11410.695876159785
Iteration 7300: Loss = -11410.711761096458
1
Iteration 7400: Loss = -11410.695640162878
Iteration 7500: Loss = -11410.823973778663
1
Iteration 7600: Loss = -11410.695355047743
Iteration 7700: Loss = -11410.695210605927
Iteration 7800: Loss = -11410.695417078312
1
Iteration 7900: Loss = -11410.695037238289
Iteration 8000: Loss = -11410.695048016885
Iteration 8100: Loss = -11410.70577570079
1
Iteration 8200: Loss = -11410.69865823612
2
Iteration 8300: Loss = -11410.7930332896
3
Iteration 8400: Loss = -11410.694478986914
Iteration 8500: Loss = -11410.745674094484
1
Iteration 8600: Loss = -11410.694355448206
Iteration 8700: Loss = -11410.729779062034
1
Iteration 8800: Loss = -11410.694241429774
Iteration 8900: Loss = -11410.694189688795
Iteration 9000: Loss = -11410.69624679316
1
Iteration 9100: Loss = -11410.694843785093
2
Iteration 9200: Loss = -11410.696215779128
3
Iteration 9300: Loss = -11410.697565593553
4
Iteration 9400: Loss = -11410.745166692668
5
Iteration 9500: Loss = -11410.695029384688
6
Iteration 9600: Loss = -11410.697778903761
7
Iteration 9700: Loss = -11410.70846789895
8
Iteration 9800: Loss = -11410.694665437044
9
Iteration 9900: Loss = -11410.776158085939
10
Iteration 10000: Loss = -11410.695043072868
11
Iteration 10100: Loss = -11410.722960020845
12
Iteration 10200: Loss = -11410.693956480249
Iteration 10300: Loss = -11410.70816025965
1
Iteration 10400: Loss = -11410.697116268893
2
Iteration 10500: Loss = -11410.699123210705
3
Iteration 10600: Loss = -11410.696041084866
4
Iteration 10700: Loss = -11410.70184727945
5
Iteration 10800: Loss = -11410.700007521182
6
Iteration 10900: Loss = -11410.700403213807
7
Iteration 11000: Loss = -11410.697715584261
8
Iteration 11100: Loss = -11410.694867231612
9
Iteration 11200: Loss = -11410.705520617392
10
Iteration 11300: Loss = -11410.725894786336
11
Iteration 11400: Loss = -11410.707392206077
12
Iteration 11500: Loss = -11410.695178244421
13
Iteration 11600: Loss = -11410.738686818015
14
Iteration 11700: Loss = -11410.694278421202
15
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[0.7369, 0.2631],
        [0.2932, 0.7068]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5103, 0.4897], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1983, 0.1014],
         [0.6317, 0.3996]],

        [[0.5158, 0.0969],
         [0.7008, 0.5234]],

        [[0.7206, 0.1053],
         [0.5267, 0.6477]],

        [[0.5822, 0.0946],
         [0.6507, 0.5673]],

        [[0.6766, 0.0951],
         [0.5703, 0.6758]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9840320626466431
Average Adjusted Rand Index: 0.9839985580570193
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23409.041777260853
Iteration 100: Loss = -12149.74327645526
Iteration 200: Loss = -11790.56123914136
Iteration 300: Loss = -11421.946691152247
Iteration 400: Loss = -11420.087624368647
Iteration 500: Loss = -11414.953683146125
Iteration 600: Loss = -11411.369002084166
Iteration 700: Loss = -11411.198537073287
Iteration 800: Loss = -11411.087869569383
Iteration 900: Loss = -11411.010684675959
Iteration 1000: Loss = -11410.954967814172
Iteration 1100: Loss = -11410.91409347578
Iteration 1200: Loss = -11410.880846218623
Iteration 1300: Loss = -11410.848408421083
Iteration 1400: Loss = -11410.8234629237
Iteration 1500: Loss = -11410.806770974868
Iteration 1600: Loss = -11410.793398484608
Iteration 1700: Loss = -11410.782343481025
Iteration 1800: Loss = -11410.77287748645
Iteration 1900: Loss = -11410.764913438077
Iteration 2000: Loss = -11410.760559495357
Iteration 2100: Loss = -11410.751912635775
Iteration 2200: Loss = -11410.746592368265
Iteration 2300: Loss = -11410.741956290105
Iteration 2400: Loss = -11410.737871799436
Iteration 2500: Loss = -11410.734226431443
Iteration 2600: Loss = -11410.730904081758
Iteration 2700: Loss = -11410.727990829642
Iteration 2800: Loss = -11410.725340392566
Iteration 2900: Loss = -11410.722964197876
Iteration 3000: Loss = -11410.720897209265
Iteration 3100: Loss = -11410.71883801958
Iteration 3200: Loss = -11410.717009348891
Iteration 3300: Loss = -11410.722963963017
1
Iteration 3400: Loss = -11410.71390939709
Iteration 3500: Loss = -11410.712552886689
Iteration 3600: Loss = -11410.71140709191
Iteration 3700: Loss = -11410.71015904459
Iteration 3800: Loss = -11410.709072330492
Iteration 3900: Loss = -11410.708231923196
Iteration 4000: Loss = -11410.707279464707
Iteration 4100: Loss = -11410.706346841886
Iteration 4200: Loss = -11410.70578602722
Iteration 4300: Loss = -11410.704818407823
Iteration 4400: Loss = -11410.713334231312
1
Iteration 4500: Loss = -11410.708331964797
2
Iteration 4600: Loss = -11410.702945681642
Iteration 4700: Loss = -11410.709150263334
1
Iteration 4800: Loss = -11410.701914930749
Iteration 4900: Loss = -11410.701438303819
Iteration 5000: Loss = -11410.701254411679
Iteration 5100: Loss = -11410.700576817602
Iteration 5200: Loss = -11410.701045848891
1
Iteration 5300: Loss = -11410.711692147075
2
Iteration 5400: Loss = -11410.699500089848
Iteration 5500: Loss = -11410.701407797553
1
Iteration 5600: Loss = -11410.698763205884
Iteration 5700: Loss = -11410.71183981837
1
Iteration 5800: Loss = -11410.698183464905
Iteration 5900: Loss = -11410.69789963359
Iteration 6000: Loss = -11410.698027984568
1
Iteration 6100: Loss = -11410.697387346729
Iteration 6200: Loss = -11410.697172679316
Iteration 6300: Loss = -11410.697140334441
Iteration 6400: Loss = -11410.696762648746
Iteration 6500: Loss = -11410.7124482941
1
Iteration 6600: Loss = -11410.696381283693
Iteration 6700: Loss = -11410.704334534566
1
Iteration 6800: Loss = -11410.69608331155
Iteration 6900: Loss = -11410.696181361986
Iteration 7000: Loss = -11410.696026620568
Iteration 7100: Loss = -11410.695948279717
Iteration 7200: Loss = -11410.695717797442
Iteration 7300: Loss = -11410.695542177835
Iteration 7400: Loss = -11410.708602427532
1
Iteration 7500: Loss = -11410.69522773528
Iteration 7600: Loss = -11410.737623499954
1
Iteration 7700: Loss = -11410.69501463589
Iteration 7800: Loss = -11410.6951982791
1
Iteration 7900: Loss = -11410.69493909191
Iteration 8000: Loss = -11410.695095517502
1
Iteration 8100: Loss = -11410.694924976693
Iteration 8200: Loss = -11410.695328711496
1
Iteration 8300: Loss = -11410.694642468896
Iteration 8400: Loss = -11410.694629119638
Iteration 8500: Loss = -11410.694549647707
Iteration 8600: Loss = -11410.69446341546
Iteration 8700: Loss = -11410.694682914604
1
Iteration 8800: Loss = -11410.694342337605
Iteration 8900: Loss = -11410.694423833738
Iteration 9000: Loss = -11410.699353889195
1
Iteration 9100: Loss = -11410.6946411093
2
Iteration 9200: Loss = -11410.724682753935
3
Iteration 9300: Loss = -11410.694897858526
4
Iteration 9400: Loss = -11410.704767660356
5
Iteration 9500: Loss = -11410.694376738764
Iteration 9600: Loss = -11410.694598779328
1
Iteration 9700: Loss = -11410.69411658217
Iteration 9800: Loss = -11410.696155312986
1
Iteration 9900: Loss = -11410.715104379586
2
Iteration 10000: Loss = -11410.695105147635
3
Iteration 10100: Loss = -11410.695018161414
4
Iteration 10200: Loss = -11410.69700415923
5
Iteration 10300: Loss = -11410.695322736152
6
Iteration 10400: Loss = -11410.694699021855
7
Iteration 10500: Loss = -11410.702943006248
8
Iteration 10600: Loss = -11410.743804992593
9
Iteration 10700: Loss = -11410.782995080212
10
Iteration 10800: Loss = -11410.713627391458
11
Iteration 10900: Loss = -11410.694156258143
Iteration 11000: Loss = -11410.695645016087
1
Iteration 11100: Loss = -11410.694634572388
2
Iteration 11200: Loss = -11410.72924456333
3
Iteration 11300: Loss = -11410.744114306894
4
Iteration 11400: Loss = -11410.698917296064
5
Iteration 11500: Loss = -11410.697772255866
6
Iteration 11600: Loss = -11410.697460912428
7
Iteration 11700: Loss = -11410.69465191841
8
Iteration 11800: Loss = -11410.782903798887
9
Iteration 11900: Loss = -11410.70064400125
10
Iteration 12000: Loss = -11410.69401980907
Iteration 12100: Loss = -11410.731722411338
1
Iteration 12200: Loss = -11410.709739091579
2
Iteration 12300: Loss = -11410.71832015343
3
Iteration 12400: Loss = -11410.695192637022
4
Iteration 12500: Loss = -11410.694521415708
5
Iteration 12600: Loss = -11410.695181554764
6
Iteration 12700: Loss = -11410.695179813842
7
Iteration 12800: Loss = -11410.701473349049
8
Iteration 12900: Loss = -11410.842081574581
9
Iteration 13000: Loss = -11410.69350777179
Iteration 13100: Loss = -11410.694705901164
1
Iteration 13200: Loss = -11410.695532508658
2
Iteration 13300: Loss = -11410.69966444633
3
Iteration 13400: Loss = -11410.694296940856
4
Iteration 13500: Loss = -11410.694413467305
5
Iteration 13600: Loss = -11410.696366803699
6
Iteration 13700: Loss = -11410.710584806831
7
Iteration 13800: Loss = -11410.702396023185
8
Iteration 13900: Loss = -11410.708697373928
9
Iteration 14000: Loss = -11410.69745650817
10
Iteration 14100: Loss = -11410.748367753742
11
Iteration 14200: Loss = -11410.69458727746
12
Iteration 14300: Loss = -11410.708595186483
13
Iteration 14400: Loss = -11410.743146733208
14
Iteration 14500: Loss = -11410.724433313098
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[0.7355, 0.2645],
        [0.2929, 0.7071]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5139, 0.4861], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.1015],
         [0.5551, 0.3993]],

        [[0.7099, 0.0967],
         [0.5113, 0.6812]],

        [[0.6832, 0.1051],
         [0.6430, 0.6987]],

        [[0.6951, 0.0946],
         [0.7300, 0.6330]],

        [[0.6963, 0.0951],
         [0.6984, 0.5376]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9840320626466431
Average Adjusted Rand Index: 0.9839985580570193
11415.882359727699
[0.9840320626466431, 0.9840320626466431] [0.9839985580570193, 0.9839985580570193] [11410.694278421202, 11410.724433313098]
-------------------------------------
This iteration is 54
True Objective function: Loss = -11625.293800397678
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22710.831814132947
Iteration 100: Loss = -12301.83610972873
Iteration 200: Loss = -12282.724807855819
Iteration 300: Loss = -11790.902206030536
Iteration 400: Loss = -11655.647606039573
Iteration 500: Loss = -11624.917741453513
Iteration 600: Loss = -11624.512216703599
Iteration 700: Loss = -11624.311171938962
Iteration 800: Loss = -11624.18938947214
Iteration 900: Loss = -11621.230267658924
Iteration 1000: Loss = -11621.131249000271
Iteration 1100: Loss = -11621.086489901127
Iteration 1200: Loss = -11621.051533535749
Iteration 1300: Loss = -11620.998341261467
Iteration 1400: Loss = -11620.97130454114
Iteration 1500: Loss = -11620.955743137496
Iteration 1600: Loss = -11620.94283641488
Iteration 1700: Loss = -11620.93199324065
Iteration 1800: Loss = -11620.922806605902
Iteration 1900: Loss = -11620.91493136037
Iteration 2000: Loss = -11620.908066788952
Iteration 2100: Loss = -11620.923276042113
1
Iteration 2200: Loss = -11620.896884357666
Iteration 2300: Loss = -11620.892308799423
Iteration 2400: Loss = -11620.888111440489
Iteration 2500: Loss = -11620.884104706642
Iteration 2600: Loss = -11620.878983843022
Iteration 2700: Loss = -11617.784369719988
Iteration 2800: Loss = -11617.772263118513
Iteration 2900: Loss = -11617.769687900045
Iteration 3000: Loss = -11617.777336886096
1
Iteration 3100: Loss = -11617.765450718809
Iteration 3200: Loss = -11617.763637987166
Iteration 3300: Loss = -11617.782616660965
1
Iteration 3400: Loss = -11617.7608924497
Iteration 3500: Loss = -11617.758655999303
Iteration 3600: Loss = -11617.756284358342
Iteration 3700: Loss = -11617.742066957671
Iteration 3800: Loss = -11617.739002939787
Iteration 3900: Loss = -11617.733950072354
Iteration 4000: Loss = -11617.70788688433
Iteration 4100: Loss = -11617.706322442598
Iteration 4200: Loss = -11617.714837086589
1
Iteration 4300: Loss = -11617.684945704556
Iteration 4400: Loss = -11617.667701433005
Iteration 4500: Loss = -11617.102361313091
Iteration 4600: Loss = -11617.101615054873
Iteration 4700: Loss = -11617.104066056425
1
Iteration 4800: Loss = -11617.103504625504
2
Iteration 4900: Loss = -11617.10008634468
Iteration 5000: Loss = -11617.099283195546
Iteration 5100: Loss = -11617.099415890953
1
Iteration 5200: Loss = -11617.101765461053
2
Iteration 5300: Loss = -11617.098237153557
Iteration 5400: Loss = -11617.099216229526
1
Iteration 5500: Loss = -11617.105620469774
2
Iteration 5600: Loss = -11617.097040818642
Iteration 5700: Loss = -11617.096779112284
Iteration 5800: Loss = -11617.10215685141
1
Iteration 5900: Loss = -11617.098862146262
2
Iteration 6000: Loss = -11617.097430312164
3
Iteration 6100: Loss = -11617.096204270365
Iteration 6200: Loss = -11617.09649443733
1
Iteration 6300: Loss = -11617.094926754547
Iteration 6400: Loss = -11617.096833419346
1
Iteration 6500: Loss = -11617.09456957415
Iteration 6600: Loss = -11617.095457987129
1
Iteration 6700: Loss = -11617.095462592759
2
Iteration 6800: Loss = -11617.094699886147
3
Iteration 6900: Loss = -11617.094444884697
Iteration 7000: Loss = -11617.160514048272
1
Iteration 7100: Loss = -11617.09372245941
Iteration 7200: Loss = -11617.097606714951
1
Iteration 7300: Loss = -11617.093532836845
Iteration 7400: Loss = -11617.094703786679
1
Iteration 7500: Loss = -11617.0941718196
2
Iteration 7600: Loss = -11617.096550725839
3
Iteration 7700: Loss = -11617.093186740218
Iteration 7800: Loss = -11617.0952368325
1
Iteration 7900: Loss = -11617.132535010369
2
Iteration 8000: Loss = -11617.092990219653
Iteration 8100: Loss = -11617.092984848672
Iteration 8200: Loss = -11617.10085099157
1
Iteration 8300: Loss = -11617.092777073269
Iteration 8400: Loss = -11617.094222843318
1
Iteration 8500: Loss = -11617.096391977455
2
Iteration 8600: Loss = -11617.094729132814
3
Iteration 8700: Loss = -11617.093082259742
4
Iteration 8800: Loss = -11617.093639975332
5
Iteration 8900: Loss = -11617.092812823113
Iteration 9000: Loss = -11617.0925784857
Iteration 9100: Loss = -11617.093599004756
1
Iteration 9200: Loss = -11617.11350849814
2
Iteration 9300: Loss = -11617.097784494215
3
Iteration 9400: Loss = -11617.097321252913
4
Iteration 9500: Loss = -11617.0986493823
5
Iteration 9600: Loss = -11617.107571034256
6
Iteration 9700: Loss = -11617.095538218562
7
Iteration 9800: Loss = -11617.09398915864
8
Iteration 9900: Loss = -11617.095313373808
9
Iteration 10000: Loss = -11617.095357288466
10
Iteration 10100: Loss = -11617.093685874923
11
Iteration 10200: Loss = -11617.098751117752
12
Iteration 10300: Loss = -11617.093844453073
13
Iteration 10400: Loss = -11617.094260230986
14
Iteration 10500: Loss = -11617.098035977662
15
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.7702, 0.2298],
        [0.2540, 0.7460]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4657, 0.5343], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2000, 0.1024],
         [0.5065, 0.3876]],

        [[0.5414, 0.1039],
         [0.5425, 0.6033]],

        [[0.7218, 0.1079],
         [0.5113, 0.5246]],

        [[0.6461, 0.0980],
         [0.6434, 0.6309]],

        [[0.6676, 0.1105],
         [0.5890, 0.5914]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999691484065
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19312.918666083522
Iteration 100: Loss = -12297.010968600527
Iteration 200: Loss = -12284.0912496847
Iteration 300: Loss = -11935.22679686446
Iteration 400: Loss = -11901.350574079339
Iteration 500: Loss = -11900.621626012793
Iteration 600: Loss = -11895.942841856639
Iteration 700: Loss = -11894.648206130312
Iteration 800: Loss = -11894.323975578034
Iteration 900: Loss = -11887.784721221007
Iteration 1000: Loss = -11886.204595531563
Iteration 1100: Loss = -11885.366398693835
Iteration 1200: Loss = -11885.302804633007
Iteration 1300: Loss = -11885.211995707083
Iteration 1400: Loss = -11885.2049371827
Iteration 1500: Loss = -11885.196965728224
Iteration 1600: Loss = -11885.194559882579
Iteration 1700: Loss = -11885.194259084608
Iteration 1800: Loss = -11885.190880631266
Iteration 1900: Loss = -11885.189485617988
Iteration 2000: Loss = -11885.21201594329
1
Iteration 2100: Loss = -11885.186648467035
Iteration 2200: Loss = -11885.179990058106
Iteration 2300: Loss = -11885.177457633357
Iteration 2400: Loss = -11885.176529991655
Iteration 2500: Loss = -11885.175777285442
Iteration 2600: Loss = -11885.17474632724
Iteration 2700: Loss = -11885.169699688195
Iteration 2800: Loss = -11885.165692829716
Iteration 2900: Loss = -11885.165178474195
Iteration 3000: Loss = -11885.164795045717
Iteration 3100: Loss = -11885.165110151012
1
Iteration 3200: Loss = -11885.168739148792
2
Iteration 3300: Loss = -11885.163232365094
Iteration 3400: Loss = -11885.132958373184
Iteration 3500: Loss = -11885.132653313312
Iteration 3600: Loss = -11885.132317529504
Iteration 3700: Loss = -11885.132005355012
Iteration 3800: Loss = -11885.131603014997
Iteration 3900: Loss = -11885.130817747198
Iteration 4000: Loss = -11885.132272417904
1
Iteration 4100: Loss = -11885.12988270983
Iteration 4200: Loss = -11885.129778714914
Iteration 4300: Loss = -11885.12966952713
Iteration 4400: Loss = -11885.129495537018
Iteration 4500: Loss = -11885.129385431874
Iteration 4600: Loss = -11885.129317496354
Iteration 4700: Loss = -11885.129204073652
Iteration 4800: Loss = -11885.131586075067
1
Iteration 4900: Loss = -11885.129071440344
Iteration 5000: Loss = -11885.129015457233
Iteration 5100: Loss = -11885.128894913545
Iteration 5200: Loss = -11885.129477815017
1
Iteration 5300: Loss = -11885.12876835887
Iteration 5400: Loss = -11885.128722846495
Iteration 5500: Loss = -11885.128993341814
1
Iteration 5600: Loss = -11885.1302607284
2
Iteration 5700: Loss = -11885.128794463339
Iteration 5800: Loss = -11885.12874069437
Iteration 5900: Loss = -11885.138202709288
1
Iteration 6000: Loss = -11885.130411887512
2
Iteration 6100: Loss = -11885.128702867325
Iteration 6200: Loss = -11885.129037762443
1
Iteration 6300: Loss = -11885.12823744471
Iteration 6400: Loss = -11885.130864740688
1
Iteration 6500: Loss = -11885.134201196572
2
Iteration 6600: Loss = -11885.126751429585
Iteration 6700: Loss = -11885.126772310896
Iteration 6800: Loss = -11885.131129532605
1
Iteration 6900: Loss = -11885.126730169382
Iteration 7000: Loss = -11885.121598434082
Iteration 7100: Loss = -11885.156568969922
1
Iteration 7200: Loss = -11885.121462178604
Iteration 7300: Loss = -11885.153810238684
1
Iteration 7400: Loss = -11885.121361992175
Iteration 7500: Loss = -11881.258356147913
Iteration 7600: Loss = -11881.258386712196
Iteration 7700: Loss = -11881.25567352627
Iteration 7800: Loss = -11881.257376285801
1
Iteration 7900: Loss = -11881.2556020001
Iteration 8000: Loss = -11881.256278825918
1
Iteration 8100: Loss = -11881.255610332979
Iteration 8200: Loss = -11881.255505906223
Iteration 8300: Loss = -11881.25518661981
Iteration 8400: Loss = -11881.255307893462
1
Iteration 8500: Loss = -11881.255099788847
Iteration 8600: Loss = -11881.255082413978
Iteration 8700: Loss = -11881.253898165729
Iteration 8800: Loss = -11881.25365029747
Iteration 8900: Loss = -11881.253665248792
Iteration 9000: Loss = -11881.253745063823
Iteration 9100: Loss = -11881.25451487911
1
Iteration 9200: Loss = -11881.253722220217
Iteration 9300: Loss = -11881.253690142361
Iteration 9400: Loss = -11881.336316148356
1
Iteration 9500: Loss = -11881.263865125133
2
Iteration 9600: Loss = -11881.253644835713
Iteration 9700: Loss = -11881.25385549918
1
Iteration 9800: Loss = -11881.266844222051
2
Iteration 9900: Loss = -11881.253881125556
3
Iteration 10000: Loss = -11881.469535040937
4
Iteration 10100: Loss = -11881.253606792661
Iteration 10200: Loss = -11881.256453333299
1
Iteration 10300: Loss = -11881.256135703652
2
Iteration 10400: Loss = -11881.456747049027
3
Iteration 10500: Loss = -11881.253566240339
Iteration 10600: Loss = -11881.253971982236
1
Iteration 10700: Loss = -11881.253648026333
Iteration 10800: Loss = -11881.253630389192
Iteration 10900: Loss = -11881.253646183795
Iteration 11000: Loss = -11881.253655555583
Iteration 11100: Loss = -11881.253523525875
Iteration 11200: Loss = -11881.253827438768
1
Iteration 11300: Loss = -11881.253532076591
Iteration 11400: Loss = -11881.61455247021
1
Iteration 11500: Loss = -11881.25354044419
Iteration 11600: Loss = -11881.2535300833
Iteration 11700: Loss = -11881.282622781406
1
Iteration 11800: Loss = -11881.253823715992
2
Iteration 11900: Loss = -11881.257473269188
3
Iteration 12000: Loss = -11881.253536623783
Iteration 12100: Loss = -11881.177662665898
Iteration 12200: Loss = -11881.214684427401
1
Iteration 12300: Loss = -11881.178542893746
2
Iteration 12400: Loss = -11881.177066977363
Iteration 12500: Loss = -11881.210507261389
1
Iteration 12600: Loss = -11881.174645094721
Iteration 12700: Loss = -11881.169427818015
Iteration 12800: Loss = -11881.181695937254
1
Iteration 12900: Loss = -11881.167562183506
Iteration 13000: Loss = -11881.168106884308
1
Iteration 13100: Loss = -11881.181282998905
2
Iteration 13200: Loss = -11881.174980910768
3
Iteration 13300: Loss = -11881.167572535716
Iteration 13400: Loss = -11881.167876232566
1
Iteration 13500: Loss = -11881.16768342674
2
Iteration 13600: Loss = -11881.197595479562
3
Iteration 13700: Loss = -11881.168169243563
4
Iteration 13800: Loss = -11881.16892106287
5
Iteration 13900: Loss = -11881.170530129313
6
Iteration 14000: Loss = -11881.123987397943
Iteration 14100: Loss = -11881.128568895527
1
Iteration 14200: Loss = -11881.19141432904
2
Iteration 14300: Loss = -11881.12058925272
Iteration 14400: Loss = -11881.11995501363
Iteration 14500: Loss = -11881.18734111799
1
Iteration 14600: Loss = -11881.119965728312
Iteration 14700: Loss = -11881.192637190754
1
Iteration 14800: Loss = -11881.119987453058
Iteration 14900: Loss = -11881.119953924666
Iteration 15000: Loss = -11881.135550382978
1
Iteration 15100: Loss = -11881.119938487189
Iteration 15200: Loss = -11881.119943324087
Iteration 15300: Loss = -11881.139730012941
1
Iteration 15400: Loss = -11881.119923748876
Iteration 15500: Loss = -11881.119921477599
Iteration 15600: Loss = -11881.167192670016
1
Iteration 15700: Loss = -11881.119924119608
Iteration 15800: Loss = -11881.119929648792
Iteration 15900: Loss = -11881.120060062054
1
Iteration 16000: Loss = -11881.142845144075
2
Iteration 16100: Loss = -11881.120842098158
3
Iteration 16200: Loss = -11881.162731292074
4
Iteration 16300: Loss = -11881.122835516418
5
Iteration 16400: Loss = -11881.175283940525
6
Iteration 16500: Loss = -11881.120888284675
7
Iteration 16600: Loss = -11881.119316653243
Iteration 16700: Loss = -11881.118228455778
Iteration 16800: Loss = -11881.127668356683
1
Iteration 16900: Loss = -11881.387337740638
2
Iteration 17000: Loss = -11881.118145923445
Iteration 17100: Loss = -11881.132763943942
1
Iteration 17200: Loss = -11881.118134234279
Iteration 17300: Loss = -11881.12239137214
1
Iteration 17400: Loss = -11881.118119272596
Iteration 17500: Loss = -11881.11845851372
1
Iteration 17600: Loss = -11881.121504031245
2
Iteration 17700: Loss = -11881.118175171203
Iteration 17800: Loss = -11881.119193793473
1
Iteration 17900: Loss = -11881.123844475309
2
Iteration 18000: Loss = -11881.131751116176
3
Iteration 18100: Loss = -11881.1181608855
Iteration 18200: Loss = -11881.118300602795
1
Iteration 18300: Loss = -11881.243308424968
2
Iteration 18400: Loss = -11881.118055509836
Iteration 18500: Loss = -11881.11850785169
1
Iteration 18600: Loss = -11881.117634412865
Iteration 18700: Loss = -11881.118783464872
1
Iteration 18800: Loss = -11881.116934185227
Iteration 18900: Loss = -11881.119853350405
1
Iteration 19000: Loss = -11881.116912213014
Iteration 19100: Loss = -11881.118266634408
1
Iteration 19200: Loss = -11881.116890641437
Iteration 19300: Loss = -11881.117011422213
1
Iteration 19400: Loss = -11881.117902326254
2
Iteration 19500: Loss = -11881.558774645011
3
Iteration 19600: Loss = -11881.116891177924
Iteration 19700: Loss = -11881.116919034472
Iteration 19800: Loss = -11881.116943211344
Iteration 19900: Loss = -11881.189353773158
1
pi: tensor([[0.6021, 0.3979],
        [0.5082, 0.4918]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4598, 0.5402], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2481, 0.1018],
         [0.5132, 0.3415]],

        [[0.5853, 0.1028],
         [0.5041, 0.7154]],

        [[0.5437, 0.1121],
         [0.6894, 0.6107]],

        [[0.6851, 0.0984],
         [0.6377, 0.7095]],

        [[0.5465, 0.1090],
         [0.5586, 0.5608]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 19
Adjusted Rand Index: 0.3783644842654133
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.702691493950761
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.08579778221745635
Average Adjusted Rand Index: 0.7922095656333904
11625.293800397678
[0.9919999691484065, 0.08579778221745635] [0.9919993417272899, 0.7922095656333904] [11617.098035977662, 11881.116890082778]
-------------------------------------
This iteration is 55
True Objective function: Loss = -11729.092566951182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22517.163935548688
Iteration 100: Loss = -12555.05291210249
Iteration 200: Loss = -12459.549254940246
Iteration 300: Loss = -12051.884870962427
Iteration 400: Loss = -11982.031556799542
Iteration 500: Loss = -11941.055467033828
Iteration 600: Loss = -11938.116821992695
Iteration 700: Loss = -11936.857782925486
Iteration 800: Loss = -11936.333524570022
Iteration 900: Loss = -11936.051900103785
Iteration 1000: Loss = -11935.720788595905
Iteration 1100: Loss = -11921.60799560042
Iteration 1200: Loss = -11920.830448758068
Iteration 1300: Loss = -11920.470899800448
Iteration 1400: Loss = -11920.29847076438
Iteration 1500: Loss = -11920.190537649667
Iteration 1600: Loss = -11917.159741764519
Iteration 1700: Loss = -11916.6956377612
Iteration 1800: Loss = -11916.603896881526
Iteration 1900: Loss = -11916.55920245877
Iteration 2000: Loss = -11916.526575958846
Iteration 2100: Loss = -11916.501226751117
Iteration 2200: Loss = -11916.479362377033
Iteration 2300: Loss = -11916.461131363752
Iteration 2400: Loss = -11916.447173979988
Iteration 2500: Loss = -11916.435208044328
Iteration 2600: Loss = -11916.423624159637
Iteration 2700: Loss = -11916.414417881006
Iteration 2800: Loss = -11916.396163468258
Iteration 2900: Loss = -11916.378158278934
Iteration 3000: Loss = -11916.391478297664
1
Iteration 3100: Loss = -11916.359371854489
Iteration 3200: Loss = -11916.353851065187
Iteration 3300: Loss = -11916.34906322456
Iteration 3400: Loss = -11916.342523080551
Iteration 3500: Loss = -11916.335427172346
Iteration 3600: Loss = -11916.331851433033
Iteration 3700: Loss = -11916.329178914599
Iteration 3800: Loss = -11916.326784965066
Iteration 3900: Loss = -11916.325506878004
Iteration 4000: Loss = -11916.323127375425
Iteration 4100: Loss = -11916.321060040611
Iteration 4200: Loss = -11916.321578964447
1
Iteration 4300: Loss = -11916.31796016347
Iteration 4400: Loss = -11916.316553873321
Iteration 4500: Loss = -11916.31891433867
1
Iteration 4600: Loss = -11916.31646698065
Iteration 4700: Loss = -11916.323752783122
1
Iteration 4800: Loss = -11916.31138114635
Iteration 4900: Loss = -11916.310018509883
Iteration 5000: Loss = -11916.311259518388
1
Iteration 5100: Loss = -11916.308007146563
Iteration 5200: Loss = -11916.307533347952
Iteration 5300: Loss = -11916.306474675584
Iteration 5400: Loss = -11916.30586474965
Iteration 5500: Loss = -11916.305145708428
Iteration 5600: Loss = -11916.305117281225
Iteration 5700: Loss = -11916.30390517417
Iteration 5800: Loss = -11916.310677110352
1
Iteration 5900: Loss = -11916.302838027235
Iteration 6000: Loss = -11916.302389815324
Iteration 6100: Loss = -11916.301953661192
Iteration 6200: Loss = -11916.301443482049
Iteration 6300: Loss = -11916.301032799476
Iteration 6400: Loss = -11916.300634406107
Iteration 6500: Loss = -11916.30024525073
Iteration 6600: Loss = -11916.327308148553
1
Iteration 6700: Loss = -11916.299558374494
Iteration 6800: Loss = -11916.299237129839
Iteration 6900: Loss = -11916.298985081601
Iteration 7000: Loss = -11916.298710225215
Iteration 7100: Loss = -11916.300479651492
1
Iteration 7200: Loss = -11916.298251962407
Iteration 7300: Loss = -11916.298072340107
Iteration 7400: Loss = -11916.297875421194
Iteration 7500: Loss = -11916.297673824092
Iteration 7600: Loss = -11916.298461549126
1
Iteration 7700: Loss = -11916.297342569644
Iteration 7800: Loss = -11916.297213789705
Iteration 7900: Loss = -11916.297015762577
Iteration 8000: Loss = -11916.323344876548
1
Iteration 8100: Loss = -11916.296781259856
Iteration 8200: Loss = -11916.315579808072
1
Iteration 8300: Loss = -11916.296544280118
Iteration 8400: Loss = -11916.298190457395
1
Iteration 8500: Loss = -11916.296338261493
Iteration 8600: Loss = -11916.296401737785
Iteration 8700: Loss = -11916.296166223305
Iteration 8800: Loss = -11916.780263322633
1
Iteration 8900: Loss = -11916.295971557985
Iteration 9000: Loss = -11916.295866461525
Iteration 9100: Loss = -11916.296093464
1
Iteration 9200: Loss = -11916.295837773201
Iteration 9300: Loss = -11916.29618710772
1
Iteration 9400: Loss = -11916.295614830186
Iteration 9500: Loss = -11916.29626455829
1
Iteration 9600: Loss = -11916.29547025885
Iteration 9700: Loss = -11916.295541389003
Iteration 9800: Loss = -11916.295391644533
Iteration 9900: Loss = -11916.295407575597
Iteration 10000: Loss = -11916.295436064771
Iteration 10100: Loss = -11916.323566814732
1
Iteration 10200: Loss = -11916.296155840788
2
Iteration 10300: Loss = -11916.294968953665
Iteration 10400: Loss = -11916.608269543492
1
Iteration 10500: Loss = -11916.294802962508
Iteration 10600: Loss = -11916.323759689561
1
Iteration 10700: Loss = -11916.294742734613
Iteration 10800: Loss = -11916.310074051637
1
Iteration 10900: Loss = -11916.294740877041
Iteration 11000: Loss = -11916.2946362313
Iteration 11100: Loss = -11916.302454048926
1
Iteration 11200: Loss = -11916.294560015353
Iteration 11300: Loss = -11916.296781042354
1
Iteration 11400: Loss = -11916.294623058093
Iteration 11500: Loss = -11916.371441009478
1
Iteration 11600: Loss = -11916.299419887879
2
Iteration 11700: Loss = -11916.294514927868
Iteration 11800: Loss = -11916.295358197272
1
Iteration 11900: Loss = -11916.29511912972
2
Iteration 12000: Loss = -11916.294656795011
3
Iteration 12100: Loss = -11916.294398344602
Iteration 12200: Loss = -11916.30332003616
1
Iteration 12300: Loss = -11916.294334754957
Iteration 12400: Loss = -11916.297483721113
1
Iteration 12500: Loss = -11916.29444843414
2
Iteration 12600: Loss = -11916.29780566081
3
Iteration 12700: Loss = -11916.327253299438
4
Iteration 12800: Loss = -11916.29721209128
5
Iteration 12900: Loss = -11916.296055175811
6
Iteration 13000: Loss = -11916.306510789946
7
Iteration 13100: Loss = -11916.294278902173
Iteration 13200: Loss = -11916.29450865437
1
Iteration 13300: Loss = -11916.30469510371
2
Iteration 13400: Loss = -11916.294891232303
3
Iteration 13500: Loss = -11916.30643773025
4
Iteration 13600: Loss = -11916.299736341181
5
Iteration 13700: Loss = -11916.2963542397
6
Iteration 13800: Loss = -11916.294458878192
7
Iteration 13900: Loss = -11916.295135931721
8
Iteration 14000: Loss = -11916.298633805403
9
Iteration 14100: Loss = -11916.295896646012
10
Iteration 14200: Loss = -11916.294327730808
Iteration 14300: Loss = -11916.306151913946
1
Iteration 14400: Loss = -11916.294250333785
Iteration 14500: Loss = -11916.294359843714
1
Iteration 14600: Loss = -11916.300925902897
2
Iteration 14700: Loss = -11916.29942656331
3
Iteration 14800: Loss = -11916.299491814474
4
Iteration 14900: Loss = -11916.294272566589
Iteration 15000: Loss = -11916.297046606178
1
Iteration 15100: Loss = -11916.300116356422
2
Iteration 15200: Loss = -11916.330520192205
3
Iteration 15300: Loss = -11916.326377233945
4
Iteration 15400: Loss = -11916.297545519725
5
Iteration 15500: Loss = -11916.294257713436
Iteration 15600: Loss = -11916.294695218196
1
Iteration 15700: Loss = -11916.36593945313
2
Iteration 15800: Loss = -11916.294365346075
3
Iteration 15900: Loss = -11916.303975911518
4
Iteration 16000: Loss = -11916.296337550959
5
Iteration 16100: Loss = -11916.318137351276
6
Iteration 16200: Loss = -11916.337408898338
7
Iteration 16300: Loss = -11916.294289146825
Iteration 16400: Loss = -11916.294665997151
1
Iteration 16500: Loss = -11916.29419950918
Iteration 16600: Loss = -11916.294501510169
1
Iteration 16700: Loss = -11916.29590071086
2
Iteration 16800: Loss = -11916.323770004421
3
Iteration 16900: Loss = -11916.294181332492
Iteration 17000: Loss = -11916.512089028707
1
Iteration 17100: Loss = -11916.294182569025
Iteration 17200: Loss = -11916.300456360048
1
Iteration 17300: Loss = -11916.30471182591
2
Iteration 17400: Loss = -11916.33596896894
3
Iteration 17500: Loss = -11916.296045978035
4
Iteration 17600: Loss = -11916.294712809828
5
Iteration 17700: Loss = -11916.302208878025
6
Iteration 17800: Loss = -11916.294185941228
Iteration 17900: Loss = -11916.296451287319
1
Iteration 18000: Loss = -11916.29421529204
Iteration 18100: Loss = -11916.29664831034
1
Iteration 18200: Loss = -11916.300100850602
2
Iteration 18300: Loss = -11916.302004520203
3
Iteration 18400: Loss = -11916.297928943408
4
Iteration 18500: Loss = -11916.315623670855
5
Iteration 18600: Loss = -11916.296379624877
6
Iteration 18700: Loss = -11916.29596984342
7
Iteration 18800: Loss = -11916.299072417436
8
Iteration 18900: Loss = -11916.29420840624
Iteration 19000: Loss = -11916.294472147634
1
Iteration 19100: Loss = -11916.311341956165
2
Iteration 19200: Loss = -11916.294351384706
3
Iteration 19300: Loss = -11916.595206795468
4
Iteration 19400: Loss = -11916.294191113819
Iteration 19500: Loss = -11916.356052973424
1
Iteration 19600: Loss = -11916.294440837239
2
Iteration 19700: Loss = -11916.294954325438
3
Iteration 19800: Loss = -11916.323159690939
4
Iteration 19900: Loss = -11916.29659731939
5
pi: tensor([[0.8087, 0.1913],
        [0.3086, 0.6914]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0244, 0.9756], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4026, 0.1011],
         [0.7109, 0.2021]],

        [[0.5461, 0.1003],
         [0.5823, 0.5684]],

        [[0.5638, 0.1011],
         [0.5256, 0.5692]],

        [[0.6811, 0.0934],
         [0.6294, 0.5806]],

        [[0.5898, 0.0925],
         [0.5325, 0.6739]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.030303030303030304
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.33972426592205685
Average Adjusted Rand Index: 0.6050785345749374
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21231.922411443888
Iteration 100: Loss = -12529.620174175685
Iteration 200: Loss = -12000.406128267847
Iteration 300: Loss = -11853.054187413833
Iteration 400: Loss = -11823.367199454065
Iteration 500: Loss = -11808.514665802491
Iteration 600: Loss = -11802.95634854579
Iteration 700: Loss = -11802.266558306339
Iteration 800: Loss = -11802.09319649982
Iteration 900: Loss = -11801.970760712462
Iteration 1000: Loss = -11801.886193927876
Iteration 1100: Loss = -11801.823605928512
Iteration 1200: Loss = -11801.773478534355
Iteration 1300: Loss = -11801.730371899006
Iteration 1400: Loss = -11801.68848891347
Iteration 1500: Loss = -11801.635893525843
Iteration 1600: Loss = -11801.511934170765
Iteration 1700: Loss = -11800.425197239043
Iteration 1800: Loss = -11787.879652941207
Iteration 1900: Loss = -11773.114299471552
Iteration 2000: Loss = -11753.14811775788
Iteration 2100: Loss = -11742.28632729191
Iteration 2200: Loss = -11738.856593430824
Iteration 2300: Loss = -11738.755772174785
Iteration 2400: Loss = -11718.846370243373
Iteration 2500: Loss = -11718.76228411281
Iteration 2600: Loss = -11718.735648360953
Iteration 2700: Loss = -11718.713506446566
Iteration 2800: Loss = -11718.70334930747
Iteration 2900: Loss = -11718.695359083647
Iteration 3000: Loss = -11718.688791087929
Iteration 3100: Loss = -11718.682877684234
Iteration 3200: Loss = -11718.677798565333
Iteration 3300: Loss = -11718.674052138122
Iteration 3400: Loss = -11718.670354518463
Iteration 3500: Loss = -11718.667496003052
Iteration 3600: Loss = -11718.665091999304
Iteration 3700: Loss = -11718.662733244442
Iteration 3800: Loss = -11718.661813265595
Iteration 3900: Loss = -11718.658924003548
Iteration 4000: Loss = -11718.664612791566
1
Iteration 4100: Loss = -11718.655834684687
Iteration 4200: Loss = -11718.654524035936
Iteration 4300: Loss = -11718.65625363074
1
Iteration 4400: Loss = -11718.652823405802
Iteration 4500: Loss = -11718.651177921212
Iteration 4600: Loss = -11718.650194762691
Iteration 4700: Loss = -11718.649438413511
Iteration 4800: Loss = -11718.649933055116
1
Iteration 4900: Loss = -11718.647805814935
Iteration 5000: Loss = -11718.647583732363
Iteration 5100: Loss = -11718.64669740653
Iteration 5200: Loss = -11718.645676666783
Iteration 5300: Loss = -11718.644970022782
Iteration 5400: Loss = -11718.644158514695
Iteration 5500: Loss = -11718.644387538001
1
Iteration 5600: Loss = -11718.649630322825
2
Iteration 5700: Loss = -11718.641708297107
Iteration 5800: Loss = -11718.643488364316
1
Iteration 5900: Loss = -11718.641501454505
Iteration 6000: Loss = -11718.642957185042
1
Iteration 6100: Loss = -11718.6416301865
2
Iteration 6200: Loss = -11718.64157457764
Iteration 6300: Loss = -11718.641270751186
Iteration 6400: Loss = -11718.64302453183
1
Iteration 6500: Loss = -11718.640572038501
Iteration 6600: Loss = -11718.640100710945
Iteration 6700: Loss = -11718.64018328878
Iteration 6800: Loss = -11718.638562496868
Iteration 6900: Loss = -11718.643027589118
1
Iteration 7000: Loss = -11718.652158774708
2
Iteration 7100: Loss = -11718.637979356328
Iteration 7200: Loss = -11718.63795052945
Iteration 7300: Loss = -11718.714033262051
1
Iteration 7400: Loss = -11718.637539246989
Iteration 7500: Loss = -11718.637473929686
Iteration 7600: Loss = -11718.637731610144
1
Iteration 7700: Loss = -11718.63784534006
2
Iteration 7800: Loss = -11718.640949334527
3
Iteration 7900: Loss = -11718.659141201944
4
Iteration 8000: Loss = -11718.636866571862
Iteration 8100: Loss = -11718.641614171256
1
Iteration 8200: Loss = -11718.638888672176
2
Iteration 8300: Loss = -11718.637867248632
3
Iteration 8400: Loss = -11718.660728778992
4
Iteration 8500: Loss = -11718.636392904253
Iteration 8600: Loss = -11718.669560328632
1
Iteration 8700: Loss = -11718.640071965066
2
Iteration 8800: Loss = -11718.636573886402
3
Iteration 8900: Loss = -11718.645835058023
4
Iteration 9000: Loss = -11718.636115269357
Iteration 9100: Loss = -11718.636028107492
Iteration 9200: Loss = -11718.660639198839
1
Iteration 9300: Loss = -11718.642077937642
2
Iteration 9400: Loss = -11718.637258434357
3
Iteration 9500: Loss = -11718.635955986048
Iteration 9600: Loss = -11718.63588825393
Iteration 9700: Loss = -11718.641613649914
1
Iteration 9800: Loss = -11718.6360457062
2
Iteration 9900: Loss = -11718.637293868433
3
Iteration 10000: Loss = -11718.639431078946
4
Iteration 10100: Loss = -11718.639251895833
5
Iteration 10200: Loss = -11718.638182668452
6
Iteration 10300: Loss = -11718.636551949006
7
Iteration 10400: Loss = -11718.637655126766
8
Iteration 10500: Loss = -11718.67880335166
9
Iteration 10600: Loss = -11718.651232768143
10
Iteration 10700: Loss = -11718.653121789668
11
Iteration 10800: Loss = -11718.640113678153
12
Iteration 10900: Loss = -11718.66876106183
13
Iteration 11000: Loss = -11718.638121126523
14
Iteration 11100: Loss = -11718.652951055767
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.8049, 0.1951],
        [0.2926, 0.7074]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4164, 0.5836], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3987, 0.1142],
         [0.6353, 0.1949]],

        [[0.6539, 0.1083],
         [0.6586, 0.7097]],

        [[0.5943, 0.1007],
         [0.7131, 0.5299]],

        [[0.5393, 0.0934],
         [0.6005, 0.5937]],

        [[0.6178, 0.0925],
         [0.6831, 0.7199]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.99199877740731
11729.092566951182
[0.33972426592205685, 0.9920000001562724] [0.6050785345749374, 0.99199877740731] [11916.29642871556, 11718.652951055767]
-------------------------------------
This iteration is 56
True Objective function: Loss = -11391.96636462653
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24934.79730106756
Iteration 100: Loss = -12059.320324897795
Iteration 200: Loss = -11816.07047429862
Iteration 300: Loss = -11486.674895802693
Iteration 400: Loss = -11443.876966224047
Iteration 500: Loss = -11443.619506152208
Iteration 600: Loss = -11443.48884629312
Iteration 700: Loss = -11432.888200247604
Iteration 800: Loss = -11416.7941666854
Iteration 900: Loss = -11416.72816890255
Iteration 1000: Loss = -11416.70136616681
Iteration 1100: Loss = -11416.681371562076
Iteration 1200: Loss = -11416.66583358191
Iteration 1300: Loss = -11416.65337691747
Iteration 1400: Loss = -11416.64306396669
Iteration 1500: Loss = -11416.631577670052
Iteration 1600: Loss = -11401.31702994189
Iteration 1700: Loss = -11401.3091039115
Iteration 1800: Loss = -11401.303124895832
Iteration 1900: Loss = -11401.298194562984
Iteration 2000: Loss = -11401.293988159605
Iteration 2100: Loss = -11401.29048425792
Iteration 2200: Loss = -11401.287379365305
Iteration 2300: Loss = -11401.28459412457
Iteration 2400: Loss = -11401.281863566204
Iteration 2500: Loss = -11401.284032714506
1
Iteration 2600: Loss = -11401.272677106419
Iteration 2700: Loss = -11401.169736846601
Iteration 2800: Loss = -11394.205761165358
Iteration 2900: Loss = -11394.203942764041
Iteration 3000: Loss = -11394.202576923326
Iteration 3100: Loss = -11394.201347211789
Iteration 3200: Loss = -11394.201095341024
Iteration 3300: Loss = -11394.199408635162
Iteration 3400: Loss = -11394.203012771008
1
Iteration 3500: Loss = -11394.197856821178
Iteration 3600: Loss = -11394.197433987552
Iteration 3700: Loss = -11394.19645733541
Iteration 3800: Loss = -11394.195924195026
Iteration 3900: Loss = -11394.196091529986
1
Iteration 4000: Loss = -11394.194749014183
Iteration 4100: Loss = -11394.194114872504
Iteration 4200: Loss = -11394.192340897343
Iteration 4300: Loss = -11393.602787834692
Iteration 4400: Loss = -11393.601635420542
Iteration 4500: Loss = -11393.601276404237
Iteration 4600: Loss = -11393.602819196618
1
Iteration 4700: Loss = -11393.602243451782
2
Iteration 4800: Loss = -11393.60104713742
Iteration 4900: Loss = -11393.602075519764
1
Iteration 5000: Loss = -11393.600363497606
Iteration 5100: Loss = -11393.603899530692
1
Iteration 5200: Loss = -11393.60293273295
2
Iteration 5300: Loss = -11393.599242342096
Iteration 5400: Loss = -11393.599080377526
Iteration 5500: Loss = -11393.600334217015
1
Iteration 5600: Loss = -11393.598761682202
Iteration 5700: Loss = -11393.598575337637
Iteration 5800: Loss = -11393.599108364619
1
Iteration 5900: Loss = -11393.60105837725
2
Iteration 6000: Loss = -11393.599908705706
3
Iteration 6100: Loss = -11393.59866198408
Iteration 6200: Loss = -11393.597954387473
Iteration 6300: Loss = -11393.597821506462
Iteration 6400: Loss = -11393.597930516433
1
Iteration 6500: Loss = -11393.60603384968
2
Iteration 6600: Loss = -11393.597631346394
Iteration 6700: Loss = -11393.597907083587
1
Iteration 6800: Loss = -11393.603274662859
2
Iteration 6900: Loss = -11393.613874941904
3
Iteration 7000: Loss = -11393.59726848499
Iteration 7100: Loss = -11393.59723169829
Iteration 7200: Loss = -11393.607693993312
1
Iteration 7300: Loss = -11393.597193811815
Iteration 7400: Loss = -11393.598015765101
1
Iteration 7500: Loss = -11393.606243524262
2
Iteration 7600: Loss = -11393.607926966699
3
Iteration 7700: Loss = -11393.597035772735
Iteration 7800: Loss = -11393.597043646918
Iteration 7900: Loss = -11393.596916738163
Iteration 8000: Loss = -11393.596762932364
Iteration 8100: Loss = -11393.596697544623
Iteration 8200: Loss = -11393.595970402106
Iteration 8300: Loss = -11393.595935234916
Iteration 8400: Loss = -11393.597538358581
1
Iteration 8500: Loss = -11393.595865648327
Iteration 8600: Loss = -11393.595946146179
Iteration 8700: Loss = -11393.597659857556
1
Iteration 8800: Loss = -11393.597798052991
2
Iteration 8900: Loss = -11383.899045628794
Iteration 9000: Loss = -11383.900658774619
1
Iteration 9100: Loss = -11383.895229913218
Iteration 9200: Loss = -11383.89512618918
Iteration 9300: Loss = -11383.967457000646
1
Iteration 9400: Loss = -11383.894790923276
Iteration 9500: Loss = -11383.90798551562
1
Iteration 9600: Loss = -11383.894752307802
Iteration 9700: Loss = -11383.902178312192
1
Iteration 9800: Loss = -11383.894737091987
Iteration 9900: Loss = -11383.898118897936
1
Iteration 10000: Loss = -11383.894691050018
Iteration 10100: Loss = -11383.895856137473
1
Iteration 10200: Loss = -11383.8946731016
Iteration 10300: Loss = -11383.901495907761
1
Iteration 10400: Loss = -11383.89651788973
2
Iteration 10500: Loss = -11383.90020832126
3
Iteration 10600: Loss = -11383.89916891863
4
Iteration 10700: Loss = -11383.972921087336
5
Iteration 10800: Loss = -11383.894371362525
Iteration 10900: Loss = -11383.894903850945
1
Iteration 11000: Loss = -11383.904260566109
2
Iteration 11100: Loss = -11383.894429880605
Iteration 11200: Loss = -11383.895341520798
1
Iteration 11300: Loss = -11383.895404367102
2
Iteration 11400: Loss = -11383.895263493292
3
Iteration 11500: Loss = -11383.924442623333
4
Iteration 11600: Loss = -11383.894159634836
Iteration 11700: Loss = -11383.894251445121
Iteration 11800: Loss = -11383.949533366544
1
Iteration 11900: Loss = -11383.911486619367
2
Iteration 12000: Loss = -11383.9404777033
3
Iteration 12100: Loss = -11383.89456097902
4
Iteration 12200: Loss = -11383.933395381935
5
Iteration 12300: Loss = -11383.920935973889
6
Iteration 12400: Loss = -11384.06233175795
7
Iteration 12500: Loss = -11383.894131464027
Iteration 12600: Loss = -11383.89444156122
1
Iteration 12700: Loss = -11383.89418862502
Iteration 12800: Loss = -11383.894149439131
Iteration 12900: Loss = -11383.911528665434
1
Iteration 13000: Loss = -11383.894108293298
Iteration 13100: Loss = -11383.894713086192
1
Iteration 13200: Loss = -11383.8941474422
Iteration 13300: Loss = -11383.89424509863
Iteration 13400: Loss = -11383.913894704927
1
Iteration 13500: Loss = -11383.905311820588
2
Iteration 13600: Loss = -11383.897291272136
3
Iteration 13700: Loss = -11383.895094027643
4
Iteration 13800: Loss = -11383.894881555014
5
Iteration 13900: Loss = -11383.899567478175
6
Iteration 14000: Loss = -11383.941845868056
7
Iteration 14100: Loss = -11383.889904364787
Iteration 14200: Loss = -11383.895273712043
1
Iteration 14300: Loss = -11383.889444322032
Iteration 14400: Loss = -11383.889015536304
Iteration 14500: Loss = -11383.913262840786
1
Iteration 14600: Loss = -11383.890215195574
2
Iteration 14700: Loss = -11383.887151433593
Iteration 14800: Loss = -11383.890704669931
1
Iteration 14900: Loss = -11383.887279333134
2
Iteration 15000: Loss = -11383.891752332906
3
Iteration 15100: Loss = -11383.888048653935
4
Iteration 15200: Loss = -11383.887304334881
5
Iteration 15300: Loss = -11383.894740741202
6
Iteration 15400: Loss = -11383.968721133067
7
Iteration 15500: Loss = -11383.894975338588
8
Iteration 15600: Loss = -11383.887276199383
9
Iteration 15700: Loss = -11383.961233501626
10
Iteration 15800: Loss = -11383.887135520636
Iteration 15900: Loss = -11383.890153060624
1
Iteration 16000: Loss = -11383.89317074278
2
Iteration 16100: Loss = -11383.89536705102
3
Iteration 16200: Loss = -11383.890576819538
4
Iteration 16300: Loss = -11383.887294018341
5
Iteration 16400: Loss = -11384.011914870165
6
Iteration 16500: Loss = -11383.887241362336
7
Iteration 16600: Loss = -11383.888901485772
8
Iteration 16700: Loss = -11383.893330622665
9
Iteration 16800: Loss = -11383.909954797824
10
Iteration 16900: Loss = -11383.887073864
Iteration 17000: Loss = -11383.88727108041
1
Iteration 17100: Loss = -11383.886781115894
Iteration 17200: Loss = -11383.886665802798
Iteration 17300: Loss = -11383.886706972044
Iteration 17400: Loss = -11383.914429092829
1
Iteration 17500: Loss = -11383.89688282886
2
Iteration 17600: Loss = -11384.081900781472
3
Iteration 17700: Loss = -11383.885913996937
Iteration 17800: Loss = -11383.891531842875
1
Iteration 17900: Loss = -11383.890651696158
2
Iteration 18000: Loss = -11383.88936183508
3
Iteration 18100: Loss = -11383.88588824537
Iteration 18200: Loss = -11383.891555760938
1
Iteration 18300: Loss = -11383.892267488174
2
Iteration 18400: Loss = -11383.887352763864
3
Iteration 18500: Loss = -11383.887568679553
4
Iteration 18600: Loss = -11383.886861946496
5
Iteration 18700: Loss = -11383.88624331338
6
Iteration 18800: Loss = -11383.93908455104
7
Iteration 18900: Loss = -11383.88711681428
8
Iteration 19000: Loss = -11383.890479028785
9
Iteration 19100: Loss = -11383.886796359506
10
Iteration 19200: Loss = -11383.90837424963
11
Iteration 19300: Loss = -11383.914606697743
12
Iteration 19400: Loss = -11383.906971361737
13
Iteration 19500: Loss = -11383.981110141951
14
Iteration 19600: Loss = -11383.89456891405
15
Stopping early at iteration 19600 due to no improvement.
pi: tensor([[0.7660, 0.2340],
        [0.3117, 0.6883]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4335, 0.5665], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1961, 0.1015],
         [0.5156, 0.3908]],

        [[0.6611, 0.1044],
         [0.5309, 0.6919]],

        [[0.6964, 0.0993],
         [0.6557, 0.6152]],

        [[0.5772, 0.0968],
         [0.6390, 0.5066]],

        [[0.5073, 0.0902],
         [0.6924, 0.6630]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999982810025
Average Adjusted Rand Index: 0.9919996552039955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21122.28253161359
Iteration 100: Loss = -12072.501161753245
Iteration 200: Loss = -12067.749396043504
Iteration 300: Loss = -12064.290121475391
Iteration 400: Loss = -12062.750269007747
Iteration 500: Loss = -12062.51311384498
Iteration 600: Loss = -12062.408383565025
Iteration 700: Loss = -12062.343854816356
Iteration 800: Loss = -12062.310095500401
Iteration 900: Loss = -12062.289013878708
Iteration 1000: Loss = -12062.275025498775
Iteration 1100: Loss = -12062.265003390256
Iteration 1200: Loss = -12062.257519369427
Iteration 1300: Loss = -12062.251574454189
Iteration 1400: Loss = -12062.246882965133
Iteration 1500: Loss = -12062.243077449904
Iteration 1600: Loss = -12062.240115306588
Iteration 1700: Loss = -12062.23785807483
Iteration 1800: Loss = -12062.236072868505
Iteration 1900: Loss = -12062.234568303516
Iteration 2000: Loss = -12062.233314915557
Iteration 2100: Loss = -12062.232299791067
Iteration 2200: Loss = -12062.231420675635
Iteration 2300: Loss = -12062.230650939615
Iteration 2400: Loss = -12062.229992038005
Iteration 2500: Loss = -12062.229389582888
Iteration 2600: Loss = -12062.228892567839
Iteration 2700: Loss = -12062.22846389704
Iteration 2800: Loss = -12062.228056599668
Iteration 2900: Loss = -12062.227726106594
Iteration 3000: Loss = -12062.227403806863
Iteration 3100: Loss = -12062.227147280704
Iteration 3200: Loss = -12062.226925672077
Iteration 3300: Loss = -12062.22670205401
Iteration 3400: Loss = -12062.226515139715
Iteration 3500: Loss = -12062.226339684667
Iteration 3600: Loss = -12062.22619070565
Iteration 3700: Loss = -12062.226030745946
Iteration 3800: Loss = -12062.22584922704
Iteration 3900: Loss = -12062.225755104188
Iteration 4000: Loss = -12062.22565547131
Iteration 4100: Loss = -12062.225584850166
Iteration 4200: Loss = -12062.225464864685
Iteration 4300: Loss = -12062.225385728014
Iteration 4400: Loss = -12062.225328637423
Iteration 4500: Loss = -12062.225300538645
Iteration 4600: Loss = -12062.225236240138
Iteration 4700: Loss = -12062.225191183556
Iteration 4800: Loss = -12062.225297050249
1
Iteration 4900: Loss = -12062.225066058823
Iteration 5000: Loss = -12062.22502929031
Iteration 5100: Loss = -12062.225129029728
Iteration 5200: Loss = -12062.225101092454
Iteration 5300: Loss = -12062.22487748832
Iteration 5400: Loss = -12062.225082611349
1
Iteration 5500: Loss = -12062.224792462473
Iteration 5600: Loss = -12062.22480088196
Iteration 5700: Loss = -12062.22475404367
Iteration 5800: Loss = -12062.225413590568
1
Iteration 5900: Loss = -12062.224733257359
Iteration 6000: Loss = -12062.22469517032
Iteration 6100: Loss = -12062.224726283213
Iteration 6200: Loss = -12062.22482555056
Iteration 6300: Loss = -12062.224646832192
Iteration 6400: Loss = -12062.224961208403
1
Iteration 6500: Loss = -12062.224626675952
Iteration 6600: Loss = -12062.225784486434
1
Iteration 6700: Loss = -12062.225582342953
2
Iteration 6800: Loss = -12062.224604158011
Iteration 6900: Loss = -12062.22480526933
1
Iteration 7000: Loss = -12062.225147709492
2
Iteration 7100: Loss = -12062.224590020565
Iteration 7200: Loss = -12062.225169984182
1
Iteration 7300: Loss = -12062.224573835114
Iteration 7400: Loss = -12062.22546212973
1
Iteration 7500: Loss = -12062.224497514382
Iteration 7600: Loss = -12062.300956347435
1
Iteration 7700: Loss = -12062.224489653425
Iteration 7800: Loss = -12062.224555521203
Iteration 7900: Loss = -12062.22569316108
1
Iteration 8000: Loss = -12062.22452538587
Iteration 8100: Loss = -12062.224707859548
1
Iteration 8200: Loss = -12062.2245034647
Iteration 8300: Loss = -12062.291981650598
1
Iteration 8400: Loss = -12062.2244569837
Iteration 8500: Loss = -12062.224479755681
Iteration 8600: Loss = -12062.313287338591
1
Iteration 8700: Loss = -12062.224461622058
Iteration 8800: Loss = -12062.224482942123
Iteration 8900: Loss = -12062.238336820228
1
Iteration 9000: Loss = -12062.22447222524
Iteration 9100: Loss = -12062.224431751998
Iteration 9200: Loss = -12062.224915887242
1
Iteration 9300: Loss = -12062.22446983004
Iteration 9400: Loss = -12062.224418822485
Iteration 9500: Loss = -12062.30619484127
1
Iteration 9600: Loss = -12062.2244341747
Iteration 9700: Loss = -12062.224460261112
Iteration 9800: Loss = -12062.275064583153
1
Iteration 9900: Loss = -12062.224456386517
Iteration 10000: Loss = -12062.22444053464
Iteration 10100: Loss = -12062.433321465121
1
Iteration 10200: Loss = -12062.224426832794
Iteration 10300: Loss = -12062.224445776616
Iteration 10400: Loss = -12062.251227053872
1
Iteration 10500: Loss = -12062.2244647771
Iteration 10600: Loss = -12062.224437764213
Iteration 10700: Loss = -12062.229960124854
1
Iteration 10800: Loss = -12062.224425382867
Iteration 10900: Loss = -12062.224482609656
Iteration 11000: Loss = -12062.22454776429
Iteration 11100: Loss = -12062.224437795305
Iteration 11200: Loss = -12062.631355569605
1
Iteration 11300: Loss = -12062.224459306926
Iteration 11400: Loss = -12062.224458186205
Iteration 11500: Loss = -12062.267710871802
1
Iteration 11600: Loss = -12062.224448559537
Iteration 11700: Loss = -12062.22579979044
1
Iteration 11800: Loss = -12062.224544268589
Iteration 11900: Loss = -12062.225622724607
1
Iteration 12000: Loss = -12062.225906839994
2
Iteration 12100: Loss = -12062.248363228147
3
Iteration 12200: Loss = -12062.224441032597
Iteration 12300: Loss = -12062.225767676453
1
Iteration 12400: Loss = -12062.224437603478
Iteration 12500: Loss = -12062.224737450033
1
Iteration 12600: Loss = -12062.22447714512
Iteration 12700: Loss = -12062.2250716809
1
Iteration 12800: Loss = -12062.224380206617
Iteration 12900: Loss = -12062.2483621118
1
Iteration 13000: Loss = -12062.224461338825
Iteration 13100: Loss = -12062.2268388188
1
Iteration 13200: Loss = -12062.224425180695
Iteration 13300: Loss = -12062.237922798611
1
Iteration 13400: Loss = -12062.224413372292
Iteration 13500: Loss = -12062.227531697114
1
Iteration 13600: Loss = -12062.224448330518
Iteration 13700: Loss = -12062.237857468663
1
Iteration 13800: Loss = -12062.224433170528
Iteration 13900: Loss = -12062.291715476558
1
Iteration 14000: Loss = -12062.224431373952
Iteration 14100: Loss = -12062.224419983688
Iteration 14200: Loss = -12062.224565538725
1
Iteration 14300: Loss = -12062.224497054121
Iteration 14400: Loss = -12062.241860638593
1
Iteration 14500: Loss = -12062.227231674815
2
Iteration 14600: Loss = -12062.22669396363
3
Iteration 14700: Loss = -12062.232286650347
4
Iteration 14800: Loss = -12062.224427125304
Iteration 14900: Loss = -12062.224443903056
Iteration 15000: Loss = -12062.22488108798
1
Iteration 15100: Loss = -12062.224423918628
Iteration 15200: Loss = -12062.257209165466
1
Iteration 15300: Loss = -12062.224422309402
Iteration 15400: Loss = -12062.229873926557
1
Iteration 15500: Loss = -12062.224427442176
Iteration 15600: Loss = -12062.224433018751
Iteration 15700: Loss = -12062.224405423809
Iteration 15800: Loss = -12062.22443811578
Iteration 15900: Loss = -12062.473867706703
1
Iteration 16000: Loss = -12062.224417039515
Iteration 16100: Loss = -12062.2251140637
1
Iteration 16200: Loss = -12062.225092997229
2
Iteration 16300: Loss = -12062.2245198083
3
Iteration 16400: Loss = -12062.224614985647
4
Iteration 16500: Loss = -12062.22540520548
5
Iteration 16600: Loss = -12062.224510598768
Iteration 16700: Loss = -12062.226883889234
1
Iteration 16800: Loss = -12062.224461197029
Iteration 16900: Loss = -12062.224502432933
Iteration 17000: Loss = -12062.230384678862
1
Iteration 17100: Loss = -12062.224441997181
Iteration 17200: Loss = -12062.224849488712
1
Iteration 17300: Loss = -12062.238163115771
2
Iteration 17400: Loss = -12062.22443924355
Iteration 17500: Loss = -12062.22675020829
1
Iteration 17600: Loss = -12062.224417888532
Iteration 17700: Loss = -12062.22992869456
1
Iteration 17800: Loss = -12062.224452833789
Iteration 17900: Loss = -12062.231052148734
1
Iteration 18000: Loss = -12062.624523034532
2
Iteration 18100: Loss = -12062.224419791752
Iteration 18200: Loss = -12062.22488949182
1
Iteration 18300: Loss = -12062.224534561881
2
Iteration 18400: Loss = -12062.241993416199
3
Iteration 18500: Loss = -12062.224455505353
Iteration 18600: Loss = -12062.2244082663
Iteration 18700: Loss = -12062.22549009692
1
Iteration 18800: Loss = -12062.22443735066
Iteration 18900: Loss = -12062.313914645256
1
Iteration 19000: Loss = -12062.224422188563
Iteration 19100: Loss = -12062.224434739364
Iteration 19200: Loss = -12062.22651035068
1
Iteration 19300: Loss = -12062.22445731307
Iteration 19400: Loss = -12062.224612233877
1
Iteration 19500: Loss = -12062.239771064165
2
Iteration 19600: Loss = -12062.224779762812
3
Iteration 19700: Loss = -12062.225620966103
4
Iteration 19800: Loss = -12062.298746316003
5
Iteration 19900: Loss = -12062.22442806262
pi: tensor([[0.0691, 0.9309],
        [0.0403, 0.9597]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9369, 0.0631], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1947, 0.3147],
         [0.7034, 0.1829]],

        [[0.6857, 0.2884],
         [0.7182, 0.6017]],

        [[0.5983, 0.2434],
         [0.6020, 0.5906]],

        [[0.5636, 0.0932],
         [0.7011, 0.7300]],

        [[0.6851, 0.2978],
         [0.5080, 0.7235]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.01333633701284073
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.012184899471542247
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.014778186472389411
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
Global Adjusted Rand Index: 0.0070022223600666855
Average Adjusted Rand Index: -3.44096512224272e-05
11391.96636462653
[0.9919999982810025, 0.0070022223600666855] [0.9919996552039955, -3.44096512224272e-05] [11383.89456891405, 12062.224476034724]
-------------------------------------
This iteration is 57
True Objective function: Loss = -11753.37770267735
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21564.222054885595
Iteration 100: Loss = -12680.734882263718
Iteration 200: Loss = -12411.870470432204
Iteration 300: Loss = -11978.711068983734
Iteration 400: Loss = -11927.359823775581
Iteration 500: Loss = -11918.40557188608
Iteration 600: Loss = -11917.958056013693
Iteration 700: Loss = -11917.68020153794
Iteration 800: Loss = -11917.425321385406
Iteration 900: Loss = -11916.11795838927
Iteration 1000: Loss = -11913.927396585685
Iteration 1100: Loss = -11913.53375205107
Iteration 1200: Loss = -11913.343826521908
Iteration 1300: Loss = -11913.216554146213
Iteration 1400: Loss = -11910.356940896028
Iteration 1500: Loss = -11910.232548955304
Iteration 1600: Loss = -11910.203599434533
Iteration 1700: Loss = -11910.182791667841
Iteration 1800: Loss = -11910.166328472229
Iteration 1900: Loss = -11910.152836068579
Iteration 2000: Loss = -11910.141539129001
Iteration 2100: Loss = -11910.131977591125
Iteration 2200: Loss = -11910.123776146731
Iteration 2300: Loss = -11910.116649440799
Iteration 2400: Loss = -11910.110387460853
Iteration 2500: Loss = -11910.104917913028
Iteration 2600: Loss = -11910.100050369552
Iteration 2700: Loss = -11910.095645706635
Iteration 2800: Loss = -11910.091745358137
Iteration 2900: Loss = -11910.102584659764
1
Iteration 3000: Loss = -11910.08502495007
Iteration 3100: Loss = -11910.082104216826
Iteration 3200: Loss = -11910.098522401378
1
Iteration 3300: Loss = -11910.076893952968
Iteration 3400: Loss = -11910.074595625281
Iteration 3500: Loss = -11910.07224669737
Iteration 3600: Loss = -11910.070029744325
Iteration 3700: Loss = -11910.06763092528
Iteration 3800: Loss = -11910.06477292818
Iteration 3900: Loss = -11910.061025548628
Iteration 4000: Loss = -11910.055775465124
Iteration 4100: Loss = -11909.991067074896
Iteration 4200: Loss = -11909.428731122975
Iteration 4300: Loss = -11909.402830747684
Iteration 4400: Loss = -11908.705671702277
Iteration 4500: Loss = -11908.023721586615
Iteration 4600: Loss = -11907.920066153662
Iteration 4700: Loss = -11906.330087686889
Iteration 4800: Loss = -11905.64846845744
Iteration 4900: Loss = -11903.736048353416
Iteration 5000: Loss = -11903.710332123908
Iteration 5100: Loss = -11903.703667908987
Iteration 5200: Loss = -11903.692924826944
Iteration 5300: Loss = -11903.681691450513
Iteration 5400: Loss = -11903.68282998663
1
Iteration 5500: Loss = -11903.679012575203
Iteration 5600: Loss = -11903.678377756294
Iteration 5700: Loss = -11903.682655634357
1
Iteration 5800: Loss = -11903.676912801948
Iteration 5900: Loss = -11903.676507702936
Iteration 6000: Loss = -11903.686042682091
1
Iteration 6100: Loss = -11903.675503539967
Iteration 6200: Loss = -11903.681062091036
1
Iteration 6300: Loss = -11903.67459681762
Iteration 6400: Loss = -11903.675159513466
1
Iteration 6500: Loss = -11903.672028678284
Iteration 6600: Loss = -11902.969800906145
Iteration 6700: Loss = -11902.963672755312
Iteration 6800: Loss = -11902.962865714984
Iteration 6900: Loss = -11902.9624870478
Iteration 7000: Loss = -11902.962170888333
Iteration 7100: Loss = -11902.961886583455
Iteration 7200: Loss = -11902.961950781704
Iteration 7300: Loss = -11902.961412919985
Iteration 7400: Loss = -11902.96205604727
1
Iteration 7500: Loss = -11902.961100444278
Iteration 7600: Loss = -11902.964869041469
1
Iteration 7700: Loss = -11902.961565544816
2
Iteration 7800: Loss = -11902.96400199379
3
Iteration 7900: Loss = -11902.970319124714
4
Iteration 8000: Loss = -11902.960133928518
Iteration 8100: Loss = -11902.970924053861
1
Iteration 8200: Loss = -11902.964887947259
2
Iteration 8300: Loss = -11902.987676915758
3
Iteration 8400: Loss = -11902.956809353254
Iteration 8500: Loss = -11902.955808286028
Iteration 8600: Loss = -11902.956959071385
1
Iteration 8700: Loss = -11903.06475974859
2
Iteration 8800: Loss = -11902.95549559423
Iteration 8900: Loss = -11902.957140385457
1
Iteration 9000: Loss = -11902.95532072782
Iteration 9100: Loss = -11902.956721634158
1
Iteration 9200: Loss = -11902.955201019704
Iteration 9300: Loss = -11902.957585104177
1
Iteration 9400: Loss = -11902.990627659688
2
Iteration 9500: Loss = -11902.95524367677
Iteration 9600: Loss = -11902.963096051588
1
Iteration 9700: Loss = -11902.95495974857
Iteration 9800: Loss = -11902.955068484818
1
Iteration 9900: Loss = -11903.073996252488
2
Iteration 10000: Loss = -11902.957697080163
3
Iteration 10100: Loss = -11902.955393461001
4
Iteration 10200: Loss = -11902.959625141224
5
Iteration 10300: Loss = -11902.955946110802
6
Iteration 10400: Loss = -11902.958087879513
7
Iteration 10500: Loss = -11902.973856383149
8
Iteration 10600: Loss = -11902.954394224907
Iteration 10700: Loss = -11902.957499842554
1
Iteration 10800: Loss = -11902.956001350123
2
Iteration 10900: Loss = -11902.95499032065
3
Iteration 11000: Loss = -11902.954707124329
4
Iteration 11100: Loss = -11902.972440172332
5
Iteration 11200: Loss = -11902.954713611505
6
Iteration 11300: Loss = -11902.961882108644
7
Iteration 11400: Loss = -11902.970320993496
8
Iteration 11500: Loss = -11902.958319471509
9
Iteration 11600: Loss = -11902.95511361351
10
Iteration 11700: Loss = -11902.954698816093
11
Iteration 11800: Loss = -11902.954935743292
12
Iteration 11900: Loss = -11902.960593525946
13
Iteration 12000: Loss = -11902.955286521443
14
Iteration 12100: Loss = -11903.032625772625
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.6121, 0.3879],
        [0.2544, 0.7456]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8039, 0.1961], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2168, 0.1054],
         [0.6781, 0.4101]],

        [[0.5004, 0.0992],
         [0.5103, 0.6069]],

        [[0.5170, 0.0910],
         [0.6633, 0.6660]],

        [[0.5292, 0.1011],
         [0.7176, 0.6702]],

        [[0.6810, 0.1023],
         [0.5395, 0.6874]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 32
Adjusted Rand Index: 0.1234375
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5232231679026662
Average Adjusted Rand Index: 0.8166867163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23863.642774294534
Iteration 100: Loss = -12691.966733958736
Iteration 200: Loss = -12444.152441755185
Iteration 300: Loss = -12243.948580929853
Iteration 400: Loss = -12050.28403880321
Iteration 500: Loss = -11965.866682016558
Iteration 600: Loss = -11910.567976186534
Iteration 700: Loss = -11872.977707805518
Iteration 800: Loss = -11847.63550180791
Iteration 900: Loss = -11819.123179581222
Iteration 1000: Loss = -11797.173793073034
Iteration 1100: Loss = -11796.941184672574
Iteration 1200: Loss = -11796.782105922966
Iteration 1300: Loss = -11792.406399984671
Iteration 1400: Loss = -11790.960220779467
Iteration 1500: Loss = -11782.289381302855
Iteration 1600: Loss = -11782.115956528889
Iteration 1700: Loss = -11763.614992439165
Iteration 1800: Loss = -11763.551357403621
Iteration 1900: Loss = -11763.517792148628
Iteration 2000: Loss = -11763.49155284941
Iteration 2100: Loss = -11763.4701849741
Iteration 2200: Loss = -11763.452241104911
Iteration 2300: Loss = -11763.436871108866
Iteration 2400: Loss = -11763.423267878927
Iteration 2500: Loss = -11763.410691623872
Iteration 2600: Loss = -11763.3974364538
Iteration 2700: Loss = -11763.366240574722
Iteration 2800: Loss = -11753.158055957936
Iteration 2900: Loss = -11753.14866212332
Iteration 3000: Loss = -11753.141200770191
Iteration 3100: Loss = -11753.134710890896
Iteration 3200: Loss = -11753.128837179629
Iteration 3300: Loss = -11753.124439006253
Iteration 3400: Loss = -11753.118632987631
Iteration 3500: Loss = -11753.11377340508
Iteration 3600: Loss = -11753.109142818028
Iteration 3700: Loss = -11753.09050449339
Iteration 3800: Loss = -11746.471571386463
Iteration 3900: Loss = -11746.468782263804
Iteration 4000: Loss = -11746.464255004512
Iteration 4100: Loss = -11746.461371121628
Iteration 4200: Loss = -11746.471417849292
1
Iteration 4300: Loss = -11746.456386907785
Iteration 4400: Loss = -11746.454115543165
Iteration 4500: Loss = -11746.467163489431
1
Iteration 4600: Loss = -11746.449786657326
Iteration 4700: Loss = -11746.44731411274
Iteration 4800: Loss = -11746.443847604907
Iteration 4900: Loss = -11746.435512743954
Iteration 5000: Loss = -11746.427842100478
Iteration 5100: Loss = -11746.42607995345
Iteration 5200: Loss = -11746.431419112621
1
Iteration 5300: Loss = -11746.436325745468
2
Iteration 5400: Loss = -11746.422652290683
Iteration 5500: Loss = -11746.421729903372
Iteration 5600: Loss = -11746.426214104098
1
Iteration 5700: Loss = -11746.419998922396
Iteration 5800: Loss = -11746.419317739697
Iteration 5900: Loss = -11746.41984820806
1
Iteration 6000: Loss = -11746.417969062624
Iteration 6100: Loss = -11746.417361203825
Iteration 6200: Loss = -11746.421210849314
1
Iteration 6300: Loss = -11746.416274173924
Iteration 6400: Loss = -11746.416129868525
Iteration 6500: Loss = -11746.415633649029
Iteration 6600: Loss = -11746.41482117726
Iteration 6700: Loss = -11746.417995192774
1
Iteration 6800: Loss = -11746.414207802274
Iteration 6900: Loss = -11746.413565062354
Iteration 7000: Loss = -11746.41320160804
Iteration 7100: Loss = -11746.412799692082
Iteration 7200: Loss = -11746.458450887254
1
Iteration 7300: Loss = -11746.413395590496
2
Iteration 7400: Loss = -11746.418098814784
3
Iteration 7500: Loss = -11746.411219148495
Iteration 7600: Loss = -11746.457956524038
1
Iteration 7700: Loss = -11746.410504734415
Iteration 7800: Loss = -11746.41556836478
1
Iteration 7900: Loss = -11746.58597558317
2
Iteration 8000: Loss = -11746.409850345455
Iteration 8100: Loss = -11746.411381179694
1
Iteration 8200: Loss = -11746.409415978447
Iteration 8300: Loss = -11746.509307476403
1
Iteration 8400: Loss = -11746.408773651938
Iteration 8500: Loss = -11746.411283497775
1
Iteration 8600: Loss = -11746.40943244183
2
Iteration 8700: Loss = -11746.407908573194
Iteration 8800: Loss = -11746.41245687648
1
Iteration 8900: Loss = -11746.407678740792
Iteration 9000: Loss = -11746.408021185482
1
Iteration 9100: Loss = -11746.407571357435
Iteration 9200: Loss = -11746.408465266875
1
Iteration 9300: Loss = -11746.407329405343
Iteration 9400: Loss = -11746.433807753558
1
Iteration 9500: Loss = -11746.413781993873
2
Iteration 9600: Loss = -11746.40885762399
3
Iteration 9700: Loss = -11746.414107402765
4
Iteration 9800: Loss = -11746.408448928081
5
Iteration 9900: Loss = -11746.425323965464
6
Iteration 10000: Loss = -11746.408187563953
7
Iteration 10100: Loss = -11746.415110099397
8
Iteration 10200: Loss = -11746.408495284535
9
Iteration 10300: Loss = -11746.407800969786
10
Iteration 10400: Loss = -11746.407451290814
11
Iteration 10500: Loss = -11746.407905724996
12
Iteration 10600: Loss = -11746.407225525865
Iteration 10700: Loss = -11746.418028744349
1
Iteration 10800: Loss = -11746.449800279255
2
Iteration 10900: Loss = -11746.406288321
Iteration 11000: Loss = -11746.406669084145
1
Iteration 11100: Loss = -11746.452078886186
2
Iteration 11200: Loss = -11746.42906952217
3
Iteration 11300: Loss = -11746.407759840056
4
Iteration 11400: Loss = -11746.409794051826
5
Iteration 11500: Loss = -11746.414293491029
6
Iteration 11600: Loss = -11746.408195748763
7
Iteration 11700: Loss = -11746.477516152203
8
Iteration 11800: Loss = -11746.415618464529
9
Iteration 11900: Loss = -11746.40691785257
10
Iteration 12000: Loss = -11746.407430448802
11
Iteration 12100: Loss = -11746.407448451535
12
Iteration 12200: Loss = -11746.542647116405
13
Iteration 12300: Loss = -11746.405780025967
Iteration 12400: Loss = -11746.469301882022
1
Iteration 12500: Loss = -11746.40689103173
2
Iteration 12600: Loss = -11746.435185050292
3
Iteration 12700: Loss = -11746.409932770955
4
Iteration 12800: Loss = -11746.408341985907
5
Iteration 12900: Loss = -11746.43265803879
6
Iteration 13000: Loss = -11746.431469410507
7
Iteration 13100: Loss = -11746.416650590818
8
Iteration 13200: Loss = -11746.413448753701
9
Iteration 13300: Loss = -11746.405909739573
10
Iteration 13400: Loss = -11746.40682725202
11
Iteration 13500: Loss = -11746.409428371235
12
Iteration 13600: Loss = -11746.420094461299
13
Iteration 13700: Loss = -11746.40999263863
14
Iteration 13800: Loss = -11746.409627629355
15
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[0.7138, 0.2862],
        [0.2154, 0.7846]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5193, 0.4807], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2068, 0.0941],
         [0.6844, 0.4084]],

        [[0.7240, 0.0992],
         [0.5061, 0.5871]],

        [[0.5435, 0.0905],
         [0.5581, 0.7003]],

        [[0.7091, 0.1009],
         [0.7280, 0.5865]],

        [[0.6909, 0.1020],
         [0.7204, 0.5186]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999034306759
Average Adjusted Rand Index: 0.9919992163297293
11753.37770267735
[0.5232231679026662, 0.9919999034306759] [0.8166867163297293, 0.9919992163297293] [11903.032625772625, 11746.409627629355]
-------------------------------------
This iteration is 58
True Objective function: Loss = -11412.113013192185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22165.31651338879
Iteration 100: Loss = -12133.915082418396
Iteration 200: Loss = -12120.406575843164
Iteration 300: Loss = -12119.692928001488
Iteration 400: Loss = -12119.455796363407
Iteration 500: Loss = -12119.33037827041
Iteration 600: Loss = -12119.247209500238
Iteration 700: Loss = -12119.167684939499
Iteration 800: Loss = -12119.001382543403
Iteration 900: Loss = -12115.14448580129
Iteration 1000: Loss = -12108.816718646747
Iteration 1100: Loss = -12108.690686604124
Iteration 1200: Loss = -12108.649665815747
Iteration 1300: Loss = -12108.628170530388
Iteration 1400: Loss = -12108.615743654278
Iteration 1500: Loss = -12108.60790174306
Iteration 1600: Loss = -12108.602272921822
Iteration 1700: Loss = -12108.59788191462
Iteration 1800: Loss = -12108.594317337758
Iteration 1900: Loss = -12108.59153178925
Iteration 2000: Loss = -12108.589378497676
Iteration 2100: Loss = -12108.587786116565
Iteration 2200: Loss = -12108.586418277575
Iteration 2300: Loss = -12108.585290329973
Iteration 2400: Loss = -12108.584327493994
Iteration 2500: Loss = -12108.583562550742
Iteration 2600: Loss = -12108.58282172438
Iteration 2700: Loss = -12108.582206801824
Iteration 2800: Loss = -12108.581667562785
Iteration 2900: Loss = -12108.581140963233
Iteration 3000: Loss = -12108.580734665033
Iteration 3100: Loss = -12108.580342083053
Iteration 3200: Loss = -12108.580053126558
Iteration 3300: Loss = -12108.579675094632
Iteration 3400: Loss = -12108.579355436494
Iteration 3500: Loss = -12108.579067801304
Iteration 3600: Loss = -12108.578849004445
Iteration 3700: Loss = -12108.578611790213
Iteration 3800: Loss = -12108.578367124699
Iteration 3900: Loss = -12108.578101921876
Iteration 4000: Loss = -12108.577779658353
Iteration 4100: Loss = -12108.577355673018
Iteration 4200: Loss = -12108.577078722858
Iteration 4300: Loss = -12108.576885547223
Iteration 4400: Loss = -12108.576739387458
Iteration 4500: Loss = -12108.576573930755
Iteration 4600: Loss = -12108.576492307851
Iteration 4700: Loss = -12108.576405595253
Iteration 4800: Loss = -12108.576271474354
Iteration 4900: Loss = -12108.576182283285
Iteration 5000: Loss = -12108.577136866112
1
Iteration 5100: Loss = -12108.57603425693
Iteration 5200: Loss = -12108.575945669923
Iteration 5300: Loss = -12108.575908196117
Iteration 5400: Loss = -12108.57585109001
Iteration 5500: Loss = -12108.575862755168
Iteration 5600: Loss = -12108.575767928252
Iteration 5700: Loss = -12108.575695347288
Iteration 5800: Loss = -12108.57570299345
Iteration 5900: Loss = -12108.57593515408
1
Iteration 6000: Loss = -12108.57562421039
Iteration 6100: Loss = -12108.57562743605
Iteration 6200: Loss = -12108.575606024342
Iteration 6300: Loss = -12108.575534968744
Iteration 6400: Loss = -12108.576057128364
1
Iteration 6500: Loss = -12108.575493966186
Iteration 6600: Loss = -12108.578893583699
1
Iteration 6700: Loss = -12108.575461586604
Iteration 6800: Loss = -12108.57543127011
Iteration 6900: Loss = -12108.575406262255
Iteration 7000: Loss = -12108.575442478697
Iteration 7100: Loss = -12108.576035683016
1
Iteration 7200: Loss = -12108.616142392535
2
Iteration 7300: Loss = -12108.575336500684
Iteration 7400: Loss = -12108.575560537858
1
Iteration 7500: Loss = -12108.651765270795
2
Iteration 7600: Loss = -12108.575304892422
Iteration 7700: Loss = -12108.919820966405
1
Iteration 7800: Loss = -12108.575234548967
Iteration 7900: Loss = -12108.575199635949
Iteration 8000: Loss = -12108.577369328841
1
Iteration 8100: Loss = -12108.575170012386
Iteration 8200: Loss = -12108.575161769688
Iteration 8300: Loss = -12108.575347979944
1
Iteration 8400: Loss = -12108.575148775275
Iteration 8500: Loss = -12108.575157587578
Iteration 8600: Loss = -12108.576229084074
1
Iteration 8700: Loss = -12108.575127743206
Iteration 8800: Loss = -12108.575101322303
Iteration 8900: Loss = -12108.575457984487
1
Iteration 9000: Loss = -12108.575090515531
Iteration 9100: Loss = -12108.575138328102
Iteration 9200: Loss = -12108.575747662948
1
Iteration 9300: Loss = -12108.575103390667
Iteration 9400: Loss = -12108.57508813262
Iteration 9500: Loss = -12108.575822321593
1
Iteration 9600: Loss = -12108.5750884027
Iteration 9700: Loss = -12108.575088299138
Iteration 9800: Loss = -12108.576537021985
1
Iteration 9900: Loss = -12108.575068931852
Iteration 10000: Loss = -12108.575081672494
Iteration 10100: Loss = -12108.577203491861
1
Iteration 10200: Loss = -12108.575074940727
Iteration 10300: Loss = -12108.57507716409
Iteration 10400: Loss = -12108.575290279494
1
Iteration 10500: Loss = -12108.575045680205
Iteration 10600: Loss = -12109.00507592863
1
Iteration 10700: Loss = -12108.575106848542
Iteration 10800: Loss = -12108.575079273432
Iteration 10900: Loss = -12108.594572845996
1
Iteration 11000: Loss = -12108.57504282778
Iteration 11100: Loss = -12108.608595964331
1
Iteration 11200: Loss = -12108.577777143544
2
Iteration 11300: Loss = -12108.5751025516
Iteration 11400: Loss = -12108.57679194742
1
Iteration 11500: Loss = -12108.575043555573
Iteration 11600: Loss = -12108.57691165549
1
Iteration 11700: Loss = -12108.575028984753
Iteration 11800: Loss = -12108.600251118874
1
Iteration 11900: Loss = -12108.575035897251
Iteration 12000: Loss = -12108.590172297048
1
Iteration 12100: Loss = -12108.575093981624
Iteration 12200: Loss = -12108.575110079897
Iteration 12300: Loss = -12108.575229664993
1
Iteration 12400: Loss = -12108.578522707625
2
Iteration 12500: Loss = -12108.575851358715
3
Iteration 12600: Loss = -12108.575199337762
Iteration 12700: Loss = -12108.575906005022
1
Iteration 12800: Loss = -12108.575229795475
Iteration 12900: Loss = -12108.575223451531
Iteration 13000: Loss = -12108.847277430494
1
Iteration 13100: Loss = -12108.575070513927
Iteration 13200: Loss = -12108.577340659318
1
Iteration 13300: Loss = -12108.575531302225
2
Iteration 13400: Loss = -12108.576876612271
3
Iteration 13500: Loss = -12108.576502005077
4
Iteration 13600: Loss = -12108.579504856221
5
Iteration 13700: Loss = -12108.580456639223
6
Iteration 13800: Loss = -12108.57507962577
Iteration 13900: Loss = -12108.576447094723
1
Iteration 14000: Loss = -12108.577009220118
2
Iteration 14100: Loss = -12108.575087667481
Iteration 14200: Loss = -12108.589310684456
1
Iteration 14300: Loss = -12108.575089493264
Iteration 14400: Loss = -12108.575905477519
1
Iteration 14500: Loss = -12108.578214924519
2
Iteration 14600: Loss = -12108.575354828832
3
Iteration 14700: Loss = -12108.576743057498
4
Iteration 14800: Loss = -12108.57665556536
5
Iteration 14900: Loss = -12108.582668076062
6
Iteration 15000: Loss = -12108.576288759412
7
Iteration 15100: Loss = -12108.577654211258
8
Iteration 15200: Loss = -12108.57576152931
9
Iteration 15300: Loss = -12108.579311120215
10
Iteration 15400: Loss = -12108.575138283955
Iteration 15500: Loss = -12108.575081638171
Iteration 15600: Loss = -12108.5756372134
1
Iteration 15700: Loss = -12108.586277397058
2
Iteration 15800: Loss = -12108.618805243992
3
Iteration 15900: Loss = -12108.6727160171
4
Iteration 16000: Loss = -12108.575482139253
5
Iteration 16100: Loss = -12108.575194234872
6
Iteration 16200: Loss = -12108.575426567082
7
Iteration 16300: Loss = -12108.589082197532
8
Iteration 16400: Loss = -12108.58818804753
9
Iteration 16500: Loss = -12108.579344628197
10
Iteration 16600: Loss = -12108.57703816559
11
Iteration 16700: Loss = -12108.575471021588
12
Iteration 16800: Loss = -12108.587229072502
13
Iteration 16900: Loss = -12108.583416788764
14
Iteration 17000: Loss = -12108.575070665715
Iteration 17100: Loss = -12108.57524803271
1
Iteration 17200: Loss = -12108.58192885467
2
Iteration 17300: Loss = -12108.576965501059
3
Iteration 17400: Loss = -12108.575093795853
Iteration 17500: Loss = -12108.583273743825
1
Iteration 17600: Loss = -12108.58862586096
2
Iteration 17700: Loss = -12108.575059910901
Iteration 17800: Loss = -12108.609805441663
1
Iteration 17900: Loss = -12108.575039035855
Iteration 18000: Loss = -12108.576060417461
1
Iteration 18100: Loss = -12108.577625935184
2
Iteration 18200: Loss = -12108.575315155143
3
Iteration 18300: Loss = -12108.57608777008
4
Iteration 18400: Loss = -12108.600877159199
5
Iteration 18500: Loss = -12108.581344933356
6
Iteration 18600: Loss = -12108.585715368
7
Iteration 18700: Loss = -12108.577139080891
8
Iteration 18800: Loss = -12108.575425793255
9
Iteration 18900: Loss = -12108.575086398045
Iteration 19000: Loss = -12108.58607855493
1
Iteration 19100: Loss = -12108.580424814794
2
Iteration 19200: Loss = -12108.577206270096
3
Iteration 19300: Loss = -12108.575127445798
Iteration 19400: Loss = -12108.575187074639
Iteration 19500: Loss = -12108.575444471184
1
Iteration 19600: Loss = -12108.575264837958
Iteration 19700: Loss = -12108.59869722881
1
Iteration 19800: Loss = -12108.588123176509
2
Iteration 19900: Loss = -12108.575077954583
pi: tensor([[0.9661, 0.0339],
        [0.8136, 0.1864]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0530, 0.9470], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.2952],
         [0.6992, 0.1730]],

        [[0.7165, 0.1056],
         [0.6609, 0.7155]],

        [[0.7304, 0.3105],
         [0.5887, 0.5947]],

        [[0.6558, 0.3309],
         [0.6286, 0.5282]],

        [[0.5683, 0.2492],
         [0.5936, 0.5712]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.030303030303030304
time is 1
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.05818181818181818
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
Global Adjusted Rand Index: 0.0007221147639520706
Average Adjusted Rand Index: 0.01861644127638166
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20341.617262698794
Iteration 100: Loss = -12137.925884391982
Iteration 200: Loss = -12115.765335212773
Iteration 300: Loss = -11652.521378730034
Iteration 400: Loss = -11464.262075417213
Iteration 500: Loss = -11429.04617680624
Iteration 600: Loss = -11414.824499001837
Iteration 700: Loss = -11414.59080415132
Iteration 800: Loss = -11414.425572664279
Iteration 900: Loss = -11414.34244367235
Iteration 1000: Loss = -11414.283749824492
Iteration 1100: Loss = -11414.237648490014
Iteration 1200: Loss = -11414.16769052116
Iteration 1300: Loss = -11414.130278024644
Iteration 1400: Loss = -11414.106740237392
Iteration 1500: Loss = -11408.764080075025
Iteration 1600: Loss = -11408.720123040814
Iteration 1700: Loss = -11408.70275748109
Iteration 1800: Loss = -11408.689875675704
Iteration 1900: Loss = -11408.662249141373
Iteration 2000: Loss = -11408.607888045597
Iteration 2100: Loss = -11408.600541766458
Iteration 2200: Loss = -11408.594734462775
Iteration 2300: Loss = -11408.58977897898
Iteration 2400: Loss = -11408.585455003862
Iteration 2500: Loss = -11408.581575944325
Iteration 2600: Loss = -11408.578213560044
Iteration 2700: Loss = -11408.575127212918
Iteration 2800: Loss = -11408.572346916499
Iteration 2900: Loss = -11408.573220269915
1
Iteration 3000: Loss = -11408.56752938407
Iteration 3100: Loss = -11408.565383516569
Iteration 3200: Loss = -11408.565340075724
Iteration 3300: Loss = -11408.56171239144
Iteration 3400: Loss = -11408.560175350249
Iteration 3500: Loss = -11408.558764750678
Iteration 3600: Loss = -11408.557477966835
Iteration 3700: Loss = -11408.556301361848
Iteration 3800: Loss = -11408.555137058585
Iteration 3900: Loss = -11408.55432472134
Iteration 4000: Loss = -11408.553139333631
Iteration 4100: Loss = -11408.552203277284
Iteration 4200: Loss = -11408.552118249225
Iteration 4300: Loss = -11408.550473736408
Iteration 4400: Loss = -11408.549657001688
Iteration 4500: Loss = -11408.548788734379
Iteration 4600: Loss = -11408.548036143344
Iteration 4700: Loss = -11408.546252811131
Iteration 4800: Loss = -11408.534406787194
Iteration 4900: Loss = -11408.522800358482
Iteration 5000: Loss = -11408.522051865062
Iteration 5100: Loss = -11408.521528989304
Iteration 5200: Loss = -11408.520995973371
Iteration 5300: Loss = -11408.52043417688
Iteration 5400: Loss = -11408.519764595196
Iteration 5500: Loss = -11408.517269142814
Iteration 5600: Loss = -11408.516334661272
Iteration 5700: Loss = -11408.515715668324
Iteration 5800: Loss = -11408.51583112375
1
Iteration 5900: Loss = -11408.515227450569
Iteration 6000: Loss = -11408.517927977713
1
Iteration 6100: Loss = -11408.519869160162
2
Iteration 6200: Loss = -11408.515339148134
3
Iteration 6300: Loss = -11408.518024922332
4
Iteration 6400: Loss = -11408.514232270107
Iteration 6500: Loss = -11408.514085218123
Iteration 6600: Loss = -11408.515235473915
1
Iteration 6700: Loss = -11408.514332816416
2
Iteration 6800: Loss = -11408.515037882316
3
Iteration 6900: Loss = -11408.51399401344
Iteration 7000: Loss = -11408.513760532176
Iteration 7100: Loss = -11408.514134892881
1
Iteration 7200: Loss = -11408.515665331724
2
Iteration 7300: Loss = -11408.521514543898
3
Iteration 7400: Loss = -11408.51308835944
Iteration 7500: Loss = -11408.633273195837
1
Iteration 7600: Loss = -11408.512710777937
Iteration 7700: Loss = -11408.517894326562
1
Iteration 7800: Loss = -11408.571300255287
2
Iteration 7900: Loss = -11408.514993448913
3
Iteration 8000: Loss = -11408.515884686418
4
Iteration 8100: Loss = -11408.51229341231
Iteration 8200: Loss = -11408.512404876667
1
Iteration 8300: Loss = -11408.525665857705
2
Iteration 8400: Loss = -11408.51119894439
Iteration 8500: Loss = -11408.511457840175
1
Iteration 8600: Loss = -11408.510778343434
Iteration 8700: Loss = -11408.510631663406
Iteration 8800: Loss = -11408.515117570028
1
Iteration 8900: Loss = -11408.511178734867
2
Iteration 9000: Loss = -11408.520309987529
3
Iteration 9100: Loss = -11408.510431413739
Iteration 9200: Loss = -11408.510292744935
Iteration 9300: Loss = -11408.510327712194
Iteration 9400: Loss = -11408.513232853113
1
Iteration 9500: Loss = -11408.512525967159
2
Iteration 9600: Loss = -11408.510035101528
Iteration 9700: Loss = -11408.569830974098
1
Iteration 9800: Loss = -11408.510591458667
2
Iteration 9900: Loss = -11408.510712394576
3
Iteration 10000: Loss = -11408.51336665043
4
Iteration 10100: Loss = -11408.523675204917
5
Iteration 10200: Loss = -11408.51035138558
6
Iteration 10300: Loss = -11408.510642643978
7
Iteration 10400: Loss = -11408.51273371637
8
Iteration 10500: Loss = -11408.571813032784
9
Iteration 10600: Loss = -11408.612682108535
10
Iteration 10700: Loss = -11408.522833290841
11
Iteration 10800: Loss = -11408.51076552348
12
Iteration 10900: Loss = -11408.516569481228
13
Iteration 11000: Loss = -11408.510371978218
14
Iteration 11100: Loss = -11408.514909012243
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.7493, 0.2507],
        [0.2565, 0.7435]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5497, 0.4503], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0984],
         [0.6376, 0.3925]],

        [[0.6274, 0.0974],
         [0.7160, 0.5349]],

        [[0.7133, 0.0959],
         [0.6537, 0.5835]],

        [[0.6542, 0.0968],
         [0.6337, 0.6893]],

        [[0.6954, 0.1003],
         [0.6757, 0.5877]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919998119331364
11412.113013192185
[0.0007221147639520706, 0.9919999997943784] [0.01861644127638166, 0.9919998119331364] [12108.57572088243, 11408.514909012243]
-------------------------------------
This iteration is 59
True Objective function: Loss = -11514.304214203867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22206.975161149985
Iteration 100: Loss = -12228.215178356142
Iteration 200: Loss = -12227.368296493805
Iteration 300: Loss = -12223.963754021752
Iteration 400: Loss = -11934.362801128316
Iteration 500: Loss = -11568.381081330368
Iteration 600: Loss = -11547.407496546692
Iteration 700: Loss = -11536.396755441678
Iteration 800: Loss = -11527.542947460337
Iteration 900: Loss = -11506.546152254487
Iteration 1000: Loss = -11506.440728481037
Iteration 1100: Loss = -11506.394604353149
Iteration 1200: Loss = -11506.362549988075
Iteration 1300: Loss = -11506.338549250291
Iteration 1400: Loss = -11506.319881348149
Iteration 1500: Loss = -11506.305041862992
Iteration 1600: Loss = -11506.292909098824
Iteration 1700: Loss = -11506.282812122732
Iteration 1800: Loss = -11506.274251525483
Iteration 1900: Loss = -11506.26676651043
Iteration 2000: Loss = -11506.259895659467
Iteration 2100: Loss = -11506.25315917629
Iteration 2200: Loss = -11506.246741505909
Iteration 2300: Loss = -11506.242308606512
Iteration 2400: Loss = -11506.238642164077
Iteration 2500: Loss = -11506.235390812051
Iteration 2600: Loss = -11506.232564456344
Iteration 2700: Loss = -11506.230069605486
Iteration 2800: Loss = -11506.227755265589
Iteration 2900: Loss = -11506.225712606263
Iteration 3000: Loss = -11506.223974767407
Iteration 3100: Loss = -11506.222149641308
Iteration 3200: Loss = -11506.220608222271
Iteration 3300: Loss = -11506.219203170695
Iteration 3400: Loss = -11506.217911080466
Iteration 3500: Loss = -11506.216663720134
Iteration 3600: Loss = -11506.226812306966
1
Iteration 3700: Loss = -11506.214505722155
Iteration 3800: Loss = -11506.21434505801
Iteration 3900: Loss = -11506.213335358469
Iteration 4000: Loss = -11506.211963323096
Iteration 4100: Loss = -11506.211215698782
Iteration 4200: Loss = -11506.210624839492
Iteration 4300: Loss = -11506.209902161352
Iteration 4400: Loss = -11506.209161485736
Iteration 4500: Loss = -11506.208323831088
Iteration 4600: Loss = -11506.206101867443
Iteration 4700: Loss = -11506.202922272558
Iteration 4800: Loss = -11506.202190483282
Iteration 4900: Loss = -11506.205748690389
1
Iteration 5000: Loss = -11506.201557265304
Iteration 5100: Loss = -11506.200973893396
Iteration 5200: Loss = -11506.201098365564
1
Iteration 5300: Loss = -11506.20033181839
Iteration 5400: Loss = -11506.205189686632
1
Iteration 5500: Loss = -11506.199800945107
Iteration 5600: Loss = -11506.199492369784
Iteration 5700: Loss = -11506.200384126927
1
Iteration 5800: Loss = -11506.199024026595
Iteration 5900: Loss = -11506.200268323599
1
Iteration 6000: Loss = -11506.198643118914
Iteration 6100: Loss = -11506.198424419876
Iteration 6200: Loss = -11506.199613051675
1
Iteration 6300: Loss = -11506.198083526242
Iteration 6400: Loss = -11506.200811830346
1
Iteration 6500: Loss = -11506.197807607308
Iteration 6600: Loss = -11506.197688859953
Iteration 6700: Loss = -11506.19758992398
Iteration 6800: Loss = -11506.202756008639
1
Iteration 6900: Loss = -11506.197534313964
Iteration 7000: Loss = -11506.197128665706
Iteration 7100: Loss = -11506.197035628735
Iteration 7200: Loss = -11506.196747035976
Iteration 7300: Loss = -11506.198425241819
1
Iteration 7400: Loss = -11506.196787038789
Iteration 7500: Loss = -11506.196596381053
Iteration 7600: Loss = -11506.196254236589
Iteration 7700: Loss = -11506.196473950184
1
Iteration 7800: Loss = -11506.196206968621
Iteration 7900: Loss = -11506.196075682967
Iteration 8000: Loss = -11506.198816424094
1
Iteration 8100: Loss = -11506.195981737457
Iteration 8200: Loss = -11506.196156034795
1
Iteration 8300: Loss = -11506.195827964006
Iteration 8400: Loss = -11506.195816216201
Iteration 8500: Loss = -11506.20245933772
1
Iteration 8600: Loss = -11506.197053152107
2
Iteration 8700: Loss = -11506.196476541361
3
Iteration 8800: Loss = -11506.197910731184
4
Iteration 8900: Loss = -11506.195614481736
Iteration 9000: Loss = -11506.195721414928
1
Iteration 9100: Loss = -11506.195593035272
Iteration 9200: Loss = -11506.201242346855
1
Iteration 9300: Loss = -11506.195481955961
Iteration 9400: Loss = -11506.195459939543
Iteration 9500: Loss = -11506.19824281236
1
Iteration 9600: Loss = -11506.196931833216
2
Iteration 9700: Loss = -11506.207833664223
3
Iteration 9800: Loss = -11506.287352511938
4
Iteration 9900: Loss = -11506.198394735158
5
Iteration 10000: Loss = -11506.195394286142
Iteration 10100: Loss = -11506.204939723866
1
Iteration 10200: Loss = -11506.238802711654
2
Iteration 10300: Loss = -11506.205228137494
3
Iteration 10400: Loss = -11506.195399412514
Iteration 10500: Loss = -11506.196904677163
1
Iteration 10600: Loss = -11506.213764167953
2
Iteration 10700: Loss = -11506.199364978216
3
Iteration 10800: Loss = -11506.207688122238
4
Iteration 10900: Loss = -11506.201264652553
5
Iteration 11000: Loss = -11506.19550787441
6
Iteration 11100: Loss = -11506.195377091186
Iteration 11200: Loss = -11506.195523708526
1
Iteration 11300: Loss = -11506.21975564801
2
Iteration 11400: Loss = -11506.339587540453
3
Iteration 11500: Loss = -11506.197251294
4
Iteration 11600: Loss = -11506.240761901372
5
Iteration 11700: Loss = -11506.198815586109
6
Iteration 11800: Loss = -11506.195955156903
7
Iteration 11900: Loss = -11506.213095072346
8
Iteration 12000: Loss = -11506.217515214514
9
Iteration 12100: Loss = -11506.198471029395
10
Iteration 12200: Loss = -11506.198482648506
11
Iteration 12300: Loss = -11506.201553729285
12
Iteration 12400: Loss = -11506.201249062417
13
Iteration 12500: Loss = -11506.204039865788
14
Iteration 12600: Loss = -11506.19780535648
15
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.7547, 0.2453],
        [0.3189, 0.6811]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4693, 0.5307], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2072, 0.1077],
         [0.6545, 0.4015]],

        [[0.6719, 0.0932],
         [0.6015, 0.6743]],

        [[0.6033, 0.1015],
         [0.5223, 0.6536]],

        [[0.6518, 0.0973],
         [0.5489, 0.5578]],

        [[0.6804, 0.0995],
         [0.7020, 0.5730]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840319571353601
Average Adjusted Rand Index: 0.9839996552039952
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19918.56926724801
Iteration 100: Loss = -12215.382997545514
Iteration 200: Loss = -12070.496657531861
Iteration 300: Loss = -11592.820988301291
Iteration 400: Loss = -11532.491653224168
Iteration 500: Loss = -11515.824598067815
Iteration 600: Loss = -11515.52346246883
Iteration 700: Loss = -11506.572125123785
Iteration 800: Loss = -11506.498994758245
Iteration 900: Loss = -11506.449549843837
Iteration 1000: Loss = -11506.412727643741
Iteration 1100: Loss = -11506.384895101875
Iteration 1200: Loss = -11506.364606288904
Iteration 1300: Loss = -11506.348120065335
Iteration 1400: Loss = -11506.334507486805
Iteration 1500: Loss = -11506.323394484754
Iteration 1600: Loss = -11506.31443549438
Iteration 1700: Loss = -11506.307324700134
Iteration 1800: Loss = -11506.301367605636
Iteration 1900: Loss = -11506.296227373048
Iteration 2000: Loss = -11506.291727430584
Iteration 2100: Loss = -11506.287819074108
Iteration 2200: Loss = -11506.290146989404
1
Iteration 2300: Loss = -11506.281109802147
Iteration 2400: Loss = -11506.278227676614
Iteration 2500: Loss = -11506.275663910774
Iteration 2600: Loss = -11506.273413106579
Iteration 2700: Loss = -11506.271263213492
Iteration 2800: Loss = -11506.268889341218
Iteration 2900: Loss = -11506.264054129995
Iteration 3000: Loss = -11506.214214937372
Iteration 3100: Loss = -11506.212640576645
Iteration 3200: Loss = -11506.211954381923
Iteration 3300: Loss = -11506.210405427111
Iteration 3400: Loss = -11506.20940746761
Iteration 3500: Loss = -11506.208694171093
Iteration 3600: Loss = -11506.207716734425
Iteration 3700: Loss = -11506.206958816205
Iteration 3800: Loss = -11506.213666597729
1
Iteration 3900: Loss = -11506.205623113845
Iteration 4000: Loss = -11506.211530866158
1
Iteration 4100: Loss = -11506.204536205041
Iteration 4200: Loss = -11506.204028534792
Iteration 4300: Loss = -11506.203681955678
Iteration 4400: Loss = -11506.203094764276
Iteration 4500: Loss = -11506.203148101358
Iteration 4600: Loss = -11506.202328229765
Iteration 4700: Loss = -11506.207968439747
1
Iteration 4800: Loss = -11506.201583326341
Iteration 4900: Loss = -11506.201297215603
Iteration 5000: Loss = -11506.201103148525
Iteration 5100: Loss = -11506.200756915714
Iteration 5200: Loss = -11506.200848814398
Iteration 5300: Loss = -11506.20026506565
Iteration 5400: Loss = -11506.200036505836
Iteration 5500: Loss = -11506.2061479294
1
Iteration 5600: Loss = -11506.199591876104
Iteration 5700: Loss = -11506.199375078342
Iteration 5800: Loss = -11506.200376718572
1
Iteration 5900: Loss = -11506.198769352248
Iteration 6000: Loss = -11506.193358690993
Iteration 6100: Loss = -11506.193172427742
Iteration 6200: Loss = -11506.193299334964
1
Iteration 6300: Loss = -11506.192861343969
Iteration 6400: Loss = -11506.198237273726
1
Iteration 6500: Loss = -11506.192479992367
Iteration 6600: Loss = -11506.192599715181
1
Iteration 6700: Loss = -11506.192048848869
Iteration 6800: Loss = -11506.192558880373
1
Iteration 6900: Loss = -11506.19618472258
2
Iteration 7000: Loss = -11506.19771769259
3
Iteration 7100: Loss = -11506.191728573018
Iteration 7200: Loss = -11506.193838431092
1
Iteration 7300: Loss = -11506.192637584132
2
Iteration 7400: Loss = -11506.194904492742
3
Iteration 7500: Loss = -11506.1914114585
Iteration 7600: Loss = -11506.1914923721
Iteration 7700: Loss = -11506.192003688479
1
Iteration 7800: Loss = -11506.193180300552
2
Iteration 7900: Loss = -11506.192836131788
3
Iteration 8000: Loss = -11506.191110817243
Iteration 8100: Loss = -11506.191433348113
1
Iteration 8200: Loss = -11506.190970965086
Iteration 8300: Loss = -11506.190923975015
Iteration 8400: Loss = -11506.190869976046
Iteration 8500: Loss = -11506.190869949643
Iteration 8600: Loss = -11506.19079206941
Iteration 8700: Loss = -11506.192359723967
1
Iteration 8800: Loss = -11506.191090001075
2
Iteration 8900: Loss = -11506.192015462238
3
Iteration 9000: Loss = -11506.19075807578
Iteration 9100: Loss = -11506.191885985914
1
Iteration 9200: Loss = -11506.191066412553
2
Iteration 9300: Loss = -11506.191776676305
3
Iteration 9400: Loss = -11506.191572437474
4
Iteration 9500: Loss = -11506.191395437589
5
Iteration 9600: Loss = -11506.190714664775
Iteration 9700: Loss = -11506.192895541331
1
Iteration 9800: Loss = -11506.190634794555
Iteration 9900: Loss = -11506.191788874883
1
Iteration 10000: Loss = -11506.191588548789
2
Iteration 10100: Loss = -11506.190698798468
Iteration 10200: Loss = -11506.19132586937
1
Iteration 10300: Loss = -11506.190686638152
Iteration 10400: Loss = -11506.197175326803
1
Iteration 10500: Loss = -11506.21969596743
2
Iteration 10600: Loss = -11506.198874758144
3
Iteration 10700: Loss = -11506.206171605394
4
Iteration 10800: Loss = -11506.192208526074
5
Iteration 10900: Loss = -11506.19040802557
Iteration 11000: Loss = -11506.209905227262
1
Iteration 11100: Loss = -11506.200704303135
2
Iteration 11200: Loss = -11506.24101999283
3
Iteration 11300: Loss = -11506.204483647225
4
Iteration 11400: Loss = -11506.265623035246
5
Iteration 11500: Loss = -11506.27552527176
6
Iteration 11600: Loss = -11506.199634327697
7
Iteration 11700: Loss = -11506.19723197078
8
Iteration 11800: Loss = -11506.256624732983
9
Iteration 11900: Loss = -11506.217150347908
10
Iteration 12000: Loss = -11506.192125496957
11
Iteration 12100: Loss = -11506.194472973955
12
Iteration 12200: Loss = -11506.194555724293
13
Iteration 12300: Loss = -11506.238038403359
14
Iteration 12400: Loss = -11506.205181116886
15
Stopping early at iteration 12400 due to no improvement.
pi: tensor([[0.6820, 0.3180],
        [0.2457, 0.7543]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5323, 0.4677], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4006, 0.1073],
         [0.6663, 0.2077]],

        [[0.6659, 0.0938],
         [0.6819, 0.6477]],

        [[0.7046, 0.1015],
         [0.5512, 0.6034]],

        [[0.7216, 0.0977],
         [0.5559, 0.5706]],

        [[0.5224, 0.0996],
         [0.5113, 0.5496]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9840319571353601
Average Adjusted Rand Index: 0.9839996552039952
11514.304214203867
[0.9840319571353601, 0.9840319571353601] [0.9839996552039952, 0.9839996552039952] [11506.19780535648, 11506.205181116886]
-------------------------------------
This iteration is 60
True Objective function: Loss = -11575.983130379685
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21849.7239248483
Iteration 100: Loss = -12312.996651708374
Iteration 200: Loss = -12282.235765807252
Iteration 300: Loss = -12098.67982792458
Iteration 400: Loss = -11687.55396189334
Iteration 500: Loss = -11645.617584134003
Iteration 600: Loss = -11604.843223922098
Iteration 700: Loss = -11597.175435039308
Iteration 800: Loss = -11587.971403763777
Iteration 900: Loss = -11579.444620123677
Iteration 1000: Loss = -11579.28060912272
Iteration 1100: Loss = -11579.172800665267
Iteration 1200: Loss = -11579.093225411501
Iteration 1300: Loss = -11579.031638191194
Iteration 1400: Loss = -11578.98054819439
Iteration 1500: Loss = -11578.898945941452
Iteration 1600: Loss = -11576.844286891104
Iteration 1700: Loss = -11576.811275365802
Iteration 1800: Loss = -11576.778842815664
Iteration 1900: Loss = -11572.101836403099
Iteration 2000: Loss = -11572.074918434546
Iteration 2100: Loss = -11572.058598243206
Iteration 2200: Loss = -11572.053417603236
Iteration 2300: Loss = -11572.033739063949
Iteration 2400: Loss = -11572.024045020145
Iteration 2500: Loss = -11572.015518859034
Iteration 2600: Loss = -11572.007927149494
Iteration 2700: Loss = -11572.001181784643
Iteration 2800: Loss = -11571.99509928223
Iteration 2900: Loss = -11571.989545623079
Iteration 3000: Loss = -11571.984434233525
Iteration 3100: Loss = -11571.979584935732
Iteration 3200: Loss = -11571.97510238113
Iteration 3300: Loss = -11571.969726158764
Iteration 3400: Loss = -11571.96446983107
Iteration 3500: Loss = -11571.971089867024
1
Iteration 3600: Loss = -11571.95819450994
Iteration 3700: Loss = -11571.955633696989
Iteration 3800: Loss = -11571.953223358298
Iteration 3900: Loss = -11571.953980308863
1
Iteration 4000: Loss = -11571.951343103201
Iteration 4100: Loss = -11571.947222036433
Iteration 4200: Loss = -11571.94524870738
Iteration 4300: Loss = -11571.943555400874
Iteration 4400: Loss = -11571.941870747609
Iteration 4500: Loss = -11571.940200395858
Iteration 4600: Loss = -11571.940658830448
1
Iteration 4700: Loss = -11571.94114695537
2
Iteration 4800: Loss = -11571.946327085107
3
Iteration 4900: Loss = -11571.933336739332
Iteration 5000: Loss = -11571.93228851061
Iteration 5100: Loss = -11571.93189595888
Iteration 5200: Loss = -11571.930407368793
Iteration 5300: Loss = -11571.929560454242
Iteration 5400: Loss = -11571.929653736861
Iteration 5500: Loss = -11571.927998018606
Iteration 5600: Loss = -11571.927323958076
Iteration 5700: Loss = -11571.927997894414
1
Iteration 5800: Loss = -11571.925977127248
Iteration 5900: Loss = -11571.942504150733
1
Iteration 6000: Loss = -11571.92473537259
Iteration 6100: Loss = -11571.924037856843
Iteration 6200: Loss = -11571.923266343374
Iteration 6300: Loss = -11571.92254497073
Iteration 6400: Loss = -11571.92168950956
Iteration 6500: Loss = -11571.92111149227
Iteration 6600: Loss = -11571.921270158331
1
Iteration 6700: Loss = -11571.920280457178
Iteration 6800: Loss = -11571.920459052308
1
Iteration 6900: Loss = -11571.928038074275
2
Iteration 7000: Loss = -11571.919353519104
Iteration 7100: Loss = -11571.92661556631
1
Iteration 7200: Loss = -11571.956386546772
2
Iteration 7300: Loss = -11571.918826159053
Iteration 7400: Loss = -11571.923039743226
1
Iteration 7500: Loss = -11571.947762906646
2
Iteration 7600: Loss = -11571.91788699322
Iteration 7700: Loss = -11571.91769428263
Iteration 7800: Loss = -11571.93259317824
1
Iteration 7900: Loss = -11571.917286312902
Iteration 8000: Loss = -11571.918816597692
1
Iteration 8100: Loss = -11571.91700304788
Iteration 8200: Loss = -11571.918532333608
1
Iteration 8300: Loss = -11571.921930334924
2
Iteration 8400: Loss = -11571.926114915348
3
Iteration 8500: Loss = -11571.925289238112
4
Iteration 8600: Loss = -11571.917036275818
Iteration 8700: Loss = -11571.916690312804
Iteration 8800: Loss = -11571.916208201783
Iteration 8900: Loss = -11571.920489519856
1
Iteration 9000: Loss = -11571.915953256355
Iteration 9100: Loss = -11571.923885001943
1
Iteration 9200: Loss = -11571.916279685194
2
Iteration 9300: Loss = -11571.965539505163
3
Iteration 9400: Loss = -11571.915739665168
Iteration 9500: Loss = -11571.915686806331
Iteration 9600: Loss = -11571.91794569761
1
Iteration 9700: Loss = -11571.997493206487
2
Iteration 9800: Loss = -11571.927143736819
3
Iteration 9900: Loss = -11571.917512036573
4
Iteration 10000: Loss = -11571.915373040298
Iteration 10100: Loss = -11571.915311517007
Iteration 10200: Loss = -11571.954409667302
1
Iteration 10300: Loss = -11571.928483866875
2
Iteration 10400: Loss = -11571.915263129466
Iteration 10500: Loss = -11571.91840868876
1
Iteration 10600: Loss = -11571.94158809686
2
Iteration 10700: Loss = -11571.9479867301
3
Iteration 10800: Loss = -11571.920630995126
4
Iteration 10900: Loss = -11571.92395174898
5
Iteration 11000: Loss = -11571.931643436046
6
Iteration 11100: Loss = -11571.9181685888
7
Iteration 11200: Loss = -11571.923467304629
8
Iteration 11300: Loss = -11571.917515501931
9
Iteration 11400: Loss = -11571.934671557898
10
Iteration 11500: Loss = -11571.916319551277
11
Iteration 11600: Loss = -11571.925769613461
12
Iteration 11700: Loss = -11571.915625796784
13
Iteration 11800: Loss = -11571.940603559236
14
Iteration 11900: Loss = -11571.914878465359
Iteration 12000: Loss = -11571.915383540529
1
Iteration 12100: Loss = -11571.923576129373
2
Iteration 12200: Loss = -11571.98982168355
3
Iteration 12300: Loss = -11571.924598129186
4
Iteration 12400: Loss = -11571.922855477636
5
Iteration 12500: Loss = -11571.939418473594
6
Iteration 12600: Loss = -11571.914705253586
Iteration 12700: Loss = -11571.914685862812
Iteration 12800: Loss = -11571.916208793675
1
Iteration 12900: Loss = -11571.914729356942
Iteration 13000: Loss = -11571.915208325972
1
Iteration 13100: Loss = -11571.91837700052
2
Iteration 13200: Loss = -11571.920462636966
3
Iteration 13300: Loss = -11571.925807215115
4
Iteration 13400: Loss = -11571.921191442712
5
Iteration 13500: Loss = -11571.916959898661
6
Iteration 13600: Loss = -11571.930329194973
7
Iteration 13700: Loss = -11571.918059855861
8
Iteration 13800: Loss = -11571.927343354128
9
Iteration 13900: Loss = -11571.938761103873
10
Iteration 14000: Loss = -11571.925836249358
11
Iteration 14100: Loss = -11571.915499137072
12
Iteration 14200: Loss = -11571.939225307107
13
Iteration 14300: Loss = -11571.929465801495
14
Iteration 14400: Loss = -11571.918015433499
15
Stopping early at iteration 14400 due to no improvement.
pi: tensor([[0.7391, 0.2609],
        [0.2466, 0.7534]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4801, 0.5199], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3968, 0.1083],
         [0.6252, 0.2034]],

        [[0.5337, 0.1043],
         [0.7299, 0.5909]],

        [[0.5539, 0.1043],
         [0.5757, 0.7155]],

        [[0.7131, 0.0930],
         [0.6803, 0.5588]],

        [[0.5697, 0.0966],
         [0.6899, 0.6050]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22674.56949939749
Iteration 100: Loss = -12313.317977266595
Iteration 200: Loss = -12276.928734463501
Iteration 300: Loss = -11854.351232968771
Iteration 400: Loss = -11717.159944748057
Iteration 500: Loss = -11676.13650180824
Iteration 600: Loss = -11626.853552056553
Iteration 700: Loss = -11603.774443206621
Iteration 800: Loss = -11592.13806862039
Iteration 900: Loss = -11572.880248558316
Iteration 1000: Loss = -11572.529175073792
Iteration 1100: Loss = -11572.400514240286
Iteration 1200: Loss = -11572.311896237792
Iteration 1300: Loss = -11572.246083789478
Iteration 1400: Loss = -11572.194422987712
Iteration 1500: Loss = -11572.153490132749
Iteration 1600: Loss = -11572.1227114304
Iteration 1700: Loss = -11572.097885989351
Iteration 1800: Loss = -11572.07726362786
Iteration 1900: Loss = -11572.059901340213
Iteration 2000: Loss = -11572.04509107668
Iteration 2100: Loss = -11572.032326053819
Iteration 2200: Loss = -11572.021218243188
Iteration 2300: Loss = -11572.01149269088
Iteration 2400: Loss = -11572.002827989132
Iteration 2500: Loss = -11571.996798446886
Iteration 2600: Loss = -11571.988044968626
Iteration 2700: Loss = -11571.981732640474
Iteration 2800: Loss = -11571.995006383084
1
Iteration 2900: Loss = -11571.971348843885
Iteration 3000: Loss = -11571.966987696433
Iteration 3100: Loss = -11571.963038107853
Iteration 3200: Loss = -11571.959517020003
Iteration 3300: Loss = -11571.956271279665
Iteration 3400: Loss = -11571.953277100241
Iteration 3500: Loss = -11571.950919337904
Iteration 3600: Loss = -11571.947943918662
Iteration 3700: Loss = -11571.955634560212
1
Iteration 3800: Loss = -11571.94365646702
Iteration 3900: Loss = -11571.941510915649
Iteration 4000: Loss = -11571.939639881677
Iteration 4100: Loss = -11571.94193606789
1
Iteration 4200: Loss = -11571.936525858746
Iteration 4300: Loss = -11571.93490689157
Iteration 4400: Loss = -11571.933937693724
Iteration 4500: Loss = -11571.93289743185
Iteration 4600: Loss = -11571.931188122171
Iteration 4700: Loss = -11571.930171384569
Iteration 4800: Loss = -11571.929383555334
Iteration 4900: Loss = -11571.928179320723
Iteration 5000: Loss = -11571.928069453164
Iteration 5100: Loss = -11571.926539384134
Iteration 5200: Loss = -11571.93622339888
1
Iteration 5300: Loss = -11571.924875789178
Iteration 5400: Loss = -11571.924109821677
Iteration 5500: Loss = -11571.927132722725
1
Iteration 5600: Loss = -11571.922865764409
Iteration 5700: Loss = -11571.922308170073
Iteration 5800: Loss = -11571.921809606618
Iteration 5900: Loss = -11571.924322787409
1
Iteration 6000: Loss = -11571.920863700112
Iteration 6100: Loss = -11571.920515175396
Iteration 6200: Loss = -11571.921402337599
1
Iteration 6300: Loss = -11571.919706032431
Iteration 6400: Loss = -11571.919361851064
Iteration 6500: Loss = -11571.923109444879
1
Iteration 6600: Loss = -11571.918747593108
Iteration 6700: Loss = -11571.918428851388
Iteration 6800: Loss = -11571.91818535001
Iteration 6900: Loss = -11571.918192929923
Iteration 7000: Loss = -11571.918560029824
1
Iteration 7100: Loss = -11571.917668462826
Iteration 7200: Loss = -11571.918347342333
1
Iteration 7300: Loss = -11571.919599112418
2
Iteration 7400: Loss = -11571.917827914293
3
Iteration 7500: Loss = -11571.933132876973
4
Iteration 7600: Loss = -11571.916609705737
Iteration 7700: Loss = -11571.95165202508
1
Iteration 7800: Loss = -11571.918215056105
2
Iteration 7900: Loss = -11571.916102269843
Iteration 8000: Loss = -11571.917721898535
1
Iteration 8100: Loss = -11571.915918686635
Iteration 8200: Loss = -11571.915747636036
Iteration 8300: Loss = -11571.917933597482
1
Iteration 8400: Loss = -11571.915546625083
Iteration 8500: Loss = -11572.007536843677
1
Iteration 8600: Loss = -11571.915386797333
Iteration 8700: Loss = -11571.92061793606
1
Iteration 8800: Loss = -11571.915431340547
Iteration 8900: Loss = -11571.915178822805
Iteration 9000: Loss = -11571.965271758301
1
Iteration 9100: Loss = -11571.91499694556
Iteration 9200: Loss = -11571.916705855323
1
Iteration 9300: Loss = -11571.991618131218
2
Iteration 9400: Loss = -11571.917580383528
3
Iteration 9500: Loss = -11571.914923115402
Iteration 9600: Loss = -11571.921850263345
1
Iteration 9700: Loss = -11571.91518177002
2
Iteration 9800: Loss = -11571.920188404834
3
Iteration 9900: Loss = -11571.915253955827
4
Iteration 10000: Loss = -11571.915225253279
5
Iteration 10100: Loss = -11571.914695730882
Iteration 10200: Loss = -11571.920067847535
1
Iteration 10300: Loss = -11571.920692839776
2
Iteration 10400: Loss = -11571.968655917522
3
Iteration 10500: Loss = -11571.914313793139
Iteration 10600: Loss = -11571.916084466484
1
Iteration 10700: Loss = -11571.915704526
2
Iteration 10800: Loss = -11571.915125214524
3
Iteration 10900: Loss = -11571.931431045836
4
Iteration 11000: Loss = -11571.918600583063
5
Iteration 11100: Loss = -11571.921376836188
6
Iteration 11200: Loss = -11571.918178599786
7
Iteration 11300: Loss = -11571.92181356819
8
Iteration 11400: Loss = -11571.916787468217
9
Iteration 11500: Loss = -11571.91715689136
10
Iteration 11600: Loss = -11571.927264065494
11
Iteration 11700: Loss = -11571.922404218347
12
Iteration 11800: Loss = -11571.926480057658
13
Iteration 11900: Loss = -11571.914489569102
14
Iteration 12000: Loss = -11571.914007593485
Iteration 12100: Loss = -11571.918891272548
1
Iteration 12200: Loss = -11571.935244351791
2
Iteration 12300: Loss = -11571.922764108444
3
Iteration 12400: Loss = -11571.92131691878
4
Iteration 12500: Loss = -11571.933098302943
5
Iteration 12600: Loss = -11571.943583116898
6
Iteration 12700: Loss = -11571.916462075582
7
Iteration 12800: Loss = -11571.915620614987
8
Iteration 12900: Loss = -11571.916704137155
9
Iteration 13000: Loss = -11571.919496148226
10
Iteration 13100: Loss = -11571.915774571258
11
Iteration 13200: Loss = -11571.928041749317
12
Iteration 13300: Loss = -11571.9165963693
13
Iteration 13400: Loss = -11571.919901943416
14
Iteration 13500: Loss = -11571.922965003127
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7397, 0.2603],
        [0.2471, 0.7529]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4806, 0.5194], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3967, 0.1099],
         [0.6101, 0.2035]],

        [[0.5019, 0.1043],
         [0.6029, 0.6294]],

        [[0.5974, 0.1042],
         [0.6827, 0.6303]],

        [[0.5957, 0.0930],
         [0.6682, 0.5256]],

        [[0.7106, 0.0965],
         [0.6515, 0.6327]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11575.983130379685
[1.0, 1.0] [1.0, 1.0] [11571.918015433499, 11571.922965003127]
-------------------------------------
This iteration is 61
True Objective function: Loss = -11842.15319592151
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23071.87098618302
Iteration 100: Loss = -12532.755771263177
Iteration 200: Loss = -11859.76031447178
Iteration 300: Loss = -11834.930002720577
Iteration 400: Loss = -11834.606123836995
Iteration 500: Loss = -11834.453780398995
Iteration 600: Loss = -11834.365963166021
Iteration 700: Loss = -11834.30958359644
Iteration 800: Loss = -11834.271048820372
Iteration 900: Loss = -11834.243307851128
Iteration 1000: Loss = -11834.222158768283
Iteration 1100: Loss = -11834.205106314279
Iteration 1200: Loss = -11834.190483680697
Iteration 1300: Loss = -11834.179740214739
Iteration 1400: Loss = -11834.17131235803
Iteration 1500: Loss = -11834.164371898347
Iteration 1600: Loss = -11834.158596095122
Iteration 1700: Loss = -11834.153675214526
Iteration 1800: Loss = -11834.149475304339
Iteration 1900: Loss = -11834.145886377044
Iteration 2000: Loss = -11834.142730262312
Iteration 2100: Loss = -11834.13998320087
Iteration 2200: Loss = -11834.137560950294
Iteration 2300: Loss = -11834.13539234043
Iteration 2400: Loss = -11834.133510370406
Iteration 2500: Loss = -11834.131839519161
Iteration 2600: Loss = -11834.13281581868
1
Iteration 2700: Loss = -11834.128894453084
Iteration 2800: Loss = -11834.127676936652
Iteration 2900: Loss = -11834.140842648529
1
Iteration 3000: Loss = -11834.125536035337
Iteration 3100: Loss = -11834.124598296778
Iteration 3200: Loss = -11834.123782450044
Iteration 3300: Loss = -11834.123014385248
Iteration 3400: Loss = -11834.122292843324
Iteration 3500: Loss = -11834.121640244419
Iteration 3600: Loss = -11834.122705446072
1
Iteration 3700: Loss = -11834.12049871387
Iteration 3800: Loss = -11834.12077428289
1
Iteration 3900: Loss = -11834.12010232979
Iteration 4000: Loss = -11834.11973967423
Iteration 4100: Loss = -11834.11868617832
Iteration 4200: Loss = -11834.118367686819
Iteration 4300: Loss = -11834.118009365959
Iteration 4400: Loss = -11834.117677785907
Iteration 4500: Loss = -11834.118275431345
1
Iteration 4600: Loss = -11834.117077491206
Iteration 4700: Loss = -11834.124127223015
1
Iteration 4800: Loss = -11834.116514373409
Iteration 4900: Loss = -11834.116816060638
1
Iteration 5000: Loss = -11834.111709932042
Iteration 5100: Loss = -11834.10875054642
Iteration 5200: Loss = -11834.114146713915
1
Iteration 5300: Loss = -11834.113393494528
2
Iteration 5400: Loss = -11834.118548657045
3
Iteration 5500: Loss = -11834.10730327059
Iteration 5600: Loss = -11834.10729863443
Iteration 5700: Loss = -11834.106800611547
Iteration 5800: Loss = -11834.106982310852
1
Iteration 5900: Loss = -11834.106695927088
Iteration 6000: Loss = -11834.106519330555
Iteration 6100: Loss = -11834.107323087608
1
Iteration 6200: Loss = -11834.108586721257
2
Iteration 6300: Loss = -11834.106004065463
Iteration 6400: Loss = -11834.106613695723
1
Iteration 6500: Loss = -11834.10921131536
2
Iteration 6600: Loss = -11834.106979971853
3
Iteration 6700: Loss = -11834.130367336318
4
Iteration 6800: Loss = -11834.105778600946
Iteration 6900: Loss = -11834.105628467592
Iteration 7000: Loss = -11834.129945125751
1
Iteration 7100: Loss = -11834.105417222649
Iteration 7200: Loss = -11834.105787992052
1
Iteration 7300: Loss = -11834.105706768096
2
Iteration 7400: Loss = -11834.105550426213
3
Iteration 7500: Loss = -11834.115318147386
4
Iteration 7600: Loss = -11834.114891804342
5
Iteration 7700: Loss = -11834.11725362965
6
Iteration 7800: Loss = -11834.105487806291
Iteration 7900: Loss = -11834.105011565287
Iteration 8000: Loss = -11834.1132171247
1
Iteration 8100: Loss = -11834.105355670348
2
Iteration 8200: Loss = -11834.106479691092
3
Iteration 8300: Loss = -11834.109788827747
4
Iteration 8400: Loss = -11834.105275071988
5
Iteration 8500: Loss = -11834.104985219075
Iteration 8600: Loss = -11834.104780296733
Iteration 8700: Loss = -11834.104964257283
1
Iteration 8800: Loss = -11834.104919446458
2
Iteration 8900: Loss = -11834.106616383177
3
Iteration 9000: Loss = -11834.105705326161
4
Iteration 9100: Loss = -11834.107938914261
5
Iteration 9200: Loss = -11834.107647269037
6
Iteration 9300: Loss = -11834.113487652416
7
Iteration 9400: Loss = -11834.106779869311
8
Iteration 9500: Loss = -11834.10800092309
9
Iteration 9600: Loss = -11834.138471603945
10
Iteration 9700: Loss = -11834.10976195012
11
Iteration 9800: Loss = -11834.112774510904
12
Iteration 9900: Loss = -11834.108824891156
13
Iteration 10000: Loss = -11834.105245008928
14
Iteration 10100: Loss = -11834.105009596124
15
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.7408, 0.2592],
        [0.2147, 0.7853]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4492, 0.5508], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2144, 0.1064],
         [0.5742, 0.3880]],

        [[0.6198, 0.0994],
         [0.7000, 0.6064]],

        [[0.7080, 0.0997],
         [0.5356, 0.6695]],

        [[0.6263, 0.0954],
         [0.5261, 0.6059]],

        [[0.7016, 0.0990],
         [0.6416, 0.6518]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20984.959336428397
Iteration 100: Loss = -12686.83865313035
Iteration 200: Loss = -12622.717015750122
Iteration 300: Loss = -12062.663627560903
Iteration 400: Loss = -11868.940977095772
Iteration 500: Loss = -11862.805105820516
Iteration 600: Loss = -11848.829482428977
Iteration 700: Loss = -11843.614619329113
Iteration 800: Loss = -11834.840675655903
Iteration 900: Loss = -11834.67910065984
Iteration 1000: Loss = -11834.571623081047
Iteration 1100: Loss = -11834.494353533504
Iteration 1200: Loss = -11834.435755911087
Iteration 1300: Loss = -11834.390038693933
Iteration 1400: Loss = -11834.353573590977
Iteration 1500: Loss = -11834.324194710203
Iteration 1600: Loss = -11834.30008149318
Iteration 1700: Loss = -11834.280039961133
Iteration 1800: Loss = -11834.263061453312
Iteration 1900: Loss = -11834.248547106261
Iteration 2000: Loss = -11834.236087582709
Iteration 2100: Loss = -11834.225267546013
Iteration 2200: Loss = -11834.215813820705
Iteration 2300: Loss = -11834.207499461647
Iteration 2400: Loss = -11834.200164743717
Iteration 2500: Loss = -11834.193580204084
Iteration 2600: Loss = -11834.187781230205
Iteration 2700: Loss = -11834.182513889158
Iteration 2800: Loss = -11834.177796250176
Iteration 2900: Loss = -11834.173525312235
Iteration 3000: Loss = -11834.16964869185
Iteration 3100: Loss = -11834.166110731612
Iteration 3200: Loss = -11834.162875925082
Iteration 3300: Loss = -11834.161446310676
Iteration 3400: Loss = -11834.157138371944
Iteration 3500: Loss = -11834.154795349576
Iteration 3600: Loss = -11834.152199645916
Iteration 3700: Loss = -11834.15003145165
Iteration 3800: Loss = -11834.161892587439
1
Iteration 3900: Loss = -11834.146298416077
Iteration 4000: Loss = -11834.144343500493
Iteration 4100: Loss = -11834.144105751759
Iteration 4200: Loss = -11834.141183831734
Iteration 4300: Loss = -11834.139549131103
Iteration 4400: Loss = -11834.140116747834
1
Iteration 4500: Loss = -11834.13657743056
Iteration 4600: Loss = -11834.134937968736
Iteration 4700: Loss = -11834.132538851401
Iteration 4800: Loss = -11834.128263632792
Iteration 4900: Loss = -11834.126191988133
Iteration 5000: Loss = -11834.124685305715
Iteration 5100: Loss = -11834.125422084922
1
Iteration 5200: Loss = -11834.123105767216
Iteration 5300: Loss = -11834.136359809956
1
Iteration 5400: Loss = -11834.121770126163
Iteration 5500: Loss = -11834.121162449237
Iteration 5600: Loss = -11834.1209032275
Iteration 5700: Loss = -11834.120033092497
Iteration 5800: Loss = -11834.121888343761
1
Iteration 5900: Loss = -11834.11899871842
Iteration 6000: Loss = -11834.122494530384
1
Iteration 6100: Loss = -11834.11795067649
Iteration 6200: Loss = -11834.117036745873
Iteration 6300: Loss = -11834.121725460293
1
Iteration 6400: Loss = -11834.115795776084
Iteration 6500: Loss = -11834.117645636208
1
Iteration 6600: Loss = -11834.159029882048
2
Iteration 6700: Loss = -11834.114908413054
Iteration 6800: Loss = -11834.115103749997
1
Iteration 6900: Loss = -11834.11728809274
2
Iteration 7000: Loss = -11834.121699721312
3
Iteration 7100: Loss = -11834.11618912145
4
Iteration 7200: Loss = -11834.113777858169
Iteration 7300: Loss = -11834.11353123604
Iteration 7400: Loss = -11834.122888173462
1
Iteration 7500: Loss = -11834.11314974791
Iteration 7600: Loss = -11834.11332716455
1
Iteration 7700: Loss = -11834.128100006348
2
Iteration 7800: Loss = -11834.11282535655
Iteration 7900: Loss = -11834.113001573773
1
Iteration 8000: Loss = -11834.128884227397
2
Iteration 8100: Loss = -11834.112440911258
Iteration 8200: Loss = -11834.116907054946
1
Iteration 8300: Loss = -11834.11251057544
Iteration 8400: Loss = -11834.145461609594
1
Iteration 8500: Loss = -11834.112020552651
Iteration 8600: Loss = -11834.125061491275
1
Iteration 8700: Loss = -11834.112742974037
2
Iteration 8800: Loss = -11834.111686849901
Iteration 8900: Loss = -11834.118381902916
1
Iteration 9000: Loss = -11834.111464637239
Iteration 9100: Loss = -11834.113331278775
1
Iteration 9200: Loss = -11834.113753467036
2
Iteration 9300: Loss = -11834.122073767758
3
Iteration 9400: Loss = -11834.111975413454
4
Iteration 9500: Loss = -11834.118772636792
5
Iteration 9600: Loss = -11834.111714199289
6
Iteration 9700: Loss = -11834.112794775181
7
Iteration 9800: Loss = -11834.117064263894
8
Iteration 9900: Loss = -11834.112424119938
9
Iteration 10000: Loss = -11834.114039680997
10
Iteration 10100: Loss = -11834.120517055835
11
Iteration 10200: Loss = -11834.111377260042
Iteration 10300: Loss = -11834.113022655622
1
Iteration 10400: Loss = -11834.113794402503
2
Iteration 10500: Loss = -11834.149969537675
3
Iteration 10600: Loss = -11834.113925807975
4
Iteration 10700: Loss = -11834.110988584165
Iteration 10800: Loss = -11834.12720951845
1
Iteration 10900: Loss = -11834.111534621255
2
Iteration 11000: Loss = -11834.111578817967
3
Iteration 11100: Loss = -11834.111007246294
Iteration 11200: Loss = -11834.112678208649
1
Iteration 11300: Loss = -11834.137394770134
2
Iteration 11400: Loss = -11834.111979925046
3
Iteration 11500: Loss = -11834.112552935854
4
Iteration 11600: Loss = -11834.116796316175
5
Iteration 11700: Loss = -11834.1132212643
6
Iteration 11800: Loss = -11834.115397063255
7
Iteration 11900: Loss = -11834.113407041177
8
Iteration 12000: Loss = -11834.106340994424
Iteration 12100: Loss = -11834.106881144013
1
Iteration 12200: Loss = -11834.10647448875
2
Iteration 12300: Loss = -11834.176254748452
3
Iteration 12400: Loss = -11834.140392721361
4
Iteration 12500: Loss = -11834.103560414398
Iteration 12600: Loss = -11834.104725415893
1
Iteration 12700: Loss = -11834.103307055857
Iteration 12800: Loss = -11834.10741322048
1
Iteration 12900: Loss = -11834.18359131792
2
Iteration 13000: Loss = -11834.12040782145
3
Iteration 13100: Loss = -11834.122973075577
4
Iteration 13200: Loss = -11834.104910962993
5
Iteration 13300: Loss = -11834.105775744083
6
Iteration 13400: Loss = -11834.108134469834
7
Iteration 13500: Loss = -11834.105562885425
8
Iteration 13600: Loss = -11834.104879763234
9
Iteration 13700: Loss = -11834.104841211898
10
Iteration 13800: Loss = -11834.122033823178
11
Iteration 13900: Loss = -11834.106630046032
12
Iteration 14000: Loss = -11834.104544624897
13
Iteration 14100: Loss = -11834.117830377107
14
Iteration 14200: Loss = -11834.106241511889
15
Stopping early at iteration 14200 due to no improvement.
pi: tensor([[0.7411, 0.2589],
        [0.2141, 0.7859]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4492, 0.5508], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2146, 0.1064],
         [0.5139, 0.3884]],

        [[0.6368, 0.0994],
         [0.6970, 0.6846]],

        [[0.6690, 0.0994],
         [0.5752, 0.7215]],

        [[0.5772, 0.0953],
         [0.6326, 0.6942]],

        [[0.7035, 0.0990],
         [0.7063, 0.5075]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11842.15319592151
[1.0, 1.0] [1.0, 1.0] [11834.105009596124, 11834.106241511889]
-------------------------------------
This iteration is 62
True Objective function: Loss = -11650.261573835178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20454.56245467987
Iteration 100: Loss = -12547.498793969164
Iteration 200: Loss = -12522.028196214727
Iteration 300: Loss = -11820.606776270288
Iteration 400: Loss = -11696.304130779385
Iteration 500: Loss = -11662.655424092465
Iteration 600: Loss = -11653.903685407022
Iteration 700: Loss = -11644.48767834238
Iteration 800: Loss = -11644.315971600729
Iteration 900: Loss = -11644.20719977999
Iteration 1000: Loss = -11644.13200492248
Iteration 1100: Loss = -11644.077113538518
Iteration 1200: Loss = -11644.035519690264
Iteration 1300: Loss = -11644.003057313419
Iteration 1400: Loss = -11643.977148639528
Iteration 1500: Loss = -11643.956008440045
Iteration 1600: Loss = -11643.938562217982
Iteration 1700: Loss = -11643.923916962629
Iteration 1800: Loss = -11643.911571585148
Iteration 1900: Loss = -11643.90098268136
Iteration 2000: Loss = -11643.891821146528
Iteration 2100: Loss = -11643.883899176015
Iteration 2200: Loss = -11643.876927097925
Iteration 2300: Loss = -11643.87079309388
Iteration 2400: Loss = -11643.865414327242
Iteration 2500: Loss = -11643.860633925986
Iteration 2600: Loss = -11643.856294614305
Iteration 2700: Loss = -11643.852430351433
Iteration 2800: Loss = -11643.873307268092
1
Iteration 2900: Loss = -11643.845831575045
Iteration 3000: Loss = -11643.843010821456
Iteration 3100: Loss = -11643.840418593265
Iteration 3200: Loss = -11643.838086852164
Iteration 3300: Loss = -11643.83591928826
Iteration 3400: Loss = -11643.833974488527
Iteration 3500: Loss = -11643.832943711777
Iteration 3600: Loss = -11643.830585983953
Iteration 3700: Loss = -11643.82903359991
Iteration 3800: Loss = -11643.844953662578
1
Iteration 3900: Loss = -11643.826373965123
Iteration 4000: Loss = -11643.82516995667
Iteration 4100: Loss = -11643.828818445532
1
Iteration 4200: Loss = -11643.823013549625
Iteration 4300: Loss = -11643.822103697104
Iteration 4400: Loss = -11643.822300934562
1
Iteration 4500: Loss = -11643.820396968316
Iteration 4600: Loss = -11643.819613174519
Iteration 4700: Loss = -11643.818969694903
Iteration 4800: Loss = -11643.81825700546
Iteration 4900: Loss = -11643.8176439205
Iteration 5000: Loss = -11643.820482242281
1
Iteration 5100: Loss = -11643.816487597262
Iteration 5200: Loss = -11643.819600408857
1
Iteration 5300: Loss = -11643.815523143885
Iteration 5400: Loss = -11643.816907693028
1
Iteration 5500: Loss = -11643.814669848582
Iteration 5600: Loss = -11643.8142787545
Iteration 5700: Loss = -11643.813916924253
Iteration 5800: Loss = -11643.813572897972
Iteration 5900: Loss = -11643.81780374967
1
Iteration 6000: Loss = -11643.812952113729
Iteration 6100: Loss = -11643.816791068397
1
Iteration 6200: Loss = -11643.812360431295
Iteration 6300: Loss = -11643.81216946262
Iteration 6400: Loss = -11643.811954956403
Iteration 6500: Loss = -11643.811686084702
Iteration 6600: Loss = -11643.820494285128
1
Iteration 6700: Loss = -11643.811376498881
Iteration 6800: Loss = -11643.814685625828
1
Iteration 6900: Loss = -11643.811180865017
Iteration 7000: Loss = -11643.810786860953
Iteration 7100: Loss = -11643.811054016003
1
Iteration 7200: Loss = -11643.811724444819
2
Iteration 7300: Loss = -11643.81028704897
Iteration 7400: Loss = -11643.814957236247
1
Iteration 7500: Loss = -11643.810033671329
Iteration 7600: Loss = -11643.851329244673
1
Iteration 7700: Loss = -11643.811050119195
2
Iteration 7800: Loss = -11643.821677886268
3
Iteration 7900: Loss = -11643.813375986127
4
Iteration 8000: Loss = -11643.811175721625
5
Iteration 8100: Loss = -11643.809938687327
Iteration 8200: Loss = -11643.809359941402
Iteration 8300: Loss = -11643.80937028041
Iteration 8400: Loss = -11643.809752693956
1
Iteration 8500: Loss = -11643.81091386721
2
Iteration 8600: Loss = -11643.809126446627
Iteration 8700: Loss = -11643.80900845999
Iteration 8800: Loss = -11643.817619425405
1
Iteration 8900: Loss = -11643.80900279733
Iteration 9000: Loss = -11643.81551979584
1
Iteration 9100: Loss = -11643.80923325144
2
Iteration 9200: Loss = -11643.808851995667
Iteration 9300: Loss = -11643.8275541906
1
Iteration 9400: Loss = -11643.811580774367
2
Iteration 9500: Loss = -11643.816533986814
3
Iteration 9600: Loss = -11643.822774993178
4
Iteration 9700: Loss = -11643.816051095517
5
Iteration 9800: Loss = -11643.81567027302
6
Iteration 9900: Loss = -11643.81255495304
7
Iteration 10000: Loss = -11643.848402387806
8
Iteration 10100: Loss = -11643.813951882024
9
Iteration 10200: Loss = -11643.812018749773
10
Iteration 10300: Loss = -11643.809458660076
11
Iteration 10400: Loss = -11643.813206312196
12
Iteration 10500: Loss = -11643.831673489922
13
Iteration 10600: Loss = -11643.810484974563
14
Iteration 10700: Loss = -11643.815782776892
15
Stopping early at iteration 10700 due to no improvement.
pi: tensor([[0.7728, 0.2272],
        [0.2537, 0.7463]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4908, 0.5092], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4077, 0.1097],
         [0.6498, 0.1982]],

        [[0.7008, 0.0921],
         [0.6281, 0.5958]],

        [[0.6332, 0.0994],
         [0.5540, 0.5213]],

        [[0.6381, 0.0946],
         [0.6182, 0.5225]],

        [[0.7036, 0.1081],
         [0.7034, 0.6523]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999944811108
Average Adjusted Rand Index: 0.9919992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21756.81572641754
Iteration 100: Loss = -12527.355418237286
Iteration 200: Loss = -12165.004557372886
Iteration 300: Loss = -11856.822720901422
Iteration 400: Loss = -11835.939690644678
Iteration 500: Loss = -11826.143465867757
Iteration 600: Loss = -11825.946104978322
Iteration 700: Loss = -11825.8324785096
Iteration 800: Loss = -11825.764708176845
Iteration 900: Loss = -11825.717847832413
Iteration 1000: Loss = -11825.683200674903
Iteration 1100: Loss = -11825.656491723486
Iteration 1200: Loss = -11825.635345233419
Iteration 1300: Loss = -11825.61849441975
Iteration 1400: Loss = -11825.605414800428
Iteration 1500: Loss = -11825.594773971348
Iteration 1600: Loss = -11825.585969048081
Iteration 1700: Loss = -11825.578546191526
Iteration 1800: Loss = -11825.572186421634
Iteration 1900: Loss = -11825.566760344813
Iteration 2000: Loss = -11825.562027132984
Iteration 2100: Loss = -11825.557895683343
Iteration 2200: Loss = -11825.554212215957
Iteration 2300: Loss = -11825.550980242953
Iteration 2400: Loss = -11825.548006972875
Iteration 2500: Loss = -11825.552577441358
1
Iteration 2600: Loss = -11825.542784016532
Iteration 2700: Loss = -11825.540282822882
Iteration 2800: Loss = -11825.549684297457
1
Iteration 2900: Loss = -11825.536031626774
Iteration 3000: Loss = -11825.535469645087
Iteration 3100: Loss = -11825.533770461148
Iteration 3200: Loss = -11825.531925370498
Iteration 3300: Loss = -11825.531101605504
Iteration 3400: Loss = -11825.529669174384
Iteration 3500: Loss = -11825.528773672077
Iteration 3600: Loss = -11825.527872394785
Iteration 3700: Loss = -11825.527106054973
Iteration 3800: Loss = -11825.526305895763
Iteration 3900: Loss = -11825.525668110793
Iteration 4000: Loss = -11825.524978664142
Iteration 4100: Loss = -11825.526097265703
1
Iteration 4200: Loss = -11825.52386156052
Iteration 4300: Loss = -11825.523359216626
Iteration 4400: Loss = -11825.52287109801
Iteration 4500: Loss = -11825.522434652821
Iteration 4600: Loss = -11825.522002423413
Iteration 4700: Loss = -11825.521646188983
Iteration 4800: Loss = -11825.522033189554
1
Iteration 4900: Loss = -11825.520857545145
Iteration 5000: Loss = -11825.53871525623
1
Iteration 5100: Loss = -11825.520203660062
Iteration 5200: Loss = -11825.53066161591
1
Iteration 5300: Loss = -11825.519632346919
Iteration 5400: Loss = -11825.534034689386
1
Iteration 5500: Loss = -11825.518950232085
Iteration 5600: Loss = -11825.520444824333
1
Iteration 5700: Loss = -11825.518028538714
Iteration 5800: Loss = -11825.518214018688
1
Iteration 5900: Loss = -11825.51759794326
Iteration 6000: Loss = -11825.517497723145
Iteration 6100: Loss = -11825.517337130887
Iteration 6200: Loss = -11825.517181048917
Iteration 6300: Loss = -11825.517352709487
1
Iteration 6400: Loss = -11825.516931207636
Iteration 6500: Loss = -11825.518902054167
1
Iteration 6600: Loss = -11825.516661217562
Iteration 6700: Loss = -11825.516531644622
Iteration 6800: Loss = -11825.518484086886
1
Iteration 6900: Loss = -11825.51610913779
Iteration 7000: Loss = -11825.512291420999
Iteration 7100: Loss = -11825.51172352327
Iteration 7200: Loss = -11825.511420039427
Iteration 7300: Loss = -11825.511130803916
Iteration 7400: Loss = -11825.512911053896
1
Iteration 7500: Loss = -11825.511421845998
2
Iteration 7600: Loss = -11825.511614055908
3
Iteration 7700: Loss = -11825.55089262789
4
Iteration 7800: Loss = -11825.510860060933
Iteration 7900: Loss = -11825.534581148991
1
Iteration 8000: Loss = -11825.510715383241
Iteration 8100: Loss = -11825.522692088522
1
Iteration 8200: Loss = -11825.510620235507
Iteration 8300: Loss = -11825.510748529103
1
Iteration 8400: Loss = -11825.511456133454
2
Iteration 8500: Loss = -11825.51101854833
3
Iteration 8600: Loss = -11825.510566860363
Iteration 8700: Loss = -11825.53361762964
1
Iteration 8800: Loss = -11825.510482842254
Iteration 8900: Loss = -11825.510486212765
Iteration 9000: Loss = -11825.51044622989
Iteration 9100: Loss = -11825.510892269576
1
Iteration 9200: Loss = -11825.514709392386
2
Iteration 9300: Loss = -11825.610662828336
3
Iteration 9400: Loss = -11825.510245993131
Iteration 9500: Loss = -11825.510386284313
1
Iteration 9600: Loss = -11825.514308492518
2
Iteration 9700: Loss = -11825.510190810539
Iteration 9800: Loss = -11825.513112590821
1
Iteration 9900: Loss = -11825.510560780984
2
Iteration 10000: Loss = -11825.512714317201
3
Iteration 10100: Loss = -11825.51308802213
4
Iteration 10200: Loss = -11825.51041393813
5
Iteration 10300: Loss = -11825.721701447681
6
Iteration 10400: Loss = -11825.51023596903
Iteration 10500: Loss = -11825.529082891613
1
Iteration 10600: Loss = -11825.510037472577
Iteration 10700: Loss = -11825.517369617872
1
Iteration 10800: Loss = -11825.51020645321
2
Iteration 10900: Loss = -11825.510428204887
3
Iteration 11000: Loss = -11825.513400661528
4
Iteration 11100: Loss = -11825.5118120915
5
Iteration 11200: Loss = -11825.563019569761
6
Iteration 11300: Loss = -11825.510982015603
7
Iteration 11400: Loss = -11825.511207673333
8
Iteration 11500: Loss = -11825.514602749165
9
Iteration 11600: Loss = -11825.510009210278
Iteration 11700: Loss = -11825.514280430367
1
Iteration 11800: Loss = -11825.5288441277
2
Iteration 11900: Loss = -11825.53455889677
3
Iteration 12000: Loss = -11825.510500457009
4
Iteration 12100: Loss = -11825.51108106094
5
Iteration 12200: Loss = -11825.514704250903
6
Iteration 12300: Loss = -11825.531276681915
7
Iteration 12400: Loss = -11825.523821922985
8
Iteration 12500: Loss = -11825.55541490302
9
Iteration 12600: Loss = -11825.5106463186
10
Iteration 12700: Loss = -11825.511303121595
11
Iteration 12800: Loss = -11825.520084608992
12
Iteration 12900: Loss = -11825.677676629546
13
Iteration 13000: Loss = -11825.509891373687
Iteration 13100: Loss = -11825.510524467696
1
Iteration 13200: Loss = -11825.517862226689
2
Iteration 13300: Loss = -11825.509911013383
Iteration 13400: Loss = -11825.513016900904
1
Iteration 13500: Loss = -11825.591347003032
2
Iteration 13600: Loss = -11825.509833049857
Iteration 13700: Loss = -11825.520546991735
1
Iteration 13800: Loss = -11825.52181536599
2
Iteration 13900: Loss = -11825.544998250261
3
Iteration 14000: Loss = -11825.509749854778
Iteration 14100: Loss = -11825.51155964191
1
Iteration 14200: Loss = -11825.513772071845
2
Iteration 14300: Loss = -11825.509819757715
Iteration 14400: Loss = -11825.518875136104
1
Iteration 14500: Loss = -11825.512599628762
2
Iteration 14600: Loss = -11825.610763341516
3
Iteration 14700: Loss = -11825.51252009011
4
Iteration 14800: Loss = -11825.510531810443
5
Iteration 14900: Loss = -11825.518460876167
6
Iteration 15000: Loss = -11825.51227031545
7
Iteration 15100: Loss = -11825.51198780965
8
Iteration 15200: Loss = -11825.511776674462
9
Iteration 15300: Loss = -11825.510545078123
10
Iteration 15400: Loss = -11825.509825863126
Iteration 15500: Loss = -11825.511578005948
1
Iteration 15600: Loss = -11825.513413781431
2
Iteration 15700: Loss = -11825.529156546345
3
Iteration 15800: Loss = -11825.514502704986
4
Iteration 15900: Loss = -11825.509935970626
5
Iteration 16000: Loss = -11825.516874580253
6
Iteration 16100: Loss = -11825.509730137224
Iteration 16200: Loss = -11825.510396419608
1
Iteration 16300: Loss = -11825.519163502191
2
Iteration 16400: Loss = -11825.52251360317
3
Iteration 16500: Loss = -11825.509782710471
Iteration 16600: Loss = -11825.510042766804
1
Iteration 16700: Loss = -11825.59589213312
2
Iteration 16800: Loss = -11825.658669724195
3
Iteration 16900: Loss = -11825.521706678663
4
Iteration 17000: Loss = -11825.509794093308
Iteration 17100: Loss = -11825.510459826253
1
Iteration 17200: Loss = -11825.581898762259
2
Iteration 17300: Loss = -11825.514577203525
3
Iteration 17400: Loss = -11825.512723583231
4
Iteration 17500: Loss = -11825.514407651437
5
Iteration 17600: Loss = -11825.515165582286
6
Iteration 17700: Loss = -11825.511700926469
7
Iteration 17800: Loss = -11825.530447505607
8
Iteration 17900: Loss = -11825.512574579252
9
Iteration 18000: Loss = -11825.514712900946
10
Iteration 18100: Loss = -11825.552562044364
11
Iteration 18200: Loss = -11825.516111273975
12
Iteration 18300: Loss = -11825.5979723
13
Iteration 18400: Loss = -11825.622518170405
14
Iteration 18500: Loss = -11825.50915995412
Iteration 18600: Loss = -11825.515076947107
1
Iteration 18700: Loss = -11825.536243012757
2
Iteration 18800: Loss = -11825.524015918316
3
Iteration 18900: Loss = -11825.512797825066
4
Iteration 19000: Loss = -11825.582388387378
5
Iteration 19100: Loss = -11825.512870663537
6
Iteration 19200: Loss = -11825.513225589344
7
Iteration 19300: Loss = -11825.511775690775
8
Iteration 19400: Loss = -11825.509054751248
Iteration 19500: Loss = -11825.51049609276
1
Iteration 19600: Loss = -11825.558051453458
2
Iteration 19700: Loss = -11825.512981628588
3
Iteration 19800: Loss = -11825.51036645367
4
Iteration 19900: Loss = -11825.508991719102
pi: tensor([[0.4922, 0.5078],
        [0.4487, 0.5513]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4888, 0.5112], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4039, 0.1096],
         [0.5228, 0.2218]],

        [[0.5503, 0.0919],
         [0.6429, 0.5421]],

        [[0.5189, 0.0991],
         [0.7093, 0.6695]],

        [[0.5560, 0.0933],
         [0.5380, 0.7106]],

        [[0.6292, 0.1081],
         [0.6667, 0.6770]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 79
Adjusted Rand Index: 0.3309002433090024
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4613232544904299
Average Adjusted Rand Index: 0.8581792649915296
11650.261573835178
[0.9919999944811108, 0.4613232544904299] [0.9919992163297293, 0.8581792649915296] [11643.815782776892, 11825.53347613318]
-------------------------------------
This iteration is 63
True Objective function: Loss = -11409.192217373846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20888.284479791564
Iteration 100: Loss = -12225.866697057892
Iteration 200: Loss = -12224.498597921713
Iteration 300: Loss = -12218.417206910699
Iteration 400: Loss = -11678.035271319704
Iteration 500: Loss = -11449.169281868832
Iteration 600: Loss = -11412.745942529664
Iteration 700: Loss = -11411.936886273412
Iteration 800: Loss = -11411.609757137667
Iteration 900: Loss = -11411.403329997525
Iteration 1000: Loss = -11404.259944199697
Iteration 1100: Loss = -11404.161264621502
Iteration 1200: Loss = -11404.090547751572
Iteration 1300: Loss = -11404.036689499804
Iteration 1400: Loss = -11403.994564634357
Iteration 1500: Loss = -11403.960764965332
Iteration 1600: Loss = -11403.933174189533
Iteration 1700: Loss = -11403.910245487277
Iteration 1800: Loss = -11403.891033558537
Iteration 1900: Loss = -11403.874691551626
Iteration 2000: Loss = -11403.860702479878
Iteration 2100: Loss = -11403.848586628048
Iteration 2200: Loss = -11403.838038892318
Iteration 2300: Loss = -11403.828747822898
Iteration 2400: Loss = -11403.820611771205
Iteration 2500: Loss = -11403.81335319481
Iteration 2600: Loss = -11403.806909792247
Iteration 2700: Loss = -11403.801119270447
Iteration 2800: Loss = -11403.795957521472
Iteration 2900: Loss = -11403.791268853343
Iteration 3000: Loss = -11403.787064623919
Iteration 3100: Loss = -11403.783240063485
Iteration 3200: Loss = -11403.779738822588
Iteration 3300: Loss = -11403.776541124693
Iteration 3400: Loss = -11403.773715727675
Iteration 3500: Loss = -11403.771137051395
Iteration 3600: Loss = -11403.76848822799
Iteration 3700: Loss = -11403.769688209448
1
Iteration 3800: Loss = -11403.774053578705
2
Iteration 3900: Loss = -11403.762195552226
Iteration 4000: Loss = -11403.760402582633
Iteration 4100: Loss = -11403.75913461182
Iteration 4200: Loss = -11403.75719139731
Iteration 4300: Loss = -11403.755762915895
Iteration 4400: Loss = -11403.755083294174
Iteration 4500: Loss = -11403.753134409828
Iteration 4600: Loss = -11403.752155533297
Iteration 4700: Loss = -11403.751976588937
Iteration 4800: Loss = -11403.749798378089
Iteration 4900: Loss = -11403.763891207973
1
Iteration 5000: Loss = -11403.7482136777
Iteration 5100: Loss = -11403.747213530247
Iteration 5200: Loss = -11403.750162797063
1
Iteration 5300: Loss = -11403.745765613856
Iteration 5400: Loss = -11403.746470265205
1
Iteration 5500: Loss = -11403.74449386442
Iteration 5600: Loss = -11403.74392830503
Iteration 5700: Loss = -11403.747414898928
1
Iteration 5800: Loss = -11403.742888250543
Iteration 5900: Loss = -11403.742398324704
Iteration 6000: Loss = -11403.741956333026
Iteration 6100: Loss = -11403.741504282765
Iteration 6200: Loss = -11403.741116965459
Iteration 6300: Loss = -11403.740769653265
Iteration 6400: Loss = -11403.740341472316
Iteration 6500: Loss = -11403.74255677357
1
Iteration 6600: Loss = -11403.739730395298
Iteration 6700: Loss = -11403.739428923956
Iteration 6800: Loss = -11403.740268595795
1
Iteration 6900: Loss = -11403.738852624727
Iteration 7000: Loss = -11403.738597564365
Iteration 7100: Loss = -11403.740546507242
1
Iteration 7200: Loss = -11403.738215083504
Iteration 7300: Loss = -11403.737967367708
Iteration 7400: Loss = -11403.737817982481
Iteration 7500: Loss = -11403.737784245448
Iteration 7600: Loss = -11403.737379761207
Iteration 7700: Loss = -11403.755185499556
1
Iteration 7800: Loss = -11403.737037226034
Iteration 7900: Loss = -11403.736912444972
Iteration 8000: Loss = -11403.736509846218
Iteration 8100: Loss = -11403.74125558403
1
Iteration 8200: Loss = -11403.73653329952
Iteration 8300: Loss = -11403.743279122751
1
Iteration 8400: Loss = -11403.739138760186
2
Iteration 8500: Loss = -11403.735792769758
Iteration 8600: Loss = -11403.736333779023
1
Iteration 8700: Loss = -11403.735552544524
Iteration 8800: Loss = -11403.740668399883
1
Iteration 8900: Loss = -11403.739886393827
2
Iteration 9000: Loss = -11403.735291145791
Iteration 9100: Loss = -11403.736094210739
1
Iteration 9200: Loss = -11403.739764544873
2
Iteration 9300: Loss = -11403.737821835171
3
Iteration 9400: Loss = -11403.73840821749
4
Iteration 9500: Loss = -11403.735456647444
5
Iteration 9600: Loss = -11403.736258099594
6
Iteration 9700: Loss = -11403.76710713081
7
Iteration 9800: Loss = -11403.735397048895
8
Iteration 9900: Loss = -11403.765655295807
9
Iteration 10000: Loss = -11403.746837795554
10
Iteration 10100: Loss = -11403.738346966113
11
Iteration 10200: Loss = -11403.743995141203
12
Iteration 10300: Loss = -11403.79053588827
13
Iteration 10400: Loss = -11403.740511411768
14
Iteration 10500: Loss = -11403.735358888607
Iteration 10600: Loss = -11403.736478037417
1
Iteration 10700: Loss = -11403.7383773321
2
Iteration 10800: Loss = -11403.758198572414
3
Iteration 10900: Loss = -11403.738486455351
4
Iteration 11000: Loss = -11403.741510463302
5
Iteration 11100: Loss = -11403.740214236212
6
Iteration 11200: Loss = -11403.75991599971
7
Iteration 11300: Loss = -11403.801370372934
8
Iteration 11400: Loss = -11403.752158575342
9
Iteration 11500: Loss = -11403.755769094036
10
Iteration 11600: Loss = -11403.736256497661
11
Iteration 11700: Loss = -11403.741155504611
12
Iteration 11800: Loss = -11403.740654374316
13
Iteration 11900: Loss = -11403.739985633503
14
Iteration 12000: Loss = -11403.735387119465
Iteration 12100: Loss = -11403.741841245455
1
Iteration 12200: Loss = -11403.886905733434
2
Iteration 12300: Loss = -11403.736263436707
3
Iteration 12400: Loss = -11403.785267097226
4
Iteration 12500: Loss = -11403.734264899542
Iteration 12600: Loss = -11403.740929330193
1
Iteration 12700: Loss = -11403.734912893924
2
Iteration 12800: Loss = -11403.73955538501
3
Iteration 12900: Loss = -11403.735043801555
4
Iteration 13000: Loss = -11403.733933832928
Iteration 13100: Loss = -11403.74125370659
1
Iteration 13200: Loss = -11403.734548864719
2
Iteration 13300: Loss = -11403.734942532259
3
Iteration 13400: Loss = -11403.734401170395
4
Iteration 13500: Loss = -11403.750011823751
5
Iteration 13600: Loss = -11403.803638944457
6
Iteration 13700: Loss = -11403.736602953866
7
Iteration 13800: Loss = -11403.734799962387
8
Iteration 13900: Loss = -11403.734531050952
9
Iteration 14000: Loss = -11403.73620327834
10
Iteration 14100: Loss = -11403.74436519062
11
Iteration 14200: Loss = -11403.745201968313
12
Iteration 14300: Loss = -11403.797077672045
13
Iteration 14400: Loss = -11403.751687995384
14
Iteration 14500: Loss = -11403.751058840537
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[0.7431, 0.2569],
        [0.2234, 0.7766]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5123, 0.4877], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4052, 0.0965],
         [0.6643, 0.1980]],

        [[0.6102, 0.0961],
         [0.6183, 0.6444]],

        [[0.6876, 0.1058],
         [0.6766, 0.5204]],

        [[0.5085, 0.0990],
         [0.5492, 0.5436]],

        [[0.5642, 0.0916],
         [0.5982, 0.7299]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320488565437
Average Adjusted Rand Index: 0.9839993730966994
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21159.17358998951
Iteration 100: Loss = -12225.51893840415
Iteration 200: Loss = -12222.770353774336
Iteration 300: Loss = -12070.631274806796
Iteration 400: Loss = -11414.821997467516
Iteration 500: Loss = -11412.743544960786
Iteration 600: Loss = -11404.870023147776
Iteration 700: Loss = -11404.53177210523
Iteration 800: Loss = -11404.337836400035
Iteration 900: Loss = -11404.211215725863
Iteration 1000: Loss = -11404.122605988896
Iteration 1100: Loss = -11404.057458558418
Iteration 1200: Loss = -11404.00788281325
Iteration 1300: Loss = -11403.968978911584
Iteration 1400: Loss = -11403.937887937069
Iteration 1500: Loss = -11403.912543690363
Iteration 1600: Loss = -11403.891677017256
Iteration 1700: Loss = -11403.874253996146
Iteration 1800: Loss = -11403.859453695544
Iteration 1900: Loss = -11403.84679251311
Iteration 2000: Loss = -11403.835870296594
Iteration 2100: Loss = -11403.826312048346
Iteration 2200: Loss = -11403.817953337788
Iteration 2300: Loss = -11403.810541310231
Iteration 2400: Loss = -11403.804036226855
Iteration 2500: Loss = -11403.81818267152
1
Iteration 2600: Loss = -11403.79292479167
Iteration 2700: Loss = -11403.788253453307
Iteration 2800: Loss = -11403.784031209812
Iteration 2900: Loss = -11403.780197603088
Iteration 3000: Loss = -11403.776758179984
Iteration 3100: Loss = -11403.773587307527
Iteration 3200: Loss = -11403.770938765942
Iteration 3300: Loss = -11403.768110655228
Iteration 3400: Loss = -11403.765683393192
Iteration 3500: Loss = -11403.774592383978
1
Iteration 3600: Loss = -11403.76146498085
Iteration 3700: Loss = -11403.75962783674
Iteration 3800: Loss = -11403.761240459868
1
Iteration 3900: Loss = -11403.756537527486
Iteration 4000: Loss = -11403.754976917486
Iteration 4100: Loss = -11403.775664685183
1
Iteration 4200: Loss = -11403.75808806248
2
Iteration 4300: Loss = -11403.751243878358
Iteration 4400: Loss = -11403.750203871861
Iteration 4500: Loss = -11403.749199576621
Iteration 4600: Loss = -11403.748479605658
Iteration 4700: Loss = -11403.747422308928
Iteration 4800: Loss = -11403.747063120321
Iteration 4900: Loss = -11403.745878073001
Iteration 5000: Loss = -11403.745204746048
Iteration 5100: Loss = -11403.773107091922
1
Iteration 5200: Loss = -11403.743903523764
Iteration 5300: Loss = -11403.743345286357
Iteration 5400: Loss = -11403.742767250405
Iteration 5500: Loss = -11403.742312165836
Iteration 5600: Loss = -11403.741797892557
Iteration 5700: Loss = -11403.74172905464
Iteration 5800: Loss = -11403.741076217859
Iteration 5900: Loss = -11403.740558162937
Iteration 6000: Loss = -11403.740179295553
Iteration 6100: Loss = -11403.74849592369
1
Iteration 6200: Loss = -11403.73948153682
Iteration 6300: Loss = -11403.739187794465
Iteration 6400: Loss = -11403.743239708412
1
Iteration 6500: Loss = -11403.738851226117
Iteration 6600: Loss = -11403.738343517809
Iteration 6700: Loss = -11403.744677733617
1
Iteration 6800: Loss = -11403.737882314117
Iteration 6900: Loss = -11403.737666887935
Iteration 7000: Loss = -11403.742904558876
1
Iteration 7100: Loss = -11403.737266271564
Iteration 7200: Loss = -11403.73707208595
Iteration 7300: Loss = -11403.746435837927
1
Iteration 7400: Loss = -11403.73670191101
Iteration 7500: Loss = -11403.736844779578
1
Iteration 7600: Loss = -11403.73798077997
2
Iteration 7700: Loss = -11403.73742874513
3
Iteration 7800: Loss = -11403.740278676029
4
Iteration 7900: Loss = -11403.753222276657
5
Iteration 8000: Loss = -11403.736015299752
Iteration 8100: Loss = -11403.735796323313
Iteration 8200: Loss = -11403.73708868961
1
Iteration 8300: Loss = -11403.736858312654
2
Iteration 8400: Loss = -11403.735466334187
Iteration 8500: Loss = -11403.777974416162
1
Iteration 8600: Loss = -11403.735251596578
Iteration 8700: Loss = -11403.73526310537
Iteration 8800: Loss = -11403.735070390088
Iteration 8900: Loss = -11403.735127873762
Iteration 9000: Loss = -11403.765865533001
1
Iteration 9100: Loss = -11403.734844470662
Iteration 9200: Loss = -11403.734883566996
Iteration 9300: Loss = -11403.751274668686
1
Iteration 9400: Loss = -11403.738755704451
2
Iteration 9500: Loss = -11403.783660107578
3
Iteration 9600: Loss = -11403.78421740666
4
Iteration 9700: Loss = -11403.740672428341
5
Iteration 9800: Loss = -11403.734676631666
Iteration 9900: Loss = -11403.746904564467
1
Iteration 10000: Loss = -11403.778470660995
2
Iteration 10100: Loss = -11403.74104328159
3
Iteration 10200: Loss = -11403.743433312035
4
Iteration 10300: Loss = -11403.74694565005
5
Iteration 10400: Loss = -11403.759235570335
6
Iteration 10500: Loss = -11403.743202657253
7
Iteration 10600: Loss = -11403.751913466485
8
Iteration 10700: Loss = -11403.793178128006
9
Iteration 10800: Loss = -11403.7830952204
10
Iteration 10900: Loss = -11403.750099235598
11
Iteration 11000: Loss = -11403.734559634651
Iteration 11100: Loss = -11403.735725389979
1
Iteration 11200: Loss = -11403.734225337239
Iteration 11300: Loss = -11403.735101170936
1
Iteration 11400: Loss = -11403.73489719469
2
Iteration 11500: Loss = -11403.747750426412
3
Iteration 11600: Loss = -11403.75931784374
4
Iteration 11700: Loss = -11403.751255375677
5
Iteration 11800: Loss = -11403.742531557511
6
Iteration 11900: Loss = -11403.766023297127
7
Iteration 12000: Loss = -11403.743005765331
8
Iteration 12100: Loss = -11403.735421257725
9
Iteration 12200: Loss = -11403.755049582922
10
Iteration 12300: Loss = -11403.73792447096
11
Iteration 12400: Loss = -11403.801511001626
12
Iteration 12500: Loss = -11403.811563437052
13
Iteration 12600: Loss = -11403.734861791609
14
Iteration 12700: Loss = -11403.735216565305
15
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.7439, 0.2561],
        [0.2248, 0.7752]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5099, 0.4901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4046, 0.0965],
         [0.6415, 0.1982]],

        [[0.7149, 0.0963],
         [0.5424, 0.6922]],

        [[0.5975, 0.1052],
         [0.5154, 0.5830]],

        [[0.6620, 0.0988],
         [0.5852, 0.7015]],

        [[0.5548, 0.0916],
         [0.5891, 0.5142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320488565437
Average Adjusted Rand Index: 0.9839993730966994
11409.192217373846
[0.9840320488565437, 0.9840320488565437] [0.9839993730966994, 0.9839993730966994] [11403.751058840537, 11403.735216565305]
-------------------------------------
This iteration is 64
True Objective function: Loss = -11522.947912558191
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23424.248370928955
Iteration 100: Loss = -12351.076950599461
Iteration 200: Loss = -12288.64884481579
Iteration 300: Loss = -12278.43797421337
Iteration 400: Loss = -12266.827252305757
Iteration 500: Loss = -11933.67549713255
Iteration 600: Loss = -11856.310174796525
Iteration 700: Loss = -11841.937928927806
Iteration 800: Loss = -11840.592294141852
Iteration 900: Loss = -11826.219448686488
Iteration 1000: Loss = -11826.133590385049
Iteration 1100: Loss = -11825.947435522337
Iteration 1200: Loss = -11825.919548708056
Iteration 1300: Loss = -11825.896627163913
Iteration 1400: Loss = -11825.881425732096
Iteration 1500: Loss = -11825.868689655
Iteration 1600: Loss = -11825.85061982548
Iteration 1700: Loss = -11820.72048720417
Iteration 1800: Loss = -11820.59274723931
Iteration 1900: Loss = -11816.576391100583
Iteration 2000: Loss = -11816.466343881233
Iteration 2100: Loss = -11816.45902859912
Iteration 2200: Loss = -11816.45905136366
Iteration 2300: Loss = -11798.018872410445
Iteration 2400: Loss = -11797.961738638034
Iteration 2500: Loss = -11797.967043495528
1
Iteration 2600: Loss = -11797.952548833924
Iteration 2700: Loss = -11797.949168812347
Iteration 2800: Loss = -11797.946776618466
Iteration 2900: Loss = -11797.944419779971
Iteration 3000: Loss = -11797.942496178664
Iteration 3100: Loss = -11797.940703689663
Iteration 3200: Loss = -11797.939148107984
Iteration 3300: Loss = -11797.937807187682
Iteration 3400: Loss = -11797.936512223007
Iteration 3500: Loss = -11797.935290189844
Iteration 3600: Loss = -11797.933749252605
Iteration 3700: Loss = -11797.932034823145
Iteration 3800: Loss = -11797.93004393586
Iteration 3900: Loss = -11797.934503131264
1
Iteration 4000: Loss = -11797.928549803484
Iteration 4100: Loss = -11797.92794844173
Iteration 4200: Loss = -11797.925196033122
Iteration 4300: Loss = -11797.921246133783
Iteration 4400: Loss = -11797.905907751385
Iteration 4500: Loss = -11797.504169111491
Iteration 4600: Loss = -11797.503649954098
Iteration 4700: Loss = -11797.503358450147
Iteration 4800: Loss = -11797.502812654526
Iteration 4900: Loss = -11797.507180937646
1
Iteration 5000: Loss = -11797.501997031251
Iteration 5100: Loss = -11797.50163384715
Iteration 5200: Loss = -11797.50313003167
1
Iteration 5300: Loss = -11797.498810432086
Iteration 5400: Loss = -11792.919230687603
Iteration 5500: Loss = -11792.9129069161
Iteration 5600: Loss = -11792.914331752481
1
Iteration 5700: Loss = -11792.915284707899
2
Iteration 5800: Loss = -11792.912782746847
Iteration 5900: Loss = -11792.911722572167
Iteration 6000: Loss = -11792.910056131765
Iteration 6100: Loss = -11790.175232772044
Iteration 6200: Loss = -11790.058305994447
Iteration 6300: Loss = -11790.058050604746
Iteration 6400: Loss = -11790.057795826902
Iteration 6500: Loss = -11790.05685274342
Iteration 6600: Loss = -11787.708424374809
Iteration 6700: Loss = -11787.695279977253
Iteration 6800: Loss = -11787.693137426482
Iteration 6900: Loss = -11787.692906830522
Iteration 7000: Loss = -11787.692836535145
Iteration 7100: Loss = -11787.698865148346
1
Iteration 7200: Loss = -11787.692444506301
Iteration 7300: Loss = -11787.692263706935
Iteration 7400: Loss = -11787.692600133043
1
Iteration 7500: Loss = -11787.821060207512
2
Iteration 7600: Loss = -11787.692046586793
Iteration 7700: Loss = -11787.691993835897
Iteration 7800: Loss = -11787.693597182213
1
Iteration 7900: Loss = -11787.691797592006
Iteration 8000: Loss = -11787.691587292164
Iteration 8100: Loss = -11784.211385041963
Iteration 8200: Loss = -11775.363687071083
Iteration 8300: Loss = -11775.362633135954
Iteration 8400: Loss = -11775.362122247505
Iteration 8500: Loss = -11775.438505754548
1
Iteration 8600: Loss = -11775.361842014421
Iteration 8700: Loss = -11775.361724523425
Iteration 8800: Loss = -11775.362018623826
1
Iteration 8900: Loss = -11775.361487120503
Iteration 9000: Loss = -11775.361354989913
Iteration 9100: Loss = -11775.36384056314
1
Iteration 9200: Loss = -11775.301070232195
Iteration 9300: Loss = -11774.226575418848
Iteration 9400: Loss = -11774.22702683988
1
Iteration 9500: Loss = -11774.295438393638
2
Iteration 9600: Loss = -11774.218752874003
Iteration 9700: Loss = -11774.31625150775
1
Iteration 9800: Loss = -11774.216878581557
Iteration 9900: Loss = -11774.26901726412
1
Iteration 10000: Loss = -11774.213881171097
Iteration 10100: Loss = -11774.225625417183
1
Iteration 10200: Loss = -11774.152452034787
Iteration 10300: Loss = -11774.152420208413
Iteration 10400: Loss = -11774.154975536698
1
Iteration 10500: Loss = -11774.15230782763
Iteration 10600: Loss = -11774.152531931111
1
Iteration 10700: Loss = -11774.17227295334
2
Iteration 10800: Loss = -11774.152267558633
Iteration 10900: Loss = -11774.261825445525
1
Iteration 11000: Loss = -11774.152231655016
Iteration 11100: Loss = -11774.152996106144
1
Iteration 11200: Loss = -11774.16051056921
2
Iteration 11300: Loss = -11774.152218898455
Iteration 11400: Loss = -11774.2553941242
1
Iteration 11500: Loss = -11774.152653193023
2
Iteration 11600: Loss = -11774.185558244104
3
Iteration 11700: Loss = -11774.155952422989
4
Iteration 11800: Loss = -11774.160322457228
5
Iteration 11900: Loss = -11774.20848450384
6
Iteration 12000: Loss = -11774.152113326381
Iteration 12100: Loss = -11774.152614289278
1
Iteration 12200: Loss = -11774.159776184299
2
Iteration 12300: Loss = -11774.15011840284
Iteration 12400: Loss = -11774.149878259159
Iteration 12500: Loss = -11769.533795415191
Iteration 12600: Loss = -11769.505692685
Iteration 12700: Loss = -11769.5057550326
Iteration 12800: Loss = -11769.516228420767
1
Iteration 12900: Loss = -11769.51098093008
2
Iteration 13000: Loss = -11769.506519340037
3
Iteration 13100: Loss = -11769.521317241193
4
Iteration 13200: Loss = -11769.506251543828
5
Iteration 13300: Loss = -11769.505489371573
Iteration 13400: Loss = -11769.506222752852
1
Iteration 13500: Loss = -11769.505453144038
Iteration 13600: Loss = -11769.50593114473
1
Iteration 13700: Loss = -11769.503467899918
Iteration 13800: Loss = -11769.509536916572
1
Iteration 13900: Loss = -11769.503354689308
Iteration 14000: Loss = -11769.508841563254
1
Iteration 14100: Loss = -11769.51646168422
2
Iteration 14200: Loss = -11769.503358328224
Iteration 14300: Loss = -11769.503316074119
Iteration 14400: Loss = -11769.503807906523
1
Iteration 14500: Loss = -11769.50317627468
Iteration 14600: Loss = -11769.503384409665
1
Iteration 14700: Loss = -11769.503214014316
Iteration 14800: Loss = -11769.502251874936
Iteration 14900: Loss = -11769.50120400746
Iteration 15000: Loss = -11769.498901068147
Iteration 15100: Loss = -11769.513125344469
1
Iteration 15200: Loss = -11769.513694359526
2
Iteration 15300: Loss = -11769.502862845795
3
Iteration 15400: Loss = -11769.499386012174
4
Iteration 15500: Loss = -11769.499138989902
5
Iteration 15600: Loss = -11769.510918265954
6
Iteration 15700: Loss = -11769.50785886702
7
Iteration 15800: Loss = -11769.498953564638
Iteration 15900: Loss = -11769.50065063933
1
Iteration 16000: Loss = -11769.503507479567
2
Iteration 16100: Loss = -11769.505086667155
3
Iteration 16200: Loss = -11769.498830344673
Iteration 16300: Loss = -11769.516524606493
1
Iteration 16400: Loss = -11769.499961107282
2
Iteration 16500: Loss = -11769.499739539342
3
Iteration 16600: Loss = -11769.643748194501
4
Iteration 16700: Loss = -11769.413782051239
Iteration 16800: Loss = -11769.410962973936
Iteration 16900: Loss = -11769.436957603206
1
Iteration 17000: Loss = -11769.409796787306
Iteration 17100: Loss = -11769.405474325744
Iteration 17200: Loss = -11769.486775648915
1
Iteration 17300: Loss = -11769.409802367314
2
Iteration 17400: Loss = -11769.408720115016
3
Iteration 17500: Loss = -11769.410570716871
4
Iteration 17600: Loss = -11769.411288352318
5
Iteration 17700: Loss = -11769.40499717367
Iteration 17800: Loss = -11769.419632241405
1
Iteration 17900: Loss = -11769.406249171463
2
Iteration 18000: Loss = -11769.404968151037
Iteration 18100: Loss = -11769.407614270684
1
Iteration 18200: Loss = -11769.398885913144
Iteration 18300: Loss = -11769.398157352536
Iteration 18400: Loss = -11769.408063149931
1
Iteration 18500: Loss = -11769.403075977052
2
Iteration 18600: Loss = -11769.408206090258
3
Iteration 18700: Loss = -11769.39806443085
Iteration 18800: Loss = -11769.408450118424
1
Iteration 18900: Loss = -11769.398041676037
Iteration 19000: Loss = -11769.429909214092
1
Iteration 19100: Loss = -11769.398058940958
Iteration 19200: Loss = -11769.4133770632
1
Iteration 19300: Loss = -11769.398043253128
Iteration 19400: Loss = -11769.470984148442
1
Iteration 19500: Loss = -11769.398033564394
Iteration 19600: Loss = -11769.47870146256
1
Iteration 19700: Loss = -11766.847157357344
Iteration 19800: Loss = -11766.851814059195
1
Iteration 19900: Loss = -11766.8408452722
pi: tensor([[0.2682, 0.7318],
        [0.7271, 0.2729]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5049, 0.4951], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2751, 0.1048],
         [0.5510, 0.3294]],

        [[0.5769, 0.0922],
         [0.6029, 0.6758]],

        [[0.6032, 0.0919],
         [0.5170, 0.6268]],

        [[0.6716, 0.0933],
         [0.7232, 0.7121]],

        [[0.5139, 0.0985],
         [0.5705, 0.5239]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7369635135591801
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 4
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.050080741377465564
Average Adjusted Rand Index: 0.9000371911794594
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22744.66507155338
Iteration 100: Loss = -11837.934033063195
Iteration 200: Loss = -11822.88810071755
Iteration 300: Loss = -11822.361380226099
Iteration 400: Loss = -11822.215973525763
Iteration 500: Loss = -11822.139785848985
Iteration 600: Loss = -11822.10272633219
Iteration 700: Loss = -11822.078045805172
Iteration 800: Loss = -11822.05999120539
Iteration 900: Loss = -11822.046441788132
Iteration 1000: Loss = -11822.035889658742
Iteration 1100: Loss = -11822.027438543239
Iteration 1200: Loss = -11822.020605618065
Iteration 1300: Loss = -11822.01501059474
Iteration 1400: Loss = -11822.010436274346
Iteration 1500: Loss = -11822.006683117068
Iteration 1600: Loss = -11822.003551841628
Iteration 1700: Loss = -11822.000920716928
Iteration 1800: Loss = -11821.998761009252
Iteration 1900: Loss = -11821.996944562183
Iteration 2000: Loss = -11821.995420725656
Iteration 2100: Loss = -11821.994098455289
Iteration 2200: Loss = -11821.992954350462
Iteration 2300: Loss = -11821.992138428313
Iteration 2400: Loss = -11821.99117192387
Iteration 2500: Loss = -11821.99036917161
Iteration 2600: Loss = -11821.989711053675
Iteration 2700: Loss = -11821.98909222048
Iteration 2800: Loss = -11821.98854027592
Iteration 2900: Loss = -11821.988118367371
Iteration 3000: Loss = -11821.99218214956
1
Iteration 3100: Loss = -11821.987258842955
Iteration 3200: Loss = -11821.986879835966
Iteration 3300: Loss = -11821.987004669081
1
Iteration 3400: Loss = -11821.986201569363
Iteration 3500: Loss = -11822.002874802329
1
Iteration 3600: Loss = -11821.985659447604
Iteration 3700: Loss = -11821.991554688388
1
Iteration 3800: Loss = -11821.985209897397
Iteration 3900: Loss = -11821.985304064108
Iteration 4000: Loss = -11821.984867177927
Iteration 4100: Loss = -11821.984839073033
Iteration 4200: Loss = -11821.984676989863
Iteration 4300: Loss = -11821.98446387219
Iteration 4400: Loss = -11821.995838785368
1
Iteration 4500: Loss = -11821.984200787438
Iteration 4600: Loss = -11821.984753686498
1
Iteration 4700: Loss = -11821.983983843553
Iteration 4800: Loss = -11821.984606774568
1
Iteration 4900: Loss = -11821.983792849049
Iteration 5000: Loss = -11821.9840560153
1
Iteration 5100: Loss = -11821.983779939044
Iteration 5200: Loss = -11821.983584484886
Iteration 5300: Loss = -11821.985466769187
1
Iteration 5400: Loss = -11821.983484515205
Iteration 5500: Loss = -11821.98750053204
1
Iteration 5600: Loss = -11821.983332420506
Iteration 5700: Loss = -11821.983330410818
Iteration 5800: Loss = -11821.983513785886
1
Iteration 5900: Loss = -11821.991489769505
2
Iteration 6000: Loss = -11821.98386618642
3
Iteration 6100: Loss = -11821.983121995667
Iteration 6200: Loss = -11821.983172367647
Iteration 6300: Loss = -11821.983821052185
1
Iteration 6400: Loss = -11821.983006322187
Iteration 6500: Loss = -11821.990933324045
1
Iteration 6600: Loss = -11821.982977803102
Iteration 6700: Loss = -11821.987622994531
1
Iteration 6800: Loss = -11821.98289173017
Iteration 6900: Loss = -11821.983495328826
1
Iteration 7000: Loss = -11821.982946299495
Iteration 7100: Loss = -11821.982989826272
Iteration 7200: Loss = -11821.983323874034
1
Iteration 7300: Loss = -11821.989428701196
2
Iteration 7400: Loss = -11821.98301976868
Iteration 7500: Loss = -11821.982814439289
Iteration 7600: Loss = -11821.982749393525
Iteration 7700: Loss = -11821.982835815863
Iteration 7800: Loss = -11821.983595989685
1
Iteration 7900: Loss = -11821.982719727559
Iteration 8000: Loss = -11821.982746177047
Iteration 8100: Loss = -11821.993485975234
1
Iteration 8200: Loss = -11821.982675978325
Iteration 8300: Loss = -11821.98401298902
1
Iteration 8400: Loss = -11821.982696066229
Iteration 8500: Loss = -11821.985305479117
1
Iteration 8600: Loss = -11821.982628262233
Iteration 8700: Loss = -11821.98269470577
Iteration 8800: Loss = -11821.98263865939
Iteration 8900: Loss = -11821.982580665788
Iteration 9000: Loss = -11822.079471025741
1
Iteration 9100: Loss = -11821.982610235935
Iteration 9200: Loss = -11821.98258456315
Iteration 9300: Loss = -11821.98379262581
1
Iteration 9400: Loss = -11821.982588928575
Iteration 9500: Loss = -11821.982577009647
Iteration 9600: Loss = -11821.98844786447
1
Iteration 9700: Loss = -11821.982592927578
Iteration 9800: Loss = -11821.996472959167
1
Iteration 9900: Loss = -11821.983886961907
2
Iteration 10000: Loss = -11821.983832440228
3
Iteration 10100: Loss = -11821.982651107319
Iteration 10200: Loss = -11821.98318937483
1
Iteration 10300: Loss = -11821.993176959082
2
Iteration 10400: Loss = -11821.989967259315
3
Iteration 10500: Loss = -11821.98288366522
4
Iteration 10600: Loss = -11821.98282882055
5
Iteration 10700: Loss = -11821.982919403315
6
Iteration 10800: Loss = -11821.99556916754
7
Iteration 10900: Loss = -11821.999802212244
8
Iteration 11000: Loss = -11821.983070473276
9
Iteration 11100: Loss = -11821.983760600277
10
Iteration 11200: Loss = -11821.990241020478
11
Iteration 11300: Loss = -11821.996441484876
12
Iteration 11400: Loss = -11821.982548583848
Iteration 11500: Loss = -11821.988279526691
1
Iteration 11600: Loss = -11821.98255416564
Iteration 11700: Loss = -11821.98267707368
1
Iteration 11800: Loss = -11822.147520378927
2
Iteration 11900: Loss = -11821.982799643712
3
Iteration 12000: Loss = -11821.983422688838
4
Iteration 12100: Loss = -11821.9932169302
5
Iteration 12200: Loss = -11821.982731289807
6
Iteration 12300: Loss = -11821.98476107891
7
Iteration 12400: Loss = -11821.982570505084
Iteration 12500: Loss = -11821.982981595906
1
Iteration 12600: Loss = -11822.054138573181
2
Iteration 12700: Loss = -11821.993195033274
3
Iteration 12800: Loss = -11821.982891692915
4
Iteration 12900: Loss = -11821.983086204911
5
Iteration 13000: Loss = -11822.002717476193
6
Iteration 13100: Loss = -11821.982563149975
Iteration 13200: Loss = -11821.982828396482
1
Iteration 13300: Loss = -11821.990125819368
2
Iteration 13400: Loss = -11821.984899973235
3
Iteration 13500: Loss = -11821.984126006522
4
Iteration 13600: Loss = -11821.983213616304
5
Iteration 13700: Loss = -11821.999690064853
6
Iteration 13800: Loss = -11821.998190568778
7
Iteration 13900: Loss = -11821.989402911904
8
Iteration 14000: Loss = -11821.987660605813
9
Iteration 14100: Loss = -11821.988795756628
10
Iteration 14200: Loss = -11821.982599090126
Iteration 14300: Loss = -11821.983915201485
1
Iteration 14400: Loss = -11821.986820020187
2
Iteration 14500: Loss = -11822.020333864759
3
Iteration 14600: Loss = -11822.034304565266
4
Iteration 14700: Loss = -11821.990222550965
5
Iteration 14800: Loss = -11821.982651046193
Iteration 14900: Loss = -11821.994204751369
1
Iteration 15000: Loss = -11822.007936763986
2
Iteration 15100: Loss = -11821.988224956627
3
Iteration 15200: Loss = -11821.993252345443
4
Iteration 15300: Loss = -11822.020112557286
5
Iteration 15400: Loss = -11821.986830413523
6
Iteration 15500: Loss = -11821.98472787306
7
Iteration 15600: Loss = -11821.989321960584
8
Iteration 15700: Loss = -11821.983403036762
9
Iteration 15800: Loss = -11821.984217714551
10
Iteration 15900: Loss = -11821.98283755259
11
Iteration 16000: Loss = -11821.982845138433
12
Iteration 16100: Loss = -11821.985918366307
13
Iteration 16200: Loss = -11821.992337308693
14
Iteration 16300: Loss = -11821.984502894089
15
Stopping early at iteration 16300 due to no improvement.
pi: tensor([[0.5334, 0.4666],
        [0.3685, 0.6315]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0120, 0.9880], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4007, 0.2923],
         [0.6395, 0.2140]],

        [[0.6061, 0.0948],
         [0.6198, 0.5943]],

        [[0.5146, 0.0937],
         [0.5179, 0.7293]],

        [[0.6136, 0.0963],
         [0.6607, 0.6994]],

        [[0.6312, 0.0933],
         [0.6514, 0.7293]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 79
Adjusted Rand Index: 0.32884850162989443
Global Adjusted Rand Index: 0.2406799973708436
Average Adjusted Rand Index: 0.6657697003259788
11522.947912558191
[0.050080741377465564, 0.2406799973708436] [0.9000371911794594, 0.6657697003259788] [11766.844578729879, 11821.984502894089]
-------------------------------------
This iteration is 65
True Objective function: Loss = -11839.077106141836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20610.300222816106
Iteration 100: Loss = -11833.548904437823
Iteration 200: Loss = -11832.023481729619
Iteration 300: Loss = -11831.666080356137
Iteration 400: Loss = -11831.505630727032
Iteration 500: Loss = -11831.416844239951
Iteration 600: Loss = -11831.36201033161
Iteration 700: Loss = -11831.325484131194
Iteration 800: Loss = -11831.299829328816
Iteration 900: Loss = -11831.281016040884
Iteration 1000: Loss = -11831.2668073246
Iteration 1100: Loss = -11831.255798811471
Iteration 1200: Loss = -11831.246994187928
Iteration 1300: Loss = -11831.239940781536
Iteration 1400: Loss = -11831.23416979583
Iteration 1500: Loss = -11831.22934578252
Iteration 1600: Loss = -11831.225286829982
Iteration 1700: Loss = -11831.2218648898
Iteration 1800: Loss = -11831.218878765892
Iteration 1900: Loss = -11831.216340062632
Iteration 2000: Loss = -11831.214137128469
Iteration 2100: Loss = -11831.212212832625
Iteration 2200: Loss = -11831.210509405273
Iteration 2300: Loss = -11831.209019432286
Iteration 2400: Loss = -11831.207662131865
Iteration 2500: Loss = -11831.206467495454
Iteration 2600: Loss = -11831.205403621296
Iteration 2700: Loss = -11831.204410977214
Iteration 2800: Loss = -11831.203594087528
Iteration 2900: Loss = -11831.202806826659
Iteration 3000: Loss = -11831.202079317583
Iteration 3100: Loss = -11831.2014659816
Iteration 3200: Loss = -11831.20108553316
Iteration 3300: Loss = -11831.20028892127
Iteration 3400: Loss = -11831.208609736794
1
Iteration 3500: Loss = -11831.199366615416
Iteration 3600: Loss = -11831.198954931588
Iteration 3700: Loss = -11831.198611825657
Iteration 3800: Loss = -11831.198497694704
Iteration 3900: Loss = -11831.197867422088
Iteration 4000: Loss = -11831.207459300536
1
Iteration 4100: Loss = -11831.20005897307
2
Iteration 4200: Loss = -11831.197034868466
Iteration 4300: Loss = -11831.196820580735
Iteration 4400: Loss = -11831.197082712557
1
Iteration 4500: Loss = -11831.199680220281
2
Iteration 4600: Loss = -11831.196129180084
Iteration 4700: Loss = -11831.207664103835
1
Iteration 4800: Loss = -11831.199707443044
2
Iteration 4900: Loss = -11831.204249318018
3
Iteration 5000: Loss = -11831.195523224093
Iteration 5100: Loss = -11831.197508972899
1
Iteration 5200: Loss = -11831.195235174257
Iteration 5300: Loss = -11831.19557674497
1
Iteration 5400: Loss = -11831.19507316131
Iteration 5500: Loss = -11831.19486414716
Iteration 5600: Loss = -11831.194912776888
Iteration 5700: Loss = -11831.194728076733
Iteration 5800: Loss = -11831.194578860384
Iteration 5900: Loss = -11831.202389875683
1
Iteration 6000: Loss = -11831.194505563457
Iteration 6100: Loss = -11831.194357474842
Iteration 6200: Loss = -11831.196052912343
1
Iteration 6300: Loss = -11831.195148339291
2
Iteration 6400: Loss = -11831.194151645179
Iteration 6500: Loss = -11831.19809721514
1
Iteration 6600: Loss = -11831.194082119708
Iteration 6700: Loss = -11831.19401443447
Iteration 6800: Loss = -11831.194013551489
Iteration 6900: Loss = -11831.193883747457
Iteration 7000: Loss = -11831.19387746577
Iteration 7100: Loss = -11831.22282777269
1
Iteration 7200: Loss = -11831.250246040467
2
Iteration 7300: Loss = -11831.193784071509
Iteration 7400: Loss = -11831.198808241823
1
Iteration 7500: Loss = -11831.245715870658
2
Iteration 7600: Loss = -11831.2146595977
3
Iteration 7700: Loss = -11831.1937231185
Iteration 7800: Loss = -11831.194437276785
1
Iteration 7900: Loss = -11831.21213348474
2
Iteration 8000: Loss = -11831.194429272575
3
Iteration 8100: Loss = -11831.19641896565
4
Iteration 8200: Loss = -11831.194076621512
5
Iteration 8300: Loss = -11831.19455164483
6
Iteration 8400: Loss = -11831.207937557496
7
Iteration 8500: Loss = -11831.193557768667
Iteration 8600: Loss = -11831.193696463304
1
Iteration 8700: Loss = -11831.19396272834
2
Iteration 8800: Loss = -11831.19402475671
3
Iteration 8900: Loss = -11831.193355442005
Iteration 9000: Loss = -11831.193693862459
1
Iteration 9100: Loss = -11831.19340347175
Iteration 9200: Loss = -11831.209236553632
1
Iteration 9300: Loss = -11831.196947647606
2
Iteration 9400: Loss = -11831.243535590867
3
Iteration 9500: Loss = -11831.194656741489
4
Iteration 9600: Loss = -11831.200138269094
5
Iteration 9700: Loss = -11831.20844342189
6
Iteration 9800: Loss = -11831.193645556688
7
Iteration 9900: Loss = -11831.199468694524
8
Iteration 10000: Loss = -11831.206722435547
9
Iteration 10100: Loss = -11831.194360789385
10
Iteration 10200: Loss = -11831.198529001906
11
Iteration 10300: Loss = -11831.19345882153
Iteration 10400: Loss = -11831.22289322913
1
Iteration 10500: Loss = -11831.288473392215
2
Iteration 10600: Loss = -11831.201627039516
3
Iteration 10700: Loss = -11831.195414163463
4
Iteration 10800: Loss = -11831.20073689356
5
Iteration 10900: Loss = -11831.19383856146
6
Iteration 11000: Loss = -11831.194812373165
7
Iteration 11100: Loss = -11831.202716212994
8
Iteration 11200: Loss = -11831.21094664465
9
Iteration 11300: Loss = -11831.200896120741
10
Iteration 11400: Loss = -11831.220190176491
11
Iteration 11500: Loss = -11831.197539923949
12
Iteration 11600: Loss = -11831.196178847122
13
Iteration 11700: Loss = -11831.202555116432
14
Iteration 11800: Loss = -11831.195093015067
15
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[0.7484, 0.2516],
        [0.1984, 0.8016]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4197, 0.5803], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1915, 0.1023],
         [0.5721, 0.3988]],

        [[0.5023, 0.0962],
         [0.5623, 0.5265]],

        [[0.5313, 0.1004],
         [0.6264, 0.7000]],

        [[0.5744, 0.1050],
         [0.6840, 0.5288]],

        [[0.6116, 0.0911],
         [0.6386, 0.5264]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.991997003537941
Average Adjusted Rand Index: 0.992
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22632.26895197012
Iteration 100: Loss = -12414.1426624537
Iteration 200: Loss = -12354.27989384689
Iteration 300: Loss = -12352.130925178692
Iteration 400: Loss = -12351.78150614947
Iteration 500: Loss = -12351.604326309145
Iteration 600: Loss = -12351.494906114769
Iteration 700: Loss = -12351.42100762506
Iteration 800: Loss = -12351.36852413307
Iteration 900: Loss = -12351.330541672329
Iteration 1000: Loss = -12351.30276743116
Iteration 1100: Loss = -12351.281727580936
Iteration 1200: Loss = -12351.265471014864
Iteration 1300: Loss = -12351.252504918628
Iteration 1400: Loss = -12351.242209222175
Iteration 1500: Loss = -12351.233755131392
Iteration 1600: Loss = -12351.226874515207
Iteration 1700: Loss = -12351.221073242548
Iteration 1800: Loss = -12351.21625113353
Iteration 1900: Loss = -12351.212154211407
Iteration 2000: Loss = -12351.20861385175
Iteration 2100: Loss = -12351.205630785362
Iteration 2200: Loss = -12351.202997346069
Iteration 2300: Loss = -12351.200781850113
Iteration 2400: Loss = -12351.19877288857
Iteration 2500: Loss = -12351.197007223735
Iteration 2600: Loss = -12351.195464470724
Iteration 2700: Loss = -12351.194128708776
Iteration 2800: Loss = -12351.19286018091
Iteration 2900: Loss = -12351.191758917092
Iteration 3000: Loss = -12351.190749001624
Iteration 3100: Loss = -12351.190834645973
Iteration 3200: Loss = -12351.189038261193
Iteration 3300: Loss = -12351.188289709458
Iteration 3400: Loss = -12351.192866777805
1
Iteration 3500: Loss = -12351.187013956678
Iteration 3600: Loss = -12351.18647386018
Iteration 3700: Loss = -12351.202498311772
1
Iteration 3800: Loss = -12351.185483614592
Iteration 3900: Loss = -12351.185025022764
Iteration 4000: Loss = -12351.185038635647
Iteration 4100: Loss = -12351.184252228017
Iteration 4200: Loss = -12351.183924232662
Iteration 4300: Loss = -12351.183729319539
Iteration 4400: Loss = -12351.183591825938
Iteration 4500: Loss = -12351.186293598075
1
Iteration 4600: Loss = -12351.183465672912
Iteration 4700: Loss = -12351.187254256576
1
Iteration 4800: Loss = -12351.182513431493
Iteration 4900: Loss = -12351.183407943954
1
Iteration 5000: Loss = -12351.18763731621
2
Iteration 5100: Loss = -12351.181716533318
Iteration 5200: Loss = -12351.181560556352
Iteration 5300: Loss = -12351.188268154445
1
Iteration 5400: Loss = -12351.181412722199
Iteration 5500: Loss = -12351.181590226764
1
Iteration 5600: Loss = -12351.181327043363
Iteration 5700: Loss = -12351.18099663239
Iteration 5800: Loss = -12351.181594511125
1
Iteration 5900: Loss = -12351.180660320746
Iteration 6000: Loss = -12351.18318689932
1
Iteration 6100: Loss = -12351.180517165181
Iteration 6200: Loss = -12351.180565801627
Iteration 6300: Loss = -12351.180280184892
Iteration 6400: Loss = -12351.180235023146
Iteration 6500: Loss = -12351.180483548158
1
Iteration 6600: Loss = -12351.192541013788
2
Iteration 6700: Loss = -12351.180038595652
Iteration 6800: Loss = -12351.184160648943
1
Iteration 6900: Loss = -12351.179902063226
Iteration 7000: Loss = -12351.17995832268
Iteration 7100: Loss = -12351.179828165255
Iteration 7200: Loss = -12351.181914806173
1
Iteration 7300: Loss = -12351.179723925843
Iteration 7400: Loss = -12351.180207783425
1
Iteration 7500: Loss = -12351.179900697982
2
Iteration 7600: Loss = -12351.179665719805
Iteration 7700: Loss = -12351.179541164605
Iteration 7800: Loss = -12351.17967593087
1
Iteration 7900: Loss = -12351.204092396418
2
Iteration 8000: Loss = -12351.179464047153
Iteration 8100: Loss = -12351.179708255819
1
Iteration 8200: Loss = -12351.188549765446
2
Iteration 8300: Loss = -12351.179374176372
Iteration 8400: Loss = -12351.179567576926
1
Iteration 8500: Loss = -12351.179339128237
Iteration 8600: Loss = -12351.179788268179
1
Iteration 8700: Loss = -12351.179294440855
Iteration 8800: Loss = -12351.179469306888
1
Iteration 8900: Loss = -12351.179287178591
Iteration 9000: Loss = -12351.179461910584
1
Iteration 9100: Loss = -12351.179221369717
Iteration 9200: Loss = -12351.179205801925
Iteration 9300: Loss = -12351.179686326335
1
Iteration 9400: Loss = -12351.179146114082
Iteration 9500: Loss = -12351.179215312937
Iteration 9600: Loss = -12351.179397671227
1
Iteration 9700: Loss = -12351.19345693542
2
Iteration 9800: Loss = -12351.179335311559
3
Iteration 9900: Loss = -12351.179164184161
Iteration 10000: Loss = -12351.180493659504
1
Iteration 10100: Loss = -12351.2027137961
2
Iteration 10200: Loss = -12351.180935228815
3
Iteration 10300: Loss = -12351.179102016682
Iteration 10400: Loss = -12351.191371480003
1
Iteration 10500: Loss = -12351.179086299811
Iteration 10600: Loss = -12351.18847482881
1
Iteration 10700: Loss = -12351.179070817898
Iteration 10800: Loss = -12351.348720541866
1
Iteration 10900: Loss = -12351.179036813188
Iteration 11000: Loss = -12351.179059538506
Iteration 11100: Loss = -12351.179230374302
1
Iteration 11200: Loss = -12351.178995232443
Iteration 11300: Loss = -12351.205878989094
1
Iteration 11400: Loss = -12351.1790453814
Iteration 11500: Loss = -12351.178997057666
Iteration 11600: Loss = -12351.269594315067
1
Iteration 11700: Loss = -12351.178990697923
Iteration 11800: Loss = -12351.179002361005
Iteration 11900: Loss = -12351.179422355563
1
Iteration 12000: Loss = -12351.178992784238
Iteration 12100: Loss = -12351.180147312523
1
Iteration 12200: Loss = -12351.178997702707
Iteration 12300: Loss = -12351.180436845905
1
Iteration 12400: Loss = -12351.187987790265
2
Iteration 12500: Loss = -12351.179086388362
Iteration 12600: Loss = -12351.195409938926
1
Iteration 12700: Loss = -12351.179031949241
Iteration 12800: Loss = -12351.179873445946
1
Iteration 12900: Loss = -12351.183040205275
2
Iteration 13000: Loss = -12351.25585900412
3
Iteration 13100: Loss = -12351.17920433337
4
Iteration 13200: Loss = -12351.181188115217
5
Iteration 13300: Loss = -12351.17920143801
6
Iteration 13400: Loss = -12351.18095812127
7
Iteration 13500: Loss = -12351.181402122082
8
Iteration 13600: Loss = -12351.207125239142
9
Iteration 13700: Loss = -12351.179058880154
Iteration 13800: Loss = -12351.189678266492
1
Iteration 13900: Loss = -12351.179057171645
Iteration 14000: Loss = -12351.179534030838
1
Iteration 14100: Loss = -12351.179045571007
Iteration 14200: Loss = -12351.18245770342
1
Iteration 14300: Loss = -12351.17904248667
Iteration 14400: Loss = -12351.17970049369
1
Iteration 14500: Loss = -12351.179041474737
Iteration 14600: Loss = -12351.179454326471
1
Iteration 14700: Loss = -12351.179823536195
2
Iteration 14800: Loss = -12351.179519280708
3
Iteration 14900: Loss = -12351.18855552964
4
Iteration 15000: Loss = -12351.179673620698
5
Iteration 15100: Loss = -12351.18036239884
6
Iteration 15200: Loss = -12351.312951438527
7
Iteration 15300: Loss = -12351.21064643702
8
Iteration 15400: Loss = -12351.17911303782
Iteration 15500: Loss = -12351.17919516132
Iteration 15600: Loss = -12351.180767439504
1
Iteration 15700: Loss = -12351.179074939166
Iteration 15800: Loss = -12351.199396206172
1
Iteration 15900: Loss = -12351.179810134594
2
Iteration 16000: Loss = -12351.182911675363
3
Iteration 16100: Loss = -12351.179098443383
Iteration 16200: Loss = -12351.179030398545
Iteration 16300: Loss = -12351.179098076493
Iteration 16400: Loss = -12351.1852452291
1
Iteration 16500: Loss = -12351.179347983569
2
Iteration 16600: Loss = -12351.263300071541
3
Iteration 16700: Loss = -12351.179308860457
4
Iteration 16800: Loss = -12351.189333356797
5
Iteration 16900: Loss = -12351.179029805797
Iteration 17000: Loss = -12351.187143334726
1
Iteration 17100: Loss = -12351.316222603637
2
Iteration 17200: Loss = -12351.179245179568
3
Iteration 17300: Loss = -12351.179236136108
4
Iteration 17400: Loss = -12351.275832184816
5
Iteration 17500: Loss = -12351.179022441302
Iteration 17600: Loss = -12351.180305165497
1
Iteration 17700: Loss = -12351.179060858638
Iteration 17800: Loss = -12351.18009970852
1
Iteration 17900: Loss = -12351.179140061495
Iteration 18000: Loss = -12351.179137206635
Iteration 18100: Loss = -12351.223943679963
1
Iteration 18200: Loss = -12351.179052133859
Iteration 18300: Loss = -12351.19092883786
1
Iteration 18400: Loss = -12351.178977003461
Iteration 18500: Loss = -12351.186132254166
1
Iteration 18600: Loss = -12351.178985400871
Iteration 18700: Loss = -12351.194462010139
1
Iteration 18800: Loss = -12351.179015250384
Iteration 18900: Loss = -12351.571374294439
1
Iteration 19000: Loss = -12351.179461914458
2
Iteration 19100: Loss = -12351.17944781531
3
Iteration 19200: Loss = -12351.179139582031
4
Iteration 19300: Loss = -12351.178988226446
Iteration 19400: Loss = -12351.181088151128
1
Iteration 19500: Loss = -12351.179001704502
Iteration 19600: Loss = -12351.323057082272
1
Iteration 19700: Loss = -12351.179027714299
Iteration 19800: Loss = -12351.179839774908
1
Iteration 19900: Loss = -12351.212887648595
2
pi: tensor([[0.5845, 0.4155],
        [0.5189, 0.4811]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9897, 0.0103], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2573, 0.3405],
         [0.5168, 0.3535]],

        [[0.7130, 0.0956],
         [0.6271, 0.5151]],

        [[0.7082, 0.0999],
         [0.5281, 0.6445]],

        [[0.6065, 0.0940],
         [0.6199, 0.6313]],

        [[0.7199, 0.0847],
         [0.6585, 0.6899]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.6015784434292446
time is 4
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 12
Adjusted Rand Index: 0.573478092283831
Global Adjusted Rand Index: -0.0004730223491081191
Average Adjusted Rand Index: 0.6270113071426151
11839.077106141836
[0.991997003537941, -0.0004730223491081191] [0.992, 0.6270113071426151] [11831.195093015067, 12351.186358310773]
-------------------------------------
This iteration is 66
True Objective function: Loss = -11445.688372695342
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23881.060809884886
Iteration 100: Loss = -12180.06860114315
Iteration 200: Loss = -12170.640937275874
Iteration 300: Loss = -12016.740247956915
Iteration 400: Loss = -11470.835662419233
Iteration 500: Loss = -11453.336627108993
Iteration 600: Loss = -11444.997921131979
Iteration 700: Loss = -11443.697598398543
Iteration 800: Loss = -11443.472442242375
Iteration 900: Loss = -11443.342515916247
Iteration 1000: Loss = -11443.251776122897
Iteration 1100: Loss = -11443.180425823637
Iteration 1200: Loss = -11438.579132954212
Iteration 1300: Loss = -11438.517367197124
Iteration 1400: Loss = -11438.484831828937
Iteration 1500: Loss = -11438.458650423687
Iteration 1600: Loss = -11438.436890662686
Iteration 1700: Loss = -11438.418384331551
Iteration 1800: Loss = -11438.402126926792
Iteration 1900: Loss = -11438.38701465926
Iteration 2000: Loss = -11438.372180453618
Iteration 2100: Loss = -11438.361517467161
Iteration 2200: Loss = -11438.35289988976
Iteration 2300: Loss = -11438.345125768283
Iteration 2400: Loss = -11438.337575407942
Iteration 2500: Loss = -11438.329769321354
Iteration 2600: Loss = -11438.322965454496
Iteration 2700: Loss = -11438.318521731622
Iteration 2800: Loss = -11438.314394803809
Iteration 2900: Loss = -11438.310747845855
Iteration 3000: Loss = -11438.307650871257
Iteration 3100: Loss = -11438.304413798325
Iteration 3200: Loss = -11438.301861335007
Iteration 3300: Loss = -11438.304149731042
1
Iteration 3400: Loss = -11438.296868500856
Iteration 3500: Loss = -11438.295986804646
Iteration 3600: Loss = -11438.308870069646
1
Iteration 3700: Loss = -11438.291129751484
Iteration 3800: Loss = -11438.289595634815
Iteration 3900: Loss = -11438.294475415452
1
Iteration 4000: Loss = -11438.286644807204
Iteration 4100: Loss = -11438.285549679087
Iteration 4200: Loss = -11438.284229081819
Iteration 4300: Loss = -11438.283111133656
Iteration 4400: Loss = -11438.284802112166
1
Iteration 4500: Loss = -11438.281159726093
Iteration 4600: Loss = -11438.29210324467
1
Iteration 4700: Loss = -11438.279463358693
Iteration 4800: Loss = -11438.27942590875
Iteration 4900: Loss = -11438.27797317052
Iteration 5000: Loss = -11438.27788481695
Iteration 5100: Loss = -11438.27669112138
Iteration 5200: Loss = -11438.276067750112
Iteration 5300: Loss = -11438.27847187593
1
Iteration 5400: Loss = -11438.27498092144
Iteration 5500: Loss = -11438.274529416612
Iteration 5600: Loss = -11438.274011202862
Iteration 5700: Loss = -11438.273596996594
Iteration 5800: Loss = -11438.273356974314
Iteration 5900: Loss = -11438.277383050536
1
Iteration 6000: Loss = -11438.273572000642
2
Iteration 6100: Loss = -11438.271777060236
Iteration 6200: Loss = -11438.279913982926
1
Iteration 6300: Loss = -11438.271090664697
Iteration 6400: Loss = -11438.271114165012
Iteration 6500: Loss = -11438.270911448713
Iteration 6600: Loss = -11438.270961248141
Iteration 6700: Loss = -11438.269843177624
Iteration 6800: Loss = -11438.270470798809
1
Iteration 6900: Loss = -11438.269764150444
Iteration 7000: Loss = -11438.269378579724
Iteration 7100: Loss = -11438.26879413688
Iteration 7200: Loss = -11438.268676841346
Iteration 7300: Loss = -11438.268395649427
Iteration 7400: Loss = -11438.268926495406
1
Iteration 7500: Loss = -11438.269912243402
2
Iteration 7600: Loss = -11438.26801126941
Iteration 7700: Loss = -11438.268271036353
1
Iteration 7800: Loss = -11438.283705874634
2
Iteration 7900: Loss = -11438.267274942678
Iteration 8000: Loss = -11438.275300931946
1
Iteration 8100: Loss = -11438.2652121898
Iteration 8200: Loss = -11438.266043092694
1
Iteration 8300: Loss = -11438.265032069661
Iteration 8400: Loss = -11438.265935407113
1
Iteration 8500: Loss = -11438.264813185093
Iteration 8600: Loss = -11438.269803058152
1
Iteration 8700: Loss = -11438.264684486148
Iteration 8800: Loss = -11438.26647386127
1
Iteration 8900: Loss = -11438.26516847858
2
Iteration 9000: Loss = -11438.269570254923
3
Iteration 9100: Loss = -11438.269379258454
4
Iteration 9200: Loss = -11438.26486202339
5
Iteration 9300: Loss = -11438.270033600264
6
Iteration 9400: Loss = -11438.264297810922
Iteration 9500: Loss = -11438.265579916591
1
Iteration 9600: Loss = -11438.283828416683
2
Iteration 9700: Loss = -11438.264131617043
Iteration 9800: Loss = -11438.271285237748
1
Iteration 9900: Loss = -11438.264044569049
Iteration 10000: Loss = -11438.264485276328
1
Iteration 10100: Loss = -11438.264376169596
2
Iteration 10200: Loss = -11438.273356034491
3
Iteration 10300: Loss = -11438.265007206151
4
Iteration 10400: Loss = -11438.271714920043
5
Iteration 10500: Loss = -11438.264590128654
6
Iteration 10600: Loss = -11438.267162536733
7
Iteration 10700: Loss = -11438.284749544118
8
Iteration 10800: Loss = -11438.277390943907
9
Iteration 10900: Loss = -11438.314801334225
10
Iteration 11000: Loss = -11438.267915977405
11
Iteration 11100: Loss = -11438.263841783224
Iteration 11200: Loss = -11438.268944181726
1
Iteration 11300: Loss = -11438.26972522352
2
Iteration 11400: Loss = -11438.26656982256
3
Iteration 11500: Loss = -11438.287344913488
4
Iteration 11600: Loss = -11438.306198463672
5
Iteration 11700: Loss = -11438.264518771251
6
Iteration 11800: Loss = -11438.271180097787
7
Iteration 11900: Loss = -11438.272644486504
8
Iteration 12000: Loss = -11438.270909937448
9
Iteration 12100: Loss = -11438.301565804186
10
Iteration 12200: Loss = -11438.264693645171
11
Iteration 12300: Loss = -11438.270056229865
12
Iteration 12400: Loss = -11438.265445289228
13
Iteration 12500: Loss = -11438.27137039886
14
Iteration 12600: Loss = -11438.27253009621
15
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.7802, 0.2198],
        [0.2356, 0.7644]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5186, 0.4814], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.1054],
         [0.7084, 0.3949]],

        [[0.6721, 0.1007],
         [0.6298, 0.6366]],

        [[0.5416, 0.0909],
         [0.6780, 0.7188]],

        [[0.6780, 0.0942],
         [0.6335, 0.7077]],

        [[0.6039, 0.1068],
         [0.6672, 0.6271]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9840320660777474
Average Adjusted Rand Index: 0.9841606232309881
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21851.93511508284
Iteration 100: Loss = -12177.549410738748
Iteration 200: Loss = -12136.206019663361
Iteration 300: Loss = -11456.353581280702
Iteration 400: Loss = -11450.542013534152
Iteration 500: Loss = -11444.033167246384
Iteration 600: Loss = -11443.684164384247
Iteration 700: Loss = -11443.490443404706
Iteration 800: Loss = -11443.36702706337
Iteration 900: Loss = -11443.279902824059
Iteration 1000: Loss = -11438.79828136272
Iteration 1100: Loss = -11438.593595284747
Iteration 1200: Loss = -11438.5573582673
Iteration 1300: Loss = -11438.511603598003
Iteration 1400: Loss = -11438.4396521956
Iteration 1500: Loss = -11438.414753098697
Iteration 1600: Loss = -11438.39788816665
Iteration 1700: Loss = -11438.384308973931
Iteration 1800: Loss = -11438.37205261917
Iteration 1900: Loss = -11438.36167455134
Iteration 2000: Loss = -11438.352427119848
Iteration 2100: Loss = -11438.344634200195
Iteration 2200: Loss = -11438.337550673727
Iteration 2300: Loss = -11438.33163072721
Iteration 2400: Loss = -11438.32647979995
Iteration 2500: Loss = -11438.321995662689
Iteration 2600: Loss = -11438.317722985808
Iteration 2700: Loss = -11438.31391837132
Iteration 2800: Loss = -11438.316377291925
1
Iteration 2900: Loss = -11438.305483039025
Iteration 3000: Loss = -11438.298415008678
Iteration 3100: Loss = -11438.302245234387
1
Iteration 3200: Loss = -11438.295908745591
Iteration 3300: Loss = -11438.291746003018
Iteration 3400: Loss = -11438.289741342154
Iteration 3500: Loss = -11438.287803198305
Iteration 3600: Loss = -11438.285622550977
Iteration 3700: Loss = -11438.283480699944
Iteration 3800: Loss = -11438.289033402929
1
Iteration 3900: Loss = -11438.280480586656
Iteration 4000: Loss = -11438.279359712287
Iteration 4100: Loss = -11438.288212827127
1
Iteration 4200: Loss = -11438.277372147313
Iteration 4300: Loss = -11438.276444671541
Iteration 4400: Loss = -11438.276991979608
1
Iteration 4500: Loss = -11438.274715298046
Iteration 4600: Loss = -11438.273907437426
Iteration 4700: Loss = -11438.276150134994
1
Iteration 4800: Loss = -11438.2723539293
Iteration 4900: Loss = -11438.271650816823
Iteration 5000: Loss = -11438.271043640345
Iteration 5100: Loss = -11438.271316022723
1
Iteration 5200: Loss = -11438.270024604279
Iteration 5300: Loss = -11438.269561823261
Iteration 5400: Loss = -11438.272327669141
1
Iteration 5500: Loss = -11438.268726881652
Iteration 5600: Loss = -11438.268410876046
Iteration 5700: Loss = -11438.268074118856
Iteration 5800: Loss = -11438.267769049713
Iteration 5900: Loss = -11438.282384843058
1
Iteration 6000: Loss = -11438.284811164753
2
Iteration 6100: Loss = -11438.266911789864
Iteration 6200: Loss = -11438.281438331132
1
Iteration 6300: Loss = -11438.26647666666
Iteration 6400: Loss = -11438.268333171396
1
Iteration 6500: Loss = -11438.26597725789
Iteration 6600: Loss = -11438.2661628689
1
Iteration 6700: Loss = -11438.265592334554
Iteration 6800: Loss = -11438.27977711262
1
Iteration 6900: Loss = -11438.265227269198
Iteration 7000: Loss = -11438.26500933787
Iteration 7100: Loss = -11438.264827552017
Iteration 7200: Loss = -11438.264407580593
Iteration 7300: Loss = -11438.26475530529
1
Iteration 7400: Loss = -11438.266186225139
2
Iteration 7500: Loss = -11438.264065317513
Iteration 7600: Loss = -11438.263908529087
Iteration 7700: Loss = -11438.2639640628
Iteration 7800: Loss = -11438.267740499947
1
Iteration 7900: Loss = -11438.266846795774
2
Iteration 8000: Loss = -11438.291668244066
3
Iteration 8100: Loss = -11438.263409783844
Iteration 8200: Loss = -11438.265699466096
1
Iteration 8300: Loss = -11438.313166992102
2
Iteration 8400: Loss = -11438.263147703852
Iteration 8500: Loss = -11438.264706171412
1
Iteration 8600: Loss = -11438.263021420422
Iteration 8700: Loss = -11438.263217335736
1
Iteration 8800: Loss = -11438.26323474452
2
Iteration 8900: Loss = -11438.264236864974
3
Iteration 9000: Loss = -11438.26593883686
4
Iteration 9100: Loss = -11438.262823268107
Iteration 9200: Loss = -11438.264893548012
1
Iteration 9300: Loss = -11438.26819654916
2
Iteration 9400: Loss = -11438.264011311334
3
Iteration 9500: Loss = -11438.263117350054
4
Iteration 9600: Loss = -11438.264895378969
5
Iteration 9700: Loss = -11438.392838491312
6
Iteration 9800: Loss = -11438.26630450062
7
Iteration 9900: Loss = -11438.27440583496
8
Iteration 10000: Loss = -11438.262117504397
Iteration 10100: Loss = -11438.262935564808
1
Iteration 10200: Loss = -11438.299280998182
2
Iteration 10300: Loss = -11438.270280129094
3
Iteration 10400: Loss = -11438.274683075706
4
Iteration 10500: Loss = -11438.301311964582
5
Iteration 10600: Loss = -11438.262710979032
6
Iteration 10700: Loss = -11438.274312627971
7
Iteration 10800: Loss = -11438.283570593543
8
Iteration 10900: Loss = -11438.265259213962
9
Iteration 11000: Loss = -11438.2657470473
10
Iteration 11100: Loss = -11438.3341795026
11
Iteration 11200: Loss = -11438.275421836093
12
Iteration 11300: Loss = -11438.262063475893
Iteration 11400: Loss = -11438.272137816764
1
Iteration 11500: Loss = -11438.26199877167
Iteration 11600: Loss = -11438.311286945225
1
Iteration 11700: Loss = -11438.2630979046
2
Iteration 11800: Loss = -11438.270089125179
3
Iteration 11900: Loss = -11438.264706670698
4
Iteration 12000: Loss = -11438.266182607198
5
Iteration 12100: Loss = -11438.270106129621
6
Iteration 12200: Loss = -11438.271459582056
7
Iteration 12300: Loss = -11438.300493845121
8
Iteration 12400: Loss = -11438.417086039048
9
Iteration 12500: Loss = -11438.262087552655
Iteration 12600: Loss = -11438.269030049654
1
Iteration 12700: Loss = -11438.26359290877
2
Iteration 12800: Loss = -11438.26654624459
3
Iteration 12900: Loss = -11438.266022087731
4
Iteration 13000: Loss = -11438.273930305548
5
Iteration 13100: Loss = -11438.264208271716
6
Iteration 13200: Loss = -11438.265066275471
7
Iteration 13300: Loss = -11438.267593593348
8
Iteration 13400: Loss = -11438.26398199552
9
Iteration 13500: Loss = -11438.262851450434
10
Iteration 13600: Loss = -11438.263258829345
11
Iteration 13700: Loss = -11438.288634223158
12
Iteration 13800: Loss = -11438.263609062273
13
Iteration 13900: Loss = -11438.263876250783
14
Iteration 14000: Loss = -11438.26314403247
15
Stopping early at iteration 14000 due to no improvement.
pi: tensor([[0.7646, 0.2354],
        [0.2199, 0.7801]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4806, 0.5194], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3950, 0.1054],
         [0.6027, 0.1996]],

        [[0.6556, 0.1014],
         [0.5769, 0.7260]],

        [[0.6047, 0.0911],
         [0.6347, 0.5827]],

        [[0.6557, 0.0943],
         [0.6626, 0.6776]],

        [[0.7089, 0.1067],
         [0.6345, 0.7306]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9840320660777474
Average Adjusted Rand Index: 0.9841606232309881
11445.688372695342
[0.9840320660777474, 0.9840320660777474] [0.9841606232309881, 0.9841606232309881] [11438.27253009621, 11438.26314403247]
-------------------------------------
This iteration is 67
True Objective function: Loss = -11533.297173028048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23923.417951828804
Iteration 100: Loss = -11609.252779135963
Iteration 200: Loss = -11526.268768682996
Iteration 300: Loss = -11525.675957851954
Iteration 400: Loss = -11525.433127710883
Iteration 500: Loss = -11525.30742577776
Iteration 600: Loss = -11525.232525618776
Iteration 700: Loss = -11525.183473595307
Iteration 800: Loss = -11525.149278828936
Iteration 900: Loss = -11525.124382453518
Iteration 1000: Loss = -11525.105632441086
Iteration 1100: Loss = -11525.091140459388
Iteration 1200: Loss = -11525.079661315012
Iteration 1300: Loss = -11525.070434638998
Iteration 1400: Loss = -11525.062882968345
Iteration 1500: Loss = -11525.056626897886
Iteration 1600: Loss = -11525.051313228149
Iteration 1700: Loss = -11525.04683600676
Iteration 1800: Loss = -11525.043019756755
Iteration 1900: Loss = -11525.039755220578
Iteration 2000: Loss = -11525.036864791078
Iteration 2100: Loss = -11525.034400244493
Iteration 2200: Loss = -11525.032235355975
Iteration 2300: Loss = -11525.03023928881
Iteration 2400: Loss = -11525.02853352531
Iteration 2500: Loss = -11525.027018939247
Iteration 2600: Loss = -11525.032137420108
1
Iteration 2700: Loss = -11525.02439530219
Iteration 2800: Loss = -11525.0233157427
Iteration 2900: Loss = -11525.022293281567
Iteration 3000: Loss = -11525.021425404539
Iteration 3100: Loss = -11525.020532185858
Iteration 3200: Loss = -11525.040537825316
1
Iteration 3300: Loss = -11525.019401759644
Iteration 3400: Loss = -11525.018449698871
Iteration 3500: Loss = -11525.017875399997
Iteration 3600: Loss = -11525.020174965257
1
Iteration 3700: Loss = -11525.01688925362
Iteration 3800: Loss = -11525.017114450306
1
Iteration 3900: Loss = -11525.017966587791
2
Iteration 4000: Loss = -11525.015638752266
Iteration 4100: Loss = -11525.01527061765
Iteration 4200: Loss = -11525.01498113857
Iteration 4300: Loss = -11525.014654337889
Iteration 4400: Loss = -11525.03194362354
1
Iteration 4500: Loss = -11525.014052014836
Iteration 4600: Loss = -11525.014074724973
Iteration 4700: Loss = -11525.014828069096
1
Iteration 4800: Loss = -11525.013680343718
Iteration 4900: Loss = -11525.013218887552
Iteration 5000: Loss = -11525.013172341896
Iteration 5100: Loss = -11525.012821522962
Iteration 5200: Loss = -11525.014012677326
1
Iteration 5300: Loss = -11525.012518804684
Iteration 5400: Loss = -11525.012353067568
Iteration 5500: Loss = -11525.017621282652
1
Iteration 5600: Loss = -11525.012087446079
Iteration 5700: Loss = -11525.032472858527
1
Iteration 5800: Loss = -11525.011830303682
Iteration 5900: Loss = -11525.01209207589
1
Iteration 6000: Loss = -11525.011701487521
Iteration 6100: Loss = -11525.02260912416
1
Iteration 6200: Loss = -11525.011497124753
Iteration 6300: Loss = -11525.012688482735
1
Iteration 6400: Loss = -11525.011330139056
Iteration 6500: Loss = -11525.012356849824
1
Iteration 6600: Loss = -11525.011910592051
2
Iteration 6700: Loss = -11525.016185123595
3
Iteration 6800: Loss = -11525.011264346542
Iteration 6900: Loss = -11525.010995556217
Iteration 7000: Loss = -11525.011009430968
Iteration 7100: Loss = -11525.010906051393
Iteration 7200: Loss = -11525.011074235857
1
Iteration 7300: Loss = -11525.01080395049
Iteration 7400: Loss = -11525.011015500053
1
Iteration 7500: Loss = -11525.010727966292
Iteration 7600: Loss = -11525.020679436708
1
Iteration 7700: Loss = -11525.01064921994
Iteration 7800: Loss = -11525.010683252705
Iteration 7900: Loss = -11525.01057878455
Iteration 8000: Loss = -11525.013877900996
1
Iteration 8100: Loss = -11525.010658393516
Iteration 8200: Loss = -11525.01051314939
Iteration 8300: Loss = -11525.010481760202
Iteration 8400: Loss = -11525.011437338335
1
Iteration 8500: Loss = -11525.010766033796
2
Iteration 8600: Loss = -11525.014960816048
3
Iteration 8700: Loss = -11525.010704832148
4
Iteration 8800: Loss = -11525.011040363099
5
Iteration 8900: Loss = -11525.015467495332
6
Iteration 9000: Loss = -11525.010825507592
7
Iteration 9100: Loss = -11525.010656178556
8
Iteration 9200: Loss = -11525.01680343768
9
Iteration 9300: Loss = -11525.01273295114
10
Iteration 9400: Loss = -11525.012092298208
11
Iteration 9500: Loss = -11525.014010451203
12
Iteration 9600: Loss = -11525.010595090562
13
Iteration 9700: Loss = -11525.030501010671
14
Iteration 9800: Loss = -11525.011957080364
15
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[0.6897, 0.3103],
        [0.2853, 0.7147]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5408, 0.4592], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1990, 0.0985],
         [0.5084, 0.4032]],

        [[0.6510, 0.0983],
         [0.7288, 0.5902]],

        [[0.7171, 0.1021],
         [0.5509, 0.5054]],

        [[0.6664, 0.1022],
         [0.6728, 0.7175]],

        [[0.5958, 0.0888],
         [0.5317, 0.7176]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999681285151
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22454.300653340626
Iteration 100: Loss = -11969.948041429452
Iteration 200: Loss = -11527.31909119543
Iteration 300: Loss = -11526.133805513227
Iteration 400: Loss = -11525.770488866774
Iteration 500: Loss = -11525.59184910027
Iteration 600: Loss = -11525.487537098123
Iteration 700: Loss = -11525.42014254997
Iteration 800: Loss = -11525.37367582044
Iteration 900: Loss = -11525.340061309427
Iteration 1000: Loss = -11525.314844766153
Iteration 1100: Loss = -11525.295381687425
Iteration 1200: Loss = -11525.280019620044
Iteration 1300: Loss = -11525.267611482763
Iteration 1400: Loss = -11525.257486736398
Iteration 1500: Loss = -11525.249099651906
Iteration 1600: Loss = -11525.242045770163
Iteration 1700: Loss = -11525.236049030878
Iteration 1800: Loss = -11525.230971811749
Iteration 1900: Loss = -11525.226555205076
Iteration 2000: Loss = -11525.222740466466
Iteration 2100: Loss = -11525.21939532713
Iteration 2200: Loss = -11525.216449421352
Iteration 2300: Loss = -11525.213856104196
Iteration 2400: Loss = -11525.211563582703
Iteration 2500: Loss = -11525.20950141482
Iteration 2600: Loss = -11525.207644614524
Iteration 2700: Loss = -11525.206032930768
Iteration 2800: Loss = -11525.20455589915
Iteration 2900: Loss = -11525.203150451944
Iteration 3000: Loss = -11525.201987518682
Iteration 3100: Loss = -11525.20077841498
Iteration 3200: Loss = -11525.200498931708
Iteration 3300: Loss = -11525.198846545765
Iteration 3400: Loss = -11525.20095981456
1
Iteration 3500: Loss = -11525.197245522533
Iteration 3600: Loss = -11525.196530119729
Iteration 3700: Loss = -11525.196471886255
Iteration 3800: Loss = -11525.195259059079
Iteration 3900: Loss = -11525.195235639008
Iteration 4000: Loss = -11525.2006261171
1
Iteration 4100: Loss = -11525.19792059339
2
Iteration 4200: Loss = -11525.193247511846
Iteration 4300: Loss = -11525.193271161454
Iteration 4400: Loss = -11525.192449429167
Iteration 4500: Loss = -11525.193420113143
1
Iteration 4600: Loss = -11525.1923629808
Iteration 4700: Loss = -11525.193101460654
1
Iteration 4800: Loss = -11525.191129662031
Iteration 4900: Loss = -11525.192409789643
1
Iteration 5000: Loss = -11525.190584582202
Iteration 5100: Loss = -11525.194758709635
1
Iteration 5200: Loss = -11525.190149747496
Iteration 5300: Loss = -11525.190259655774
1
Iteration 5400: Loss = -11525.197705183906
2
Iteration 5500: Loss = -11525.18955318823
Iteration 5600: Loss = -11525.191974346893
1
Iteration 5700: Loss = -11525.190725233191
2
Iteration 5800: Loss = -11525.189236816148
Iteration 5900: Loss = -11525.189354281458
1
Iteration 6000: Loss = -11525.188772396483
Iteration 6100: Loss = -11525.18958997102
1
Iteration 6200: Loss = -11525.18853255075
Iteration 6300: Loss = -11525.18855006285
Iteration 6400: Loss = -11525.188367843322
Iteration 6500: Loss = -11525.188254539724
Iteration 6600: Loss = -11525.18823553072
Iteration 6700: Loss = -11525.188102170332
Iteration 6800: Loss = -11525.189257092497
1
Iteration 6900: Loss = -11525.188389899753
2
Iteration 7000: Loss = -11525.187807687416
Iteration 7100: Loss = -11525.188756814621
1
Iteration 7200: Loss = -11525.188947773062
2
Iteration 7300: Loss = -11525.187420573551
Iteration 7400: Loss = -11525.187405451305
Iteration 7500: Loss = -11525.190358739203
1
Iteration 7600: Loss = -11525.187801135453
2
Iteration 7700: Loss = -11525.190515385224
3
Iteration 7800: Loss = -11525.187206138577
Iteration 7900: Loss = -11525.1886439732
1
Iteration 8000: Loss = -11525.187108642844
Iteration 8100: Loss = -11525.187054588732
Iteration 8200: Loss = -11525.190083543048
1
Iteration 8300: Loss = -11525.187039651399
Iteration 8400: Loss = -11525.186928698045
Iteration 8500: Loss = -11525.186893418722
Iteration 8600: Loss = -11525.187537047826
1
Iteration 8700: Loss = -11525.189512300747
2
Iteration 8800: Loss = -11525.186937500852
Iteration 8900: Loss = -11525.188109289793
1
Iteration 9000: Loss = -11525.191125844962
2
Iteration 9100: Loss = -11525.197276919569
3
Iteration 9200: Loss = -11525.187866621618
4
Iteration 9300: Loss = -11525.187945484462
5
Iteration 9400: Loss = -11525.188470442004
6
Iteration 9500: Loss = -11525.188327091102
7
Iteration 9600: Loss = -11525.231848308387
8
Iteration 9700: Loss = -11525.203930191952
9
Iteration 9800: Loss = -11525.188290136704
10
Iteration 9900: Loss = -11525.203275544558
11
Iteration 10000: Loss = -11525.197779928489
12
Iteration 10100: Loss = -11525.18894128427
13
Iteration 10200: Loss = -11525.197216261637
14
Iteration 10300: Loss = -11525.194624836391
15
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[0.6897, 0.3103],
        [0.2854, 0.7146]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5406, 0.4594], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.0985],
         [0.5939, 0.4032]],

        [[0.6644, 0.0980],
         [0.5966, 0.5043]],

        [[0.6367, 0.1018],
         [0.5114, 0.6920]],

        [[0.7080, 0.1022],
         [0.7277, 0.6675]],

        [[0.6716, 0.0888],
         [0.5353, 0.7189]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999681285151
Average Adjusted Rand Index: 0.9919995611635631
11533.297173028048
[0.9919999681285151, 0.9919999681285151] [0.9919995611635631, 0.9919995611635631] [11525.011957080364, 11525.194624836391]
-------------------------------------
This iteration is 68
True Objective function: Loss = -11642.831115010997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25409.336660706595
Iteration 100: Loss = -12485.81100764557
Iteration 200: Loss = -11838.08175342724
Iteration 300: Loss = -11720.412872210734
Iteration 400: Loss = -11693.83575043074
Iteration 500: Loss = -11637.210803079464
Iteration 600: Loss = -11636.978148776901
Iteration 700: Loss = -11636.857924575994
Iteration 800: Loss = -11636.77970975893
Iteration 900: Loss = -11636.7063313436
Iteration 1000: Loss = -11636.660078442379
Iteration 1100: Loss = -11636.63154583024
Iteration 1200: Loss = -11636.60931938111
Iteration 1300: Loss = -11636.591612399783
Iteration 1400: Loss = -11636.576410193078
Iteration 1500: Loss = -11636.556855124898
Iteration 1600: Loss = -11633.479120230084
Iteration 1700: Loss = -11633.467990226842
Iteration 1800: Loss = -11633.460512580505
Iteration 1900: Loss = -11633.453971080908
Iteration 2000: Loss = -11633.448134283974
Iteration 2100: Loss = -11633.441166241875
Iteration 2200: Loss = -11633.434393349455
Iteration 2300: Loss = -11633.430459716534
Iteration 2400: Loss = -11633.42703666128
Iteration 2500: Loss = -11633.424035440505
Iteration 2600: Loss = -11633.421322159427
Iteration 2700: Loss = -11633.418860919082
Iteration 2800: Loss = -11633.416492613194
Iteration 2900: Loss = -11633.414059415723
Iteration 3000: Loss = -11633.410533848408
Iteration 3100: Loss = -11633.393876741968
Iteration 3200: Loss = -11633.372023380609
Iteration 3300: Loss = -11633.37039584978
Iteration 3400: Loss = -11633.366206021625
Iteration 3500: Loss = -11633.367964627947
1
Iteration 3600: Loss = -11633.353126991511
Iteration 3700: Loss = -11633.352147500884
Iteration 3800: Loss = -11633.35123001671
Iteration 3900: Loss = -11633.35039228447
Iteration 4000: Loss = -11633.353397435147
1
Iteration 4100: Loss = -11633.349894667508
Iteration 4200: Loss = -11633.34716581486
Iteration 4300: Loss = -11633.33486545447
Iteration 4400: Loss = -11633.320777288342
Iteration 4500: Loss = -11633.320270989436
Iteration 4600: Loss = -11633.3208134099
1
Iteration 4700: Loss = -11633.319311904977
Iteration 4800: Loss = -11633.320489621738
1
Iteration 4900: Loss = -11633.318428292832
Iteration 5000: Loss = -11633.31797896323
Iteration 5100: Loss = -11633.317647060881
Iteration 5200: Loss = -11633.31689747004
Iteration 5300: Loss = -11633.323543335056
1
Iteration 5400: Loss = -11633.315990074418
Iteration 5500: Loss = -11633.315760263407
Iteration 5600: Loss = -11633.315872968036
1
Iteration 5700: Loss = -11633.31523142879
Iteration 5800: Loss = -11633.315631184301
1
Iteration 5900: Loss = -11633.314791731435
Iteration 6000: Loss = -11633.314582516474
Iteration 6100: Loss = -11633.319700692207
1
Iteration 6200: Loss = -11633.314262616452
Iteration 6300: Loss = -11633.314102772254
Iteration 6400: Loss = -11633.31393960138
Iteration 6500: Loss = -11633.31420860604
1
Iteration 6600: Loss = -11633.313670565072
Iteration 6700: Loss = -11633.313539717768
Iteration 6800: Loss = -11633.3134418266
Iteration 6900: Loss = -11633.314703923488
1
Iteration 7000: Loss = -11633.32982462651
2
Iteration 7100: Loss = -11633.313588182249
3
Iteration 7200: Loss = -11633.317991861008
4
Iteration 7300: Loss = -11633.312900141325
Iteration 7400: Loss = -11633.3138085192
1
Iteration 7500: Loss = -11633.320362564316
2
Iteration 7600: Loss = -11633.345538689098
3
Iteration 7700: Loss = -11633.312589781286
Iteration 7800: Loss = -11633.316073112595
1
Iteration 7900: Loss = -11633.319793417044
2
Iteration 8000: Loss = -11633.312445139312
Iteration 8100: Loss = -11633.312459578581
Iteration 8200: Loss = -11633.392382223763
1
Iteration 8300: Loss = -11633.313386970363
2
Iteration 8400: Loss = -11633.335735334162
3
Iteration 8500: Loss = -11633.314235975067
4
Iteration 8600: Loss = -11633.312518092831
Iteration 8700: Loss = -11633.31718752475
1
Iteration 8800: Loss = -11633.312545878262
Iteration 8900: Loss = -11633.311950029372
Iteration 9000: Loss = -11633.312637731542
1
Iteration 9100: Loss = -11633.311989346012
Iteration 9200: Loss = -11633.311903532194
Iteration 9300: Loss = -11633.362394283822
1
Iteration 9400: Loss = -11633.312180422627
2
Iteration 9500: Loss = -11633.372417395996
3
Iteration 9600: Loss = -11633.311617129972
Iteration 9700: Loss = -11633.312444688272
1
Iteration 9800: Loss = -11633.318124583328
2
Iteration 9900: Loss = -11633.312486618033
3
Iteration 10000: Loss = -11633.328670419553
4
Iteration 10100: Loss = -11633.32431344647
5
Iteration 10200: Loss = -11633.31580815095
6
Iteration 10300: Loss = -11633.312803687015
7
Iteration 10400: Loss = -11633.311702658248
Iteration 10500: Loss = -11633.31338106524
1
Iteration 10600: Loss = -11633.318982734365
2
Iteration 10700: Loss = -11633.31206465454
3
Iteration 10800: Loss = -11633.372542185913
4
Iteration 10900: Loss = -11633.31184150372
5
Iteration 11000: Loss = -11633.311313217415
Iteration 11100: Loss = -11633.312321662512
1
Iteration 11200: Loss = -11633.317371272955
2
Iteration 11300: Loss = -11633.316292091053
3
Iteration 11400: Loss = -11633.3392543076
4
Iteration 11500: Loss = -11633.346479672708
5
Iteration 11600: Loss = -11633.378576269764
6
Iteration 11700: Loss = -11633.3215461737
7
Iteration 11800: Loss = -11633.31824340883
8
Iteration 11900: Loss = -11633.316070050267
9
Iteration 12000: Loss = -11633.31488057774
10
Iteration 12100: Loss = -11633.31837180367
11
Iteration 12200: Loss = -11633.320088184251
12
Iteration 12300: Loss = -11633.326541815286
13
Iteration 12400: Loss = -11633.314056040344
14
Iteration 12500: Loss = -11633.314017763101
15
Stopping early at iteration 12500 due to no improvement.
pi: tensor([[0.7861, 0.2139],
        [0.2094, 0.7906]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5399, 0.4601], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4120, 0.0879],
         [0.6761, 0.1984]],

        [[0.6778, 0.1130],
         [0.7047, 0.5905]],

        [[0.5242, 0.0966],
         [0.6834, 0.6716]],

        [[0.5672, 0.1068],
         [0.6752, 0.6890]],

        [[0.5661, 0.0957],
         [0.5916, 0.7156]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21554.380902468245
Iteration 100: Loss = -12566.98249764785
Iteration 200: Loss = -12494.968257775046
Iteration 300: Loss = -12127.478458915484
Iteration 400: Loss = -12062.365141173828
Iteration 500: Loss = -12033.997190303371
Iteration 600: Loss = -12031.61229756226
Iteration 700: Loss = -12023.313032038744
Iteration 800: Loss = -12021.197914978024
Iteration 900: Loss = -12021.11269247479
Iteration 1000: Loss = -12021.066964983787
Iteration 1100: Loss = -12021.031005686504
Iteration 1200: Loss = -12008.751845939261
Iteration 1300: Loss = -12000.744727834048
Iteration 1400: Loss = -11998.998157840297
Iteration 1500: Loss = -11997.387514647231
Iteration 1600: Loss = -11991.95471885929
Iteration 1700: Loss = -11990.820645333019
Iteration 1800: Loss = -11990.803384166831
Iteration 1900: Loss = -11990.790129264102
Iteration 2000: Loss = -11990.774002725855
Iteration 2100: Loss = -11990.756498654324
Iteration 2200: Loss = -11990.751754739471
Iteration 2300: Loss = -11990.747664708008
Iteration 2400: Loss = -11990.742822868611
Iteration 2500: Loss = -11981.075980497098
Iteration 2600: Loss = -11981.02698705508
Iteration 2700: Loss = -11981.015677014087
Iteration 2800: Loss = -11981.000160431777
Iteration 2900: Loss = -11972.196507010962
Iteration 3000: Loss = -11972.07481557575
Iteration 3100: Loss = -11972.075806415874
1
Iteration 3200: Loss = -11972.069547026536
Iteration 3300: Loss = -11972.05417420491
Iteration 3400: Loss = -11972.046985006915
Iteration 3500: Loss = -11972.041372398215
Iteration 3600: Loss = -11972.041431716372
Iteration 3700: Loss = -11972.037471279697
Iteration 3800: Loss = -11972.031403889563
Iteration 3900: Loss = -11970.644221998327
Iteration 4000: Loss = -11970.638217264237
Iteration 4100: Loss = -11970.654724741464
1
Iteration 4200: Loss = -11970.63384388541
Iteration 4300: Loss = -11970.632570138207
Iteration 4400: Loss = -11970.629397096021
Iteration 4500: Loss = -11970.62856778649
Iteration 4600: Loss = -11970.628919073346
1
Iteration 4700: Loss = -11970.62792183375
Iteration 4800: Loss = -11970.628113581617
1
Iteration 4900: Loss = -11970.633224156172
2
Iteration 5000: Loss = -11970.627265120538
Iteration 5100: Loss = -11970.626832934435
Iteration 5200: Loss = -11970.626953522626
1
Iteration 5300: Loss = -11970.628739978578
2
Iteration 5400: Loss = -11970.62743008484
3
Iteration 5500: Loss = -11970.627892201253
4
Iteration 5600: Loss = -11970.625832555022
Iteration 5700: Loss = -11970.626447365396
1
Iteration 5800: Loss = -11970.638848322295
2
Iteration 5900: Loss = -11970.62543057595
Iteration 6000: Loss = -11970.625516674432
Iteration 6100: Loss = -11970.633167630367
1
Iteration 6200: Loss = -11970.630204888219
2
Iteration 6300: Loss = -11970.625821332302
3
Iteration 6400: Loss = -11970.640012752248
4
Iteration 6500: Loss = -11970.62472737932
Iteration 6600: Loss = -11970.62450990471
Iteration 6700: Loss = -11970.62452306936
Iteration 6800: Loss = -11970.65428939482
1
Iteration 6900: Loss = -11970.67309453641
2
Iteration 7000: Loss = -11970.624691123608
3
Iteration 7100: Loss = -11970.623995330305
Iteration 7200: Loss = -11970.624297776909
1
Iteration 7300: Loss = -11970.623877290542
Iteration 7400: Loss = -11970.631494792635
1
Iteration 7500: Loss = -11970.623782087454
Iteration 7600: Loss = -11970.623649775536
Iteration 7700: Loss = -11970.492956008688
Iteration 7800: Loss = -11970.492334061812
Iteration 7900: Loss = -11970.492311399496
Iteration 8000: Loss = -11970.492322957962
Iteration 8100: Loss = -11970.492216875096
Iteration 8200: Loss = -11970.492197010693
Iteration 8300: Loss = -11970.492195339579
Iteration 8400: Loss = -11970.492111284733
Iteration 8500: Loss = -11970.492378468398
1
Iteration 8600: Loss = -11970.492000939568
Iteration 8700: Loss = -11970.49193082266
Iteration 8800: Loss = -11970.618794105454
1
Iteration 8900: Loss = -11970.49187043132
Iteration 9000: Loss = -11970.491850175362
Iteration 9100: Loss = -11970.547548983375
1
Iteration 9200: Loss = -11970.491828880324
Iteration 9300: Loss = -11970.49179083298
Iteration 9400: Loss = -11970.50457553593
1
Iteration 9500: Loss = -11970.49180290847
Iteration 9600: Loss = -11970.491781251872
Iteration 9700: Loss = -11970.50416753786
1
Iteration 9800: Loss = -11970.49186401971
Iteration 9900: Loss = -11970.493399223724
1
Iteration 10000: Loss = -11970.492662390834
2
Iteration 10100: Loss = -11970.5188373907
3
Iteration 10200: Loss = -11970.491713125073
Iteration 10300: Loss = -11970.491816749904
1
Iteration 10400: Loss = -11970.494053406746
2
Iteration 10500: Loss = -11970.491725972253
Iteration 10600: Loss = -11970.49167548595
Iteration 10700: Loss = -11970.507781407592
1
Iteration 10800: Loss = -11970.491631863766
Iteration 10900: Loss = -11970.491638064519
Iteration 11000: Loss = -11970.854546321118
1
Iteration 11100: Loss = -11970.4916033492
Iteration 11200: Loss = -11970.491609212218
Iteration 11300: Loss = -11970.575508796514
1
Iteration 11400: Loss = -11970.490024445045
Iteration 11500: Loss = -11970.490009438518
Iteration 11600: Loss = -11970.491841712515
1
Iteration 11700: Loss = -11970.492109442768
2
Iteration 11800: Loss = -11970.49000519696
Iteration 11900: Loss = -11970.490174539977
1
Iteration 12000: Loss = -11970.489944644674
Iteration 12100: Loss = -11970.489944349934
Iteration 12200: Loss = -11970.489946101972
Iteration 12300: Loss = -11970.506049459944
1
Iteration 12400: Loss = -11970.489920491485
Iteration 12500: Loss = -11970.493055418274
1
Iteration 12600: Loss = -11970.490006592527
Iteration 12700: Loss = -11970.49053550939
1
Iteration 12800: Loss = -11970.58981188896
2
Iteration 12900: Loss = -11970.490649660682
3
Iteration 13000: Loss = -11970.489796940059
Iteration 13100: Loss = -11970.55709793763
1
Iteration 13200: Loss = -11970.489769436424
Iteration 13300: Loss = -11970.85137035075
1
Iteration 13400: Loss = -11970.489792805658
Iteration 13500: Loss = -11970.489768053987
Iteration 13600: Loss = -11970.489991430266
1
Iteration 13700: Loss = -11970.489810400335
Iteration 13800: Loss = -11970.528430171984
1
Iteration 13900: Loss = -11970.489775185195
Iteration 14000: Loss = -11970.489773386915
Iteration 14100: Loss = -11970.490839297005
1
Iteration 14200: Loss = -11970.489759553453
Iteration 14300: Loss = -11970.491307930799
1
Iteration 14400: Loss = -11970.489775187778
Iteration 14500: Loss = -11970.48975637195
Iteration 14600: Loss = -11970.490253152759
1
Iteration 14700: Loss = -11970.5386251222
2
Iteration 14800: Loss = -11970.489755101988
Iteration 14900: Loss = -11970.49647662349
1
Iteration 15000: Loss = -11970.489744047158
Iteration 15100: Loss = -11970.48985483222
1
Iteration 15200: Loss = -11970.489925570331
2
Iteration 15300: Loss = -11970.489867462857
3
Iteration 15400: Loss = -11970.489737922491
Iteration 15500: Loss = -11970.49003038373
1
Iteration 15600: Loss = -11970.490409619324
2
Iteration 15700: Loss = -11970.490062312841
3
Iteration 15800: Loss = -11970.49202482927
4
Iteration 15900: Loss = -11970.732301947608
5
Iteration 16000: Loss = -11970.489761161625
Iteration 16100: Loss = -11970.497295883408
1
Iteration 16200: Loss = -11970.489723452652
Iteration 16300: Loss = -11970.489839235339
1
Iteration 16400: Loss = -11970.561665511104
2
Iteration 16500: Loss = -11970.48974272147
Iteration 16600: Loss = -11970.48971576612
Iteration 16700: Loss = -11970.490091438343
1
Iteration 16800: Loss = -11970.489718391384
Iteration 16900: Loss = -11970.857585619564
1
Iteration 17000: Loss = -11970.489727733415
Iteration 17100: Loss = -11970.489704314183
Iteration 17200: Loss = -11970.499768433696
1
Iteration 17300: Loss = -11970.489710210244
Iteration 17400: Loss = -11970.516915271657
1
Iteration 17500: Loss = -11970.490909684977
2
Iteration 17600: Loss = -11970.489766364532
Iteration 17700: Loss = -11970.489964843275
1
Iteration 17800: Loss = -11970.489717469414
Iteration 17900: Loss = -11970.489754598339
Iteration 18000: Loss = -11970.490029825458
1
Iteration 18100: Loss = -11970.489760765817
Iteration 18200: Loss = -11970.490155311598
1
Iteration 18300: Loss = -11970.49038241861
2
Iteration 18400: Loss = -11970.489728626058
Iteration 18500: Loss = -11970.491816382706
1
Iteration 18600: Loss = -11970.490048520418
2
Iteration 18700: Loss = -11970.492911495556
3
Iteration 18800: Loss = -11970.489691389848
Iteration 18900: Loss = -11970.517599827821
1
Iteration 19000: Loss = -11970.489692637768
Iteration 19100: Loss = -11970.489697712987
Iteration 19200: Loss = -11970.489758932328
Iteration 19300: Loss = -11970.4896938707
Iteration 19400: Loss = -11970.578134236383
1
Iteration 19500: Loss = -11970.489694400547
Iteration 19600: Loss = -11970.48969721664
Iteration 19700: Loss = -11970.629059480429
1
Iteration 19800: Loss = -11970.489692969752
Iteration 19900: Loss = -11970.489686468452
pi: tensor([[0.5237, 0.4763],
        [0.4586, 0.5414]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6074, 0.3926], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2772, 0.0849],
         [0.5431, 0.3517]],

        [[0.5770, 0.1119],
         [0.6970, 0.7024]],

        [[0.5268, 0.0949],
         [0.6985, 0.6738]],

        [[0.5432, 0.1061],
         [0.7025, 0.6078]],

        [[0.6147, 0.0933],
         [0.5922, 0.6351]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721463199647421
time is 1
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 8
Adjusted Rand Index: 0.7026542224128787
Global Adjusted Rand Index: 0.05962287789684733
Average Adjusted Rand Index: 0.8789599204086607
11642.831115010997
[1.0, 0.05962287789684733] [1.0, 0.8789599204086607] [11633.314017763101, 11970.490249965716]
-------------------------------------
This iteration is 69
True Objective function: Loss = -11646.85210681403
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21463.2942180226
Iteration 100: Loss = -12347.48851429682
Iteration 200: Loss = -12334.387555307376
Iteration 300: Loss = -12121.665533919058
Iteration 400: Loss = -11853.801183105732
Iteration 500: Loss = -11835.12638097973
Iteration 600: Loss = -11828.31484214652
Iteration 700: Loss = -11818.070161180816
Iteration 800: Loss = -11817.459090459677
Iteration 900: Loss = -11814.311070028658
Iteration 1000: Loss = -11808.412806832357
Iteration 1100: Loss = -11803.083695073052
Iteration 1200: Loss = -11802.902952421122
Iteration 1300: Loss = -11802.865390529523
Iteration 1400: Loss = -11802.836683149313
Iteration 1500: Loss = -11802.813800920965
Iteration 1600: Loss = -11802.794977761176
Iteration 1700: Loss = -11802.779147740739
Iteration 1800: Loss = -11802.765568677069
Iteration 1900: Loss = -11802.753624111467
Iteration 2000: Loss = -11802.742938100846
Iteration 2100: Loss = -11802.73304943961
Iteration 2200: Loss = -11802.723736785714
Iteration 2300: Loss = -11802.714687410855
Iteration 2400: Loss = -11802.705354780024
Iteration 2500: Loss = -11802.695259533557
Iteration 2600: Loss = -11802.682291504714
Iteration 2700: Loss = -11802.660924017846
Iteration 2800: Loss = -11802.64428166366
Iteration 2900: Loss = -11802.63283809617
Iteration 3000: Loss = -11802.62321696425
Iteration 3100: Loss = -11802.605986435165
Iteration 3200: Loss = -11802.594648630784
Iteration 3300: Loss = -11802.592675095098
Iteration 3400: Loss = -11802.585873946224
Iteration 3500: Loss = -11802.582834806866
Iteration 3600: Loss = -11802.580220558495
Iteration 3700: Loss = -11802.577694679203
Iteration 3800: Loss = -11802.575286416979
Iteration 3900: Loss = -11802.573278961745
Iteration 4000: Loss = -11802.564877260103
Iteration 4100: Loss = -11802.229086845906
Iteration 4200: Loss = -11800.021982960865
Iteration 4300: Loss = -11792.791721479014
Iteration 4400: Loss = -11792.618338643017
Iteration 4500: Loss = -11792.610636015124
Iteration 4600: Loss = -11792.594126133274
Iteration 4700: Loss = -11792.60002837588
1
Iteration 4800: Loss = -11792.583644327688
Iteration 4900: Loss = -11792.577417541808
Iteration 5000: Loss = -11792.575975321228
Iteration 5100: Loss = -11792.574949446604
Iteration 5200: Loss = -11792.574396566826
Iteration 5300: Loss = -11792.573329262095
Iteration 5400: Loss = -11792.572822621256
Iteration 5500: Loss = -11792.572085327902
Iteration 5600: Loss = -11792.571328387316
Iteration 5700: Loss = -11792.575580100292
1
Iteration 5800: Loss = -11792.571169861367
Iteration 5900: Loss = -11792.569333125857
Iteration 6000: Loss = -11792.572022996163
1
Iteration 6100: Loss = -11791.395344251223
Iteration 6200: Loss = -11786.965290262277
Iteration 6300: Loss = -11786.713340479604
Iteration 6400: Loss = -11786.71259874326
Iteration 6500: Loss = -11786.710876681685
Iteration 6600: Loss = -11786.710520335739
Iteration 6700: Loss = -11786.709462210829
Iteration 6800: Loss = -11786.709198495888
Iteration 6900: Loss = -11786.708819359079
Iteration 7000: Loss = -11786.708545021747
Iteration 7100: Loss = -11786.708322664166
Iteration 7200: Loss = -11786.715362978286
1
Iteration 7300: Loss = -11786.70787421347
Iteration 7400: Loss = -11786.705722146675
Iteration 7500: Loss = -11784.087267574756
Iteration 7600: Loss = -11784.087056766823
Iteration 7700: Loss = -11784.082772403424
Iteration 7800: Loss = -11784.085668295353
1
Iteration 7900: Loss = -11784.082408016395
Iteration 8000: Loss = -11784.082325785514
Iteration 8100: Loss = -11784.0830114222
1
Iteration 8200: Loss = -11784.08201705037
Iteration 8300: Loss = -11784.08200828426
Iteration 8400: Loss = -11784.08935063679
1
Iteration 8500: Loss = -11784.08170414001
Iteration 8600: Loss = -11784.081933374518
1
Iteration 8700: Loss = -11784.093343856013
2
Iteration 8800: Loss = -11784.14689742232
3
Iteration 8900: Loss = -11784.089261450548
4
Iteration 9000: Loss = -11784.08146175896
Iteration 9100: Loss = -11784.08181236504
1
Iteration 9200: Loss = -11784.086065978881
2
Iteration 9300: Loss = -11784.081422418649
Iteration 9400: Loss = -11784.08176543792
1
Iteration 9500: Loss = -11784.081505469032
Iteration 9600: Loss = -11784.081454738707
Iteration 9700: Loss = -11784.084117032018
1
Iteration 9800: Loss = -11784.051750015375
Iteration 9900: Loss = -11784.065849343157
1
Iteration 10000: Loss = -11784.053660700358
2
Iteration 10100: Loss = -11784.070247515898
3
Iteration 10200: Loss = -11784.05129207452
Iteration 10300: Loss = -11784.051506077345
1
Iteration 10400: Loss = -11784.049695011421
Iteration 10500: Loss = -11784.048739210528
Iteration 10600: Loss = -11784.125204603331
1
Iteration 10700: Loss = -11784.105080795305
2
Iteration 10800: Loss = -11784.045259136537
Iteration 10900: Loss = -11784.045653796082
1
Iteration 11000: Loss = -11784.053271347893
2
Iteration 11100: Loss = -11784.070684139391
3
Iteration 11200: Loss = -11784.052627951012
4
Iteration 11300: Loss = -11784.045578610305
5
Iteration 11400: Loss = -11784.054606251353
6
Iteration 11500: Loss = -11784.059467794534
7
Iteration 11600: Loss = -11784.05984255584
8
Iteration 11700: Loss = -11784.04561464098
9
Iteration 11800: Loss = -11784.055896765645
10
Iteration 11900: Loss = -11784.045980743038
11
Iteration 12000: Loss = -11784.04634927497
12
Iteration 12100: Loss = -11784.047409556526
13
Iteration 12200: Loss = -11784.047779770155
14
Iteration 12300: Loss = -11784.04571880631
15
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[0.6463, 0.3537],
        [0.3450, 0.6550]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8015, 0.1985], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2151, 0.1033],
         [0.5271, 0.3963]],

        [[0.5232, 0.1083],
         [0.6701, 0.5310]],

        [[0.5332, 0.1076],
         [0.7249, 0.7059]],

        [[0.5431, 0.1043],
         [0.5830, 0.6082]],

        [[0.6857, 0.1004],
         [0.6117, 0.5441]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 35
Adjusted Rand Index: 0.08207042363046008
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
Global Adjusted Rand Index: 0.534894418293144
Average Adjusted Rand Index: 0.8004105731557019
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22818.037238168265
Iteration 100: Loss = -11796.76851215468
Iteration 200: Loss = -11791.70116309644
Iteration 300: Loss = -11641.404676601034
Iteration 400: Loss = -11640.72696946855
Iteration 500: Loss = -11640.607345326503
Iteration 600: Loss = -11640.547449564163
Iteration 700: Loss = -11640.511417793037
Iteration 800: Loss = -11640.487602443396
Iteration 900: Loss = -11640.47074559151
Iteration 1000: Loss = -11640.458438958978
Iteration 1100: Loss = -11640.448993541899
Iteration 1200: Loss = -11640.44163791681
Iteration 1300: Loss = -11640.435756861918
Iteration 1400: Loss = -11640.430965560045
Iteration 1500: Loss = -11640.427001991698
Iteration 1600: Loss = -11640.423741805174
Iteration 1700: Loss = -11640.420922300062
Iteration 1800: Loss = -11640.418541587898
Iteration 1900: Loss = -11640.416512837064
Iteration 2000: Loss = -11640.414756375685
Iteration 2100: Loss = -11640.413183607663
Iteration 2200: Loss = -11640.411824279188
Iteration 2300: Loss = -11640.410624173182
Iteration 2400: Loss = -11640.409569789092
Iteration 2500: Loss = -11640.408603702777
Iteration 2600: Loss = -11640.407950571265
Iteration 2700: Loss = -11640.40702809765
Iteration 2800: Loss = -11640.406325098347
Iteration 2900: Loss = -11640.406882349993
1
Iteration 3000: Loss = -11640.405158557684
Iteration 3100: Loss = -11640.413799739843
1
Iteration 3200: Loss = -11640.404639968812
Iteration 3300: Loss = -11640.403773813227
Iteration 3400: Loss = -11640.403362461446
Iteration 3500: Loss = -11640.407951091669
1
Iteration 3600: Loss = -11640.402788614676
Iteration 3700: Loss = -11640.402543935541
Iteration 3800: Loss = -11640.402536681087
Iteration 3900: Loss = -11640.402110601199
Iteration 4000: Loss = -11640.401655036296
Iteration 4100: Loss = -11640.405489687013
1
Iteration 4200: Loss = -11640.401251992218
Iteration 4300: Loss = -11640.40108018622
Iteration 4400: Loss = -11640.408348474431
1
Iteration 4500: Loss = -11640.401104240696
Iteration 4600: Loss = -11640.403654743095
1
Iteration 4700: Loss = -11640.40307253417
2
Iteration 4800: Loss = -11640.400398436375
Iteration 4900: Loss = -11640.400395166736
Iteration 5000: Loss = -11640.402415027389
1
Iteration 5100: Loss = -11640.400580570284
2
Iteration 5200: Loss = -11640.399980143216
Iteration 5300: Loss = -11640.400484256677
1
Iteration 5400: Loss = -11640.399896433784
Iteration 5500: Loss = -11640.399623541483
Iteration 5600: Loss = -11640.39969420728
Iteration 5700: Loss = -11640.400627392662
1
Iteration 5800: Loss = -11640.399523957343
Iteration 5900: Loss = -11640.406429581613
1
Iteration 6000: Loss = -11640.401882756694
2
Iteration 6100: Loss = -11640.399380615398
Iteration 6200: Loss = -11640.399349862255
Iteration 6300: Loss = -11640.399540427536
1
Iteration 6400: Loss = -11640.399167906857
Iteration 6500: Loss = -11640.405362396012
1
Iteration 6600: Loss = -11640.413086895252
2
Iteration 6700: Loss = -11640.398975289663
Iteration 6800: Loss = -11640.398976584942
Iteration 6900: Loss = -11640.413305320102
1
Iteration 7000: Loss = -11640.399139078236
2
Iteration 7100: Loss = -11640.399567173023
3
Iteration 7200: Loss = -11640.400897072397
4
Iteration 7300: Loss = -11640.400284846972
5
Iteration 7400: Loss = -11640.400123391615
6
Iteration 7500: Loss = -11640.39873046657
Iteration 7600: Loss = -11640.398829572103
Iteration 7700: Loss = -11640.398831334844
Iteration 7800: Loss = -11640.398648769338
Iteration 7900: Loss = -11640.398768984649
1
Iteration 8000: Loss = -11640.419299567511
2
Iteration 8100: Loss = -11640.398631676544
Iteration 8200: Loss = -11640.438820661238
1
Iteration 8300: Loss = -11640.398555011376
Iteration 8400: Loss = -11640.41907541649
1
Iteration 8500: Loss = -11640.398673005899
2
Iteration 8600: Loss = -11640.408097457639
3
Iteration 8700: Loss = -11640.489252239757
4
Iteration 8800: Loss = -11640.39847167948
Iteration 8900: Loss = -11640.399553963545
1
Iteration 9000: Loss = -11640.398504543437
Iteration 9100: Loss = -11640.400176870988
1
Iteration 9200: Loss = -11640.399172301297
2
Iteration 9300: Loss = -11640.398473218125
Iteration 9400: Loss = -11640.398519856926
Iteration 9500: Loss = -11640.39871729327
1
Iteration 9600: Loss = -11640.404935264005
2
Iteration 9700: Loss = -11640.430630803012
3
Iteration 9800: Loss = -11640.398444130196
Iteration 9900: Loss = -11640.399282108541
1
Iteration 10000: Loss = -11640.398605680748
2
Iteration 10100: Loss = -11640.398680060325
3
Iteration 10200: Loss = -11640.406045088805
4
Iteration 10300: Loss = -11640.399264525908
5
Iteration 10400: Loss = -11640.475400792584
6
Iteration 10500: Loss = -11640.39838523765
Iteration 10600: Loss = -11640.403904401774
1
Iteration 10700: Loss = -11640.398398653117
Iteration 10800: Loss = -11640.398351583994
Iteration 10900: Loss = -11640.39852122425
1
Iteration 11000: Loss = -11640.409536265428
2
Iteration 11100: Loss = -11640.400879839628
3
Iteration 11200: Loss = -11640.413863454749
4
Iteration 11300: Loss = -11640.433363939665
5
Iteration 11400: Loss = -11640.410248964568
6
Iteration 11500: Loss = -11640.398645172203
7
Iteration 11600: Loss = -11640.398411634176
Iteration 11700: Loss = -11640.398539762737
1
Iteration 11800: Loss = -11640.401963613696
2
Iteration 11900: Loss = -11640.39861483654
3
Iteration 12000: Loss = -11640.398646811396
4
Iteration 12100: Loss = -11640.399569317276
5
Iteration 12200: Loss = -11640.400547205574
6
Iteration 12300: Loss = -11640.40060053862
7
Iteration 12400: Loss = -11640.398417227498
Iteration 12500: Loss = -11640.399008827164
1
Iteration 12600: Loss = -11640.399561836493
2
Iteration 12700: Loss = -11640.39868326871
3
Iteration 12800: Loss = -11640.398481519022
Iteration 12900: Loss = -11640.399009860317
1
Iteration 13000: Loss = -11640.405677995997
2
Iteration 13100: Loss = -11640.398926781947
3
Iteration 13200: Loss = -11640.398941009804
4
Iteration 13300: Loss = -11640.400544802975
5
Iteration 13400: Loss = -11640.400445531206
6
Iteration 13500: Loss = -11640.398869653036
7
Iteration 13600: Loss = -11640.400212634971
8
Iteration 13700: Loss = -11640.428724852218
9
Iteration 13800: Loss = -11640.398621564023
10
Iteration 13900: Loss = -11640.398607913086
11
Iteration 14000: Loss = -11640.399836645893
12
Iteration 14100: Loss = -11640.566728601088
13
Iteration 14200: Loss = -11640.39839069856
Iteration 14300: Loss = -11640.399351010296
1
Iteration 14400: Loss = -11640.445986495588
2
Iteration 14500: Loss = -11640.39842121795
Iteration 14600: Loss = -11640.398919892232
1
Iteration 14700: Loss = -11640.398662789648
2
Iteration 14800: Loss = -11640.398566582819
3
Iteration 14900: Loss = -11640.398487273787
Iteration 15000: Loss = -11640.4000402662
1
Iteration 15100: Loss = -11640.522657129375
2
Iteration 15200: Loss = -11640.40357347253
3
Iteration 15300: Loss = -11640.400300248684
4
Iteration 15400: Loss = -11640.39840457942
Iteration 15500: Loss = -11640.398857694516
1
Iteration 15600: Loss = -11640.398824143598
2
Iteration 15700: Loss = -11640.399286789958
3
Iteration 15800: Loss = -11640.398787261081
4
Iteration 15900: Loss = -11640.398543260997
5
Iteration 16000: Loss = -11640.39883559896
6
Iteration 16100: Loss = -11640.400367440583
7
Iteration 16200: Loss = -11640.398568612363
8
Iteration 16300: Loss = -11640.398861775206
9
Iteration 16400: Loss = -11640.398498589426
Iteration 16500: Loss = -11640.39939390549
1
Iteration 16600: Loss = -11640.406975456339
2
Iteration 16700: Loss = -11640.40330190511
3
Iteration 16800: Loss = -11640.40368589378
4
Iteration 16900: Loss = -11640.399790593761
5
Iteration 17000: Loss = -11640.399505038604
6
Iteration 17100: Loss = -11640.400885917457
7
Iteration 17200: Loss = -11640.419175471663
8
Iteration 17300: Loss = -11640.421901523037
9
Iteration 17400: Loss = -11640.440942493398
10
Iteration 17500: Loss = -11640.410296689317
11
Iteration 17600: Loss = -11640.409870878933
12
Iteration 17700: Loss = -11640.448115691868
13
Iteration 17800: Loss = -11640.399709666348
14
Iteration 17900: Loss = -11640.398439578747
Iteration 18000: Loss = -11640.399424662764
1
Iteration 18100: Loss = -11640.408983823003
2
Iteration 18200: Loss = -11640.400256660843
3
Iteration 18300: Loss = -11640.3986427382
4
Iteration 18400: Loss = -11640.39850298452
Iteration 18500: Loss = -11640.401932925553
1
Iteration 18600: Loss = -11640.41310366727
2
Iteration 18700: Loss = -11640.469406012773
3
Iteration 18800: Loss = -11640.398463217234
Iteration 18900: Loss = -11640.398394094633
Iteration 19000: Loss = -11640.418045688159
1
Iteration 19100: Loss = -11640.398368456945
Iteration 19200: Loss = -11640.41466152065
1
Iteration 19300: Loss = -11640.398352180193
Iteration 19400: Loss = -11640.402017487886
1
Iteration 19500: Loss = -11640.410346731665
2
Iteration 19600: Loss = -11640.41839889649
3
Iteration 19700: Loss = -11640.400140891994
4
Iteration 19800: Loss = -11640.40417704205
5
Iteration 19900: Loss = -11640.40073778534
6
pi: tensor([[0.7088, 0.2912],
        [0.2607, 0.7393]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4580, 0.5420], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4012, 0.0981],
         [0.6967, 0.2054]],

        [[0.6180, 0.1087],
         [0.6561, 0.6763]],

        [[0.5973, 0.1078],
         [0.5496, 0.6124]],

        [[0.6667, 0.1042],
         [0.6143, 0.6220]],

        [[0.6531, 0.1007],
         [0.6594, 0.6495]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9920000001562724
Average Adjusted Rand Index: 0.9919993417272899
11646.85210681403
[0.534894418293144, 0.9920000001562724] [0.8004105731557019, 0.9919993417272899] [11784.04571880631, 11640.398447331125]
-------------------------------------
This iteration is 70
True Objective function: Loss = -11724.240901832021
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21914.57447246916
Iteration 100: Loss = -12560.572265577213
Iteration 200: Loss = -12546.19785786275
Iteration 300: Loss = -12170.406420691264
Iteration 400: Loss = -12029.820467303845
Iteration 500: Loss = -12023.626581489976
Iteration 600: Loss = -12021.783043212654
Iteration 700: Loss = -12021.365333082578
Iteration 800: Loss = -12021.118843496213
Iteration 900: Loss = -12020.93573612046
Iteration 1000: Loss = -12020.747072614851
Iteration 1100: Loss = -12020.020443857304
Iteration 1200: Loss = -12015.845098427997
Iteration 1300: Loss = -12009.582465134561
Iteration 1400: Loss = -12006.921149627578
Iteration 1500: Loss = -12004.380202539978
Iteration 1600: Loss = -12002.540040451213
Iteration 1700: Loss = -11999.339362912006
Iteration 1800: Loss = -11998.940398141684
Iteration 1900: Loss = -11998.096300331019
Iteration 2000: Loss = -11997.992692555821
Iteration 2100: Loss = -11997.95642172615
Iteration 2200: Loss = -11997.934020975574
Iteration 2300: Loss = -11997.918886104066
Iteration 2400: Loss = -11997.906056429623
Iteration 2500: Loss = -11997.896152424804
Iteration 2600: Loss = -11997.8885688819
Iteration 2700: Loss = -11997.882516695596
Iteration 2800: Loss = -11997.877515158063
Iteration 2900: Loss = -11997.873705029318
Iteration 3000: Loss = -11997.869359034801
Iteration 3100: Loss = -11997.86588630989
Iteration 3200: Loss = -11997.864157282
Iteration 3300: Loss = -11997.860064357426
Iteration 3400: Loss = -11997.860451339513
1
Iteration 3500: Loss = -11997.856206184126
Iteration 3600: Loss = -11997.860939669261
1
Iteration 3700: Loss = -11997.851465754
Iteration 3800: Loss = -11997.850795154785
Iteration 3900: Loss = -11997.851655938293
1
Iteration 4000: Loss = -11997.848667466795
Iteration 4100: Loss = -11997.846087393398
Iteration 4200: Loss = -11997.843971169017
Iteration 4300: Loss = -11997.842705902127
Iteration 4400: Loss = -11997.841866486753
Iteration 4500: Loss = -11997.840602866749
Iteration 4600: Loss = -11997.840057810408
Iteration 4700: Loss = -11997.838791955057
Iteration 4800: Loss = -11997.846362624286
1
Iteration 4900: Loss = -11997.837277080123
Iteration 5000: Loss = -11997.836725254376
Iteration 5100: Loss = -11997.83604146879
Iteration 5200: Loss = -11997.835462159677
Iteration 5300: Loss = -11997.834837646835
Iteration 5400: Loss = -11997.83438821735
Iteration 5500: Loss = -11997.833888096173
Iteration 5600: Loss = -11997.833500946737
Iteration 5700: Loss = -11997.83339446714
Iteration 5800: Loss = -11997.837073640992
1
Iteration 5900: Loss = -11997.832775991621
Iteration 6000: Loss = -11997.831958991288
Iteration 6100: Loss = -11997.832764248724
1
Iteration 6200: Loss = -11997.831347806039
Iteration 6300: Loss = -11997.837595295636
1
Iteration 6400: Loss = -11997.831964205056
2
Iteration 6500: Loss = -11997.832295054024
3
Iteration 6600: Loss = -11997.830871799779
Iteration 6700: Loss = -11997.8298527199
Iteration 6800: Loss = -11997.83300131159
1
Iteration 6900: Loss = -11997.829405341789
Iteration 7000: Loss = -11997.832427438943
1
Iteration 7100: Loss = -11997.829071210464
Iteration 7200: Loss = -11997.829461768455
1
Iteration 7300: Loss = -11997.828709778392
Iteration 7400: Loss = -11997.828616030158
Iteration 7500: Loss = -11997.828431823009
Iteration 7600: Loss = -11997.82830287395
Iteration 7700: Loss = -11997.828155316056
Iteration 7800: Loss = -11997.8287051121
1
Iteration 7900: Loss = -11997.827906741877
Iteration 8000: Loss = -11997.827830665765
Iteration 8100: Loss = -11997.838632441026
1
Iteration 8200: Loss = -11997.827743298287
Iteration 8300: Loss = -11997.827608436959
Iteration 8400: Loss = -11997.8277831022
1
Iteration 8500: Loss = -11997.82814345862
2
Iteration 8600: Loss = -11997.827268229694
Iteration 8700: Loss = -11997.827304228485
Iteration 8800: Loss = -11997.827164001563
Iteration 8900: Loss = -11997.828527492042
1
Iteration 9000: Loss = -11997.833090087848
2
Iteration 9100: Loss = -11997.958967017823
3
Iteration 9200: Loss = -11997.826901742148
Iteration 9300: Loss = -11997.84322893765
1
Iteration 9400: Loss = -11997.831661956252
2
Iteration 9500: Loss = -11997.8265849217
Iteration 9600: Loss = -11997.826925872898
1
Iteration 9700: Loss = -11997.826836495839
2
Iteration 9800: Loss = -11997.826782689697
3
Iteration 9900: Loss = -11997.826429943454
Iteration 10000: Loss = -11997.843526474382
1
Iteration 10100: Loss = -11997.834297777485
2
Iteration 10200: Loss = -11997.82706089298
3
Iteration 10300: Loss = -11997.82676535264
4
Iteration 10400: Loss = -11997.847138505174
5
Iteration 10500: Loss = -11997.82660092637
6
Iteration 10600: Loss = -11997.826798907385
7
Iteration 10700: Loss = -11997.841044196863
8
Iteration 10800: Loss = -11997.829390736877
9
Iteration 10900: Loss = -11997.826193310146
Iteration 11000: Loss = -11997.85966451854
1
Iteration 11100: Loss = -11997.827244720609
2
Iteration 11200: Loss = -11997.826244587563
Iteration 11300: Loss = -11997.83367589838
1
Iteration 11400: Loss = -11997.832329835448
2
Iteration 11500: Loss = -11997.82602558123
Iteration 11600: Loss = -11997.826938552054
1
Iteration 11700: Loss = -11997.827061676018
2
Iteration 11800: Loss = -11997.826008060658
Iteration 11900: Loss = -11997.827893643967
1
Iteration 12000: Loss = -11997.825910080666
Iteration 12100: Loss = -11997.826894126116
1
Iteration 12200: Loss = -11997.825889765301
Iteration 12300: Loss = -11997.833305355703
1
Iteration 12400: Loss = -11997.82569451302
Iteration 12500: Loss = -11997.830428651598
1
Iteration 12600: Loss = -11997.826556035558
2
Iteration 12700: Loss = -11997.826895407696
3
Iteration 12800: Loss = -11997.82606604148
4
Iteration 12900: Loss = -11997.82610778196
5
Iteration 13000: Loss = -11997.826410337168
6
Iteration 13100: Loss = -11997.825858250902
7
Iteration 13200: Loss = -11997.863018236216
8
Iteration 13300: Loss = -11997.827209253806
9
Iteration 13400: Loss = -11997.82580395841
10
Iteration 13500: Loss = -11997.82678847925
11
Iteration 13600: Loss = -11997.852522722545
12
Iteration 13700: Loss = -11997.825826091319
13
Iteration 13800: Loss = -11997.826406373573
14
Iteration 13900: Loss = -11997.934255376797
15
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[0.6703, 0.3297],
        [0.3481, 0.6519]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8199, 0.1801], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2252, 0.0992],
         [0.7223, 0.4045]],

        [[0.5464, 0.1096],
         [0.6197, 0.7000]],

        [[0.5254, 0.1088],
         [0.5047, 0.7052]],

        [[0.6060, 0.0962],
         [0.5701, 0.6724]],

        [[0.7018, 0.1086],
         [0.5453, 0.5049]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 35
Adjusted Rand Index: 0.08320034701004249
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 31
Adjusted Rand Index: 0.13904643419078636
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.20641241871584917
Average Adjusted Rand Index: 0.6284481337334581
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20237.230253102665
Iteration 100: Loss = -12533.090709802565
Iteration 200: Loss = -12000.327481636576
Iteration 300: Loss = -11996.144218123602
Iteration 400: Loss = -11995.591702160247
Iteration 500: Loss = -11990.582028520195
Iteration 600: Loss = -11986.321597245735
Iteration 700: Loss = -11985.686032703961
Iteration 800: Loss = -11985.639680677004
Iteration 900: Loss = -11985.617172890788
Iteration 1000: Loss = -11985.601441972762
Iteration 1100: Loss = -11985.588740093845
Iteration 1200: Loss = -11985.55285120017
Iteration 1300: Loss = -11985.449621432903
Iteration 1400: Loss = -11985.443062986351
Iteration 1500: Loss = -11985.437450302214
Iteration 1600: Loss = -11985.432982030406
Iteration 1700: Loss = -11985.429333891121
Iteration 1800: Loss = -11985.426145201733
Iteration 1900: Loss = -11985.42338897927
Iteration 2000: Loss = -11985.421172820597
Iteration 2100: Loss = -11985.427102266563
1
Iteration 2200: Loss = -11985.417637096609
Iteration 2300: Loss = -11985.41618205456
Iteration 2400: Loss = -11985.414878456533
Iteration 2500: Loss = -11985.41371078651
Iteration 2600: Loss = -11985.412642790345
Iteration 2700: Loss = -11985.411685511328
Iteration 2800: Loss = -11985.411213565025
Iteration 2900: Loss = -11985.410053741649
Iteration 3000: Loss = -11985.40951466618
Iteration 3100: Loss = -11985.408756567309
Iteration 3200: Loss = -11985.409123850914
1
Iteration 3300: Loss = -11985.412202045543
2
Iteration 3400: Loss = -11985.407060391788
Iteration 3500: Loss = -11985.408147860799
1
Iteration 3600: Loss = -11985.406374554212
Iteration 3700: Loss = -11985.405861335857
Iteration 3800: Loss = -11985.405460998516
Iteration 3900: Loss = -11985.413634974604
1
Iteration 4000: Loss = -11985.404779834764
Iteration 4100: Loss = -11985.404544188752
Iteration 4200: Loss = -11985.403204383114
Iteration 4300: Loss = -11985.396872055502
Iteration 4400: Loss = -11985.396535617565
Iteration 4500: Loss = -11985.396366095181
Iteration 4600: Loss = -11985.396135508123
Iteration 4700: Loss = -11985.396073432721
Iteration 4800: Loss = -11985.395819381554
Iteration 4900: Loss = -11985.39576409488
Iteration 5000: Loss = -11985.395501096935
Iteration 5100: Loss = -11985.395400651309
Iteration 5200: Loss = -11985.395478574863
Iteration 5300: Loss = -11985.395153140174
Iteration 5400: Loss = -11985.395022544344
Iteration 5500: Loss = -11985.394920622004
Iteration 5600: Loss = -11985.394791054392
Iteration 5700: Loss = -11985.394866205195
Iteration 5800: Loss = -11985.39470405394
Iteration 5900: Loss = -11985.394530479183
Iteration 6000: Loss = -11985.394987393913
1
Iteration 6100: Loss = -11985.394788348089
2
Iteration 6200: Loss = -11985.39552868351
3
Iteration 6300: Loss = -11985.394422500862
Iteration 6400: Loss = -11985.394212884294
Iteration 6500: Loss = -11985.40774033119
1
Iteration 6600: Loss = -11985.3971543806
2
Iteration 6700: Loss = -11985.395131843223
3
Iteration 6800: Loss = -11985.394148494386
Iteration 6900: Loss = -11985.39397512504
Iteration 7000: Loss = -11985.394041670379
Iteration 7100: Loss = -11985.409735935751
1
Iteration 7200: Loss = -11985.39386115996
Iteration 7300: Loss = -11985.393844182348
Iteration 7400: Loss = -11985.393966493375
1
Iteration 7500: Loss = -11985.39377494067
Iteration 7600: Loss = -11985.475221825465
1
Iteration 7700: Loss = -11985.393660293223
Iteration 7800: Loss = -11985.393663593084
Iteration 7900: Loss = -11985.39567565565
1
Iteration 8000: Loss = -11985.39359736732
Iteration 8100: Loss = -11985.44374327072
1
Iteration 8200: Loss = -11985.393550624176
Iteration 8300: Loss = -11985.393535588071
Iteration 8400: Loss = -11985.422231694783
1
Iteration 8500: Loss = -11985.39346195991
Iteration 8600: Loss = -11985.39457246218
1
Iteration 8700: Loss = -11985.393459197949
Iteration 8800: Loss = -11985.393493123625
Iteration 8900: Loss = -11985.394549062736
1
Iteration 9000: Loss = -11985.406928096047
2
Iteration 9100: Loss = -11985.392860589536
Iteration 9200: Loss = -11985.404190748697
1
Iteration 9300: Loss = -11985.397524036856
2
Iteration 9400: Loss = -11985.393209356855
3
Iteration 9500: Loss = -11985.39276321319
Iteration 9600: Loss = -11985.43566517
1
Iteration 9700: Loss = -11985.392701801438
Iteration 9800: Loss = -11985.700919342044
1
Iteration 9900: Loss = -11985.392733299199
Iteration 10000: Loss = -11985.503257859811
1
Iteration 10100: Loss = -11985.392685580206
Iteration 10200: Loss = -11985.393091762695
1
Iteration 10300: Loss = -11985.39271415381
Iteration 10400: Loss = -11985.392673675304
Iteration 10500: Loss = -11985.40099846582
1
Iteration 10600: Loss = -11985.392672798558
Iteration 10700: Loss = -11985.39355659494
1
Iteration 10800: Loss = -11985.392871328118
2
Iteration 10900: Loss = -11985.393815353016
3
Iteration 11000: Loss = -11985.701578075033
4
Iteration 11100: Loss = -11985.392626475252
Iteration 11200: Loss = -11985.453272655366
1
Iteration 11300: Loss = -11985.392593868608
Iteration 11400: Loss = -11985.39262215617
Iteration 11500: Loss = -11985.392693759833
Iteration 11600: Loss = -11985.392631917215
Iteration 11700: Loss = -11985.40073231263
1
Iteration 11800: Loss = -11985.392628483316
Iteration 11900: Loss = -11985.392595496043
Iteration 12000: Loss = -11985.392969294657
1
Iteration 12100: Loss = -11985.394667612358
2
Iteration 12200: Loss = -11985.418047543164
3
Iteration 12300: Loss = -11985.412625414183
4
Iteration 12400: Loss = -11985.396684989979
5
Iteration 12500: Loss = -11985.394775651517
6
Iteration 12600: Loss = -11985.395401408497
7
Iteration 12700: Loss = -11985.392666742564
Iteration 12800: Loss = -11985.392872145629
1
Iteration 12900: Loss = -11985.421474361616
2
Iteration 13000: Loss = -11985.392620583285
Iteration 13100: Loss = -11985.397244100042
1
Iteration 13200: Loss = -11985.397065609108
2
Iteration 13300: Loss = -11985.394907970081
3
Iteration 13400: Loss = -11985.393164349538
4
Iteration 13500: Loss = -11985.392600123094
Iteration 13600: Loss = -11985.393343982774
1
Iteration 13700: Loss = -11985.392667128475
Iteration 13800: Loss = -11985.39753093978
1
Iteration 13900: Loss = -11985.393113766997
2
Iteration 14000: Loss = -11985.394120667548
3
Iteration 14100: Loss = -11985.403024801511
4
Iteration 14200: Loss = -11985.396781596735
5
Iteration 14300: Loss = -11985.392629636812
Iteration 14400: Loss = -11985.393435894102
1
Iteration 14500: Loss = -11985.409269211055
2
Iteration 14600: Loss = -11985.392615607667
Iteration 14700: Loss = -11985.421281722349
1
Iteration 14800: Loss = -11985.393281288105
2
Iteration 14900: Loss = -11985.397967230898
3
Iteration 15000: Loss = -11985.402906827952
4
Iteration 15100: Loss = -11985.394592588316
5
Iteration 15200: Loss = -11985.450795911698
6
Iteration 15300: Loss = -11985.393290617112
7
Iteration 15400: Loss = -11985.47668793426
8
Iteration 15500: Loss = -11985.393847753976
9
Iteration 15600: Loss = -11985.39109274836
Iteration 15700: Loss = -11985.390772156965
Iteration 15800: Loss = -11985.395691103075
1
Iteration 15900: Loss = -11985.389305913981
Iteration 16000: Loss = -11985.405597330348
1
Iteration 16100: Loss = -11985.389360311296
Iteration 16200: Loss = -11985.39090803845
1
Iteration 16300: Loss = -11985.388546235077
Iteration 16400: Loss = -11985.388452606612
Iteration 16500: Loss = -11985.507417050212
1
Iteration 16600: Loss = -11985.3947086603
2
Iteration 16700: Loss = -11985.387063792234
Iteration 16800: Loss = -11985.386737172013
Iteration 16900: Loss = -11985.429646563756
1
Iteration 17000: Loss = -11985.386684838595
Iteration 17100: Loss = -11985.39350567248
1
Iteration 17200: Loss = -11985.420768720845
2
Iteration 17300: Loss = -11985.387101571121
3
Iteration 17400: Loss = -11985.38679668217
4
Iteration 17500: Loss = -11985.489607953703
5
Iteration 17600: Loss = -11985.387307062852
6
Iteration 17700: Loss = -11985.390338203882
7
Iteration 17800: Loss = -11985.386752264818
Iteration 17900: Loss = -11985.418064678166
1
Iteration 18000: Loss = -11985.386668357802
Iteration 18100: Loss = -11985.38671622284
Iteration 18200: Loss = -11985.387312945899
1
Iteration 18300: Loss = -11985.386666309572
Iteration 18400: Loss = -11985.390446358879
1
Iteration 18500: Loss = -11985.386660551134
Iteration 18600: Loss = -11985.560954496916
1
Iteration 18700: Loss = -11985.386683281784
Iteration 18800: Loss = -11985.386883871215
1
Iteration 18900: Loss = -11985.393148647287
2
Iteration 19000: Loss = -11985.386657743224
Iteration 19100: Loss = -11985.466208925876
1
Iteration 19200: Loss = -11985.38864983468
2
Iteration 19300: Loss = -11985.388435875784
3
Iteration 19400: Loss = -11985.39761412788
4
Iteration 19500: Loss = -11985.386886559489
5
Iteration 19600: Loss = -11985.603729631248
6
Iteration 19700: Loss = -11985.386869285081
7
Iteration 19800: Loss = -11985.440149917365
8
Iteration 19900: Loss = -11985.38683053311
9
pi: tensor([[0.5981, 0.4019],
        [0.3883, 0.6117]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2556, 0.7444], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3882, 0.0992],
         [0.6076, 0.2391]],

        [[0.6157, 0.1035],
         [0.5914, 0.6030]],

        [[0.5467, 0.1078],
         [0.6738, 0.5119]],

        [[0.6695, 0.0962],
         [0.5622, 0.6074]],

        [[0.6377, 0.1068],
         [0.5190, 0.6545]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.1867290930650465
time is 1
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 82
Adjusted Rand Index: 0.4043632094699255
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.13967963444071926
Average Adjusted Rand Index: 0.70221748876986
11724.240901832021
[0.20641241871584917, 0.13967963444071926] [0.6284481337334581, 0.70221748876986] [11997.934255376797, 11985.488189058033]
-------------------------------------
This iteration is 71
True Objective function: Loss = -11704.401932025363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19499.19525702144
Iteration 100: Loss = -12510.78385195018
Iteration 200: Loss = -12384.761733612648
Iteration 300: Loss = -11701.582609291354
Iteration 400: Loss = -11691.60384789564
Iteration 500: Loss = -11690.767637316985
Iteration 600: Loss = -11690.36899429682
Iteration 700: Loss = -11690.102257890443
Iteration 800: Loss = -11689.967725679688
Iteration 900: Loss = -11689.874872856719
Iteration 1000: Loss = -11689.806962136952
Iteration 1100: Loss = -11689.7553275906
Iteration 1200: Loss = -11689.715159729049
Iteration 1300: Loss = -11689.68352168194
Iteration 1400: Loss = -11689.658304877712
Iteration 1500: Loss = -11689.637668837315
Iteration 1600: Loss = -11689.620336363983
Iteration 1700: Loss = -11689.60561232719
Iteration 1800: Loss = -11689.593080982855
Iteration 1900: Loss = -11689.582330345407
Iteration 2000: Loss = -11689.57306009411
Iteration 2100: Loss = -11689.564811895647
Iteration 2200: Loss = -11689.55755055602
Iteration 2300: Loss = -11689.550701886303
Iteration 2400: Loss = -11689.545860053495
Iteration 2500: Loss = -11689.536757982823
Iteration 2600: Loss = -11689.530101240945
Iteration 2700: Loss = -11689.498310954255
Iteration 2800: Loss = -11689.47917581894
Iteration 2900: Loss = -11689.475553141316
Iteration 3000: Loss = -11689.472411420455
Iteration 3100: Loss = -11689.469692821125
Iteration 3200: Loss = -11689.466527087594
Iteration 3300: Loss = -11689.46969038227
1
Iteration 3400: Loss = -11689.46151814235
Iteration 3500: Loss = -11689.459431681638
Iteration 3600: Loss = -11689.457542275806
Iteration 3700: Loss = -11689.455822515474
Iteration 3800: Loss = -11689.454198901143
Iteration 3900: Loss = -11689.452846404307
Iteration 4000: Loss = -11689.451657730842
Iteration 4100: Loss = -11689.452347743752
1
Iteration 4200: Loss = -11689.449110564477
Iteration 4300: Loss = -11689.447967412509
Iteration 4400: Loss = -11689.447094730343
Iteration 4500: Loss = -11689.446224718713
Iteration 4600: Loss = -11689.445418020741
Iteration 4700: Loss = -11689.445150466316
Iteration 4800: Loss = -11689.453035816212
1
Iteration 4900: Loss = -11689.443617710966
Iteration 5000: Loss = -11689.448957674145
1
Iteration 5100: Loss = -11689.442582894051
Iteration 5200: Loss = -11689.44475386845
1
Iteration 5300: Loss = -11689.441100877099
Iteration 5400: Loss = -11689.440671654771
Iteration 5500: Loss = -11689.440548406643
Iteration 5600: Loss = -11689.440306516946
Iteration 5700: Loss = -11689.439541588588
Iteration 5800: Loss = -11689.439102912173
Iteration 5900: Loss = -11689.440817766994
1
Iteration 6000: Loss = -11689.443230788562
2
Iteration 6100: Loss = -11689.438549589058
Iteration 6200: Loss = -11689.437910390314
Iteration 6300: Loss = -11689.437761615965
Iteration 6400: Loss = -11689.437819349521
Iteration 6500: Loss = -11689.437704801652
Iteration 6600: Loss = -11689.439503062747
1
Iteration 6700: Loss = -11689.43674089296
Iteration 6800: Loss = -11689.44935901234
1
Iteration 6900: Loss = -11689.440912710026
2
Iteration 7000: Loss = -11689.44822926515
3
Iteration 7100: Loss = -11689.440835917736
4
Iteration 7200: Loss = -11689.502066678435
5
Iteration 7300: Loss = -11689.435747534133
Iteration 7400: Loss = -11689.436918730262
1
Iteration 7500: Loss = -11689.435483116968
Iteration 7600: Loss = -11689.435905355194
1
Iteration 7700: Loss = -11689.435222572762
Iteration 7800: Loss = -11689.435069354427
Iteration 7900: Loss = -11689.435199976351
1
Iteration 8000: Loss = -11689.434696828572
Iteration 8100: Loss = -11689.43451621234
Iteration 8200: Loss = -11689.43443239367
Iteration 8300: Loss = -11689.434156306903
Iteration 8400: Loss = -11689.43405696904
Iteration 8500: Loss = -11689.497480470956
1
Iteration 8600: Loss = -11689.433953912432
Iteration 8700: Loss = -11689.439336373096
1
Iteration 8800: Loss = -11689.446180243038
2
Iteration 8900: Loss = -11689.440468444072
3
Iteration 9000: Loss = -11689.434360598527
4
Iteration 9100: Loss = -11689.434669256301
5
Iteration 9200: Loss = -11689.43453470507
6
Iteration 9300: Loss = -11689.438394623077
7
Iteration 9400: Loss = -11689.440304022457
8
Iteration 9500: Loss = -11689.440708321912
9
Iteration 9600: Loss = -11689.43801759028
10
Iteration 9700: Loss = -11689.435165749188
11
Iteration 9800: Loss = -11689.434579147608
12
Iteration 9900: Loss = -11689.433398724696
Iteration 10000: Loss = -11689.433604146881
1
Iteration 10100: Loss = -11689.433917676315
2
Iteration 10200: Loss = -11689.441187515882
3
Iteration 10300: Loss = -11689.436625632972
4
Iteration 10400: Loss = -11689.494817850427
5
Iteration 10500: Loss = -11689.435101946112
6
Iteration 10600: Loss = -11689.434696074499
7
Iteration 10700: Loss = -11689.569781029826
8
Iteration 10800: Loss = -11689.436753710886
9
Iteration 10900: Loss = -11689.43306403915
Iteration 11000: Loss = -11689.434182573024
1
Iteration 11100: Loss = -11689.43679376844
2
Iteration 11200: Loss = -11689.433293817117
3
Iteration 11300: Loss = -11689.438686207744
4
Iteration 11400: Loss = -11689.433926439542
5
Iteration 11500: Loss = -11689.433056625583
Iteration 11600: Loss = -11689.433500810797
1
Iteration 11700: Loss = -11689.436895461606
2
Iteration 11800: Loss = -11689.467031939887
3
Iteration 11900: Loss = -11689.488198458757
4
Iteration 12000: Loss = -11689.443027266369
5
Iteration 12100: Loss = -11689.45016962009
6
Iteration 12200: Loss = -11689.444809058516
7
Iteration 12300: Loss = -11689.469064436636
8
Iteration 12400: Loss = -11689.436710472357
9
Iteration 12500: Loss = -11689.442249871763
10
Iteration 12600: Loss = -11689.432828822208
Iteration 12700: Loss = -11689.438978038723
1
Iteration 12800: Loss = -11689.510790711607
2
Iteration 12900: Loss = -11689.435763199808
3
Iteration 13000: Loss = -11689.432735448456
Iteration 13100: Loss = -11689.43421429189
1
Iteration 13200: Loss = -11689.433311218194
2
Iteration 13300: Loss = -11689.47842846033
3
Iteration 13400: Loss = -11689.440454326466
4
Iteration 13500: Loss = -11689.436312474241
5
Iteration 13600: Loss = -11689.51820781438
6
Iteration 13700: Loss = -11689.432546518636
Iteration 13800: Loss = -11689.432823437726
1
Iteration 13900: Loss = -11689.463783478941
2
Iteration 14000: Loss = -11689.476658952444
3
Iteration 14100: Loss = -11689.461960354585
4
Iteration 14200: Loss = -11689.43271415206
5
Iteration 14300: Loss = -11689.433009308614
6
Iteration 14400: Loss = -11689.440546405682
7
Iteration 14500: Loss = -11689.435290720896
8
Iteration 14600: Loss = -11689.432323946694
Iteration 14700: Loss = -11689.452809614695
1
Iteration 14800: Loss = -11689.445407716748
2
Iteration 14900: Loss = -11689.474892581671
3
Iteration 15000: Loss = -11689.43374052855
4
Iteration 15100: Loss = -11689.444131738825
5
Iteration 15200: Loss = -11689.432837052367
6
Iteration 15300: Loss = -11689.433146843958
7
Iteration 15400: Loss = -11689.43307590192
8
Iteration 15500: Loss = -11689.44794247446
9
Iteration 15600: Loss = -11689.435170102719
10
Iteration 15700: Loss = -11689.44091376942
11
Iteration 15800: Loss = -11689.432661057715
12
Iteration 15900: Loss = -11689.435482210514
13
Iteration 16000: Loss = -11689.440584603375
14
Iteration 16100: Loss = -11689.438070317168
15
Stopping early at iteration 16100 due to no improvement.
pi: tensor([[0.7396, 0.2604],
        [0.2663, 0.7337]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5337, 0.4663], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3980, 0.1062],
         [0.5732, 0.2049]],

        [[0.6038, 0.1035],
         [0.6786, 0.5719]],

        [[0.5094, 0.1055],
         [0.7125, 0.7107]],

        [[0.5696, 0.1016],
         [0.6505, 0.7286]],

        [[0.5371, 0.0884],
         [0.5955, 0.6281]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681923796599017
Average Adjusted Rand Index: 0.9681606230554014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21441.912978637927
Iteration 100: Loss = -12521.407778974994
Iteration 200: Loss = -12519.074961052635
Iteration 300: Loss = -12485.709333757937
Iteration 400: Loss = -12149.454037101275
Iteration 500: Loss = -11997.874376100017
Iteration 600: Loss = -11884.190606697694
Iteration 700: Loss = -11864.001811724367
Iteration 800: Loss = -11863.477823680043
Iteration 900: Loss = -11863.225723057969
Iteration 1000: Loss = -11863.068619592901
Iteration 1100: Loss = -11862.958516493496
Iteration 1200: Loss = -11862.869904119525
Iteration 1300: Loss = -11862.799523668946
Iteration 1400: Loss = -11862.752002277264
Iteration 1500: Loss = -11862.71460089092
Iteration 1600: Loss = -11862.684960121904
Iteration 1700: Loss = -11862.668611435005
Iteration 1800: Loss = -11862.640900985305
Iteration 1900: Loss = -11862.628399384064
Iteration 2000: Loss = -11862.609546146781
Iteration 2100: Loss = -11862.597137166427
Iteration 2200: Loss = -11862.586041743973
Iteration 2300: Loss = -11862.576162792226
Iteration 2400: Loss = -11862.567027541203
Iteration 2500: Loss = -11862.567310822658
1
Iteration 2600: Loss = -11862.55186246503
Iteration 2700: Loss = -11862.54581546572
Iteration 2800: Loss = -11862.540382439502
Iteration 2900: Loss = -11862.53519909501
Iteration 3000: Loss = -11862.530194333765
Iteration 3100: Loss = -11862.524961631156
Iteration 3200: Loss = -11862.520661309769
Iteration 3300: Loss = -11862.515962458829
Iteration 3400: Loss = -11862.514330812708
Iteration 3500: Loss = -11862.510485053686
Iteration 3600: Loss = -11862.507216348975
Iteration 3700: Loss = -11862.504918554874
Iteration 3800: Loss = -11862.504485765627
Iteration 3900: Loss = -11862.50099641915
Iteration 4000: Loss = -11862.499301076921
Iteration 4100: Loss = -11862.498399068023
Iteration 4200: Loss = -11862.496299438906
Iteration 4300: Loss = -11862.497197076817
1
Iteration 4400: Loss = -11862.496655646579
2
Iteration 4500: Loss = -11862.49603458447
Iteration 4600: Loss = -11862.491664387702
Iteration 4700: Loss = -11862.49048691549
Iteration 4800: Loss = -11862.490608627395
1
Iteration 4900: Loss = -11862.488659798
Iteration 5000: Loss = -11862.4880727459
Iteration 5100: Loss = -11862.487442727177
Iteration 5200: Loss = -11862.487112245532
Iteration 5300: Loss = -11862.486006835254
Iteration 5400: Loss = -11862.48532805033
Iteration 5500: Loss = -11862.485316486032
Iteration 5600: Loss = -11862.484749633843
Iteration 5700: Loss = -11862.483448932615
Iteration 5800: Loss = -11862.483715002922
1
Iteration 5900: Loss = -11862.482619221917
Iteration 6000: Loss = -11862.482559728393
Iteration 6100: Loss = -11862.482217982997
Iteration 6200: Loss = -11862.48952622386
1
Iteration 6300: Loss = -11862.481047906163
Iteration 6400: Loss = -11862.480706708271
Iteration 6500: Loss = -11862.48035871795
Iteration 6600: Loss = -11862.48008340505
Iteration 6700: Loss = -11862.479866589514
Iteration 6800: Loss = -11862.479578769775
Iteration 6900: Loss = -11862.488662504678
1
Iteration 7000: Loss = -11862.479077964801
Iteration 7100: Loss = -11862.478920537282
Iteration 7200: Loss = -11862.480946190337
1
Iteration 7300: Loss = -11862.482118460404
2
Iteration 7400: Loss = -11862.484670659594
3
Iteration 7500: Loss = -11862.572676327647
4
Iteration 7600: Loss = -11862.47799739648
Iteration 7700: Loss = -11862.478471789273
1
Iteration 7800: Loss = -11862.477661663466
Iteration 7900: Loss = -11862.478701445587
1
Iteration 8000: Loss = -11862.477429795417
Iteration 8100: Loss = -11862.477255513484
Iteration 8200: Loss = -11862.477257934945
Iteration 8300: Loss = -11862.477039620286
Iteration 8400: Loss = -11862.476911651793
Iteration 8500: Loss = -11862.478199443847
1
Iteration 8600: Loss = -11862.476750032958
Iteration 8700: Loss = -11862.49201989252
1
Iteration 8800: Loss = -11862.528243064779
2
Iteration 8900: Loss = -11862.476520981045
Iteration 9000: Loss = -11862.476548124663
Iteration 9100: Loss = -11862.507167844276
1
Iteration 9200: Loss = -11862.476372072071
Iteration 9300: Loss = -11862.538356993053
1
Iteration 9400: Loss = -11862.478926899763
2
Iteration 9500: Loss = -11862.476177368004
Iteration 9600: Loss = -11862.476126392836
Iteration 9700: Loss = -11862.514082282021
1
Iteration 9800: Loss = -11862.476342998703
2
Iteration 9900: Loss = -11862.477310940107
3
Iteration 10000: Loss = -11862.480532553818
4
Iteration 10100: Loss = -11862.643139022219
5
Iteration 10200: Loss = -11862.475815538592
Iteration 10300: Loss = -11862.477380592021
1
Iteration 10400: Loss = -11862.475689705081
Iteration 10500: Loss = -11862.475834746989
1
Iteration 10600: Loss = -11862.476173704372
2
Iteration 10700: Loss = -11862.475813582336
3
Iteration 10800: Loss = -11862.482929911375
4
Iteration 10900: Loss = -11862.487156960764
5
Iteration 11000: Loss = -11862.475522481145
Iteration 11100: Loss = -11862.47834894583
1
Iteration 11200: Loss = -11862.47803728776
2
Iteration 11300: Loss = -11862.476999222254
3
Iteration 11400: Loss = -11862.475783988773
4
Iteration 11500: Loss = -11862.47626415215
5
Iteration 11600: Loss = -11862.476985934576
6
Iteration 11700: Loss = -11862.47907547353
7
Iteration 11800: Loss = -11862.487103512452
8
Iteration 11900: Loss = -11862.475833171688
9
Iteration 12000: Loss = -11862.475534123156
Iteration 12100: Loss = -11862.500199957127
1
Iteration 12200: Loss = -11862.475423296992
Iteration 12300: Loss = -11862.475315514172
Iteration 12400: Loss = -11862.503371196864
1
Iteration 12500: Loss = -11862.477322704866
2
Iteration 12600: Loss = -11862.479749581302
3
Iteration 12700: Loss = -11862.504184618076
4
Iteration 12800: Loss = -11862.475174242514
Iteration 12900: Loss = -11862.489092879077
1
Iteration 13000: Loss = -11862.476617134247
2
Iteration 13100: Loss = -11862.478393970434
3
Iteration 13200: Loss = -11862.48521372085
4
Iteration 13300: Loss = -11862.476869505444
5
Iteration 13400: Loss = -11862.477306471314
6
Iteration 13500: Loss = -11862.520080421138
7
Iteration 13600: Loss = -11862.47512809625
Iteration 13700: Loss = -11862.47529768121
1
Iteration 13800: Loss = -11862.476587732086
2
Iteration 13900: Loss = -11862.481451464568
3
Iteration 14000: Loss = -11862.481156305534
4
Iteration 14100: Loss = -11862.487601647881
5
Iteration 14200: Loss = -11862.52907764517
6
Iteration 14300: Loss = -11862.49061118627
7
Iteration 14400: Loss = -11862.475159980633
Iteration 14500: Loss = -11862.47816050873
1
Iteration 14600: Loss = -11862.479489264937
2
Iteration 14700: Loss = -11862.477277502361
3
Iteration 14800: Loss = -11862.478310811563
4
Iteration 14900: Loss = -11862.476036755677
5
Iteration 15000: Loss = -11862.477539292035
6
Iteration 15100: Loss = -11862.481674946857
7
Iteration 15200: Loss = -11862.475170514073
Iteration 15300: Loss = -11862.475204448723
Iteration 15400: Loss = -11862.475705075272
1
Iteration 15500: Loss = -11862.56466280999
2
Iteration 15600: Loss = -11862.475072244817
Iteration 15700: Loss = -11862.489287079134
1
Iteration 15800: Loss = -11862.475074540484
Iteration 15900: Loss = -11862.479633854517
1
Iteration 16000: Loss = -11862.4750907984
Iteration 16100: Loss = -11862.475514526948
1
Iteration 16200: Loss = -11862.48920572581
2
Iteration 16300: Loss = -11862.60836009254
3
Iteration 16400: Loss = -11862.47774460696
4
Iteration 16500: Loss = -11862.479877166741
5
Iteration 16600: Loss = -11862.475683842082
6
Iteration 16700: Loss = -11862.47540696137
7
Iteration 16800: Loss = -11862.475302429055
8
Iteration 16900: Loss = -11862.476587178822
9
Iteration 17000: Loss = -11862.49663115315
10
Iteration 17100: Loss = -11862.475116654972
Iteration 17200: Loss = -11862.48070978493
1
Iteration 17300: Loss = -11862.480890772009
2
Iteration 17400: Loss = -11862.475345958066
3
Iteration 17500: Loss = -11862.475281461444
4
Iteration 17600: Loss = -11862.475865861117
5
Iteration 17700: Loss = -11862.475385729978
6
Iteration 17800: Loss = -11862.475250630298
7
Iteration 17900: Loss = -11862.477722788082
8
Iteration 18000: Loss = -11862.49051828433
9
Iteration 18100: Loss = -11862.491609435625
10
Iteration 18200: Loss = -11862.478653809092
11
Iteration 18300: Loss = -11862.475408336819
12
Iteration 18400: Loss = -11862.47569957377
13
Iteration 18500: Loss = -11862.49269120326
14
Iteration 18600: Loss = -11862.488244577855
15
Stopping early at iteration 18600 due to no improvement.
pi: tensor([[0.7501, 0.2499],
        [0.3535, 0.6465]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4635, 0.5365], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2231, 0.1061],
         [0.5772, 0.3899]],

        [[0.6237, 0.1031],
         [0.7135, 0.5966]],

        [[0.7235, 0.1048],
         [0.7172, 0.7244]],

        [[0.6261, 0.1015],
         [0.6882, 0.6797]],

        [[0.5933, 0.1016],
         [0.6050, 0.5452]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 1
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 24
Adjusted Rand Index: 0.2646838453269049
Global Adjusted Rand Index: 0.4558881251798184
Average Adjusted Rand Index: 0.8130966084505115
11704.401932025363
[0.9681923796599017, 0.4558881251798184] [0.9681606230554014, 0.8130966084505115] [11689.438070317168, 11862.488244577855]
-------------------------------------
This iteration is 72
True Objective function: Loss = -11516.907266362165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21004.92586242381
Iteration 100: Loss = -12390.295511616581
Iteration 200: Loss = -12189.709785475308
Iteration 300: Loss = -11639.303074638243
Iteration 400: Loss = -11577.60013073601
Iteration 500: Loss = -11513.66034001865
Iteration 600: Loss = -11510.749856884431
Iteration 700: Loss = -11510.463624570322
Iteration 800: Loss = -11510.348139743726
Iteration 900: Loss = -11510.271715772362
Iteration 1000: Loss = -11510.217820552634
Iteration 1100: Loss = -11510.178103323135
Iteration 1200: Loss = -11510.14780494664
Iteration 1300: Loss = -11510.12389988138
Iteration 1400: Loss = -11510.104650832722
Iteration 1500: Loss = -11510.08874834785
Iteration 1600: Loss = -11510.075602855517
Iteration 1700: Loss = -11510.064528961708
Iteration 1800: Loss = -11510.055286009212
Iteration 1900: Loss = -11510.047345172696
Iteration 2000: Loss = -11510.04049772824
Iteration 2100: Loss = -11510.034628042986
Iteration 2200: Loss = -11510.029405439358
Iteration 2300: Loss = -11510.024850225884
Iteration 2400: Loss = -11510.0207817124
Iteration 2500: Loss = -11510.017186429941
Iteration 2600: Loss = -11510.013966932293
Iteration 2700: Loss = -11510.011020290693
Iteration 2800: Loss = -11510.009564955091
Iteration 2900: Loss = -11510.006803927976
Iteration 3000: Loss = -11510.003735793285
Iteration 3100: Loss = -11510.001713157899
Iteration 3200: Loss = -11509.999786807832
Iteration 3300: Loss = -11509.99771311208
Iteration 3400: Loss = -11509.995152969484
Iteration 3500: Loss = -11509.991465198596
Iteration 3600: Loss = -11509.984779088385
Iteration 3700: Loss = -11509.983197549573
Iteration 3800: Loss = -11509.982958632192
Iteration 3900: Loss = -11509.981018459162
Iteration 4000: Loss = -11509.980583553219
Iteration 4100: Loss = -11509.979228536004
Iteration 4200: Loss = -11509.978505127345
Iteration 4300: Loss = -11509.977900013559
Iteration 4400: Loss = -11509.977068808099
Iteration 4500: Loss = -11509.99339874664
1
Iteration 4600: Loss = -11509.984528147399
2
Iteration 4700: Loss = -11509.975338120687
Iteration 4800: Loss = -11509.974849389404
Iteration 4900: Loss = -11509.975139120344
1
Iteration 5000: Loss = -11509.974297806952
Iteration 5100: Loss = -11509.973541199372
Iteration 5200: Loss = -11509.974605291865
1
Iteration 5300: Loss = -11509.972827380629
Iteration 5400: Loss = -11509.972457414018
Iteration 5500: Loss = -11509.97548502529
1
Iteration 5600: Loss = -11509.971846082706
Iteration 5700: Loss = -11509.981517906572
1
Iteration 5800: Loss = -11509.971354649635
Iteration 5900: Loss = -11509.971076565384
Iteration 6000: Loss = -11509.972084327945
1
Iteration 6100: Loss = -11509.97064078963
Iteration 6200: Loss = -11509.973526160207
1
Iteration 6300: Loss = -11509.97027232599
Iteration 6400: Loss = -11509.970100001628
Iteration 6500: Loss = -11509.969932917425
Iteration 6600: Loss = -11509.970094900975
1
Iteration 6700: Loss = -11510.038912740087
2
Iteration 6800: Loss = -11509.969494163019
Iteration 6900: Loss = -11509.973327271538
1
Iteration 7000: Loss = -11509.969266740676
Iteration 7100: Loss = -11509.969903256773
1
Iteration 7200: Loss = -11509.971487908215
2
Iteration 7300: Loss = -11509.974512805404
3
Iteration 7400: Loss = -11509.993311460548
4
Iteration 7500: Loss = -11509.968672020363
Iteration 7600: Loss = -11509.97178619498
1
Iteration 7700: Loss = -11509.968480823763
Iteration 7800: Loss = -11509.968993884917
1
Iteration 7900: Loss = -11509.968375916103
Iteration 8000: Loss = -11509.972066322818
1
Iteration 8100: Loss = -11509.972402319336
2
Iteration 8200: Loss = -11509.968179979067
Iteration 8300: Loss = -11509.971528481257
1
Iteration 8400: Loss = -11509.96827867886
Iteration 8500: Loss = -11509.967984874613
Iteration 8600: Loss = -11509.992598203597
1
Iteration 8700: Loss = -11509.968035706963
Iteration 8800: Loss = -11509.968377304396
1
Iteration 8900: Loss = -11509.967908351806
Iteration 9000: Loss = -11509.968169518177
1
Iteration 9100: Loss = -11509.969057193573
2
Iteration 9200: Loss = -11509.971762208661
3
Iteration 9300: Loss = -11509.968203015718
4
Iteration 9400: Loss = -11509.967972618933
Iteration 9500: Loss = -11509.967908984223
Iteration 9600: Loss = -11509.971345122653
1
Iteration 9700: Loss = -11509.971738974347
2
Iteration 9800: Loss = -11509.971129533003
3
Iteration 9900: Loss = -11510.001472425198
4
Iteration 10000: Loss = -11510.008563399364
5
Iteration 10100: Loss = -11509.985183744386
6
Iteration 10200: Loss = -11509.972236577012
7
Iteration 10300: Loss = -11509.968197492943
8
Iteration 10400: Loss = -11509.981770225526
9
Iteration 10500: Loss = -11509.975021826533
10
Iteration 10600: Loss = -11509.967988431332
Iteration 10700: Loss = -11509.967885737853
Iteration 10800: Loss = -11509.968283427377
1
Iteration 10900: Loss = -11509.984732548526
2
Iteration 11000: Loss = -11509.967595099935
Iteration 11100: Loss = -11509.970067939119
1
Iteration 11200: Loss = -11509.972393639564
2
Iteration 11300: Loss = -11509.984234749527
3
Iteration 11400: Loss = -11509.970380595649
4
Iteration 11500: Loss = -11510.02375210384
5
Iteration 11600: Loss = -11509.980195157461
6
Iteration 11700: Loss = -11509.973386944104
7
Iteration 11800: Loss = -11509.979135356374
8
Iteration 11900: Loss = -11509.973373565947
9
Iteration 12000: Loss = -11509.970700932176
10
Iteration 12100: Loss = -11509.984626692787
11
Iteration 12200: Loss = -11509.973507798322
12
Iteration 12300: Loss = -11509.970495221554
13
Iteration 12400: Loss = -11509.975021782153
14
Iteration 12500: Loss = -11509.979015807216
15
Stopping early at iteration 12500 due to no improvement.
pi: tensor([[0.7826, 0.2174],
        [0.2174, 0.7826]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4604, 0.5396], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1950, 0.0913],
         [0.6366, 0.3968]],

        [[0.5807, 0.0943],
         [0.5427, 0.5077]],

        [[0.5168, 0.0989],
         [0.7025, 0.6803]],

        [[0.5553, 0.0991],
         [0.6537, 0.6299]],

        [[0.6329, 0.1040],
         [0.5189, 0.5578]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23085.642228197907
Iteration 100: Loss = -11755.057465527047
Iteration 200: Loss = -11512.091992625452
Iteration 300: Loss = -11511.027915797564
Iteration 400: Loss = -11510.638616212875
Iteration 500: Loss = -11510.441124295057
Iteration 600: Loss = -11510.325252418683
Iteration 700: Loss = -11510.250085813741
Iteration 800: Loss = -11510.197935464463
Iteration 900: Loss = -11510.160066138036
Iteration 1000: Loss = -11510.131640639795
Iteration 1100: Loss = -11510.109636818112
Iteration 1200: Loss = -11510.092222600055
Iteration 1300: Loss = -11510.078226257565
Iteration 1400: Loss = -11510.066716022098
Iteration 1500: Loss = -11510.057172482975
Iteration 1600: Loss = -11510.049174711517
Iteration 1700: Loss = -11510.042317274823
Iteration 1800: Loss = -11510.036517250148
Iteration 1900: Loss = -11510.031405842987
Iteration 2000: Loss = -11510.026912782132
Iteration 2100: Loss = -11510.022699487152
Iteration 2200: Loss = -11510.018227461123
Iteration 2300: Loss = -11510.014746740118
Iteration 2400: Loss = -11510.011952467465
Iteration 2500: Loss = -11510.00952374183
Iteration 2600: Loss = -11510.007425328404
Iteration 2700: Loss = -11510.00546285356
Iteration 2800: Loss = -11510.003738503086
Iteration 2900: Loss = -11510.002138777867
Iteration 3000: Loss = -11510.000595197334
Iteration 3100: Loss = -11509.998739035032
Iteration 3200: Loss = -11509.9956419202
Iteration 3300: Loss = -11509.994379580567
Iteration 3400: Loss = -11509.993310395279
Iteration 3500: Loss = -11509.992347944177
Iteration 3600: Loss = -11509.99148894838
Iteration 3700: Loss = -11509.99192918864
1
Iteration 3800: Loss = -11509.990058666584
Iteration 3900: Loss = -11509.989368719365
Iteration 4000: Loss = -11509.988786919115
Iteration 4100: Loss = -11509.988189304351
Iteration 4200: Loss = -11509.987706694646
Iteration 4300: Loss = -11509.987215485819
Iteration 4400: Loss = -11509.98678646305
Iteration 4500: Loss = -11509.986741243956
Iteration 4600: Loss = -11509.98598096793
Iteration 4700: Loss = -11509.993023265208
1
Iteration 4800: Loss = -11509.985290712271
Iteration 4900: Loss = -11509.984964687612
Iteration 5000: Loss = -11509.985971105283
1
Iteration 5100: Loss = -11509.984405184783
Iteration 5200: Loss = -11509.984106569795
Iteration 5300: Loss = -11509.984373779513
1
Iteration 5400: Loss = -11509.983640371047
Iteration 5500: Loss = -11509.983453134668
Iteration 5600: Loss = -11509.983300022566
Iteration 5700: Loss = -11509.983053011536
Iteration 5800: Loss = -11509.982917879883
Iteration 5900: Loss = -11509.986034487674
1
Iteration 6000: Loss = -11509.98259133481
Iteration 6100: Loss = -11509.98242430307
Iteration 6200: Loss = -11509.98252542152
1
Iteration 6300: Loss = -11509.982154058038
Iteration 6400: Loss = -11509.982048028702
Iteration 6500: Loss = -11509.982088007206
Iteration 6600: Loss = -11509.981813728411
Iteration 6700: Loss = -11509.981728899857
Iteration 6800: Loss = -11509.983399221039
1
Iteration 6900: Loss = -11509.981551537003
Iteration 7000: Loss = -11509.981439237006
Iteration 7100: Loss = -11509.999304649413
1
Iteration 7200: Loss = -11509.98128196158
Iteration 7300: Loss = -11509.983120974537
1
Iteration 7400: Loss = -11509.98136810352
Iteration 7500: Loss = -11509.9823990427
1
Iteration 7600: Loss = -11509.981388369395
Iteration 7700: Loss = -11509.984741276427
1
Iteration 7800: Loss = -11509.980916332786
Iteration 7900: Loss = -11509.984946282517
1
Iteration 8000: Loss = -11509.981710980797
2
Iteration 8100: Loss = -11509.98073173904
Iteration 8200: Loss = -11509.980973856163
1
Iteration 8300: Loss = -11509.980908163563
2
Iteration 8400: Loss = -11509.980818998634
Iteration 8500: Loss = -11509.985071012889
1
Iteration 8600: Loss = -11509.983435078824
2
Iteration 8700: Loss = -11510.09385125213
3
Iteration 8800: Loss = -11509.980242263915
Iteration 8900: Loss = -11510.025768619611
1
Iteration 9000: Loss = -11509.980384016057
2
Iteration 9100: Loss = -11509.979895636741
Iteration 9200: Loss = -11509.98104004612
1
Iteration 9300: Loss = -11509.99417667604
2
Iteration 9400: Loss = -11509.979744213706
Iteration 9500: Loss = -11509.99071544349
1
Iteration 9600: Loss = -11509.980143492592
2
Iteration 9700: Loss = -11509.980326487448
3
Iteration 9800: Loss = -11509.981201924391
4
Iteration 9900: Loss = -11510.000573911258
5
Iteration 10000: Loss = -11510.013151610363
6
Iteration 10100: Loss = -11509.981837448666
7
Iteration 10200: Loss = -11509.98423125832
8
Iteration 10300: Loss = -11509.989962190213
9
Iteration 10400: Loss = -11509.982896426096
10
Iteration 10500: Loss = -11509.98345820078
11
Iteration 10600: Loss = -11509.98085839774
12
Iteration 10700: Loss = -11509.981185729364
13
Iteration 10800: Loss = -11510.005270555706
14
Iteration 10900: Loss = -11510.043094910565
15
Stopping early at iteration 10900 due to no improvement.
pi: tensor([[0.7852, 0.2148],
        [0.2160, 0.7840]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5398, 0.4602], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3988, 0.0914],
         [0.6849, 0.1945]],

        [[0.7124, 0.0945],
         [0.6125, 0.6850]],

        [[0.7243, 0.0997],
         [0.6286, 0.5522]],

        [[0.5335, 0.0992],
         [0.6747, 0.5019]],

        [[0.5829, 0.1042],
         [0.6578, 0.5451]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999997943784
Average Adjusted Rand Index: 0.9919993417272899
11516.907266362165
[0.9919999997943784, 0.9919999997943784] [0.9919993417272899, 0.9919993417272899] [11509.979015807216, 11510.043094910565]
-------------------------------------
This iteration is 73
True Objective function: Loss = -11626.697284900833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21614.954904030426
Iteration 100: Loss = -12419.090070043389
Iteration 200: Loss = -12215.630386273184
Iteration 300: Loss = -11696.022586406803
Iteration 400: Loss = -11653.746837277555
Iteration 500: Loss = -11626.007986361268
Iteration 600: Loss = -11618.427514979983
Iteration 700: Loss = -11618.069706182854
Iteration 800: Loss = -11617.853498636408
Iteration 900: Loss = -11617.708687267617
Iteration 1000: Loss = -11617.605284220183
Iteration 1100: Loss = -11617.527494620685
Iteration 1200: Loss = -11617.464456371405
Iteration 1300: Loss = -11617.38483854274
Iteration 1400: Loss = -11617.179987718413
Iteration 1500: Loss = -11617.148007366473
Iteration 1600: Loss = -11617.122617490226
Iteration 1700: Loss = -11617.101560996818
Iteration 1800: Loss = -11617.083314905809
Iteration 1900: Loss = -11617.067666481673
Iteration 2000: Loss = -11617.053838251728
Iteration 2100: Loss = -11617.041314893797
Iteration 2200: Loss = -11617.02944037792
Iteration 2300: Loss = -11617.018176357142
Iteration 2400: Loss = -11617.008884986093
Iteration 2500: Loss = -11617.001856233872
Iteration 2600: Loss = -11616.99422818248
Iteration 2700: Loss = -11616.989293392166
Iteration 2800: Loss = -11616.983541383956
Iteration 2900: Loss = -11616.97870865127
Iteration 3000: Loss = -11616.974910191686
Iteration 3100: Loss = -11616.970995054451
Iteration 3200: Loss = -11616.967640894189
Iteration 3300: Loss = -11616.96815811175
1
Iteration 3400: Loss = -11616.961896045432
Iteration 3500: Loss = -11616.959357494494
Iteration 3600: Loss = -11616.957323536608
Iteration 3700: Loss = -11616.955004110474
Iteration 3800: Loss = -11616.966317974027
1
Iteration 3900: Loss = -11616.951125802903
Iteration 4000: Loss = -11616.950046184864
Iteration 4100: Loss = -11616.947862385572
Iteration 4200: Loss = -11616.948946261155
1
Iteration 4300: Loss = -11616.945130397666
Iteration 4400: Loss = -11616.946166450178
1
Iteration 4500: Loss = -11616.942873005572
Iteration 4600: Loss = -11616.941627057224
Iteration 4700: Loss = -11616.94163346394
Iteration 4800: Loss = -11616.939608411918
Iteration 4900: Loss = -11616.938971442341
Iteration 5000: Loss = -11616.944006763366
1
Iteration 5100: Loss = -11616.93794996057
Iteration 5200: Loss = -11616.93767229298
Iteration 5300: Loss = -11616.936532348593
Iteration 5400: Loss = -11616.934798628474
Iteration 5500: Loss = -11616.934476520362
Iteration 5600: Loss = -11616.933528564805
Iteration 5700: Loss = -11616.932705245286
Iteration 5800: Loss = -11616.93171596486
Iteration 5900: Loss = -11616.93128481421
Iteration 6000: Loss = -11616.932262112437
1
Iteration 6100: Loss = -11616.932863376634
2
Iteration 6200: Loss = -11616.93032746359
Iteration 6300: Loss = -11616.92944906749
Iteration 6400: Loss = -11616.929077863535
Iteration 6500: Loss = -11616.928567890433
Iteration 6600: Loss = -11616.928224132345
Iteration 6700: Loss = -11616.92765652169
Iteration 6800: Loss = -11616.927701659817
Iteration 6900: Loss = -11616.928930743212
1
Iteration 7000: Loss = -11616.92423842211
Iteration 7100: Loss = -11616.922908378776
Iteration 7200: Loss = -11616.920853447917
Iteration 7300: Loss = -11616.923893381208
1
Iteration 7400: Loss = -11616.92105345908
2
Iteration 7500: Loss = -11616.920151740183
Iteration 7600: Loss = -11616.943098571031
1
Iteration 7700: Loss = -11616.807403714596
Iteration 7800: Loss = -11616.808044143583
1
Iteration 7900: Loss = -11616.807034890911
Iteration 8000: Loss = -11616.808146056848
1
Iteration 8100: Loss = -11616.806818085832
Iteration 8200: Loss = -11616.820728070834
1
Iteration 8300: Loss = -11616.806547757651
Iteration 8400: Loss = -11616.836571178186
1
Iteration 8500: Loss = -11616.806459703
Iteration 8600: Loss = -11616.807337524839
1
Iteration 8700: Loss = -11616.813197069723
2
Iteration 8800: Loss = -11616.814110905669
3
Iteration 8900: Loss = -11616.805966908158
Iteration 9000: Loss = -11616.808781695412
1
Iteration 9100: Loss = -11616.807049572484
2
Iteration 9200: Loss = -11616.813524044821
3
Iteration 9300: Loss = -11616.831203845437
4
Iteration 9400: Loss = -11616.817741632878
5
Iteration 9500: Loss = -11616.80588068795
Iteration 9600: Loss = -11616.805658765988
Iteration 9700: Loss = -11616.81532513515
1
Iteration 9800: Loss = -11616.82265716103
2
Iteration 9900: Loss = -11616.813595362133
3
Iteration 10000: Loss = -11616.851354105012
4
Iteration 10100: Loss = -11616.80529965087
Iteration 10200: Loss = -11616.808158450904
1
Iteration 10300: Loss = -11616.86940288463
2
Iteration 10400: Loss = -11616.859110577807
3
Iteration 10500: Loss = -11616.88730140041
4
Iteration 10600: Loss = -11616.815882666142
5
Iteration 10700: Loss = -11616.865757179994
6
Iteration 10800: Loss = -11616.805716177805
7
Iteration 10900: Loss = -11616.80851881564
8
Iteration 11000: Loss = -11616.805566951356
9
Iteration 11100: Loss = -11616.80521740999
Iteration 11200: Loss = -11616.806924600784
1
Iteration 11300: Loss = -11616.82018789623
2
Iteration 11400: Loss = -11616.805121332534
Iteration 11500: Loss = -11616.89204981202
1
Iteration 11600: Loss = -11616.815051557458
2
Iteration 11700: Loss = -11616.81408069972
3
Iteration 11800: Loss = -11616.807389268504
4
Iteration 11900: Loss = -11616.809743878906
5
Iteration 12000: Loss = -11616.805978782206
6
Iteration 12100: Loss = -11616.820970644156
7
Iteration 12200: Loss = -11616.810009186542
8
Iteration 12300: Loss = -11616.81300965183
9
Iteration 12400: Loss = -11616.807111377504
10
Iteration 12500: Loss = -11616.807161431907
11
Iteration 12600: Loss = -11616.839777863453
12
Iteration 12700: Loss = -11616.871426046973
13
Iteration 12800: Loss = -11616.806718104792
14
Iteration 12900: Loss = -11616.804341348263
Iteration 13000: Loss = -11616.805167969875
1
Iteration 13100: Loss = -11616.80503186667
2
Iteration 13200: Loss = -11616.814383697898
3
Iteration 13300: Loss = -11616.809731639287
4
Iteration 13400: Loss = -11616.804012434197
Iteration 13500: Loss = -11616.810453608163
1
Iteration 13600: Loss = -11616.823593270647
2
Iteration 13700: Loss = -11616.807065855015
3
Iteration 13800: Loss = -11616.80483070728
4
Iteration 13900: Loss = -11616.80439581088
5
Iteration 14000: Loss = -11616.808278084984
6
Iteration 14100: Loss = -11616.80514125261
7
Iteration 14200: Loss = -11616.830182949569
8
Iteration 14300: Loss = -11616.810448317701
9
Iteration 14400: Loss = -11616.805706003564
10
Iteration 14500: Loss = -11616.809028213098
11
Iteration 14600: Loss = -11616.804996300993
12
Iteration 14700: Loss = -11616.804253235818
13
Iteration 14800: Loss = -11616.878063164495
14
Iteration 14900: Loss = -11616.819813838327
15
Stopping early at iteration 14900 due to no improvement.
pi: tensor([[0.7855, 0.2145],
        [0.2178, 0.7822]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4572, 0.5428], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1919, 0.1073],
         [0.6806, 0.3991]],

        [[0.5335, 0.1045],
         [0.5620, 0.5435]],

        [[0.6984, 0.1068],
         [0.6498, 0.5780]],

        [[0.7245, 0.0968],
         [0.5725, 0.7239]],

        [[0.6956, 0.1030],
         [0.7032, 0.6246]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.984032031618886
Average Adjusted Rand Index: 0.9839993730966995
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20005.13876406007
Iteration 100: Loss = -12416.153781207026
Iteration 200: Loss = -12002.97111429056
Iteration 300: Loss = -11622.229383191065
Iteration 400: Loss = -11619.61145585995
Iteration 500: Loss = -11618.653331212163
Iteration 600: Loss = -11618.089510844213
Iteration 700: Loss = -11617.762586449004
Iteration 800: Loss = -11617.588330794872
Iteration 900: Loss = -11617.412974824989
Iteration 1000: Loss = -11617.32292869081
Iteration 1100: Loss = -11617.25947001561
Iteration 1200: Loss = -11617.210294488508
Iteration 1300: Loss = -11617.171458276438
Iteration 1400: Loss = -11617.1402619881
Iteration 1500: Loss = -11617.114621688299
Iteration 1600: Loss = -11617.093317751001
Iteration 1700: Loss = -11617.075507586449
Iteration 1800: Loss = -11617.060240076426
Iteration 1900: Loss = -11617.04722290449
Iteration 2000: Loss = -11617.036046579342
Iteration 2100: Loss = -11617.026361901926
Iteration 2200: Loss = -11617.017859125926
Iteration 2300: Loss = -11617.010343582402
Iteration 2400: Loss = -11617.004277617296
Iteration 2500: Loss = -11616.997776010345
Iteration 2600: Loss = -11616.992457168055
Iteration 2700: Loss = -11616.9877003874
Iteration 2800: Loss = -11616.98343216986
Iteration 2900: Loss = -11616.979539447508
Iteration 3000: Loss = -11616.990374582323
1
Iteration 3100: Loss = -11616.972751355426
Iteration 3200: Loss = -11616.97096468652
Iteration 3300: Loss = -11616.968135459096
Iteration 3400: Loss = -11616.965572822723
Iteration 3500: Loss = -11616.967460158517
1
Iteration 3600: Loss = -11616.960136797925
Iteration 3700: Loss = -11616.957971396836
Iteration 3800: Loss = -11616.956638590338
Iteration 3900: Loss = -11616.954056792296
Iteration 4000: Loss = -11616.952658306182
Iteration 4100: Loss = -11616.94923997243
Iteration 4200: Loss = -11616.943453902739
Iteration 4300: Loss = -11616.910682797134
Iteration 4400: Loss = -11616.822872924246
Iteration 4500: Loss = -11616.82323568784
1
Iteration 4600: Loss = -11616.81999584124
Iteration 4700: Loss = -11616.820424384174
1
Iteration 4800: Loss = -11616.818647173612
Iteration 4900: Loss = -11616.817016336241
Iteration 5000: Loss = -11616.816224023318
Iteration 5100: Loss = -11616.816563648106
1
Iteration 5200: Loss = -11616.81423902978
Iteration 5300: Loss = -11616.813539012272
Iteration 5400: Loss = -11616.812961359776
Iteration 5500: Loss = -11616.817991710288
1
Iteration 5600: Loss = -11616.813725871283
2
Iteration 5700: Loss = -11616.811549970467
Iteration 5800: Loss = -11616.81106663857
Iteration 5900: Loss = -11616.810693385762
Iteration 6000: Loss = -11616.810254320322
Iteration 6100: Loss = -11616.809927945773
Iteration 6200: Loss = -11616.809722297474
Iteration 6300: Loss = -11616.809655022653
Iteration 6400: Loss = -11616.81126941202
1
Iteration 6500: Loss = -11616.80892982907
Iteration 6600: Loss = -11616.80908591808
1
Iteration 6700: Loss = -11616.813501319673
2
Iteration 6800: Loss = -11616.807900108215
Iteration 6900: Loss = -11616.807810468845
Iteration 7000: Loss = -11616.807464097294
Iteration 7100: Loss = -11616.809273683462
1
Iteration 7200: Loss = -11616.807164536822
Iteration 7300: Loss = -11616.809148454418
1
Iteration 7400: Loss = -11616.806880766262
Iteration 7500: Loss = -11616.808751635139
1
Iteration 7600: Loss = -11616.80835195881
2
Iteration 7700: Loss = -11616.80878873094
3
Iteration 7800: Loss = -11616.806326258644
Iteration 7900: Loss = -11616.805995174498
Iteration 8000: Loss = -11616.806580278553
1
Iteration 8100: Loss = -11616.806255082041
2
Iteration 8200: Loss = -11616.805699357343
Iteration 8300: Loss = -11616.808295319246
1
Iteration 8400: Loss = -11616.81785934038
2
Iteration 8500: Loss = -11616.809542730538
3
Iteration 8600: Loss = -11616.806605701157
4
Iteration 8700: Loss = -11616.806908847175
5
Iteration 8800: Loss = -11616.806683073568
6
Iteration 8900: Loss = -11616.814221067745
7
Iteration 9000: Loss = -11616.831091410193
8
Iteration 9100: Loss = -11616.879877691543
9
Iteration 9200: Loss = -11616.807025655924
10
Iteration 9300: Loss = -11616.812688195512
11
Iteration 9400: Loss = -11616.810638098304
12
Iteration 9500: Loss = -11616.843261420016
13
Iteration 9600: Loss = -11616.804746133124
Iteration 9700: Loss = -11616.80663413826
1
Iteration 9800: Loss = -11616.808536894488
2
Iteration 9900: Loss = -11616.919699691165
3
Iteration 10000: Loss = -11616.805429151536
4
Iteration 10100: Loss = -11616.804578175299
Iteration 10200: Loss = -11616.818333822344
1
Iteration 10300: Loss = -11616.809011204414
2
Iteration 10400: Loss = -11616.822667260141
3
Iteration 10500: Loss = -11616.809807253881
4
Iteration 10600: Loss = -11616.805620755027
5
Iteration 10700: Loss = -11616.804495655631
Iteration 10800: Loss = -11616.808134475748
1
Iteration 10900: Loss = -11616.805787205343
2
Iteration 11000: Loss = -11616.804830081319
3
Iteration 11100: Loss = -11616.814158605144
4
Iteration 11200: Loss = -11616.814839003971
5
Iteration 11300: Loss = -11616.805173188159
6
Iteration 11400: Loss = -11616.809181362227
7
Iteration 11500: Loss = -11616.81198034903
8
Iteration 11600: Loss = -11616.812358351252
9
Iteration 11700: Loss = -11616.826302763504
10
Iteration 11800: Loss = -11616.809761109977
11
Iteration 11900: Loss = -11616.819632970752
12
Iteration 12000: Loss = -11616.90417284481
13
Iteration 12100: Loss = -11616.807437532692
14
Iteration 12200: Loss = -11616.822572661467
15
Stopping early at iteration 12200 due to no improvement.
pi: tensor([[0.7876, 0.2124],
        [0.2156, 0.7844]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4544, 0.5456], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1924, 0.1068],
         [0.6018, 0.3973]],

        [[0.7080, 0.1039],
         [0.6220, 0.6338]],

        [[0.5822, 0.1064],
         [0.6539, 0.5220]],

        [[0.7197, 0.0965],
         [0.5358, 0.6214]],

        [[0.5591, 0.1028],
         [0.6015, 0.6340]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.984032031618886
Average Adjusted Rand Index: 0.9839993730966995
11626.697284900833
[0.984032031618886, 0.984032031618886] [0.9839993730966995, 0.9839993730966995] [11616.819813838327, 11616.822572661467]
-------------------------------------
This iteration is 74
True Objective function: Loss = -11702.58571048985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20279.09826204612
Iteration 100: Loss = -12677.825476617041
Iteration 200: Loss = -12665.29804038417
Iteration 300: Loss = -11906.300563008044
Iteration 400: Loss = -11822.695709068565
Iteration 500: Loss = -11771.827498046103
Iteration 600: Loss = -11737.638429093
Iteration 700: Loss = -11713.756736252268
Iteration 800: Loss = -11711.742553647657
Iteration 900: Loss = -11711.408388395595
Iteration 1000: Loss = -11702.880046744829
Iteration 1100: Loss = -11702.82712699517
Iteration 1200: Loss = -11702.778121527983
Iteration 1300: Loss = -11695.489462419391
Iteration 1400: Loss = -11695.459762979543
Iteration 1500: Loss = -11695.440339240611
Iteration 1600: Loss = -11695.424644913424
Iteration 1700: Loss = -11695.411695465762
Iteration 1800: Loss = -11695.40074533109
Iteration 1900: Loss = -11695.391315892659
Iteration 2000: Loss = -11695.383237090307
Iteration 2100: Loss = -11695.376225118674
Iteration 2200: Loss = -11695.370200700509
Iteration 2300: Loss = -11695.36496695814
Iteration 2400: Loss = -11695.360358252496
Iteration 2500: Loss = -11695.356326271438
Iteration 2600: Loss = -11695.352647906133
Iteration 2700: Loss = -11695.34936223106
Iteration 2800: Loss = -11695.346417942681
Iteration 2900: Loss = -11695.343685895628
Iteration 3000: Loss = -11695.34119122253
Iteration 3100: Loss = -11695.338936899205
Iteration 3200: Loss = -11695.337190210594
Iteration 3300: Loss = -11695.335023594002
Iteration 3400: Loss = -11695.33460218643
Iteration 3500: Loss = -11695.331819785619
Iteration 3600: Loss = -11695.330431353323
Iteration 3700: Loss = -11695.34841486116
1
Iteration 3800: Loss = -11695.32940199814
Iteration 3900: Loss = -11695.326831703089
Iteration 4000: Loss = -11695.325963995987
Iteration 4100: Loss = -11695.324898891786
Iteration 4200: Loss = -11695.324410853735
Iteration 4300: Loss = -11695.32320636373
Iteration 4400: Loss = -11695.324348608307
1
Iteration 4500: Loss = -11695.32160085939
Iteration 4600: Loss = -11695.320934089757
Iteration 4700: Loss = -11695.320168431912
Iteration 4800: Loss = -11695.31863509247
Iteration 4900: Loss = -11695.312609826742
Iteration 5000: Loss = -11695.301985006578
Iteration 5100: Loss = -11695.301422755681
Iteration 5200: Loss = -11695.301040112176
Iteration 5300: Loss = -11695.30054810133
Iteration 5400: Loss = -11695.3003255566
Iteration 5500: Loss = -11695.299803384592
Iteration 5600: Loss = -11695.299675365039
Iteration 5700: Loss = -11695.299094650982
Iteration 5800: Loss = -11695.298903338513
Iteration 5900: Loss = -11695.298600629878
Iteration 6000: Loss = -11695.298313824615
Iteration 6100: Loss = -11695.298354510052
Iteration 6200: Loss = -11695.297834165944
Iteration 6300: Loss = -11695.297647681336
Iteration 6400: Loss = -11695.297711953268
Iteration 6500: Loss = -11695.298100434473
1
Iteration 6600: Loss = -11695.333013049825
2
Iteration 6700: Loss = -11695.298034229018
3
Iteration 6800: Loss = -11695.304060085358
4
Iteration 6900: Loss = -11695.296553216922
Iteration 7000: Loss = -11695.296745026062
1
Iteration 7100: Loss = -11695.29694935711
2
Iteration 7200: Loss = -11695.2963556567
Iteration 7300: Loss = -11695.320686786563
1
Iteration 7400: Loss = -11695.296234977559
Iteration 7500: Loss = -11695.295862584562
Iteration 7600: Loss = -11695.297016452698
1
Iteration 7700: Loss = -11695.304667323353
2
Iteration 7800: Loss = -11695.29564311748
Iteration 7900: Loss = -11695.300068364962
1
Iteration 8000: Loss = -11695.295413047266
Iteration 8100: Loss = -11695.295269425756
Iteration 8200: Loss = -11695.33051487754
1
Iteration 8300: Loss = -11695.294989032167
Iteration 8400: Loss = -11695.2955066319
1
Iteration 8500: Loss = -11695.306088798674
2
Iteration 8600: Loss = -11695.294872831617
Iteration 8700: Loss = -11695.296685825258
1
Iteration 8800: Loss = -11695.297201514291
2
Iteration 8900: Loss = -11695.300371509407
3
Iteration 9000: Loss = -11695.331568082183
4
Iteration 9100: Loss = -11695.299981171076
5
Iteration 9200: Loss = -11695.297602518458
6
Iteration 9300: Loss = -11695.294450028185
Iteration 9400: Loss = -11695.296015123457
1
Iteration 9500: Loss = -11695.29519895528
2
Iteration 9600: Loss = -11695.295539040992
3
Iteration 9700: Loss = -11695.29793000317
4
Iteration 9800: Loss = -11695.295348745338
5
Iteration 9900: Loss = -11695.346360464906
6
Iteration 10000: Loss = -11695.306339609222
7
Iteration 10100: Loss = -11695.329802081082
8
Iteration 10200: Loss = -11695.301952179227
9
Iteration 10300: Loss = -11695.294473131604
Iteration 10400: Loss = -11695.301858527515
1
Iteration 10500: Loss = -11695.298330110929
2
Iteration 10600: Loss = -11695.29641389871
3
Iteration 10700: Loss = -11695.308276861815
4
Iteration 10800: Loss = -11695.313563552589
5
Iteration 10900: Loss = -11695.302987303436
6
Iteration 11000: Loss = -11695.334140239607
7
Iteration 11100: Loss = -11695.32764163317
8
Iteration 11200: Loss = -11695.296750115704
9
Iteration 11300: Loss = -11695.302022489455
10
Iteration 11400: Loss = -11695.300776589438
11
Iteration 11500: Loss = -11695.29708818809
12
Iteration 11600: Loss = -11695.333349990882
13
Iteration 11700: Loss = -11695.297372173123
14
Iteration 11800: Loss = -11695.319957164485
15
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[0.7729, 0.2271],
        [0.2732, 0.7268]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5502, 0.4498], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4011, 0.0954],
         [0.5557, 0.1969]],

        [[0.7128, 0.0905],
         [0.5813, 0.6819]],

        [[0.7181, 0.0934],
         [0.6510, 0.5106]],

        [[0.6642, 0.1038],
         [0.7304, 0.6059]],

        [[0.6163, 0.1017],
         [0.5639, 0.6192]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9840312350384399
Average Adjusted Rand Index: 0.9841611274691303
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20311.47254262114
Iteration 100: Loss = -12660.307667792194
Iteration 200: Loss = -12471.837628055087
Iteration 300: Loss = -12195.881599687596
Iteration 400: Loss = -12189.822911390564
Iteration 500: Loss = -12135.701938630424
Iteration 600: Loss = -12076.275981599641
Iteration 700: Loss = -12051.60742245477
Iteration 800: Loss = -12041.960626880562
Iteration 900: Loss = -12038.01878507758
Iteration 1000: Loss = -12021.714780640537
Iteration 1100: Loss = -12016.498673821194
Iteration 1200: Loss = -12016.44265554434
Iteration 1300: Loss = -12016.363341042748
Iteration 1400: Loss = -12015.989594618553
Iteration 1500: Loss = -12015.972207574125
Iteration 1600: Loss = -12015.898566840326
Iteration 1700: Loss = -12015.875046200374
Iteration 1800: Loss = -12015.868738675088
Iteration 1900: Loss = -12015.86361121431
Iteration 2000: Loss = -12015.859278909918
Iteration 2100: Loss = -12015.855580671332
Iteration 2200: Loss = -12015.853759641039
Iteration 2300: Loss = -12015.849724081803
Iteration 2400: Loss = -12015.847274202617
Iteration 2500: Loss = -12015.847167097227
Iteration 2600: Loss = -12015.84314742806
Iteration 2700: Loss = -12015.841310269825
Iteration 2800: Loss = -12015.842937912876
1
Iteration 2900: Loss = -12015.837895470453
Iteration 3000: Loss = -12015.845048690444
1
Iteration 3100: Loss = -12015.848409405278
2
Iteration 3200: Loss = -12015.835772525443
Iteration 3300: Loss = -12015.832422327421
Iteration 3400: Loss = -12015.832582038836
1
Iteration 3500: Loss = -12015.830248730292
Iteration 3600: Loss = -12015.834380312363
1
Iteration 3700: Loss = -12015.84285197249
2
Iteration 3800: Loss = -12015.82696249653
Iteration 3900: Loss = -12015.660580314092
Iteration 4000: Loss = -12015.668655090285
1
Iteration 4100: Loss = -12015.666328128264
2
Iteration 4200: Loss = -12015.656891779858
Iteration 4300: Loss = -12015.655116942642
Iteration 4400: Loss = -12015.66953401466
1
Iteration 4500: Loss = -12015.654062882288
Iteration 4600: Loss = -12015.653845158073
Iteration 4700: Loss = -12015.479306540661
Iteration 4800: Loss = -12015.477221064964
Iteration 4900: Loss = -12015.476172047716
Iteration 5000: Loss = -12015.475829840614
Iteration 5100: Loss = -12015.475710262188
Iteration 5200: Loss = -12015.47540387632
Iteration 5300: Loss = -12015.478292165406
1
Iteration 5400: Loss = -12015.474964173338
Iteration 5500: Loss = -12015.47573027634
1
Iteration 5600: Loss = -12015.475307093988
2
Iteration 5700: Loss = -12015.474792747702
Iteration 5800: Loss = -12015.474851410758
Iteration 5900: Loss = -12015.477822768928
1
Iteration 6000: Loss = -12015.474276481604
Iteration 6100: Loss = -12015.474583654843
1
Iteration 6200: Loss = -12015.47423019602
Iteration 6300: Loss = -12015.473783366717
Iteration 6400: Loss = -12015.474147807081
1
Iteration 6500: Loss = -12015.473869650397
Iteration 6600: Loss = -12015.473916304816
Iteration 6700: Loss = -12015.473475677554
Iteration 6800: Loss = -12015.473450063602
Iteration 6900: Loss = -12015.47367924281
1
Iteration 7000: Loss = -12015.488095321027
2
Iteration 7100: Loss = -12015.473104108058
Iteration 7200: Loss = -12015.473086271486
Iteration 7300: Loss = -12015.473047986268
Iteration 7400: Loss = -12015.472929088884
Iteration 7500: Loss = -12015.478836504877
1
Iteration 7600: Loss = -12015.47286274797
Iteration 7700: Loss = -12015.472807517459
Iteration 7800: Loss = -12015.474229685162
1
Iteration 7900: Loss = -12015.472710480875
Iteration 8000: Loss = -12015.472631279801
Iteration 8100: Loss = -12015.47265139516
Iteration 8200: Loss = -12015.472597122933
Iteration 8300: Loss = -12015.472549518443
Iteration 8400: Loss = -12015.473119727298
1
Iteration 8500: Loss = -12015.472507178816
Iteration 8600: Loss = -12015.472469980967
Iteration 8700: Loss = -12015.757429479876
1
Iteration 8800: Loss = -12015.472377469852
Iteration 8900: Loss = -12015.472913301237
1
Iteration 9000: Loss = -12015.472610280787
2
Iteration 9100: Loss = -12015.472405649394
Iteration 9200: Loss = -12015.475809473035
1
Iteration 9300: Loss = -12015.472433315352
Iteration 9400: Loss = -12015.472407696741
Iteration 9500: Loss = -12015.481731827722
1
Iteration 9600: Loss = -12015.472987378413
2
Iteration 9700: Loss = -12015.472736438236
3
Iteration 9800: Loss = -12015.50096524005
4
Iteration 9900: Loss = -12015.551251493775
5
Iteration 10000: Loss = -12015.47961321913
6
Iteration 10100: Loss = -12015.47227309585
Iteration 10200: Loss = -12015.476390853835
1
Iteration 10300: Loss = -12015.591595898017
2
Iteration 10400: Loss = -12015.482634394762
3
Iteration 10500: Loss = -12015.472475271185
4
Iteration 10600: Loss = -12015.476504520244
5
Iteration 10700: Loss = -12015.48807586658
6
Iteration 10800: Loss = -12015.472481019759
7
Iteration 10900: Loss = -12015.47321014071
8
Iteration 11000: Loss = -12015.475135769675
9
Iteration 11100: Loss = -12015.4907626619
10
Iteration 11200: Loss = -12015.474238841458
11
Iteration 11300: Loss = -12015.485316806693
12
Iteration 11400: Loss = -12015.47198803659
Iteration 11500: Loss = -12015.481099818848
1
Iteration 11600: Loss = -12015.479405146363
2
Iteration 11700: Loss = -12015.472896562665
3
Iteration 11800: Loss = -12015.476903634817
4
Iteration 11900: Loss = -12015.53706709828
5
Iteration 12000: Loss = -12015.472002733059
Iteration 12100: Loss = -12015.47199806843
Iteration 12200: Loss = -12015.563810632908
1
Iteration 12300: Loss = -12015.47191965436
Iteration 12400: Loss = -12015.587318487344
1
Iteration 12500: Loss = -12015.470567267885
Iteration 12600: Loss = -12015.475299006099
1
Iteration 12700: Loss = -12015.473831740326
2
Iteration 12800: Loss = -12015.470600895718
Iteration 12900: Loss = -12015.529022812598
1
Iteration 13000: Loss = -12015.472589147783
2
Iteration 13100: Loss = -12015.478981675758
3
Iteration 13200: Loss = -12015.478685455142
4
Iteration 13300: Loss = -12015.568604321557
5
Iteration 13400: Loss = -12015.470789714815
6
Iteration 13500: Loss = -12015.485995233526
7
Iteration 13600: Loss = -12015.471552569965
8
Iteration 13700: Loss = -12015.476715317849
9
Iteration 13800: Loss = -12015.478069487348
10
Iteration 13900: Loss = -12015.47049556367
Iteration 14000: Loss = -12015.471284205973
1
Iteration 14100: Loss = -12015.470507249052
Iteration 14200: Loss = -12015.479339544543
1
Iteration 14300: Loss = -12015.493654109006
2
Iteration 14400: Loss = -12015.477965723889
3
Iteration 14500: Loss = -12015.471525283632
4
Iteration 14600: Loss = -12015.470951999547
5
Iteration 14700: Loss = -12015.470684729846
6
Iteration 14800: Loss = -12015.472852916584
7
Iteration 14900: Loss = -12015.477911580452
8
Iteration 15000: Loss = -12015.470503487832
Iteration 15100: Loss = -12015.470485057274
Iteration 15200: Loss = -12015.573460502234
1
Iteration 15300: Loss = -12015.470440510213
Iteration 15400: Loss = -12015.541927741826
1
Iteration 15500: Loss = -12015.470445809173
Iteration 15600: Loss = -12015.476551121246
1
Iteration 15700: Loss = -12015.476177328204
2
Iteration 15800: Loss = -12015.471176824838
3
Iteration 15900: Loss = -12015.474300311538
4
Iteration 16000: Loss = -12015.475312137325
5
Iteration 16100: Loss = -12015.475552178954
6
Iteration 16200: Loss = -12015.523643298562
7
Iteration 16300: Loss = -12015.471144192823
8
Iteration 16400: Loss = -12015.549878403233
9
Iteration 16500: Loss = -12015.47021482728
Iteration 16600: Loss = -12015.555073082553
1
Iteration 16700: Loss = -12015.470201404556
Iteration 16800: Loss = -12015.470209089697
Iteration 16900: Loss = -12015.471126667475
1
Iteration 17000: Loss = -12015.473992938169
2
Iteration 17100: Loss = -12015.512953573241
3
Iteration 17200: Loss = -12015.547908797
4
Iteration 17300: Loss = -12015.477224044149
5
Iteration 17400: Loss = -12015.478013617969
6
Iteration 17500: Loss = -12015.613391410985
7
Iteration 17600: Loss = -12015.479538652608
8
Iteration 17700: Loss = -12015.471169380276
9
Iteration 17800: Loss = -12015.472745775554
10
Iteration 17900: Loss = -12015.526623686432
11
Iteration 18000: Loss = -12015.47366682041
12
Iteration 18100: Loss = -12015.48158412256
13
Iteration 18200: Loss = -12015.541978774108
14
Iteration 18300: Loss = -12015.479799382043
15
Stopping early at iteration 18300 due to no improvement.
pi: tensor([[0.4693, 0.5307],
        [0.4610, 0.5390]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5711, 0.4289], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3409, 0.0928],
         [0.5900, 0.2980]],

        [[0.6083, 0.0900],
         [0.6803, 0.5405]],

        [[0.6399, 0.0915],
         [0.7037, 0.6188]],

        [[0.6931, 0.0986],
         [0.5407, 0.6168]],

        [[0.6523, 0.1006],
         [0.6421, 0.7290]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.882296193749233
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7370212140872312
time is 4
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.04828005583506332
Average Adjusted Rand Index: 0.8921862251980393
11702.58571048985
[0.9840312350384399, 0.04828005583506332] [0.9841611274691303, 0.8921862251980393] [11695.319957164485, 12015.479799382043]
-------------------------------------
This iteration is 75
True Objective function: Loss = -11801.974341317185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22028.49877290599
Iteration 100: Loss = -12659.230226028285
Iteration 200: Loss = -12642.54534651776
Iteration 300: Loss = -12573.881720116666
Iteration 400: Loss = -11951.019933489672
Iteration 500: Loss = -11889.759590935824
Iteration 600: Loss = -11848.038617897311
Iteration 700: Loss = -11835.931316661123
Iteration 800: Loss = -11814.900210970127
Iteration 900: Loss = -11814.656671040093
Iteration 1000: Loss = -11804.867426336481
Iteration 1100: Loss = -11804.809769105068
Iteration 1200: Loss = -11804.76661695084
Iteration 1300: Loss = -11804.727356097761
Iteration 1400: Loss = -11796.899158004659
Iteration 1500: Loss = -11796.875392975007
Iteration 1600: Loss = -11796.85837445317
Iteration 1700: Loss = -11796.844229476574
Iteration 1800: Loss = -11796.832320901687
Iteration 1900: Loss = -11796.822030340405
Iteration 2000: Loss = -11796.812829789345
Iteration 2100: Loss = -11796.802780883389
Iteration 2200: Loss = -11796.757106659496
Iteration 2300: Loss = -11796.749647726761
Iteration 2400: Loss = -11796.74433895807
Iteration 2500: Loss = -11796.73978319205
Iteration 2600: Loss = -11796.735745077927
Iteration 2700: Loss = -11796.732146211756
Iteration 2800: Loss = -11796.72874031593
Iteration 2900: Loss = -11796.725683525066
Iteration 3000: Loss = -11796.722820506855
Iteration 3100: Loss = -11796.719926396474
Iteration 3200: Loss = -11796.71678328781
Iteration 3300: Loss = -11796.71347267467
Iteration 3400: Loss = -11796.71437882101
1
Iteration 3500: Loss = -11796.708203460386
Iteration 3600: Loss = -11796.706691858748
Iteration 3700: Loss = -11796.70522546002
Iteration 3800: Loss = -11796.703894524366
Iteration 3900: Loss = -11796.71706155566
1
Iteration 4000: Loss = -11796.701660343422
Iteration 4100: Loss = -11796.700607963783
Iteration 4200: Loss = -11796.706003868023
1
Iteration 4300: Loss = -11796.698813171368
Iteration 4400: Loss = -11796.697969519802
Iteration 4500: Loss = -11796.71043073531
1
Iteration 4600: Loss = -11796.69651917355
Iteration 4700: Loss = -11796.695833446272
Iteration 4800: Loss = -11796.695903520787
Iteration 4900: Loss = -11796.694664861996
Iteration 5000: Loss = -11796.694085803918
Iteration 5100: Loss = -11796.69359422286
Iteration 5200: Loss = -11796.693139807438
Iteration 5300: Loss = -11796.692655761339
Iteration 5400: Loss = -11796.692252352937
Iteration 5500: Loss = -11796.69190374144
Iteration 5600: Loss = -11796.691473275445
Iteration 5700: Loss = -11796.691180275842
Iteration 5800: Loss = -11796.691309734764
1
Iteration 5900: Loss = -11796.690485039167
Iteration 6000: Loss = -11796.6902085544
Iteration 6100: Loss = -11796.68985757712
Iteration 6200: Loss = -11796.691006016492
1
Iteration 6300: Loss = -11796.689347830787
Iteration 6400: Loss = -11796.68936670732
Iteration 6500: Loss = -11796.689088966457
Iteration 6600: Loss = -11796.68935330517
1
Iteration 6700: Loss = -11796.694333365858
2
Iteration 6800: Loss = -11796.688580001852
Iteration 6900: Loss = -11796.688886167307
1
Iteration 7000: Loss = -11796.692893334974
2
Iteration 7100: Loss = -11796.75845765295
3
Iteration 7200: Loss = -11796.68766372215
Iteration 7300: Loss = -11796.768869754085
1
Iteration 7400: Loss = -11796.687388528808
Iteration 7500: Loss = -11796.712444140274
1
Iteration 7600: Loss = -11796.681518065336
Iteration 7700: Loss = -11796.684729613433
1
Iteration 7800: Loss = -11796.680450340371
Iteration 7900: Loss = -11796.758804022611
1
Iteration 8000: Loss = -11796.68035755294
Iteration 8100: Loss = -11796.680505871273
1
Iteration 8200: Loss = -11796.68031322001
Iteration 8300: Loss = -11796.684363929002
1
Iteration 8400: Loss = -11796.68535628738
2
Iteration 8500: Loss = -11796.680024547375
Iteration 8600: Loss = -11796.714971478912
1
Iteration 8700: Loss = -11796.681428554444
2
Iteration 8800: Loss = -11796.787016944067
3
Iteration 8900: Loss = -11796.690578762791
4
Iteration 9000: Loss = -11796.683030211505
5
Iteration 9100: Loss = -11796.680611312644
6
Iteration 9200: Loss = -11796.679522738776
Iteration 9300: Loss = -11796.679459627572
Iteration 9400: Loss = -11796.679919087475
1
Iteration 9500: Loss = -11796.680438203237
2
Iteration 9600: Loss = -11796.679793828047
3
Iteration 9700: Loss = -11796.680198313252
4
Iteration 9800: Loss = -11796.703789581034
5
Iteration 9900: Loss = -11796.702986152774
6
Iteration 10000: Loss = -11796.682565302975
7
Iteration 10100: Loss = -11796.680398524566
8
Iteration 10200: Loss = -11796.70073873385
9
Iteration 10300: Loss = -11796.704140903597
10
Iteration 10400: Loss = -11796.696965050778
11
Iteration 10500: Loss = -11796.679155675845
Iteration 10600: Loss = -11796.68356924055
1
Iteration 10700: Loss = -11796.686724138448
2
Iteration 10800: Loss = -11796.702099280117
3
Iteration 10900: Loss = -11796.679685123408
4
Iteration 11000: Loss = -11796.679786256758
5
Iteration 11100: Loss = -11796.686032410744
6
Iteration 11200: Loss = -11796.67905515966
Iteration 11300: Loss = -11796.687750231893
1
Iteration 11400: Loss = -11796.698420225204
2
Iteration 11500: Loss = -11796.724359495585
3
Iteration 11600: Loss = -11796.681574580862
4
Iteration 11700: Loss = -11796.679486122222
5
Iteration 11800: Loss = -11796.67886169033
Iteration 11900: Loss = -11796.741829509947
1
Iteration 12000: Loss = -11796.696739562365
2
Iteration 12100: Loss = -11796.678946179973
Iteration 12200: Loss = -11796.67893944036
Iteration 12300: Loss = -11796.694053166111
1
Iteration 12400: Loss = -11796.69323844634
2
Iteration 12500: Loss = -11796.688644302041
3
Iteration 12600: Loss = -11796.690056709727
4
Iteration 12700: Loss = -11796.694959844088
5
Iteration 12800: Loss = -11796.690406901813
6
Iteration 12900: Loss = -11796.694791933254
7
Iteration 13000: Loss = -11796.680891170203
8
Iteration 13100: Loss = -11796.739756162415
9
Iteration 13200: Loss = -11796.679860605262
10
Iteration 13300: Loss = -11796.681776571797
11
Iteration 13400: Loss = -11796.702175363262
12
Iteration 13500: Loss = -11796.68171464981
13
Iteration 13600: Loss = -11796.686706539625
14
Iteration 13700: Loss = -11796.684506640233
15
Stopping early at iteration 13700 due to no improvement.
pi: tensor([[0.7374, 0.2626],
        [0.2406, 0.7594]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5604, 0.4396], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4019, 0.1026],
         [0.5367, 0.2118]],

        [[0.6729, 0.0937],
         [0.5401, 0.5954]],

        [[0.6917, 0.1011],
         [0.6968, 0.7193]],

        [[0.6337, 0.1068],
         [0.5678, 0.6747]],

        [[0.6096, 0.1012],
         [0.5714, 0.6526]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21970.789583876536
Iteration 100: Loss = -12658.448200281671
Iteration 200: Loss = -12645.839509287347
Iteration 300: Loss = -12592.459160068927
Iteration 400: Loss = -12191.9517621671
Iteration 500: Loss = -12168.622485545502
Iteration 600: Loss = -12124.924032050827
Iteration 700: Loss = -12105.82310177637
Iteration 800: Loss = -12103.669849812326
Iteration 900: Loss = -12096.820735042596
Iteration 1000: Loss = -12096.47195004777
Iteration 1100: Loss = -12096.178800326277
Iteration 1200: Loss = -12096.062094322204
Iteration 1300: Loss = -12091.17295976462
Iteration 1400: Loss = -12091.069796575417
Iteration 1500: Loss = -12091.035882828366
Iteration 1600: Loss = -12090.978510067787
Iteration 1700: Loss = -12083.039173741756
Iteration 1800: Loss = -12081.95661080572
Iteration 1900: Loss = -12081.930162577202
Iteration 2000: Loss = -12081.916882303312
Iteration 2100: Loss = -12081.90639711941
Iteration 2200: Loss = -12081.897719141476
Iteration 2300: Loss = -12081.890120249564
Iteration 2400: Loss = -12081.883169376732
Iteration 2500: Loss = -12081.876463862503
Iteration 2600: Loss = -12081.866665116682
Iteration 2700: Loss = -12081.740273149246
Iteration 2800: Loss = -12079.879072825168
Iteration 2900: Loss = -12079.857629925667
Iteration 3000: Loss = -12079.831724278332
Iteration 3100: Loss = -12073.106120034852
Iteration 3200: Loss = -12073.085128322069
Iteration 3300: Loss = -12073.079471185827
Iteration 3400: Loss = -12073.079594524472
1
Iteration 3500: Loss = -12073.073180057194
Iteration 3600: Loss = -12073.070054260657
Iteration 3700: Loss = -12073.065884632348
Iteration 3800: Loss = -12073.059323070653
Iteration 3900: Loss = -12073.037070354185
Iteration 4000: Loss = -12071.979489421497
Iteration 4100: Loss = -12071.97092248194
Iteration 4200: Loss = -12071.967804380545
Iteration 4300: Loss = -12071.965901038986
Iteration 4400: Loss = -12071.975648044589
1
Iteration 4500: Loss = -12071.963305393801
Iteration 4600: Loss = -12071.961659045359
Iteration 4700: Loss = -12071.951509435867
Iteration 4800: Loss = -12066.254351163452
Iteration 4900: Loss = -12066.209533416328
Iteration 5000: Loss = -12066.092318856308
Iteration 5100: Loss = -12059.808665004735
Iteration 5200: Loss = -12059.805476027894
Iteration 5300: Loss = -12059.804649891577
Iteration 5400: Loss = -12059.80392220183
Iteration 5500: Loss = -12059.803297644543
Iteration 5600: Loss = -12059.803188272566
Iteration 5700: Loss = -12059.802207860546
Iteration 5800: Loss = -12059.801709059962
Iteration 5900: Loss = -12059.802112875772
1
Iteration 6000: Loss = -12059.808988805615
2
Iteration 6100: Loss = -12059.80101202928
Iteration 6200: Loss = -12059.800353951487
Iteration 6300: Loss = -12059.800873158612
1
Iteration 6400: Loss = -12059.618942451014
Iteration 6500: Loss = -12055.922292756448
Iteration 6600: Loss = -12055.918435600774
Iteration 6700: Loss = -12055.918118485384
Iteration 6800: Loss = -12055.919715041107
1
Iteration 6900: Loss = -12055.917431713697
Iteration 7000: Loss = -12055.917197974684
Iteration 7100: Loss = -12055.915604522903
Iteration 7200: Loss = -12055.903990633518
Iteration 7300: Loss = -12055.905208713271
1
Iteration 7400: Loss = -12055.890393451205
Iteration 7500: Loss = -12055.8786561941
Iteration 7600: Loss = -12055.888979502764
1
Iteration 7700: Loss = -12055.863908224208
Iteration 7800: Loss = -12055.863998057805
Iteration 7900: Loss = -12055.86362698795
Iteration 8000: Loss = -12055.863506621454
Iteration 8100: Loss = -12055.863193776031
Iteration 8200: Loss = -12055.863789979645
1
Iteration 8300: Loss = -12055.860936792182
Iteration 8400: Loss = -12055.82383384193
Iteration 8500: Loss = -12055.823644913353
Iteration 8600: Loss = -12055.82357028518
Iteration 8700: Loss = -12055.82347930636
Iteration 8800: Loss = -12055.823510892507
Iteration 8900: Loss = -12055.823381840082
Iteration 9000: Loss = -12055.840150916438
1
Iteration 9100: Loss = -12055.823233582367
Iteration 9200: Loss = -12055.827491317354
1
Iteration 9300: Loss = -12055.825103024323
2
Iteration 9400: Loss = -12055.83081205545
3
Iteration 9500: Loss = -12055.823054535562
Iteration 9600: Loss = -12055.823349491575
1
Iteration 9700: Loss = -12055.823286815254
2
Iteration 9800: Loss = -12055.827618171586
3
Iteration 9900: Loss = -12055.823639871127
4
Iteration 10000: Loss = -12055.830440551363
5
Iteration 10100: Loss = -12055.825483047442
6
Iteration 10200: Loss = -12055.83060966692
7
Iteration 10300: Loss = -12055.875285084076
8
Iteration 10400: Loss = -12055.870786239397
9
Iteration 10500: Loss = -12055.84084288173
10
Iteration 10600: Loss = -12055.822807674562
Iteration 10700: Loss = -12055.823290299953
1
Iteration 10800: Loss = -12055.825786807003
2
Iteration 10900: Loss = -12055.854978958041
3
Iteration 11000: Loss = -12055.826122977944
4
Iteration 11100: Loss = -12055.824838835922
5
Iteration 11200: Loss = -12055.822942120583
6
Iteration 11300: Loss = -12055.823640935074
7
Iteration 11400: Loss = -12055.838219850777
8
Iteration 11500: Loss = -12055.823525981481
9
Iteration 11600: Loss = -12056.038442152516
10
Iteration 11700: Loss = -12055.823256644064
11
Iteration 11800: Loss = -12056.267325840892
12
Iteration 11900: Loss = -12055.822391453221
Iteration 12000: Loss = -12055.831289296846
1
Iteration 12100: Loss = -12055.822295832362
Iteration 12200: Loss = -12055.825339565665
1
Iteration 12300: Loss = -12055.924422259262
2
Iteration 12400: Loss = -12055.826630747099
3
Iteration 12500: Loss = -12055.940612698872
4
Iteration 12600: Loss = -12055.821664377356
Iteration 12700: Loss = -12055.862435859417
1
Iteration 12800: Loss = -12055.847649021121
2
Iteration 12900: Loss = -12055.85287251858
3
Iteration 13000: Loss = -12055.821800343267
4
Iteration 13100: Loss = -12055.861234734684
5
Iteration 13200: Loss = -12055.821619903078
Iteration 13300: Loss = -12056.019963288221
1
Iteration 13400: Loss = -12055.841805217226
2
Iteration 13500: Loss = -12055.85567869009
3
Iteration 13600: Loss = -12055.823810162208
4
Iteration 13700: Loss = -12056.090620416198
5
Iteration 13800: Loss = -12055.823194545268
6
Iteration 13900: Loss = -12055.823554706416
7
Iteration 14000: Loss = -12055.821952862047
8
Iteration 14100: Loss = -12055.821472050855
Iteration 14200: Loss = -12055.824208005513
1
Iteration 14300: Loss = -12055.969196378319
2
Iteration 14400: Loss = -12055.824967830362
3
Iteration 14500: Loss = -12055.824881483313
4
Iteration 14600: Loss = -12055.85715417371
5
Iteration 14700: Loss = -12055.828178108759
6
Iteration 14800: Loss = -12055.821941782282
7
Iteration 14900: Loss = -12055.821226194488
Iteration 15000: Loss = -12053.884918430997
Iteration 15100: Loss = -12053.882442554466
Iteration 15200: Loss = -12053.880818650283
Iteration 15300: Loss = -12053.880891358916
Iteration 15400: Loss = -12053.601492811831
Iteration 15500: Loss = -12053.60187873186
1
Iteration 15600: Loss = -12053.602716224168
2
Iteration 15700: Loss = -12053.603454847282
3
Iteration 15800: Loss = -12053.610675034934
4
Iteration 15900: Loss = -12053.60510103045
5
Iteration 16000: Loss = -12053.60070765735
Iteration 16100: Loss = -12053.5294049334
Iteration 16200: Loss = -12053.547951945791
1
Iteration 16300: Loss = -12053.526805764508
Iteration 16400: Loss = -12053.532745741133
1
Iteration 16500: Loss = -12053.526768305903
Iteration 16600: Loss = -12053.53274597424
1
Iteration 16700: Loss = -12053.527620542949
2
Iteration 16800: Loss = -12053.840118181704
3
Iteration 16900: Loss = -12053.527170520218
4
Iteration 17000: Loss = -12053.597357800347
5
Iteration 17100: Loss = -12053.5307953943
6
Iteration 17200: Loss = -12053.534195280725
7
Iteration 17300: Loss = -12053.768060170827
8
Iteration 17400: Loss = -12053.526797427445
Iteration 17500: Loss = -12053.526845120094
Iteration 17600: Loss = -12053.526837033529
Iteration 17700: Loss = -12053.526776673725
Iteration 17800: Loss = -12053.529429199678
1
Iteration 17900: Loss = -12053.526988033356
2
Iteration 18000: Loss = -12053.546557097154
3
Iteration 18100: Loss = -12053.527293971767
4
Iteration 18200: Loss = -12053.528984823413
5
Iteration 18300: Loss = -12053.579318362534
6
Iteration 18400: Loss = -12053.529953614636
7
Iteration 18500: Loss = -12053.536009321966
8
Iteration 18600: Loss = -12053.535699643713
9
Iteration 18700: Loss = -12053.53069024955
10
Iteration 18800: Loss = -12053.527056501505
11
Iteration 18900: Loss = -12053.560410458736
12
Iteration 19000: Loss = -12053.528444858823
13
Iteration 19100: Loss = -12053.528543949851
14
Iteration 19200: Loss = -12053.554848514163
15
Stopping early at iteration 19200 due to no improvement.
pi: tensor([[0.3408, 0.6592],
        [0.6271, 0.3729]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3913, 0.6087], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3456, 0.0985],
         [0.6124, 0.2890]],

        [[0.6915, 0.0926],
         [0.5382, 0.6354]],

        [[0.6131, 0.0991],
         [0.6269, 0.7169]],

        [[0.7041, 0.1066],
         [0.5312, 0.6884]],

        [[0.5682, 0.1007],
         [0.7079, 0.6587]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080477173169247
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.05192865376345545
Average Adjusted Rand Index: 0.922580840726672
11801.974341317185
[1.0, 0.05192865376345545] [1.0, 0.922580840726672] [11796.684506640233, 12053.554848514163]
-------------------------------------
This iteration is 76
True Objective function: Loss = -11559.744931225014
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23247.08901037772
Iteration 100: Loss = -12217.885957380491
Iteration 200: Loss = -12211.474893250017
Iteration 300: Loss = -12175.617921209716
Iteration 400: Loss = -11735.830774686294
Iteration 500: Loss = -11596.89768338726
Iteration 600: Loss = -11560.372165214714
Iteration 700: Loss = -11551.430913308599
Iteration 800: Loss = -11550.94344414459
Iteration 900: Loss = -11550.74723051986
Iteration 1000: Loss = -11550.567092556827
Iteration 1100: Loss = -11550.32965329378
Iteration 1200: Loss = -11550.267083300132
Iteration 1300: Loss = -11550.220478636435
Iteration 1400: Loss = -11550.184195468457
Iteration 1500: Loss = -11550.155283083492
Iteration 1600: Loss = -11550.131720440811
Iteration 1700: Loss = -11550.112259192407
Iteration 1800: Loss = -11550.095959203154
Iteration 1900: Loss = -11550.082146642642
Iteration 2000: Loss = -11550.070297943837
Iteration 2100: Loss = -11550.060136671955
Iteration 2200: Loss = -11550.05123890997
Iteration 2300: Loss = -11550.043453135038
Iteration 2400: Loss = -11550.036597645301
Iteration 2500: Loss = -11550.03051704414
Iteration 2600: Loss = -11550.025105929228
Iteration 2700: Loss = -11550.020227519733
Iteration 2800: Loss = -11550.015882283486
Iteration 2900: Loss = -11550.011898739827
Iteration 3000: Loss = -11550.008317288226
Iteration 3100: Loss = -11550.005978625122
Iteration 3200: Loss = -11550.002079771282
Iteration 3300: Loss = -11549.999332634963
Iteration 3400: Loss = -11549.998217050976
Iteration 3500: Loss = -11549.994453703346
Iteration 3600: Loss = -11549.992237815404
Iteration 3700: Loss = -11549.990612727497
Iteration 3800: Loss = -11549.987800285051
Iteration 3900: Loss = -11549.985929452774
Iteration 4000: Loss = -11549.983615020466
Iteration 4100: Loss = -11549.981673074095
Iteration 4200: Loss = -11549.979860296333
Iteration 4300: Loss = -11549.977987805667
Iteration 4400: Loss = -11549.976623394536
Iteration 4500: Loss = -11549.98037927073
1
Iteration 4600: Loss = -11549.98213881838
2
Iteration 4700: Loss = -11549.975518688132
Iteration 4800: Loss = -11549.972921952261
Iteration 4900: Loss = -11549.972493864056
Iteration 5000: Loss = -11549.972499506479
Iteration 5100: Loss = -11549.973554311411
1
Iteration 5200: Loss = -11549.97460628491
2
Iteration 5300: Loss = -11549.969663872565
Iteration 5400: Loss = -11549.97556771263
1
Iteration 5500: Loss = -11549.96917321449
Iteration 5600: Loss = -11549.969103099145
Iteration 5700: Loss = -11549.97288500334
1
Iteration 5800: Loss = -11549.968301234803
Iteration 5900: Loss = -11549.968787715768
1
Iteration 6000: Loss = -11549.967070551324
Iteration 6100: Loss = -11549.969741577323
1
Iteration 6200: Loss = -11549.966437914545
Iteration 6300: Loss = -11549.967798989679
1
Iteration 6400: Loss = -11549.967244236681
2
Iteration 6500: Loss = -11549.970973211466
3
Iteration 6600: Loss = -11549.964942732724
Iteration 6700: Loss = -11549.96558024929
1
Iteration 6800: Loss = -11549.966883946849
2
Iteration 6900: Loss = -11549.964812022155
Iteration 7000: Loss = -11549.964069928941
Iteration 7100: Loss = -11549.963893343014
Iteration 7200: Loss = -11549.964552617417
1
Iteration 7300: Loss = -11549.963978689364
Iteration 7400: Loss = -11549.964006231674
Iteration 7500: Loss = -11549.966200220226
1
Iteration 7600: Loss = -11549.971620474962
2
Iteration 7700: Loss = -11549.966503116964
3
Iteration 7800: Loss = -11549.970255515711
4
Iteration 7900: Loss = -11549.962901762454
Iteration 8000: Loss = -11549.962666227155
Iteration 8100: Loss = -11549.962786046362
1
Iteration 8200: Loss = -11549.96304136469
2
Iteration 8300: Loss = -11549.966721166395
3
Iteration 8400: Loss = -11549.970801645864
4
Iteration 8500: Loss = -11549.970690477823
5
Iteration 8600: Loss = -11549.962085950656
Iteration 8700: Loss = -11549.962090183193
Iteration 8800: Loss = -11549.977887810264
1
Iteration 8900: Loss = -11549.961799780178
Iteration 9000: Loss = -11549.962202435474
1
Iteration 9100: Loss = -11549.961727052674
Iteration 9200: Loss = -11549.961736149002
Iteration 9300: Loss = -11549.96216321496
1
Iteration 9400: Loss = -11549.978835976353
2
Iteration 9500: Loss = -11549.961645706584
Iteration 9600: Loss = -11549.961705604579
Iteration 9700: Loss = -11550.018746851643
1
Iteration 9800: Loss = -11550.02278920957
2
Iteration 9900: Loss = -11550.01723313061
3
Iteration 10000: Loss = -11549.980718239973
4
Iteration 10100: Loss = -11549.971199854977
5
Iteration 10200: Loss = -11550.03083731975
6
Iteration 10300: Loss = -11549.967548550854
7
Iteration 10400: Loss = -11549.984598272367
8
Iteration 10500: Loss = -11549.96393438525
9
Iteration 10600: Loss = -11549.968156400762
10
Iteration 10700: Loss = -11549.967269687248
11
Iteration 10800: Loss = -11549.96939306401
12
Iteration 10900: Loss = -11550.064245091153
13
Iteration 11000: Loss = -11549.961754501177
Iteration 11100: Loss = -11549.984927142585
1
Iteration 11200: Loss = -11549.961847446575
Iteration 11300: Loss = -11549.962514351382
1
Iteration 11400: Loss = -11549.964522337083
2
Iteration 11500: Loss = -11549.973169037527
3
Iteration 11600: Loss = -11550.026338955186
4
Iteration 11700: Loss = -11549.965819708981
5
Iteration 11800: Loss = -11549.967464782982
6
Iteration 11900: Loss = -11549.9626016542
7
Iteration 12000: Loss = -11549.963582683953
8
Iteration 12100: Loss = -11549.96068906657
Iteration 12200: Loss = -11549.961639383606
1
Iteration 12300: Loss = -11549.966260582361
2
Iteration 12400: Loss = -11549.964040918194
3
Iteration 12500: Loss = -11549.967183000505
4
Iteration 12600: Loss = -11549.962600390694
5
Iteration 12700: Loss = -11549.988688591646
6
Iteration 12800: Loss = -11550.014728947845
7
Iteration 12900: Loss = -11549.979491455517
8
Iteration 13000: Loss = -11549.960558563589
Iteration 13100: Loss = -11549.961415232325
1
Iteration 13200: Loss = -11549.962309808581
2
Iteration 13300: Loss = -11549.960797951835
3
Iteration 13400: Loss = -11549.96755019827
4
Iteration 13500: Loss = -11549.994119744913
5
Iteration 13600: Loss = -11549.962721641932
6
Iteration 13700: Loss = -11549.961984814072
7
Iteration 13800: Loss = -11549.9675091576
8
Iteration 13900: Loss = -11549.989406672677
9
Iteration 14000: Loss = -11549.968065889962
10
Iteration 14100: Loss = -11550.04502275299
11
Iteration 14200: Loss = -11549.99947292956
12
Iteration 14300: Loss = -11549.974424908645
13
Iteration 14400: Loss = -11549.960544782643
Iteration 14500: Loss = -11549.969879739458
1
Iteration 14600: Loss = -11549.961410718686
2
Iteration 14700: Loss = -11549.968947230245
3
Iteration 14800: Loss = -11549.965990301765
4
Iteration 14900: Loss = -11550.036795255877
5
Iteration 15000: Loss = -11549.984010753198
6
Iteration 15100: Loss = -11549.963892163918
7
Iteration 15200: Loss = -11549.987245987013
8
Iteration 15300: Loss = -11549.966577747055
9
Iteration 15400: Loss = -11549.965828839951
10
Iteration 15500: Loss = -11549.964265447492
11
Iteration 15600: Loss = -11550.009144372001
12
Iteration 15700: Loss = -11549.967119841906
13
Iteration 15800: Loss = -11549.975913701512
14
Iteration 15900: Loss = -11549.970379387916
15
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[0.7547, 0.2453],
        [0.2441, 0.7559]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4719, 0.5281], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3858, 0.1036],
         [0.6350, 0.1989]],

        [[0.7288, 0.0994],
         [0.5121, 0.6640]],

        [[0.6380, 0.1049],
         [0.7144, 0.5670]],

        [[0.5126, 0.1150],
         [0.5532, 0.5363]],

        [[0.6247, 0.0956],
         [0.5844, 0.6018]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22008.478094291826
Iteration 100: Loss = -12218.409355432264
Iteration 200: Loss = -12215.777674325785
Iteration 300: Loss = -12203.639631395907
Iteration 400: Loss = -11877.847776160294
Iteration 500: Loss = -11725.402012765124
Iteration 600: Loss = -11649.561777913259
Iteration 700: Loss = -11609.374481013245
Iteration 800: Loss = -11589.09454527634
Iteration 900: Loss = -11554.606293271718
Iteration 1000: Loss = -11554.144057040088
Iteration 1100: Loss = -11553.967897615195
Iteration 1200: Loss = -11553.82867401821
Iteration 1300: Loss = -11550.870622537055
Iteration 1400: Loss = -11550.752834218802
Iteration 1500: Loss = -11550.583848667744
Iteration 1600: Loss = -11550.515976883102
Iteration 1700: Loss = -11550.48326249489
Iteration 1800: Loss = -11550.457333467019
Iteration 1900: Loss = -11550.435843443007
Iteration 2000: Loss = -11550.41752661892
Iteration 2100: Loss = -11550.401588212637
Iteration 2200: Loss = -11550.38728298354
Iteration 2300: Loss = -11550.373272257
Iteration 2400: Loss = -11550.355210339654
Iteration 2500: Loss = -11550.230214847388
Iteration 2600: Loss = -11550.062945692964
Iteration 2700: Loss = -11550.05442390851
Iteration 2800: Loss = -11550.04941311898
Iteration 2900: Loss = -11550.041686555824
Iteration 3000: Loss = -11550.036535585952
Iteration 3100: Loss = -11550.031790134324
Iteration 3200: Loss = -11550.027574709198
Iteration 3300: Loss = -11550.02624416077
Iteration 3400: Loss = -11550.023900041304
Iteration 3500: Loss = -11550.022827744504
Iteration 3600: Loss = -11550.014186252021
Iteration 3700: Loss = -11550.011520560152
Iteration 3800: Loss = -11550.009293468649
Iteration 3900: Loss = -11550.006952565906
Iteration 4000: Loss = -11550.022691019794
1
Iteration 4100: Loss = -11550.003104376216
Iteration 4200: Loss = -11550.001952572593
Iteration 4300: Loss = -11549.999827298527
Iteration 4400: Loss = -11550.000029776314
1
Iteration 4500: Loss = -11549.997898141146
Iteration 4600: Loss = -11549.995752631125
Iteration 4700: Loss = -11549.99695986957
1
Iteration 4800: Loss = -11549.993466563554
Iteration 4900: Loss = -11549.994259418752
1
Iteration 5000: Loss = -11549.99149345024
Iteration 5100: Loss = -11549.990982749841
Iteration 5200: Loss = -11549.989658558829
Iteration 5300: Loss = -11549.992512494897
1
Iteration 5400: Loss = -11549.987914375655
Iteration 5500: Loss = -11549.990653853
1
Iteration 5600: Loss = -11549.985254972546
Iteration 5700: Loss = -11549.988495159645
1
Iteration 5800: Loss = -11549.983391791284
Iteration 5900: Loss = -11549.982740471149
Iteration 6000: Loss = -11549.982220257772
Iteration 6100: Loss = -11549.981964556739
Iteration 6200: Loss = -11549.9812759396
Iteration 6300: Loss = -11549.994065392906
1
Iteration 6400: Loss = -11549.98046302356
Iteration 6500: Loss = -11549.98054383037
Iteration 6600: Loss = -11549.983555187708
1
Iteration 6700: Loss = -11549.98091629877
2
Iteration 6800: Loss = -11549.981113718039
3
Iteration 6900: Loss = -11549.978642111086
Iteration 7000: Loss = -11549.979172038082
1
Iteration 7100: Loss = -11549.74620906153
Iteration 7200: Loss = -11549.743651109098
Iteration 7300: Loss = -11549.748958135238
1
Iteration 7400: Loss = -11549.74318446813
Iteration 7500: Loss = -11549.744200818544
1
Iteration 7600: Loss = -11549.742862688077
Iteration 7700: Loss = -11549.742973500286
1
Iteration 7800: Loss = -11549.74241991485
Iteration 7900: Loss = -11549.742362292383
Iteration 8000: Loss = -11549.751559322256
1
Iteration 8100: Loss = -11549.75530048287
2
Iteration 8200: Loss = -11549.751930960078
3
Iteration 8300: Loss = -11549.743210498344
4
Iteration 8400: Loss = -11549.745090789334
5
Iteration 8500: Loss = -11549.76619343167
6
Iteration 8600: Loss = -11549.742505744038
7
Iteration 8700: Loss = -11549.741360435246
Iteration 8800: Loss = -11549.741220931488
Iteration 8900: Loss = -11549.742482567926
1
Iteration 9000: Loss = -11549.741260480361
Iteration 9100: Loss = -11549.742505050115
1
Iteration 9200: Loss = -11549.74089730053
Iteration 9300: Loss = -11549.740772146703
Iteration 9400: Loss = -11549.75567258663
1
Iteration 9500: Loss = -11549.74404632705
2
Iteration 9600: Loss = -11549.748039371701
3
Iteration 9700: Loss = -11549.74096038369
4
Iteration 9800: Loss = -11549.740226304242
Iteration 9900: Loss = -11549.737154715904
Iteration 10000: Loss = -11549.729087181211
Iteration 10100: Loss = -11549.743599373207
1
Iteration 10200: Loss = -11549.727816663284
Iteration 10300: Loss = -11549.74932981917
1
Iteration 10400: Loss = -11549.730019300574
2
Iteration 10500: Loss = -11549.727024886495
Iteration 10600: Loss = -11549.727907871933
1
Iteration 10700: Loss = -11549.728751407089
2
Iteration 10800: Loss = -11549.728795764988
3
Iteration 10900: Loss = -11549.738508494653
4
Iteration 11000: Loss = -11549.729640747042
5
Iteration 11100: Loss = -11549.735548146564
6
Iteration 11200: Loss = -11549.736434772542
7
Iteration 11300: Loss = -11549.750382825987
8
Iteration 11400: Loss = -11549.73237713627
9
Iteration 11500: Loss = -11549.73242963679
10
Iteration 11600: Loss = -11549.728774526842
11
Iteration 11700: Loss = -11549.726954196301
Iteration 11800: Loss = -11549.72968211137
1
Iteration 11900: Loss = -11549.737495981379
2
Iteration 12000: Loss = -11549.737760451575
3
Iteration 12100: Loss = -11549.728485382564
4
Iteration 12200: Loss = -11549.755127461103
5
Iteration 12300: Loss = -11549.755729462096
6
Iteration 12400: Loss = -11549.728238251362
7
Iteration 12500: Loss = -11549.729843893923
8
Iteration 12600: Loss = -11549.727967448154
9
Iteration 12700: Loss = -11549.733443977813
10
Iteration 12800: Loss = -11549.738286256928
11
Iteration 12900: Loss = -11549.727302542278
12
Iteration 13000: Loss = -11549.745161039036
13
Iteration 13100: Loss = -11549.745905922562
14
Iteration 13200: Loss = -11549.740520955616
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[0.7555, 0.2445],
        [0.2466, 0.7534]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5266, 0.4734], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.1035],
         [0.5250, 0.3861]],

        [[0.6773, 0.0993],
         [0.5784, 0.6648]],

        [[0.6689, 0.1057],
         [0.6971, 0.6263]],

        [[0.7289, 0.1155],
         [0.6172, 0.5740]],

        [[0.5720, 0.0957],
         [0.6448, 0.6777]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.99199998169963
Average Adjusted Rand Index: 0.9919995611635631
11559.744931225014
[0.99199998169963, 0.99199998169963] [0.9919995611635631, 0.9919995611635631] [11549.970379387916, 11549.740520955616]
-------------------------------------
This iteration is 77
True Objective function: Loss = -11775.194293641836
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21797.19159148481
Iteration 100: Loss = -12694.130269454912
Iteration 200: Loss = -12638.932492137657
Iteration 300: Loss = -11947.158238684095
Iteration 400: Loss = -11862.192157786547
Iteration 500: Loss = -11828.069020231555
Iteration 600: Loss = -11798.229044687263
Iteration 700: Loss = -11769.865175977528
Iteration 800: Loss = -11769.534612147432
Iteration 900: Loss = -11769.34808394298
Iteration 1000: Loss = -11769.143826691648
Iteration 1100: Loss = -11768.980336790277
Iteration 1200: Loss = -11768.918226646329
Iteration 1300: Loss = -11768.871349229421
Iteration 1400: Loss = -11768.83444315092
Iteration 1500: Loss = -11768.808945127435
Iteration 1600: Loss = -11768.780385018685
Iteration 1700: Loss = -11768.760095132378
Iteration 1800: Loss = -11768.743052904963
Iteration 1900: Loss = -11768.728614856413
Iteration 2000: Loss = -11768.715944475065
Iteration 2100: Loss = -11768.70515862718
Iteration 2200: Loss = -11768.6956803388
Iteration 2300: Loss = -11768.687427044548
Iteration 2400: Loss = -11768.680067671325
Iteration 2500: Loss = -11768.67464115377
Iteration 2600: Loss = -11768.667629654494
Iteration 2700: Loss = -11768.662408929411
Iteration 2800: Loss = -11768.662199002
Iteration 2900: Loss = -11768.65343517862
Iteration 3000: Loss = -11768.649537365543
Iteration 3100: Loss = -11768.646340596395
Iteration 3200: Loss = -11768.642848192501
Iteration 3300: Loss = -11768.641016758911
Iteration 3400: Loss = -11768.637280108798
Iteration 3500: Loss = -11768.635899756773
Iteration 3600: Loss = -11768.63260441432
Iteration 3700: Loss = -11768.630583696684
Iteration 3800: Loss = -11768.628858973867
Iteration 3900: Loss = -11768.626994774773
Iteration 4000: Loss = -11768.625381526503
Iteration 4100: Loss = -11768.624026913207
Iteration 4200: Loss = -11768.622494019246
Iteration 4300: Loss = -11768.621311476654
Iteration 4400: Loss = -11768.622731139258
1
Iteration 4500: Loss = -11768.618897016284
Iteration 4600: Loss = -11768.6178704604
Iteration 4700: Loss = -11768.61687023484
Iteration 4800: Loss = -11768.617268207803
1
Iteration 4900: Loss = -11768.615053653479
Iteration 5000: Loss = -11768.614623115369
Iteration 5100: Loss = -11768.613345423448
Iteration 5200: Loss = -11768.61389825483
1
Iteration 5300: Loss = -11768.611387378007
Iteration 5400: Loss = -11768.625131284201
1
Iteration 5500: Loss = -11768.598541550899
Iteration 5600: Loss = -11768.57629662702
Iteration 5700: Loss = -11768.575002712005
Iteration 5800: Loss = -11768.574428332115
Iteration 5900: Loss = -11768.576699019637
1
Iteration 6000: Loss = -11768.57355344673
Iteration 6100: Loss = -11768.583481057765
1
Iteration 6200: Loss = -11768.572793700074
Iteration 6300: Loss = -11768.57493366444
1
Iteration 6400: Loss = -11768.572378158224
Iteration 6500: Loss = -11768.574181554575
1
Iteration 6600: Loss = -11768.572459803165
Iteration 6700: Loss = -11768.581375026184
1
Iteration 6800: Loss = -11768.571784417907
Iteration 6900: Loss = -11768.57100626656
Iteration 7000: Loss = -11768.57115559601
1
Iteration 7100: Loss = -11768.57221899824
2
Iteration 7200: Loss = -11768.57029670621
Iteration 7300: Loss = -11768.57014503918
Iteration 7400: Loss = -11768.575609559904
1
Iteration 7500: Loss = -11768.574164815154
2
Iteration 7600: Loss = -11768.569534071235
Iteration 7700: Loss = -11768.573612504211
1
Iteration 7800: Loss = -11768.569351069018
Iteration 7900: Loss = -11768.569810139423
1
Iteration 8000: Loss = -11768.57629339065
2
Iteration 8100: Loss = -11768.577751119796
3
Iteration 8200: Loss = -11768.582602018274
4
Iteration 8300: Loss = -11768.568535540417
Iteration 8400: Loss = -11768.569231342723
1
Iteration 8500: Loss = -11768.568550483544
Iteration 8600: Loss = -11768.568891444582
1
Iteration 8700: Loss = -11768.569456221845
2
Iteration 8800: Loss = -11768.574549930623
3
Iteration 8900: Loss = -11768.568274949075
Iteration 9000: Loss = -11768.568658154543
1
Iteration 9100: Loss = -11768.56793005526
Iteration 9200: Loss = -11768.569763580903
1
Iteration 9300: Loss = -11768.5727988898
2
Iteration 9400: Loss = -11768.582220901373
3
Iteration 9500: Loss = -11768.57056076818
4
Iteration 9600: Loss = -11768.567756389582
Iteration 9700: Loss = -11768.583989496892
1
Iteration 9800: Loss = -11768.586187673776
2
Iteration 9900: Loss = -11768.57003846402
3
Iteration 10000: Loss = -11768.568050245192
4
Iteration 10100: Loss = -11768.567660236227
Iteration 10200: Loss = -11768.677317008896
1
Iteration 10300: Loss = -11768.575156397641
2
Iteration 10400: Loss = -11768.5687884657
3
Iteration 10500: Loss = -11768.581998956086
4
Iteration 10600: Loss = -11768.567327525552
Iteration 10700: Loss = -11768.573970142641
1
Iteration 10800: Loss = -11768.5762632684
2
Iteration 10900: Loss = -11768.58257949956
3
Iteration 11000: Loss = -11768.584188648263
4
Iteration 11100: Loss = -11768.58049523533
5
Iteration 11200: Loss = -11768.571594720757
6
Iteration 11300: Loss = -11768.569610088907
7
Iteration 11400: Loss = -11768.58001211816
8
Iteration 11500: Loss = -11768.61793921059
9
Iteration 11600: Loss = -11768.634875113974
10
Iteration 11700: Loss = -11768.582830111705
11
Iteration 11800: Loss = -11768.570184787992
12
Iteration 11900: Loss = -11768.568794561834
13
Iteration 12000: Loss = -11768.573653585663
14
Iteration 12100: Loss = -11768.573533646295
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.7578, 0.2422],
        [0.2089, 0.7911]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4171, 0.5829], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1975, 0.1088],
         [0.5093, 0.3965]],

        [[0.5218, 0.1040],
         [0.7221, 0.7300]],

        [[0.5615, 0.1020],
         [0.5435, 0.6063]],

        [[0.7110, 0.0978],
         [0.7042, 0.6388]],

        [[0.6810, 0.0874],
         [0.6131, 0.5389]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23520.40968211202
Iteration 100: Loss = -12696.105724181625
Iteration 200: Loss = -12584.417403417705
Iteration 300: Loss = -12319.055710355935
Iteration 400: Loss = -11812.604133278439
Iteration 500: Loss = -11802.505611400422
Iteration 600: Loss = -11800.635129748483
Iteration 700: Loss = -11796.02075484292
Iteration 800: Loss = -11795.840034318338
Iteration 900: Loss = -11795.775717792338
Iteration 1000: Loss = -11795.729094096032
Iteration 1100: Loss = -11795.688740245403
Iteration 1200: Loss = -11795.65543649968
Iteration 1300: Loss = -11795.635348739343
Iteration 1400: Loss = -11795.619096704251
Iteration 1500: Loss = -11795.60568876356
Iteration 1600: Loss = -11795.594503680572
Iteration 1700: Loss = -11795.584908231933
Iteration 1800: Loss = -11795.576569621886
Iteration 1900: Loss = -11795.569193160569
Iteration 2000: Loss = -11795.56249916939
Iteration 2100: Loss = -11795.556294150672
Iteration 2200: Loss = -11795.550190448745
Iteration 2300: Loss = -11795.540950543884
Iteration 2400: Loss = -11792.340153360205
Iteration 2500: Loss = -11790.819302160438
Iteration 2600: Loss = -11790.820489642962
1
Iteration 2700: Loss = -11790.8128461775
Iteration 2800: Loss = -11790.810194712405
Iteration 2900: Loss = -11790.823995510036
1
Iteration 3000: Loss = -11790.805390955624
Iteration 3100: Loss = -11790.803353741421
Iteration 3200: Loss = -11790.80185581906
Iteration 3300: Loss = -11790.80019051762
Iteration 3400: Loss = -11790.803795254253
1
Iteration 3500: Loss = -11790.7975987365
Iteration 3600: Loss = -11790.810943809935
1
Iteration 3700: Loss = -11790.7961892133
Iteration 3800: Loss = -11790.796334833498
1
Iteration 3900: Loss = -11790.797502953992
2
Iteration 4000: Loss = -11790.793604471588
Iteration 4100: Loss = -11790.791936867618
Iteration 4200: Loss = -11790.791671166997
Iteration 4300: Loss = -11790.790633837443
Iteration 4400: Loss = -11790.789961847457
Iteration 4500: Loss = -11790.798187815173
1
Iteration 4600: Loss = -11790.788830409792
Iteration 4700: Loss = -11790.79080677107
1
Iteration 4800: Loss = -11790.787855129156
Iteration 4900: Loss = -11790.78781246061
Iteration 5000: Loss = -11790.790090776703
1
Iteration 5100: Loss = -11790.786677737751
Iteration 5200: Loss = -11790.786303288647
Iteration 5300: Loss = -11790.787021888349
1
Iteration 5400: Loss = -11790.785856219596
Iteration 5500: Loss = -11790.784714959093
Iteration 5600: Loss = -11790.784443024177
Iteration 5700: Loss = -11790.781317051904
Iteration 5800: Loss = -11790.780900003587
Iteration 5900: Loss = -11790.780802582147
Iteration 6000: Loss = -11790.781898893954
1
Iteration 6100: Loss = -11790.78043503031
Iteration 6200: Loss = -11790.779613469222
Iteration 6300: Loss = -11784.146430427694
Iteration 6400: Loss = -11784.131064847466
Iteration 6500: Loss = -11784.131893388665
1
Iteration 6600: Loss = -11784.12891408725
Iteration 6700: Loss = -11775.789451814488
Iteration 6800: Loss = -11775.790671155297
1
Iteration 6900: Loss = -11775.786854107153
Iteration 7000: Loss = -11775.786439381656
Iteration 7100: Loss = -11775.785792813558
Iteration 7200: Loss = -11775.787842035234
1
Iteration 7300: Loss = -11775.785379046305
Iteration 7400: Loss = -11775.78531510312
Iteration 7500: Loss = -11775.792037716681
1
Iteration 7600: Loss = -11775.78510063931
Iteration 7700: Loss = -11775.785067111534
Iteration 7800: Loss = -11775.786718514897
1
Iteration 7900: Loss = -11768.600263438628
Iteration 8000: Loss = -11768.60379280395
1
Iteration 8100: Loss = -11768.605162020725
2
Iteration 8200: Loss = -11768.600657175966
3
Iteration 8300: Loss = -11768.599904268514
Iteration 8400: Loss = -11768.599904380371
Iteration 8500: Loss = -11768.599884430914
Iteration 8600: Loss = -11768.601863549335
1
Iteration 8700: Loss = -11768.601632202784
2
Iteration 8800: Loss = -11768.602178203682
3
Iteration 8900: Loss = -11768.60140192575
4
Iteration 9000: Loss = -11768.599551975905
Iteration 9100: Loss = -11768.601416221301
1
Iteration 9200: Loss = -11768.612679647573
2
Iteration 9300: Loss = -11768.59982012563
3
Iteration 9400: Loss = -11768.60361741531
4
Iteration 9500: Loss = -11768.601363790789
5
Iteration 9600: Loss = -11768.609020006827
6
Iteration 9700: Loss = -11768.602021284461
7
Iteration 9800: Loss = -11768.599647018118
Iteration 9900: Loss = -11768.599690681229
Iteration 10000: Loss = -11768.599859905498
1
Iteration 10100: Loss = -11768.599657196864
Iteration 10200: Loss = -11768.599489583205
Iteration 10300: Loss = -11768.602468779944
1
Iteration 10400: Loss = -11768.599483330567
Iteration 10500: Loss = -11768.600652612373
1
Iteration 10600: Loss = -11768.64901247767
2
Iteration 10700: Loss = -11768.61092853014
3
Iteration 10800: Loss = -11768.604300115094
4
Iteration 10900: Loss = -11768.605605735094
5
Iteration 11000: Loss = -11768.6534031957
6
Iteration 11100: Loss = -11768.61204158961
7
Iteration 11200: Loss = -11768.614413890713
8
Iteration 11300: Loss = -11768.60072568207
9
Iteration 11400: Loss = -11768.697944629654
10
Iteration 11500: Loss = -11768.606108905546
11
Iteration 11600: Loss = -11768.610027492108
12
Iteration 11700: Loss = -11768.601689651396
13
Iteration 11800: Loss = -11768.603029381386
14
Iteration 11900: Loss = -11768.607742490163
15
Stopping early at iteration 11900 due to no improvement.
pi: tensor([[0.7578, 0.2422],
        [0.2098, 0.7902]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4176, 0.5824], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1088],
         [0.6637, 0.3966]],

        [[0.5667, 0.1040],
         [0.7106, 0.7017]],

        [[0.6425, 0.1021],
         [0.7070, 0.6661]],

        [[0.6844, 0.0978],
         [0.6490, 0.5220]],

        [[0.7109, 0.0881],
         [0.6594, 0.5198]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11775.194293641836
[1.0, 1.0] [1.0, 1.0] [11768.573533646295, 11768.607742490163]
-------------------------------------
This iteration is 78
True Objective function: Loss = -11361.425513192185
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23564.858758080423
Iteration 100: Loss = -11901.784487808443
Iteration 200: Loss = -11881.673933591306
Iteration 300: Loss = -11640.245039156722
Iteration 400: Loss = -11636.645266081818
Iteration 500: Loss = -11635.083881286455
Iteration 600: Loss = -11633.702172680407
Iteration 700: Loss = -11631.762336144659
Iteration 800: Loss = -11631.733575137623
Iteration 900: Loss = -11631.70205828837
Iteration 1000: Loss = -11631.35708811981
Iteration 1100: Loss = -11631.346269868242
Iteration 1200: Loss = -11631.338549737431
Iteration 1300: Loss = -11631.332212518993
Iteration 1400: Loss = -11631.327527398123
Iteration 1500: Loss = -11631.323213378599
Iteration 1600: Loss = -11631.323173018172
Iteration 1700: Loss = -11631.3111009992
Iteration 1800: Loss = -11631.306864859956
Iteration 1900: Loss = -11631.304741540554
Iteration 2000: Loss = -11631.302854839929
Iteration 2100: Loss = -11631.301641517783
Iteration 2200: Loss = -11631.300371520965
Iteration 2300: Loss = -11631.301048402276
1
Iteration 2400: Loss = -11631.298497444444
Iteration 2500: Loss = -11631.297760900487
Iteration 2600: Loss = -11631.298217338614
1
Iteration 2700: Loss = -11631.296403425591
Iteration 2800: Loss = -11631.295850908593
Iteration 2900: Loss = -11631.295462809983
Iteration 3000: Loss = -11631.303364547432
1
Iteration 3100: Loss = -11631.29429253384
Iteration 3200: Loss = -11631.293860633672
Iteration 3300: Loss = -11631.293411610508
Iteration 3400: Loss = -11631.294624802258
1
Iteration 3500: Loss = -11631.292672681277
Iteration 3600: Loss = -11631.292413842662
Iteration 3700: Loss = -11631.292161183694
Iteration 3800: Loss = -11631.291863525306
Iteration 3900: Loss = -11631.301228169212
1
Iteration 4000: Loss = -11631.291467925312
Iteration 4100: Loss = -11631.29149647241
Iteration 4200: Loss = -11631.293944440147
1
Iteration 4300: Loss = -11631.29098135582
Iteration 4400: Loss = -11631.293151418651
1
Iteration 4500: Loss = -11631.292879841532
2
Iteration 4600: Loss = -11631.293879724119
3
Iteration 4700: Loss = -11631.292320710163
4
Iteration 4800: Loss = -11631.29394650706
5
Iteration 4900: Loss = -11631.291017771575
Iteration 5000: Loss = -11631.29039947097
Iteration 5100: Loss = -11631.291723047618
1
Iteration 5200: Loss = -11631.294675933113
2
Iteration 5300: Loss = -11631.290070441073
Iteration 5400: Loss = -11631.28994713584
Iteration 5500: Loss = -11631.290296042318
1
Iteration 5600: Loss = -11631.304866729028
2
Iteration 5700: Loss = -11631.289777005895
Iteration 5800: Loss = -11631.289712998898
Iteration 5900: Loss = -11631.291676717176
1
Iteration 6000: Loss = -11631.289591162815
Iteration 6100: Loss = -11631.28971335175
1
Iteration 6200: Loss = -11631.28952119333
Iteration 6300: Loss = -11631.289720237117
1
Iteration 6400: Loss = -11631.289513639693
Iteration 6500: Loss = -11631.289385634896
Iteration 6600: Loss = -11631.289355133124
Iteration 6700: Loss = -11631.291763322457
1
Iteration 6800: Loss = -11631.289228082376
Iteration 6900: Loss = -11631.289225201992
Iteration 7000: Loss = -11631.289911480464
1
Iteration 7100: Loss = -11631.289574272327
2
Iteration 7200: Loss = -11631.294978907235
3
Iteration 7300: Loss = -11631.288977882628
Iteration 7400: Loss = -11627.96325724678
Iteration 7500: Loss = -11627.889597452391
Iteration 7600: Loss = -11627.821712351077
Iteration 7700: Loss = -11627.820783829813
Iteration 7800: Loss = -11627.816506581965
Iteration 7900: Loss = -11627.815901076054
Iteration 8000: Loss = -11627.81577174549
Iteration 8100: Loss = -11627.81569762875
Iteration 8200: Loss = -11627.815684722056
Iteration 8300: Loss = -11627.824425013725
1
Iteration 8400: Loss = -11627.815650467206
Iteration 8500: Loss = -11627.816012501731
1
Iteration 8600: Loss = -11627.81569528746
Iteration 8700: Loss = -11627.815886041366
1
Iteration 8800: Loss = -11627.841677677028
2
Iteration 8900: Loss = -11627.815987720049
3
Iteration 9000: Loss = -11627.816115564
4
Iteration 9100: Loss = -11627.819417770608
5
Iteration 9200: Loss = -11627.946446695849
6
Iteration 9300: Loss = -11627.815789020684
Iteration 9400: Loss = -11627.815825866574
Iteration 9500: Loss = -11627.8164191734
1
Iteration 9600: Loss = -11627.818962763084
2
Iteration 9700: Loss = -11627.815531482416
Iteration 9800: Loss = -11627.815720094764
1
Iteration 9900: Loss = -11624.577471094768
Iteration 10000: Loss = -11623.880109392365
Iteration 10100: Loss = -11623.86774682178
Iteration 10200: Loss = -11623.878026624208
1
Iteration 10300: Loss = -11623.856131934612
Iteration 10400: Loss = -11623.855281683298
Iteration 10500: Loss = -11623.856121732268
1
Iteration 10600: Loss = -11623.852169500882
Iteration 10700: Loss = -11623.853355551502
1
Iteration 10800: Loss = -11623.858675481268
2
Iteration 10900: Loss = -11623.851921163929
Iteration 11000: Loss = -11623.857059734108
1
Iteration 11100: Loss = -11623.853679824577
2
Iteration 11200: Loss = -11623.888228690093
3
Iteration 11300: Loss = -11623.85149910588
Iteration 11400: Loss = -11623.851421011545
Iteration 11500: Loss = -11623.876600349326
1
Iteration 11600: Loss = -11623.285828951939
Iteration 11700: Loss = -11622.765369898154
Iteration 11800: Loss = -11622.763307885132
Iteration 11900: Loss = -11622.76476440723
1
Iteration 12000: Loss = -11622.76690156851
2
Iteration 12100: Loss = -11622.763229118878
Iteration 12200: Loss = -11622.795471287433
1
Iteration 12300: Loss = -11622.763203882227
Iteration 12400: Loss = -11622.767236513131
1
Iteration 12500: Loss = -11622.763226677274
Iteration 12600: Loss = -11622.763387020826
1
Iteration 12700: Loss = -11622.78647870894
2
Iteration 12800: Loss = -11622.772305306233
3
Iteration 12900: Loss = -11622.834999510116
4
Iteration 13000: Loss = -11622.764985982169
5
Iteration 13100: Loss = -11622.76207920839
Iteration 13200: Loss = -11622.76219664125
1
Iteration 13300: Loss = -11622.772688131436
2
Iteration 13400: Loss = -11622.789363560214
3
Iteration 13500: Loss = -11622.76044288378
Iteration 13600: Loss = -11622.77290236418
1
Iteration 13700: Loss = -11622.761335917268
2
Iteration 13800: Loss = -11622.75673257517
Iteration 13900: Loss = -11622.764123727327
1
Iteration 14000: Loss = -11622.75666715962
Iteration 14100: Loss = -11622.766132334717
1
Iteration 14200: Loss = -11622.757125994369
2
Iteration 14300: Loss = -11622.756814172968
3
Iteration 14400: Loss = -11622.758198524643
4
Iteration 14500: Loss = -11622.982246827583
5
Iteration 14600: Loss = -11622.761925319046
6
Iteration 14700: Loss = -11622.75834102258
7
Iteration 14800: Loss = -11622.756722461429
Iteration 14900: Loss = -11622.756771928201
Iteration 15000: Loss = -11622.761553200615
1
Iteration 15100: Loss = -11622.756772067798
Iteration 15200: Loss = -11622.761401279944
1
Iteration 15300: Loss = -11622.756688727108
Iteration 15400: Loss = -11622.792016480711
1
Iteration 15500: Loss = -11622.756231585228
Iteration 15600: Loss = -11622.85352700151
1
Iteration 15700: Loss = -11622.756238363225
Iteration 15800: Loss = -11622.75926595977
1
Iteration 15900: Loss = -11622.79377043396
2
Iteration 16000: Loss = -11622.757178312235
3
Iteration 16100: Loss = -11622.756417689077
4
Iteration 16200: Loss = -11622.804575823726
5
Iteration 16300: Loss = -11622.760961893324
6
Iteration 16400: Loss = -11622.756380471206
7
Iteration 16500: Loss = -11622.764870237064
8
Iteration 16600: Loss = -11622.763267283723
9
Iteration 16700: Loss = -11622.759267773241
10
Iteration 16800: Loss = -11622.782106137369
11
Iteration 16900: Loss = -11622.757437750754
12
Iteration 17000: Loss = -11623.012311099295
13
Iteration 17100: Loss = -11622.755253487858
Iteration 17200: Loss = -11622.756576744918
1
Iteration 17300: Loss = -11622.76522488896
2
Iteration 17400: Loss = -11622.755593607944
3
Iteration 17500: Loss = -11622.777937980065
4
Iteration 17600: Loss = -11622.755346007487
Iteration 17700: Loss = -11622.75622701093
1
Iteration 17800: Loss = -11622.755155405483
Iteration 17900: Loss = -11622.851269049854
1
Iteration 18000: Loss = -11622.755177846682
Iteration 18100: Loss = -11622.756580390207
1
Iteration 18200: Loss = -11622.761796699917
2
Iteration 18300: Loss = -11622.756487539571
3
Iteration 18400: Loss = -11622.758386497842
4
Iteration 18500: Loss = -11622.761803332956
5
Iteration 18600: Loss = -11622.76629124205
6
Iteration 18700: Loss = -11622.762059044622
7
Iteration 18800: Loss = -11622.776790767142
8
Iteration 18900: Loss = -11622.756311756244
9
Iteration 19000: Loss = -11622.755190587784
Iteration 19100: Loss = -11622.756640072577
1
Iteration 19200: Loss = -11622.862326015977
2
Iteration 19300: Loss = -11622.755105869497
Iteration 19400: Loss = -11622.75783703189
1
Iteration 19500: Loss = -11622.759127260437
2
Iteration 19600: Loss = -11622.76250158152
3
Iteration 19700: Loss = -11622.755177370089
Iteration 19800: Loss = -11622.755499422967
1
Iteration 19900: Loss = -11622.772897007431
2
pi: tensor([[0.4282, 0.5718],
        [0.3218, 0.6782]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3788, 0.6212], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3765, 0.1055],
         [0.6138, 0.2233]],

        [[0.5469, 0.0948],
         [0.6628, 0.6739]],

        [[0.5603, 0.1071],
         [0.5878, 0.6679]],

        [[0.5888, 0.1021],
         [0.7306, 0.5747]],

        [[0.6295, 0.1018],
         [0.5605, 0.5599]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 70
Adjusted Rand Index: 0.14902834509221438
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 72
Adjusted Rand Index: 0.18478260869565216
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.18010606757098188
Average Adjusted Rand Index: 0.6587397485464144
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23485.927169095456
Iteration 100: Loss = -11988.164497910446
Iteration 200: Loss = -11986.329251427549
Iteration 300: Loss = -11976.069776256818
Iteration 400: Loss = -11961.967326845224
Iteration 500: Loss = -11824.198492363656
Iteration 600: Loss = -11807.513136884712
Iteration 700: Loss = -11779.923113402934
Iteration 800: Loss = -11672.549163334623
Iteration 900: Loss = -11598.200034566218
Iteration 1000: Loss = -11585.37166133025
Iteration 1100: Loss = -11568.688748421524
Iteration 1200: Loss = -11546.060536050105
Iteration 1300: Loss = -11545.975868679234
Iteration 1400: Loss = -11545.425367166401
Iteration 1500: Loss = -11544.87303060212
Iteration 1600: Loss = -11536.78437562586
Iteration 1700: Loss = -11530.909667451224
Iteration 1800: Loss = -11530.712205107178
Iteration 1900: Loss = -11527.527172083399
Iteration 2000: Loss = -11527.50895990474
Iteration 2100: Loss = -11527.385230846146
Iteration 2200: Loss = -11527.346801428706
Iteration 2300: Loss = -11527.338027220661
Iteration 2400: Loss = -11527.329604427561
Iteration 2500: Loss = -11527.235665927308
Iteration 2600: Loss = -11527.231052157507
Iteration 2700: Loss = -11527.228731441413
Iteration 2800: Loss = -11527.226559825356
Iteration 2900: Loss = -11527.22446966003
Iteration 3000: Loss = -11527.2224281719
Iteration 3100: Loss = -11527.220648852337
Iteration 3200: Loss = -11527.213965344434
Iteration 3300: Loss = -11527.209151586107
Iteration 3400: Loss = -11526.88074407691
Iteration 3500: Loss = -11526.831596955817
Iteration 3600: Loss = -11526.825718433338
Iteration 3700: Loss = -11525.184657164535
Iteration 3800: Loss = -11525.106113150834
Iteration 3900: Loss = -11525.101326292996
Iteration 4000: Loss = -11520.474803884998
Iteration 4100: Loss = -11520.304671778238
Iteration 4200: Loss = -11520.296372402805
Iteration 4300: Loss = -11520.291617998757
Iteration 4400: Loss = -11520.278166183285
Iteration 4500: Loss = -11520.277676759477
Iteration 4600: Loss = -11520.277223828973
Iteration 4700: Loss = -11520.27664812639
Iteration 4800: Loss = -11520.27611518503
Iteration 4900: Loss = -11520.275320413857
Iteration 5000: Loss = -11520.261953010513
Iteration 5100: Loss = -11520.267833576163
1
Iteration 5200: Loss = -11520.263885216604
2
Iteration 5300: Loss = -11520.267932714387
3
Iteration 5400: Loss = -11520.258350782538
Iteration 5500: Loss = -11520.257805016336
Iteration 5600: Loss = -11520.257163980772
Iteration 5700: Loss = -11520.256380281025
Iteration 5800: Loss = -11520.256958292295
1
Iteration 5900: Loss = -11520.25313618912
Iteration 6000: Loss = -11520.194730121862
Iteration 6100: Loss = -11520.19551581286
1
Iteration 6200: Loss = -11520.197554055372
2
Iteration 6300: Loss = -11520.202863285595
3
Iteration 6400: Loss = -11520.195220421656
4
Iteration 6500: Loss = -11520.19537188068
5
Iteration 6600: Loss = -11520.197475731571
6
Iteration 6700: Loss = -11520.194766781979
Iteration 6800: Loss = -11520.193882262409
Iteration 6900: Loss = -11520.18578731272
Iteration 7000: Loss = -11520.1853552646
Iteration 7100: Loss = -11520.184904181526
Iteration 7200: Loss = -11520.185072799879
1
Iteration 7300: Loss = -11520.18650478125
2
Iteration 7400: Loss = -11520.186035655574
3
Iteration 7500: Loss = -11520.184395001877
Iteration 7600: Loss = -11520.1840775087
Iteration 7700: Loss = -11520.18783610677
1
Iteration 7800: Loss = -11520.185792794622
2
Iteration 7900: Loss = -11520.183758648263
Iteration 8000: Loss = -11520.18297193982
Iteration 8100: Loss = -11520.184746238507
1
Iteration 8200: Loss = -11520.18631503569
2
Iteration 8300: Loss = -11520.182894412492
Iteration 8400: Loss = -11520.219092561427
1
Iteration 8500: Loss = -11520.270578968257
2
Iteration 8600: Loss = -11520.181943654952
Iteration 8700: Loss = -11520.182753403667
1
Iteration 8800: Loss = -11520.182068836952
2
Iteration 8900: Loss = -11520.20361443545
3
Iteration 9000: Loss = -11520.181789003156
Iteration 9100: Loss = -11520.183091748206
1
Iteration 9200: Loss = -11520.2221178616
2
Iteration 9300: Loss = -11520.186873891882
3
Iteration 9400: Loss = -11520.221516744941
4
Iteration 9500: Loss = -11520.207601726435
5
Iteration 9600: Loss = -11520.18149991091
Iteration 9700: Loss = -11520.169387519805
Iteration 9800: Loss = -11519.85679124814
Iteration 9900: Loss = -11519.856965177156
1
Iteration 10000: Loss = -11519.92357367589
2
Iteration 10100: Loss = -11519.858416462366
3
Iteration 10200: Loss = -11519.856916952709
4
Iteration 10300: Loss = -11519.891259719165
5
Iteration 10400: Loss = -11519.864449712553
6
Iteration 10500: Loss = -11519.857047602674
7
Iteration 10600: Loss = -11519.960285486346
8
Iteration 10700: Loss = -11519.857140325348
9
Iteration 10800: Loss = -11519.861967663506
10
Iteration 10900: Loss = -11519.966014814214
11
Iteration 11000: Loss = -11519.85904228382
12
Iteration 11100: Loss = -11519.858523602406
13
Iteration 11200: Loss = -11519.859462409371
14
Iteration 11300: Loss = -11519.991027995884
15
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[0.4260, 0.5740],
        [0.3904, 0.6096]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3775, 0.6225], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3945, 0.1052],
         [0.6500, 0.2129]],

        [[0.5813, 0.0948],
         [0.6173, 0.7019]],

        [[0.6353, 0.1115],
         [0.7075, 0.6196]],

        [[0.6091, 0.1021],
         [0.6364, 0.5921]],

        [[0.6522, 0.1018],
         [0.5443, 0.6348]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 77
Adjusted Rand Index: 0.28410505721230805
time is 3
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.4773290139826178
Average Adjusted Rand Index: 0.8568210114424616
11361.425513192185
[0.18010606757098188, 0.4773290139826178] [0.6587397485464144, 0.8568210114424616] [11622.755600703334, 11519.991027995884]
-------------------------------------
This iteration is 79
True Objective function: Loss = -11593.789627337863
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22695.999382763348
Iteration 100: Loss = -12413.818513342307
Iteration 200: Loss = -12390.617239347417
Iteration 300: Loss = -12017.358478103213
Iteration 400: Loss = -11953.427244711278
Iteration 500: Loss = -11909.918450462917
Iteration 600: Loss = -11906.577780357906
Iteration 700: Loss = -11904.304657206281
Iteration 800: Loss = -11896.382532396352
Iteration 900: Loss = -11895.37499716348
Iteration 1000: Loss = -11891.610208969872
Iteration 1100: Loss = -11891.366486771502
Iteration 1200: Loss = -11884.826243767882
Iteration 1300: Loss = -11884.589501555487
Iteration 1400: Loss = -11884.541924962066
Iteration 1500: Loss = -11883.989346886137
Iteration 1600: Loss = -11883.91959321148
Iteration 1700: Loss = -11883.890028313735
Iteration 1800: Loss = -11883.87342068061
Iteration 1900: Loss = -11883.859370760832
Iteration 2000: Loss = -11883.848503547153
Iteration 2100: Loss = -11883.83889974605
Iteration 2200: Loss = -11883.040304582546
Iteration 2300: Loss = -11882.881806213883
Iteration 2400: Loss = -11882.57447466072
Iteration 2500: Loss = -11882.563550872008
Iteration 2600: Loss = -11882.551756787885
Iteration 2700: Loss = -11882.549002439777
Iteration 2800: Loss = -11882.539858122542
Iteration 2900: Loss = -11882.524924064825
Iteration 3000: Loss = -11882.522161990657
Iteration 3100: Loss = -11882.518554257691
Iteration 3200: Loss = -11882.516734201377
Iteration 3300: Loss = -11882.521936357425
1
Iteration 3400: Loss = -11882.512569628672
Iteration 3500: Loss = -11882.511819885816
Iteration 3600: Loss = -11882.509341845765
Iteration 3700: Loss = -11882.508354399743
Iteration 3800: Loss = -11882.507574561392
Iteration 3900: Loss = -11882.508922740351
1
Iteration 4000: Loss = -11882.504691636901
Iteration 4100: Loss = -11882.505471739245
1
Iteration 4200: Loss = -11882.503187221424
Iteration 4300: Loss = -11882.502047766806
Iteration 4400: Loss = -11882.50634546081
1
Iteration 4500: Loss = -11882.500461540702
Iteration 4600: Loss = -11882.499783945115
Iteration 4700: Loss = -11882.500494528238
1
Iteration 4800: Loss = -11882.50477924011
2
Iteration 4900: Loss = -11882.510263110402
3
Iteration 5000: Loss = -11882.49754297624
Iteration 5100: Loss = -11882.497107650712
Iteration 5200: Loss = -11882.496739129156
Iteration 5300: Loss = -11882.496249509442
Iteration 5400: Loss = -11882.498882764748
1
Iteration 5500: Loss = -11882.495712081389
Iteration 5600: Loss = -11882.495409999974
Iteration 5700: Loss = -11882.495330577627
Iteration 5800: Loss = -11882.494683883633
Iteration 5900: Loss = -11882.495424594652
1
Iteration 6000: Loss = -11882.495718705411
2
Iteration 6100: Loss = -11882.497415218037
3
Iteration 6200: Loss = -11882.493655732158
Iteration 6300: Loss = -11882.493510352962
Iteration 6400: Loss = -11882.49334918609
Iteration 6500: Loss = -11882.493065272982
Iteration 6600: Loss = -11882.492879340329
Iteration 6700: Loss = -11882.49275209273
Iteration 6800: Loss = -11882.492575591943
Iteration 6900: Loss = -11882.492717454907
1
Iteration 7000: Loss = -11882.492254906814
Iteration 7100: Loss = -11882.520632489724
1
Iteration 7200: Loss = -11882.492066346196
Iteration 7300: Loss = -11882.491925257787
Iteration 7400: Loss = -11882.491873876334
Iteration 7500: Loss = -11882.491751314052
Iteration 7600: Loss = -11882.492039341643
1
Iteration 7700: Loss = -11882.492223202264
2
Iteration 7800: Loss = -11882.51520273847
3
Iteration 7900: Loss = -11882.491311150265
Iteration 8000: Loss = -11882.491240903646
Iteration 8100: Loss = -11882.521421334717
1
Iteration 8200: Loss = -11882.49106603258
Iteration 8300: Loss = -11882.491234135481
1
Iteration 8400: Loss = -11882.49090260639
Iteration 8500: Loss = -11882.49089655207
Iteration 8600: Loss = -11882.490886651252
Iteration 8700: Loss = -11882.490930037178
Iteration 8800: Loss = -11882.490719044374
Iteration 8900: Loss = -11882.490743480332
Iteration 9000: Loss = -11882.490647662198
Iteration 9100: Loss = -11882.49068234514
Iteration 9200: Loss = -11882.624929118925
1
Iteration 9300: Loss = -11882.490558173286
Iteration 9400: Loss = -11882.490902309915
1
Iteration 9500: Loss = -11882.494608969691
2
Iteration 9600: Loss = -11882.491167086031
3
Iteration 9700: Loss = -11882.66911309801
4
Iteration 9800: Loss = -11882.493092162307
5
Iteration 9900: Loss = -11882.494962692035
6
Iteration 10000: Loss = -11882.491604092213
7
Iteration 10100: Loss = -11882.492655145681
8
Iteration 10200: Loss = -11882.491212575396
9
Iteration 10300: Loss = -11882.497720007688
10
Iteration 10400: Loss = -11882.500203795746
11
Iteration 10500: Loss = -11882.492110109113
12
Iteration 10600: Loss = -11882.492215238213
13
Iteration 10700: Loss = -11882.49185655038
14
Iteration 10800: Loss = -11882.491340713903
15
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[0.5053, 0.4947],
        [0.6134, 0.3866]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7589, 0.2411], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2464, 0.0935],
         [0.5681, 0.3748]],

        [[0.6498, 0.1075],
         [0.7110, 0.5145]],

        [[0.6350, 0.0995],
         [0.6920, 0.5949]],

        [[0.6509, 0.1068],
         [0.5661, 0.5513]],

        [[0.5973, 0.0881],
         [0.6584, 0.6783]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 18
Adjusted Rand Index: 0.402231140237841
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 22
Adjusted Rand Index: 0.3069711157192286
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.1279085214282292
Average Adjusted Rand Index: 0.741840451191414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21242.890485974935
Iteration 100: Loss = -12405.309820280096
Iteration 200: Loss = -12221.207525622107
Iteration 300: Loss = -11712.124133825106
Iteration 400: Loss = -11590.002648337737
Iteration 500: Loss = -11587.747597161331
Iteration 600: Loss = -11587.284676927446
Iteration 700: Loss = -11586.985783068114
Iteration 800: Loss = -11586.744523188021
Iteration 900: Loss = -11586.627196093923
Iteration 1000: Loss = -11586.47019448786
Iteration 1100: Loss = -11586.29897821657
Iteration 1200: Loss = -11586.252766888494
Iteration 1300: Loss = -11586.217128927856
Iteration 1400: Loss = -11586.188561700894
Iteration 1500: Loss = -11586.165022683399
Iteration 1600: Loss = -11586.145191365362
Iteration 1700: Loss = -11586.128048264449
Iteration 1800: Loss = -11586.112841858645
Iteration 1900: Loss = -11586.100068500922
Iteration 2000: Loss = -11586.08984415482
Iteration 2100: Loss = -11586.080988981314
Iteration 2200: Loss = -11586.073237958757
Iteration 2300: Loss = -11586.066408584662
Iteration 2400: Loss = -11586.06029105908
Iteration 2500: Loss = -11586.064190204446
1
Iteration 2600: Loss = -11586.050106870674
Iteration 2700: Loss = -11586.045717089217
Iteration 2800: Loss = -11586.041777107102
Iteration 2900: Loss = -11586.038190961726
Iteration 3000: Loss = -11586.034902200508
Iteration 3100: Loss = -11586.031891207842
Iteration 3200: Loss = -11586.031161310579
Iteration 3300: Loss = -11586.02639665101
Iteration 3400: Loss = -11586.023661753716
Iteration 3500: Loss = -11586.020367332265
Iteration 3600: Loss = -11586.013251963546
Iteration 3700: Loss = -11586.015160200213
1
Iteration 3800: Loss = -11586.00645399647
Iteration 3900: Loss = -11586.002310764634
Iteration 4000: Loss = -11586.007043245225
1
Iteration 4100: Loss = -11585.999642583649
Iteration 4200: Loss = -11585.998439257037
Iteration 4300: Loss = -11585.997382106945
Iteration 4400: Loss = -11586.010020152708
1
Iteration 4500: Loss = -11585.996324584987
Iteration 4600: Loss = -11585.999010621781
1
Iteration 4700: Loss = -11585.993764129007
Iteration 4800: Loss = -11585.992928282829
Iteration 4900: Loss = -11585.994644955836
1
Iteration 5000: Loss = -11585.991519960191
Iteration 5100: Loss = -11585.990200895552
Iteration 5200: Loss = -11585.989750010098
Iteration 5300: Loss = -11585.989132042734
Iteration 5400: Loss = -11586.00549418331
1
Iteration 5500: Loss = -11585.988338661382
Iteration 5600: Loss = -11585.987668139132
Iteration 5700: Loss = -11585.991564376907
1
Iteration 5800: Loss = -11585.98710483003
Iteration 5900: Loss = -11585.986401050995
Iteration 6000: Loss = -11585.988163041226
1
Iteration 6100: Loss = -11585.98619969921
Iteration 6200: Loss = -11585.986733368827
1
Iteration 6300: Loss = -11585.992064172768
2
Iteration 6400: Loss = -11585.984856569685
Iteration 6500: Loss = -11585.984779045897
Iteration 6600: Loss = -11585.984746558759
Iteration 6700: Loss = -11585.985746347846
1
Iteration 6800: Loss = -11585.983859301818
Iteration 6900: Loss = -11585.984113239527
1
Iteration 7000: Loss = -11585.991182007769
2
Iteration 7100: Loss = -11585.985097775278
3
Iteration 7200: Loss = -11585.98423630289
4
Iteration 7300: Loss = -11585.987288234388
5
Iteration 7400: Loss = -11585.990358372192
6
Iteration 7500: Loss = -11585.98282074264
Iteration 7600: Loss = -11585.982476705014
Iteration 7700: Loss = -11585.99318221055
1
Iteration 7800: Loss = -11585.982180343573
Iteration 7900: Loss = -11585.981148717434
Iteration 8000: Loss = -11585.981088642173
Iteration 8100: Loss = -11585.98358489327
1
Iteration 8200: Loss = -11585.989203171463
2
Iteration 8300: Loss = -11586.00635922215
3
Iteration 8400: Loss = -11585.98301970763
4
Iteration 8500: Loss = -11585.984322610218
5
Iteration 8600: Loss = -11586.016967867803
6
Iteration 8700: Loss = -11585.980397806514
Iteration 8800: Loss = -11585.980464890252
Iteration 8900: Loss = -11585.980928575578
1
Iteration 9000: Loss = -11585.980364116356
Iteration 9100: Loss = -11585.989281760392
1
Iteration 9200: Loss = -11586.026331787241
2
Iteration 9300: Loss = -11586.06133053072
3
Iteration 9400: Loss = -11585.982394030669
4
Iteration 9500: Loss = -11585.983680323854
5
Iteration 9600: Loss = -11585.981045547358
6
Iteration 9700: Loss = -11585.998320578256
7
Iteration 9800: Loss = -11585.980148420214
Iteration 9900: Loss = -11585.979797436217
Iteration 10000: Loss = -11585.982031260199
1
Iteration 10100: Loss = -11585.98163980726
2
Iteration 10200: Loss = -11585.982927474413
3
Iteration 10300: Loss = -11585.981287949364
4
Iteration 10400: Loss = -11585.993477016471
5
Iteration 10500: Loss = -11585.980501283586
6
Iteration 10600: Loss = -11585.979632454353
Iteration 10700: Loss = -11585.979395837721
Iteration 10800: Loss = -11585.980412955258
1
Iteration 10900: Loss = -11585.980543252514
2
Iteration 11000: Loss = -11585.981397188774
3
Iteration 11100: Loss = -11585.980277138207
4
Iteration 11200: Loss = -11585.989326801771
5
Iteration 11300: Loss = -11586.002341426325
6
Iteration 11400: Loss = -11585.981119900254
7
Iteration 11500: Loss = -11585.98013688379
8
Iteration 11600: Loss = -11585.987236120474
9
Iteration 11700: Loss = -11586.013562152366
10
Iteration 11800: Loss = -11585.993970053474
11
Iteration 11900: Loss = -11586.016981957808
12
Iteration 12000: Loss = -11586.053247788579
13
Iteration 12100: Loss = -11586.00075670207
14
Iteration 12200: Loss = -11585.981413644113
15
Stopping early at iteration 12200 due to no improvement.
pi: tensor([[0.7524, 0.2476],
        [0.2881, 0.7119]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4284, 0.5716], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1941, 0.1034],
         [0.5497, 0.4054]],

        [[0.5842, 0.1079],
         [0.6629, 0.5205]],

        [[0.5580, 0.1005],
         [0.5400, 0.6589]],

        [[0.6623, 0.1085],
         [0.7211, 0.5482]],

        [[0.5220, 0.0884],
         [0.6805, 0.6715]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11593.789627337863
[0.1279085214282292, 1.0] [0.741840451191414, 1.0] [11882.491340713903, 11585.981413644113]
-------------------------------------
This iteration is 80
True Objective function: Loss = -11499.722511335178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20328.477733475975
Iteration 100: Loss = -11493.060164457296
Iteration 200: Loss = -11491.886423366795
Iteration 300: Loss = -11491.670170673144
Iteration 400: Loss = -11491.576538530902
Iteration 500: Loss = -11491.527224970545
Iteration 600: Loss = -11491.497346741393
Iteration 700: Loss = -11491.47774187464
Iteration 800: Loss = -11491.46410973787
Iteration 900: Loss = -11491.454213713596
Iteration 1000: Loss = -11491.44679650214
Iteration 1100: Loss = -11491.44112323174
Iteration 1200: Loss = -11491.436616108047
Iteration 1300: Loss = -11491.433068171467
Iteration 1400: Loss = -11491.430094474874
Iteration 1500: Loss = -11491.427696116836
Iteration 1600: Loss = -11491.425677822128
Iteration 1700: Loss = -11491.423960052834
Iteration 1800: Loss = -11491.422490736175
Iteration 1900: Loss = -11491.421257300804
Iteration 2000: Loss = -11491.42017231766
Iteration 2100: Loss = -11491.419225356129
Iteration 2200: Loss = -11491.418425682981
Iteration 2300: Loss = -11491.41769130686
Iteration 2400: Loss = -11491.417040630058
Iteration 2500: Loss = -11491.416500851861
Iteration 2600: Loss = -11491.415969765863
Iteration 2700: Loss = -11491.415480534477
Iteration 2800: Loss = -11491.415121390526
Iteration 2900: Loss = -11491.415050527952
Iteration 3000: Loss = -11491.414633541284
Iteration 3100: Loss = -11491.41411602039
Iteration 3200: Loss = -11491.413864166581
Iteration 3300: Loss = -11491.420017541559
1
Iteration 3400: Loss = -11491.413350484225
Iteration 3500: Loss = -11491.41567131546
1
Iteration 3600: Loss = -11491.412998768106
Iteration 3700: Loss = -11491.412806348504
Iteration 3800: Loss = -11491.412630240231
Iteration 3900: Loss = -11491.413610219413
1
Iteration 4000: Loss = -11491.41234746736
Iteration 4100: Loss = -11491.412897204844
1
Iteration 4200: Loss = -11491.412702106481
2
Iteration 4300: Loss = -11491.412073112575
Iteration 4400: Loss = -11491.412008589974
Iteration 4500: Loss = -11491.413150451515
1
Iteration 4600: Loss = -11491.411824563138
Iteration 4700: Loss = -11491.412483456428
1
Iteration 4800: Loss = -11491.41155746776
Iteration 4900: Loss = -11491.412991925503
1
Iteration 5000: Loss = -11491.411431319728
Iteration 5100: Loss = -11491.411602269814
1
Iteration 5200: Loss = -11491.411262672493
Iteration 5300: Loss = -11491.411567546778
1
Iteration 5400: Loss = -11491.411709090753
2
Iteration 5500: Loss = -11491.41868515445
3
Iteration 5600: Loss = -11491.411112935304
Iteration 5700: Loss = -11491.411316762114
1
Iteration 5800: Loss = -11491.411575906677
2
Iteration 5900: Loss = -11491.410964444442
Iteration 6000: Loss = -11491.411048404962
Iteration 6100: Loss = -11491.410939343092
Iteration 6200: Loss = -11491.410895370118
Iteration 6300: Loss = -11491.411493786241
1
Iteration 6400: Loss = -11491.410877510321
Iteration 6500: Loss = -11491.412604845378
1
Iteration 6600: Loss = -11491.410852762243
Iteration 6700: Loss = -11491.41078187576
Iteration 6800: Loss = -11491.411228276353
1
Iteration 6900: Loss = -11491.41074403231
Iteration 7000: Loss = -11491.411047869356
1
Iteration 7100: Loss = -11491.413065803874
2
Iteration 7200: Loss = -11491.410875083107
3
Iteration 7300: Loss = -11491.410691077923
Iteration 7400: Loss = -11491.411264884535
1
Iteration 7500: Loss = -11491.412263517825
2
Iteration 7600: Loss = -11491.41753464402
3
Iteration 7700: Loss = -11491.41084425843
4
Iteration 7800: Loss = -11491.410722193199
Iteration 7900: Loss = -11491.410618146328
Iteration 8000: Loss = -11491.410687792626
Iteration 8100: Loss = -11491.411526993625
1
Iteration 8200: Loss = -11491.413628015694
2
Iteration 8300: Loss = -11491.426581991494
3
Iteration 8400: Loss = -11491.411749246492
4
Iteration 8500: Loss = -11491.410534650233
Iteration 8600: Loss = -11491.41078637016
1
Iteration 8700: Loss = -11491.410530410723
Iteration 8800: Loss = -11491.410841518364
1
Iteration 8900: Loss = -11491.411595565485
2
Iteration 9000: Loss = -11491.415005129576
3
Iteration 9100: Loss = -11491.429420868531
4
Iteration 9200: Loss = -11491.497244213071
5
Iteration 9300: Loss = -11491.414192779517
6
Iteration 9400: Loss = -11491.410911848367
7
Iteration 9500: Loss = -11491.410765036384
8
Iteration 9600: Loss = -11491.41062097083
Iteration 9700: Loss = -11491.410972650261
1
Iteration 9800: Loss = -11491.41659341273
2
Iteration 9900: Loss = -11491.436635062673
3
Iteration 10000: Loss = -11491.447149391804
4
Iteration 10100: Loss = -11491.411872184413
5
Iteration 10200: Loss = -11491.418867574292
6
Iteration 10300: Loss = -11491.41264748119
7
Iteration 10400: Loss = -11491.411875912281
8
Iteration 10500: Loss = -11491.42576356421
9
Iteration 10600: Loss = -11491.425682790732
10
Iteration 10700: Loss = -11491.414289812376
11
Iteration 10800: Loss = -11491.462379044906
12
Iteration 10900: Loss = -11491.41414238527
13
Iteration 11000: Loss = -11491.470848916686
14
Iteration 11100: Loss = -11491.418185355995
15
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[0.8219, 0.1781],
        [0.3063, 0.6937]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5189, 0.4811], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2033, 0.0990],
         [0.5511, 0.4050]],

        [[0.5442, 0.1075],
         [0.6430, 0.5400]],

        [[0.6188, 0.1031],
         [0.6643, 0.5028]],

        [[0.5270, 0.1040],
         [0.6411, 0.5736]],

        [[0.7275, 0.1095],
         [0.6498, 0.5663]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22200.093389339825
Iteration 100: Loss = -12093.794125669003
Iteration 200: Loss = -12089.997928311557
Iteration 300: Loss = -12035.186600522695
Iteration 400: Loss = -11567.182971667688
Iteration 500: Loss = -11508.863995151274
Iteration 600: Loss = -11496.489650965596
Iteration 700: Loss = -11493.662829184565
Iteration 800: Loss = -11492.1495222342
Iteration 900: Loss = -11491.99222654286
Iteration 1000: Loss = -11491.887130916455
Iteration 1100: Loss = -11491.81125557389
Iteration 1200: Loss = -11491.753598754185
Iteration 1300: Loss = -11491.708168561689
Iteration 1400: Loss = -11491.671403632872
Iteration 1500: Loss = -11491.639544709262
Iteration 1600: Loss = -11491.605414633597
Iteration 1700: Loss = -11491.583676012566
Iteration 1800: Loss = -11491.567318322052
Iteration 1900: Loss = -11491.553644254498
Iteration 2000: Loss = -11491.541984375654
Iteration 2100: Loss = -11491.53193679095
Iteration 2200: Loss = -11491.523192236797
Iteration 2300: Loss = -11491.515502115455
Iteration 2400: Loss = -11491.508688231364
Iteration 2500: Loss = -11491.502705418252
Iteration 2600: Loss = -11491.497327056808
Iteration 2700: Loss = -11491.492479726883
Iteration 2800: Loss = -11491.487996851081
Iteration 2900: Loss = -11491.483941378834
Iteration 3000: Loss = -11491.480060904523
Iteration 3100: Loss = -11491.47611331558
Iteration 3200: Loss = -11491.471705063177
Iteration 3300: Loss = -11491.466974071349
Iteration 3400: Loss = -11491.46393805357
Iteration 3500: Loss = -11491.46171205454
Iteration 3600: Loss = -11491.45959351934
Iteration 3700: Loss = -11491.457723413805
Iteration 3800: Loss = -11491.45724103317
Iteration 3900: Loss = -11491.454430080224
Iteration 4000: Loss = -11491.453008800527
Iteration 4100: Loss = -11491.479156784471
1
Iteration 4200: Loss = -11491.450431458059
Iteration 4300: Loss = -11491.4492881502
Iteration 4400: Loss = -11491.458502401274
1
Iteration 4500: Loss = -11491.447264506593
Iteration 4600: Loss = -11491.446301430724
Iteration 4700: Loss = -11491.45344490217
1
Iteration 4800: Loss = -11491.446728501885
2
Iteration 4900: Loss = -11491.443927787423
Iteration 5000: Loss = -11491.443196515716
Iteration 5100: Loss = -11491.442506820647
Iteration 5200: Loss = -11491.4418509995
Iteration 5300: Loss = -11491.441209178744
Iteration 5400: Loss = -11491.440418310336
Iteration 5500: Loss = -11491.439384247575
Iteration 5600: Loss = -11491.435691422237
Iteration 5700: Loss = -11491.434991579388
Iteration 5800: Loss = -11491.438871115219
1
Iteration 5900: Loss = -11491.434144475481
Iteration 6000: Loss = -11491.43370695322
Iteration 6100: Loss = -11491.437891145302
1
Iteration 6200: Loss = -11491.433032037265
Iteration 6300: Loss = -11491.433080330627
Iteration 6400: Loss = -11491.432924747993
Iteration 6500: Loss = -11491.43194955449
Iteration 6600: Loss = -11491.453250158349
1
Iteration 6700: Loss = -11491.433253769492
2
Iteration 6800: Loss = -11491.430725206596
Iteration 6900: Loss = -11491.430995576668
1
Iteration 7000: Loss = -11491.430304758063
Iteration 7100: Loss = -11491.43823364565
1
Iteration 7200: Loss = -11491.432206744506
2
Iteration 7300: Loss = -11491.429789071673
Iteration 7400: Loss = -11491.429829946866
Iteration 7500: Loss = -11491.429485178083
Iteration 7600: Loss = -11491.42933246291
Iteration 7700: Loss = -11491.430188096967
1
Iteration 7800: Loss = -11491.429063426096
Iteration 7900: Loss = -11491.429412512161
1
Iteration 8000: Loss = -11491.429318790333
2
Iteration 8100: Loss = -11491.448782985293
3
Iteration 8200: Loss = -11491.432997247299
4
Iteration 8300: Loss = -11491.432378474497
5
Iteration 8400: Loss = -11491.428868927085
Iteration 8500: Loss = -11491.43665304861
1
Iteration 8600: Loss = -11491.436254847566
2
Iteration 8700: Loss = -11491.428217946024
Iteration 8800: Loss = -11491.428217128463
Iteration 8900: Loss = -11491.428929179354
1
Iteration 9000: Loss = -11491.43863270424
2
Iteration 9100: Loss = -11491.436291929343
3
Iteration 9200: Loss = -11491.433781405889
4
Iteration 9300: Loss = -11491.48330093421
5
Iteration 9400: Loss = -11491.42776098352
Iteration 9500: Loss = -11491.427963192617
1
Iteration 9600: Loss = -11491.515422461378
2
Iteration 9700: Loss = -11491.452270335649
3
Iteration 9800: Loss = -11491.46654378387
4
Iteration 9900: Loss = -11491.435263279009
5
Iteration 10000: Loss = -11491.438082327302
6
Iteration 10100: Loss = -11491.43101303574
7
Iteration 10200: Loss = -11491.43455993021
8
Iteration 10300: Loss = -11491.47098990816
9
Iteration 10400: Loss = -11491.42793548337
10
Iteration 10500: Loss = -11491.433510309565
11
Iteration 10600: Loss = -11491.542451966328
12
Iteration 10700: Loss = -11491.433954119595
13
Iteration 10800: Loss = -11491.42781268996
Iteration 10900: Loss = -11491.435901165336
1
Iteration 11000: Loss = -11491.457422325153
2
Iteration 11100: Loss = -11491.428522300925
3
Iteration 11200: Loss = -11491.452526173252
4
Iteration 11300: Loss = -11491.427546319726
Iteration 11400: Loss = -11491.452327972718
1
Iteration 11500: Loss = -11491.428708217003
2
Iteration 11600: Loss = -11491.452625297903
3
Iteration 11700: Loss = -11491.429049497385
4
Iteration 11800: Loss = -11491.453693236055
5
Iteration 11900: Loss = -11491.471246272691
6
Iteration 12000: Loss = -11491.450423528651
7
Iteration 12100: Loss = -11491.428010219086
8
Iteration 12200: Loss = -11491.430980937022
9
Iteration 12300: Loss = -11491.444876234988
10
Iteration 12400: Loss = -11491.427916166218
11
Iteration 12500: Loss = -11491.435876627474
12
Iteration 12600: Loss = -11491.429337561149
13
Iteration 12700: Loss = -11491.429248423015
14
Iteration 12800: Loss = -11491.429424328082
15
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[0.6903, 0.3097],
        [0.1782, 0.8218]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4800, 0.5200], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4049, 0.0987],
         [0.6349, 0.2031]],

        [[0.5013, 0.1077],
         [0.6419, 0.5485]],

        [[0.5967, 0.1031],
         [0.6795, 0.5494]],

        [[0.5156, 0.1045],
         [0.5752, 0.7043]],

        [[0.5526, 0.1096],
         [0.5199, 0.6273]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11499.722511335178
[1.0, 1.0] [1.0, 1.0] [11491.418185355995, 11491.429424328082]
-------------------------------------
This iteration is 81
True Objective function: Loss = -11525.258192623376
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23634.63859760338
Iteration 100: Loss = -12251.657687595503
Iteration 200: Loss = -12212.513961688124
Iteration 300: Loss = -11973.581316066417
Iteration 400: Loss = -11726.761602523058
Iteration 500: Loss = -11662.498068897097
Iteration 600: Loss = -11658.752374255446
Iteration 700: Loss = -11643.180903512482
Iteration 800: Loss = -11599.759840679451
Iteration 900: Loss = -11593.670771884696
Iteration 1000: Loss = -11593.240378742983
Iteration 1100: Loss = -11577.374620594845
Iteration 1200: Loss = -11547.365973615377
Iteration 1300: Loss = -11521.362929570405
Iteration 1400: Loss = -11510.608110764195
Iteration 1500: Loss = -11510.49599000399
Iteration 1600: Loss = -11510.419749873145
Iteration 1700: Loss = -11510.357809164567
Iteration 1800: Loss = -11510.315156181336
Iteration 1900: Loss = -11510.282802483729
Iteration 2000: Loss = -11510.256617034394
Iteration 2100: Loss = -11510.234831346617
Iteration 2200: Loss = -11510.216406188529
Iteration 2300: Loss = -11510.200743325115
Iteration 2400: Loss = -11510.187151326407
Iteration 2500: Loss = -11510.175369423785
Iteration 2600: Loss = -11510.165016198536
Iteration 2700: Loss = -11510.155762389382
Iteration 2800: Loss = -11510.147662321164
Iteration 2900: Loss = -11510.14039987244
Iteration 3000: Loss = -11510.133896207148
Iteration 3100: Loss = -11510.128043061062
Iteration 3200: Loss = -11510.122731453277
Iteration 3300: Loss = -11510.118209862154
Iteration 3400: Loss = -11510.1136036129
Iteration 3500: Loss = -11510.109665849139
Iteration 3600: Loss = -11510.106765010829
Iteration 3700: Loss = -11510.105198576615
Iteration 3800: Loss = -11510.09965819682
Iteration 3900: Loss = -11510.114551606735
1
Iteration 4000: Loss = -11510.094289758
Iteration 4100: Loss = -11510.091912237474
Iteration 4200: Loss = -11510.089699145063
Iteration 4300: Loss = -11510.087984128786
Iteration 4400: Loss = -11510.085835916032
Iteration 4500: Loss = -11510.084037367324
Iteration 4600: Loss = -11510.082471680014
Iteration 4700: Loss = -11510.083583008496
1
Iteration 4800: Loss = -11510.079705798262
Iteration 4900: Loss = -11510.081072635017
1
Iteration 5000: Loss = -11510.08128826787
2
Iteration 5100: Loss = -11510.075656791809
Iteration 5200: Loss = -11510.07456561104
Iteration 5300: Loss = -11510.075645121657
1
Iteration 5400: Loss = -11510.072470527211
Iteration 5500: Loss = -11510.071472540441
Iteration 5600: Loss = -11510.070679608882
Iteration 5700: Loss = -11510.069394527192
Iteration 5800: Loss = -11510.069598392385
1
Iteration 5900: Loss = -11510.06855776049
Iteration 6000: Loss = -11510.06921245095
1
Iteration 6100: Loss = -11510.066073090386
Iteration 6200: Loss = -11510.065642432546
Iteration 6300: Loss = -11510.064854887456
Iteration 6400: Loss = -11510.073626967205
1
Iteration 6500: Loss = -11510.063748756946
Iteration 6600: Loss = -11510.075899987269
1
Iteration 6700: Loss = -11510.062583586638
Iteration 6800: Loss = -11510.062134307613
Iteration 6900: Loss = -11510.063197228299
1
Iteration 7000: Loss = -11510.062569571364
2
Iteration 7100: Loss = -11510.060724386629
Iteration 7200: Loss = -11510.06056526544
Iteration 7300: Loss = -11510.061332427727
1
Iteration 7400: Loss = -11510.060078505192
Iteration 7500: Loss = -11510.05966380042
Iteration 7600: Loss = -11510.059254972857
Iteration 7700: Loss = -11510.059510371439
1
Iteration 7800: Loss = -11510.080499365135
2
Iteration 7900: Loss = -11510.058507159456
Iteration 8000: Loss = -11510.058595049764
Iteration 8100: Loss = -11510.0580234565
Iteration 8200: Loss = -11510.058419601197
1
Iteration 8300: Loss = -11510.057564025541
Iteration 8400: Loss = -11510.059876293539
1
Iteration 8500: Loss = -11510.057725997907
2
Iteration 8600: Loss = -11510.058231761555
3
Iteration 8700: Loss = -11510.05702196953
Iteration 8800: Loss = -11510.05685013241
Iteration 8900: Loss = -11510.063165448719
1
Iteration 9000: Loss = -11510.070366595568
2
Iteration 9100: Loss = -11510.10075143823
3
Iteration 9200: Loss = -11510.056418464013
Iteration 9300: Loss = -11510.057329710065
1
Iteration 9400: Loss = -11510.056312332352
Iteration 9500: Loss = -11510.056599499549
1
Iteration 9600: Loss = -11510.059146100653
2
Iteration 9700: Loss = -11510.055758215536
Iteration 9800: Loss = -11510.055768270857
Iteration 9900: Loss = -11510.23222745041
1
Iteration 10000: Loss = -11510.055466772112
Iteration 10100: Loss = -11510.082093542553
1
Iteration 10200: Loss = -11510.055361089306
Iteration 10300: Loss = -11510.05553760533
1
Iteration 10400: Loss = -11510.063882815966
2
Iteration 10500: Loss = -11510.208025067486
3
Iteration 10600: Loss = -11510.055296665252
Iteration 10700: Loss = -11510.056538296483
1
Iteration 10800: Loss = -11510.055454741538
2
Iteration 10900: Loss = -11510.065242418186
3
Iteration 11000: Loss = -11510.059866969656
4
Iteration 11100: Loss = -11510.075578591299
5
Iteration 11200: Loss = -11510.066995267003
6
Iteration 11300: Loss = -11510.063816515665
7
Iteration 11400: Loss = -11510.054879366637
Iteration 11500: Loss = -11510.055074503525
1
Iteration 11600: Loss = -11510.062663938115
2
Iteration 11700: Loss = -11510.075550507589
3
Iteration 11800: Loss = -11510.178318309521
4
Iteration 11900: Loss = -11510.055128840691
5
Iteration 12000: Loss = -11510.056415884064
6
Iteration 12100: Loss = -11510.054792056733
Iteration 12200: Loss = -11510.067606778124
1
Iteration 12300: Loss = -11510.059102482146
2
Iteration 12400: Loss = -11510.133781402834
3
Iteration 12500: Loss = -11510.054708691587
Iteration 12600: Loss = -11510.055459902504
1
Iteration 12700: Loss = -11510.095043664385
2
Iteration 12800: Loss = -11510.054581836783
Iteration 12900: Loss = -11510.055757269683
1
Iteration 13000: Loss = -11510.062728119992
2
Iteration 13100: Loss = -11510.06835042906
3
Iteration 13200: Loss = -11510.072768359669
4
Iteration 13300: Loss = -11510.057548712231
5
Iteration 13400: Loss = -11510.057567034577
6
Iteration 13500: Loss = -11510.056048786246
7
Iteration 13600: Loss = -11510.060514778219
8
Iteration 13700: Loss = -11510.060433397824
9
Iteration 13800: Loss = -11510.055045542847
10
Iteration 13900: Loss = -11510.063454036013
11
Iteration 14000: Loss = -11510.058770407455
12
Iteration 14100: Loss = -11510.055559244905
13
Iteration 14200: Loss = -11510.08841790669
14
Iteration 14300: Loss = -11510.055615963936
15
Stopping early at iteration 14300 due to no improvement.
pi: tensor([[0.7096, 0.2904],
        [0.2894, 0.7106]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5888, 0.4112], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1928, 0.0981],
         [0.5858, 0.4106]],

        [[0.7236, 0.1057],
         [0.6488, 0.5677]],

        [[0.5629, 0.0993],
         [0.5966, 0.6320]],

        [[0.6257, 0.1064],
         [0.6736, 0.6540]],

        [[0.6522, 0.1080],
         [0.6808, 0.6842]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9841599999999999
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24378.457136867368
Iteration 100: Loss = -12252.547701288035
Iteration 200: Loss = -12196.34815273175
Iteration 300: Loss = -11721.993767722655
Iteration 400: Loss = -11658.610081328
Iteration 500: Loss = -11612.28457990248
Iteration 600: Loss = -11593.277997224694
Iteration 700: Loss = -11546.370522414896
Iteration 800: Loss = -11510.783427453993
Iteration 900: Loss = -11510.573629596145
Iteration 1000: Loss = -11510.449821845166
Iteration 1100: Loss = -11510.364040363353
Iteration 1200: Loss = -11510.30966123955
Iteration 1300: Loss = -11510.26970795701
Iteration 1400: Loss = -11510.238960145216
Iteration 1500: Loss = -11510.21457467871
Iteration 1600: Loss = -11510.19488401311
Iteration 1700: Loss = -11510.178648531119
Iteration 1800: Loss = -11510.16510086543
Iteration 1900: Loss = -11510.153631370933
Iteration 2000: Loss = -11510.143835425752
Iteration 2100: Loss = -11510.135403106553
Iteration 2200: Loss = -11510.128096906652
Iteration 2300: Loss = -11510.121643096041
Iteration 2400: Loss = -11510.116035026258
Iteration 2500: Loss = -11510.11102668689
Iteration 2600: Loss = -11510.10658675095
Iteration 2700: Loss = -11510.102576806887
Iteration 2800: Loss = -11510.09901464944
Iteration 2900: Loss = -11510.095784508645
Iteration 3000: Loss = -11510.093101707233
Iteration 3100: Loss = -11510.090171187085
Iteration 3200: Loss = -11510.08775336011
Iteration 3300: Loss = -11510.085530428838
Iteration 3400: Loss = -11510.083338538374
Iteration 3500: Loss = -11510.082324681925
Iteration 3600: Loss = -11510.079335383562
Iteration 3700: Loss = -11510.077775147509
Iteration 3800: Loss = -11510.07619070624
Iteration 3900: Loss = -11510.074853866678
Iteration 4000: Loss = -11510.078186047018
1
Iteration 4100: Loss = -11510.072443340536
Iteration 4200: Loss = -11510.071355625383
Iteration 4300: Loss = -11510.07224780102
1
Iteration 4400: Loss = -11510.069561436974
Iteration 4500: Loss = -11510.069070654463
Iteration 4600: Loss = -11510.083506103863
1
Iteration 4700: Loss = -11510.07011859223
2
Iteration 4800: Loss = -11510.06649543782
Iteration 4900: Loss = -11510.065886417868
Iteration 5000: Loss = -11510.065347699347
Iteration 5100: Loss = -11510.064771370051
Iteration 5200: Loss = -11510.064282255467
Iteration 5300: Loss = -11510.063772418413
Iteration 5400: Loss = -11510.06385587507
Iteration 5500: Loss = -11510.067151348174
1
Iteration 5600: Loss = -11510.075037083201
2
Iteration 5700: Loss = -11510.06211890872
Iteration 5800: Loss = -11510.062825230101
1
Iteration 5900: Loss = -11510.06149126526
Iteration 6000: Loss = -11510.061427684606
Iteration 6100: Loss = -11510.06100884933
Iteration 6200: Loss = -11510.065251891236
1
Iteration 6300: Loss = -11510.060545103664
Iteration 6400: Loss = -11510.063629200476
1
Iteration 6500: Loss = -11510.059966591858
Iteration 6600: Loss = -11510.061989443631
1
Iteration 6700: Loss = -11510.059435270636
Iteration 6800: Loss = -11510.059442761461
Iteration 6900: Loss = -11510.062485851735
1
Iteration 7000: Loss = -11510.059070204976
Iteration 7100: Loss = -11510.060184288483
1
Iteration 7200: Loss = -11510.05823669595
Iteration 7300: Loss = -11510.066173824363
1
Iteration 7400: Loss = -11510.059931743
2
Iteration 7500: Loss = -11510.05756713045
Iteration 7600: Loss = -11510.057261801665
Iteration 7700: Loss = -11510.065573235594
1
Iteration 7800: Loss = -11510.05857159278
2
Iteration 7900: Loss = -11510.062134819435
3
Iteration 8000: Loss = -11510.068654294528
4
Iteration 8100: Loss = -11510.05639898143
Iteration 8200: Loss = -11510.056157694076
Iteration 8300: Loss = -11510.056269557648
1
Iteration 8400: Loss = -11510.056038881663
Iteration 8500: Loss = -11510.057018401007
1
Iteration 8600: Loss = -11510.05588551855
Iteration 8700: Loss = -11510.056073607868
1
Iteration 8800: Loss = -11510.055748626664
Iteration 8900: Loss = -11510.055699565126
Iteration 9000: Loss = -11510.055736721933
Iteration 9100: Loss = -11510.078878919481
1
Iteration 9200: Loss = -11510.05570288643
Iteration 9300: Loss = -11510.059456148958
1
Iteration 9400: Loss = -11510.055446141818
Iteration 9500: Loss = -11510.056092063001
1
Iteration 9600: Loss = -11510.05538132006
Iteration 9700: Loss = -11510.055584898202
1
Iteration 9800: Loss = -11510.074973311599
2
Iteration 9900: Loss = -11510.054970188065
Iteration 10000: Loss = -11510.056702896516
1
Iteration 10100: Loss = -11510.055023329234
Iteration 10200: Loss = -11510.05688247822
1
Iteration 10300: Loss = -11510.057950072918
2
Iteration 10400: Loss = -11510.058717572465
3
Iteration 10500: Loss = -11510.070513562972
4
Iteration 10600: Loss = -11510.063654175421
5
Iteration 10700: Loss = -11510.055774904826
6
Iteration 10800: Loss = -11510.055838130253
7
Iteration 10900: Loss = -11510.055604258712
8
Iteration 11000: Loss = -11510.217644832372
9
Iteration 11100: Loss = -11510.062475078848
10
Iteration 11200: Loss = -11510.160112633996
11
Iteration 11300: Loss = -11510.05486784131
Iteration 11400: Loss = -11510.054688898213
Iteration 11500: Loss = -11510.064124702047
1
Iteration 11600: Loss = -11510.062134612142
2
Iteration 11700: Loss = -11510.055426746243
3
Iteration 11800: Loss = -11510.054898767228
4
Iteration 11900: Loss = -11510.062732121438
5
Iteration 12000: Loss = -11510.111781961301
6
Iteration 12100: Loss = -11510.09384239804
7
Iteration 12200: Loss = -11510.05486399571
8
Iteration 12300: Loss = -11510.054733944899
Iteration 12400: Loss = -11510.059812763593
1
Iteration 12500: Loss = -11510.055903889785
2
Iteration 12600: Loss = -11510.06737287066
3
Iteration 12700: Loss = -11510.066279929666
4
Iteration 12800: Loss = -11510.054511823922
Iteration 12900: Loss = -11510.054744803016
1
Iteration 13000: Loss = -11510.059790316778
2
Iteration 13100: Loss = -11510.054553436757
Iteration 13200: Loss = -11510.059259482241
1
Iteration 13300: Loss = -11510.056052779128
2
Iteration 13400: Loss = -11510.070852701527
3
Iteration 13500: Loss = -11510.05447107403
Iteration 13600: Loss = -11510.05528749664
1
Iteration 13700: Loss = -11510.058829867277
2
Iteration 13800: Loss = -11510.06014854392
3
Iteration 13900: Loss = -11510.054536539772
Iteration 14000: Loss = -11510.055890036268
1
Iteration 14100: Loss = -11510.055119283557
2
Iteration 14200: Loss = -11510.054698526348
3
Iteration 14300: Loss = -11510.055708257427
4
Iteration 14400: Loss = -11510.097152794628
5
Iteration 14500: Loss = -11510.099231929074
6
Iteration 14600: Loss = -11510.054579333191
Iteration 14700: Loss = -11510.057564671388
1
Iteration 14800: Loss = -11510.056451466644
2
Iteration 14900: Loss = -11510.05960517502
3
Iteration 15000: Loss = -11510.054471772066
Iteration 15100: Loss = -11510.054408123839
Iteration 15200: Loss = -11510.121439572293
1
Iteration 15300: Loss = -11510.054940364376
2
Iteration 15400: Loss = -11510.055274024775
3
Iteration 15500: Loss = -11510.063943790836
4
Iteration 15600: Loss = -11510.054685283005
5
Iteration 15700: Loss = -11510.058528925318
6
Iteration 15800: Loss = -11510.066482376371
7
Iteration 15900: Loss = -11510.06090249563
8
Iteration 16000: Loss = -11510.056537110842
9
Iteration 16100: Loss = -11510.056660243683
10
Iteration 16200: Loss = -11510.082759087938
11
Iteration 16300: Loss = -11510.06347869918
12
Iteration 16400: Loss = -11510.056808286541
13
Iteration 16500: Loss = -11510.0558056801
14
Iteration 16600: Loss = -11510.094774745174
15
Stopping early at iteration 16600 due to no improvement.
pi: tensor([[0.7130, 0.2870],
        [0.2895, 0.7105]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5906, 0.4094], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1931, 0.0981],
         [0.5747, 0.4122]],

        [[0.6822, 0.1064],
         [0.5029, 0.5771]],

        [[0.5161, 0.0987],
         [0.7001, 0.6783]],

        [[0.7092, 0.1064],
         [0.6112, 0.6450]],

        [[0.6022, 0.1084],
         [0.5061, 0.6018]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
Global Adjusted Rand Index: 0.9840320230862317
Average Adjusted Rand Index: 0.9841599999999999
11525.258192623376
[0.9840320230862317, 0.9840320230862317] [0.9841599999999999, 0.9841599999999999] [11510.055615963936, 11510.094774745174]
-------------------------------------
This iteration is 82
True Objective function: Loss = -11678.908891105853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20725.461488693163
Iteration 100: Loss = -12511.210644132021
Iteration 200: Loss = -12115.221233161581
Iteration 300: Loss = -11849.352967990537
Iteration 400: Loss = -11811.622333119416
Iteration 500: Loss = -11759.940889998983
Iteration 600: Loss = -11746.093870944958
Iteration 700: Loss = -11693.264084976445
Iteration 800: Loss = -11687.317045376376
Iteration 900: Loss = -11686.028134708911
Iteration 1000: Loss = -11683.015841705661
Iteration 1100: Loss = -11682.83529092294
Iteration 1200: Loss = -11675.067748700645
Iteration 1300: Loss = -11675.038566957275
Iteration 1400: Loss = -11675.01747006484
Iteration 1500: Loss = -11675.000993456631
Iteration 1600: Loss = -11674.987760347094
Iteration 1700: Loss = -11674.976878095222
Iteration 1800: Loss = -11674.967707885893
Iteration 1900: Loss = -11674.959889781467
Iteration 2000: Loss = -11674.95313515165
Iteration 2100: Loss = -11674.947322824031
Iteration 2200: Loss = -11674.942162047399
Iteration 2300: Loss = -11674.937614543554
Iteration 2400: Loss = -11674.933491112499
Iteration 2500: Loss = -11674.929785951083
Iteration 2600: Loss = -11674.95017689628
1
Iteration 2700: Loss = -11674.92350476176
Iteration 2800: Loss = -11674.921039552668
Iteration 2900: Loss = -11674.918835269458
Iteration 3000: Loss = -11674.918707860626
Iteration 3100: Loss = -11674.91508382478
Iteration 3200: Loss = -11674.91341685567
Iteration 3300: Loss = -11674.911889175472
Iteration 3400: Loss = -11674.911217992889
Iteration 3500: Loss = -11674.909116207813
Iteration 3600: Loss = -11674.907844915686
Iteration 3700: Loss = -11674.906632422957
Iteration 3800: Loss = -11674.905525446904
Iteration 3900: Loss = -11674.904563129177
Iteration 4000: Loss = -11674.903816287508
Iteration 4100: Loss = -11674.904526725779
1
Iteration 4200: Loss = -11674.903726102762
Iteration 4300: Loss = -11674.901737293694
Iteration 4400: Loss = -11674.90114305242
Iteration 4500: Loss = -11674.901047370622
Iteration 4600: Loss = -11674.90009467087
Iteration 4700: Loss = -11674.899615287446
Iteration 4800: Loss = -11674.899179576283
Iteration 4900: Loss = -11674.89885791757
Iteration 5000: Loss = -11674.901413273194
1
Iteration 5100: Loss = -11674.898129285732
Iteration 5200: Loss = -11674.897673369262
Iteration 5300: Loss = -11674.901184669712
1
Iteration 5400: Loss = -11674.8970506329
Iteration 5500: Loss = -11674.897903030349
1
Iteration 5600: Loss = -11674.896504286602
Iteration 5700: Loss = -11674.896272848076
Iteration 5800: Loss = -11674.896063508437
Iteration 5900: Loss = -11674.895847153492
Iteration 6000: Loss = -11674.898125474714
1
Iteration 6100: Loss = -11674.895391445736
Iteration 6200: Loss = -11674.895206999736
Iteration 6300: Loss = -11674.895534221356
1
Iteration 6400: Loss = -11674.894775091585
Iteration 6500: Loss = -11674.894581904642
Iteration 6600: Loss = -11674.894626767775
Iteration 6700: Loss = -11674.905183244467
1
Iteration 6800: Loss = -11674.894316690375
Iteration 6900: Loss = -11674.895982774922
1
Iteration 7000: Loss = -11674.893946305952
Iteration 7100: Loss = -11674.893890325393
Iteration 7200: Loss = -11674.893804303127
Iteration 7300: Loss = -11674.933582568972
1
Iteration 7400: Loss = -11674.893575296122
Iteration 7500: Loss = -11674.893576547307
Iteration 7600: Loss = -11674.893422494122
Iteration 7700: Loss = -11674.893451677632
Iteration 7800: Loss = -11674.896284867607
1
Iteration 7900: Loss = -11674.893187281386
Iteration 8000: Loss = -11674.8972083336
1
Iteration 8100: Loss = -11674.902878502597
2
Iteration 8200: Loss = -11674.893003870493
Iteration 8300: Loss = -11674.894960427604
1
Iteration 8400: Loss = -11674.89335481388
2
Iteration 8500: Loss = -11674.89929609044
3
Iteration 8600: Loss = -11674.893440877897
4
Iteration 8700: Loss = -11674.925638348172
5
Iteration 8800: Loss = -11674.893464843666
6
Iteration 8900: Loss = -11674.896357867945
7
Iteration 9000: Loss = -11674.893347724657
8
Iteration 9100: Loss = -11674.892617855834
Iteration 9200: Loss = -11674.89874661699
1
Iteration 9300: Loss = -11674.892560276465
Iteration 9400: Loss = -11674.904038778319
1
Iteration 9500: Loss = -11674.895860794306
2
Iteration 9600: Loss = -11674.901754271988
3
Iteration 9700: Loss = -11674.922519613408
4
Iteration 9800: Loss = -11674.898492926373
5
Iteration 9900: Loss = -11674.912706631967
6
Iteration 10000: Loss = -11674.893470616222
7
Iteration 10100: Loss = -11674.894523182355
8
Iteration 10200: Loss = -11674.89537532121
9
Iteration 10300: Loss = -11674.904518087009
10
Iteration 10400: Loss = -11674.90122763138
11
Iteration 10500: Loss = -11674.89690926277
12
Iteration 10600: Loss = -11674.918891091684
13
Iteration 10700: Loss = -11674.897191681184
14
Iteration 10800: Loss = -11674.914688105135
15
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[0.7488, 0.2512],
        [0.2651, 0.7349]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5086, 0.4914], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4053, 0.1012],
         [0.5079, 0.2039]],

        [[0.6870, 0.0955],
         [0.6840, 0.5756]],

        [[0.6875, 0.1024],
         [0.7288, 0.5249]],

        [[0.5503, 0.1069],
         [0.5399, 0.7293]],

        [[0.5677, 0.1020],
         [0.5549, 0.5446]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23629.448459590225
Iteration 100: Loss = -12511.84025730751
Iteration 200: Loss = -12421.909571430951
Iteration 300: Loss = -12044.420626316103
Iteration 400: Loss = -11870.481493386751
Iteration 500: Loss = -11816.543331961077
Iteration 600: Loss = -11793.547244170448
Iteration 700: Loss = -11765.503141292413
Iteration 800: Loss = -11762.018262277314
Iteration 900: Loss = -11744.191794628343
Iteration 1000: Loss = -11703.797716842957
Iteration 1100: Loss = -11703.55622868721
Iteration 1200: Loss = -11697.420831084892
Iteration 1300: Loss = -11682.134098613362
Iteration 1400: Loss = -11681.903947774594
Iteration 1500: Loss = -11681.83876652694
Iteration 1600: Loss = -11675.396785356745
Iteration 1700: Loss = -11675.126561055948
Iteration 1800: Loss = -11675.097491031545
Iteration 1900: Loss = -11675.07413284149
Iteration 2000: Loss = -11675.05481605261
Iteration 2100: Loss = -11675.038536727434
Iteration 2200: Loss = -11675.024548997093
Iteration 2300: Loss = -11675.012493952112
Iteration 2400: Loss = -11675.001963958464
Iteration 2500: Loss = -11674.99272279658
Iteration 2600: Loss = -11674.984597817369
Iteration 2700: Loss = -11674.977212100286
Iteration 2800: Loss = -11674.970723471739
Iteration 2900: Loss = -11674.96635361008
Iteration 3000: Loss = -11674.959636690506
Iteration 3100: Loss = -11674.95493280411
Iteration 3200: Loss = -11674.95058136783
Iteration 3300: Loss = -11674.946780852499
Iteration 3400: Loss = -11674.943137007134
Iteration 3500: Loss = -11674.940033415538
Iteration 3600: Loss = -11674.93723665067
Iteration 3700: Loss = -11674.934255131348
Iteration 3800: Loss = -11674.93238470712
Iteration 3900: Loss = -11674.931510236374
Iteration 4000: Loss = -11674.927539166889
Iteration 4100: Loss = -11674.925350258185
Iteration 4200: Loss = -11674.923606534847
Iteration 4300: Loss = -11674.922415821688
Iteration 4400: Loss = -11674.920371487508
Iteration 4500: Loss = -11674.936646087168
1
Iteration 4600: Loss = -11674.91748529551
Iteration 4700: Loss = -11674.917410327727
Iteration 4800: Loss = -11674.915025225095
Iteration 4900: Loss = -11674.913957330586
Iteration 5000: Loss = -11674.912869338445
Iteration 5100: Loss = -11674.91193816121
Iteration 5200: Loss = -11674.91097310482
Iteration 5300: Loss = -11674.910065127662
Iteration 5400: Loss = -11674.909218066323
Iteration 5500: Loss = -11674.908377495378
Iteration 5600: Loss = -11674.913913416354
1
Iteration 5700: Loss = -11674.906098548921
Iteration 5800: Loss = -11674.905497669397
Iteration 5900: Loss = -11674.90449285507
Iteration 6000: Loss = -11674.91824541187
1
Iteration 6100: Loss = -11674.903388995419
Iteration 6200: Loss = -11674.909570609394
1
Iteration 6300: Loss = -11674.90245692649
Iteration 6400: Loss = -11674.90318308649
1
Iteration 6500: Loss = -11674.90162465393
Iteration 6600: Loss = -11674.90131157639
Iteration 6700: Loss = -11674.900911901908
Iteration 6800: Loss = -11674.900701463019
Iteration 6900: Loss = -11674.901007040819
1
Iteration 7000: Loss = -11674.901151208254
2
Iteration 7100: Loss = -11674.90121667216
3
Iteration 7200: Loss = -11674.917648187755
4
Iteration 7300: Loss = -11674.899457807809
Iteration 7400: Loss = -11674.899018149976
Iteration 7500: Loss = -11674.913112402686
1
Iteration 7600: Loss = -11674.902453842498
2
Iteration 7700: Loss = -11674.900463127378
3
Iteration 7800: Loss = -11674.912821797727
4
Iteration 7900: Loss = -11674.898075928026
Iteration 8000: Loss = -11674.898675080532
1
Iteration 8100: Loss = -11674.898781415399
2
Iteration 8200: Loss = -11674.900057454754
3
Iteration 8300: Loss = -11674.899801713396
4
Iteration 8400: Loss = -11674.898379942582
5
Iteration 8500: Loss = -11674.9014517907
6
Iteration 8600: Loss = -11674.897395137692
Iteration 8700: Loss = -11674.896933942162
Iteration 8800: Loss = -11674.89682899595
Iteration 8900: Loss = -11674.903306784216
1
Iteration 9000: Loss = -11674.896604874515
Iteration 9100: Loss = -11674.896759139536
1
Iteration 9200: Loss = -11674.896238759748
Iteration 9300: Loss = -11675.004646962481
1
Iteration 9400: Loss = -11674.895972854305
Iteration 9500: Loss = -11674.895974946448
Iteration 9600: Loss = -11674.895851435147
Iteration 9700: Loss = -11674.922559730676
1
Iteration 9800: Loss = -11674.913040848982
2
Iteration 9900: Loss = -11674.895845276864
Iteration 10000: Loss = -11674.895865366741
Iteration 10100: Loss = -11674.898734378701
1
Iteration 10200: Loss = -11674.945129681098
2
Iteration 10300: Loss = -11674.906464035366
3
Iteration 10400: Loss = -11674.89545582594
Iteration 10500: Loss = -11674.962594997696
1
Iteration 10600: Loss = -11674.913809151161
2
Iteration 10700: Loss = -11674.898074019868
3
Iteration 10800: Loss = -11674.923223574466
4
Iteration 10900: Loss = -11674.938984386386
5
Iteration 11000: Loss = -11674.900362327413
6
Iteration 11100: Loss = -11674.909039582386
7
Iteration 11200: Loss = -11674.957279080074
8
Iteration 11300: Loss = -11674.934214468989
9
Iteration 11400: Loss = -11674.905852043346
10
Iteration 11500: Loss = -11674.895966823808
11
Iteration 11600: Loss = -11674.895346121397
Iteration 11700: Loss = -11674.89652189847
1
Iteration 11800: Loss = -11674.897683209478
2
Iteration 11900: Loss = -11674.91083972353
3
Iteration 12000: Loss = -11674.901435207124
4
Iteration 12100: Loss = -11674.90016747378
5
Iteration 12200: Loss = -11674.916578390312
6
Iteration 12300: Loss = -11674.896104458672
7
Iteration 12400: Loss = -11674.896214374978
8
Iteration 12500: Loss = -11674.895627652719
9
Iteration 12600: Loss = -11674.902604006018
10
Iteration 12700: Loss = -11674.897460601946
11
Iteration 12800: Loss = -11674.952405177688
12
Iteration 12900: Loss = -11674.912687039608
13
Iteration 13000: Loss = -11674.89863095709
14
Iteration 13100: Loss = -11674.896822787956
15
Stopping early at iteration 13100 due to no improvement.
pi: tensor([[0.7336, 0.2664],
        [0.2530, 0.7470]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4902, 0.5098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2037, 0.1013],
         [0.7182, 0.4047]],

        [[0.6750, 0.0953],
         [0.6484, 0.7225]],

        [[0.5871, 0.1023],
         [0.5423, 0.6885]],

        [[0.6954, 0.1064],
         [0.7260, 0.5605]],

        [[0.5218, 0.1019],
         [0.6182, 0.6037]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919998119331364
11678.908891105853
[0.9919999775871758, 0.9919999775871758] [0.9919998119331364, 0.9919998119331364] [11674.914688105135, 11674.896822787956]
-------------------------------------
This iteration is 83
True Objective function: Loss = -11481.089739882842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21220.220839580423
Iteration 100: Loss = -12163.639653745477
Iteration 200: Loss = -12156.119155947863
Iteration 300: Loss = -11902.159764978782
Iteration 400: Loss = -11581.251284271153
Iteration 500: Loss = -11533.655160756336
Iteration 600: Loss = -11503.620920399111
Iteration 700: Loss = -11498.835476296692
Iteration 800: Loss = -11495.730377421849
Iteration 900: Loss = -11495.513374085618
Iteration 1000: Loss = -11495.254239168089
Iteration 1100: Loss = -11473.68038009869
Iteration 1200: Loss = -11472.329397102398
Iteration 1300: Loss = -11472.256847043047
Iteration 1400: Loss = -11472.202598639351
Iteration 1500: Loss = -11472.151714012438
Iteration 1600: Loss = -11472.11248963766
Iteration 1700: Loss = -11472.087689666932
Iteration 1800: Loss = -11472.067702648428
Iteration 1900: Loss = -11472.051083078966
Iteration 2000: Loss = -11472.037019078434
Iteration 2100: Loss = -11472.025039705331
Iteration 2200: Loss = -11472.014420710611
Iteration 2300: Loss = -11472.005273539768
Iteration 2400: Loss = -11471.997170146084
Iteration 2500: Loss = -11471.990169430506
Iteration 2600: Loss = -11471.983444735251
Iteration 2700: Loss = -11471.977425871934
Iteration 2800: Loss = -11471.971562155724
Iteration 2900: Loss = -11471.964618421773
Iteration 3000: Loss = -11471.947233124098
Iteration 3100: Loss = -11471.939033288616
Iteration 3200: Loss = -11471.935188062595
Iteration 3300: Loss = -11471.931394560772
Iteration 3400: Loss = -11471.928331508157
Iteration 3500: Loss = -11471.928328829574
Iteration 3600: Loss = -11471.92323825412
Iteration 3700: Loss = -11471.921125441331
Iteration 3800: Loss = -11471.919057858782
Iteration 3900: Loss = -11471.917104422795
Iteration 4000: Loss = -11471.934964941167
1
Iteration 4100: Loss = -11471.913774144883
Iteration 4200: Loss = -11471.912256141508
Iteration 4300: Loss = -11471.91303646159
1
Iteration 4400: Loss = -11471.90965812014
Iteration 4500: Loss = -11471.908816161753
Iteration 4600: Loss = -11471.907722924076
Iteration 4700: Loss = -11471.906581271482
Iteration 4800: Loss = -11471.922551273825
1
Iteration 4900: Loss = -11471.909444353825
2
Iteration 5000: Loss = -11471.903851829684
Iteration 5100: Loss = -11471.90360473876
Iteration 5200: Loss = -11471.902452060225
Iteration 5300: Loss = -11471.901898973952
Iteration 5400: Loss = -11471.901216039329
Iteration 5500: Loss = -11471.900667987955
Iteration 5600: Loss = -11471.90040550672
Iteration 5700: Loss = -11471.899661147329
Iteration 5800: Loss = -11471.89925407766
Iteration 5900: Loss = -11471.89898957512
Iteration 6000: Loss = -11471.898374134638
Iteration 6100: Loss = -11471.8979713203
Iteration 6200: Loss = -11471.90009721067
1
Iteration 6300: Loss = -11471.89727708191
Iteration 6400: Loss = -11471.8969453991
Iteration 6500: Loss = -11471.897670900427
1
Iteration 6600: Loss = -11471.89661328452
Iteration 6700: Loss = -11471.89726920596
1
Iteration 6800: Loss = -11471.900429835072
2
Iteration 6900: Loss = -11471.943707283432
3
Iteration 7000: Loss = -11471.89558517419
Iteration 7100: Loss = -11471.896562337211
1
Iteration 7200: Loss = -11471.932351976067
2
Iteration 7300: Loss = -11471.89376013544
Iteration 7400: Loss = -11471.895332924692
1
Iteration 7500: Loss = -11471.897895040367
2
Iteration 7600: Loss = -11471.893781363793
Iteration 7700: Loss = -11471.893172428714
Iteration 7800: Loss = -11471.893122476187
Iteration 7900: Loss = -11471.893284066538
1
Iteration 8000: Loss = -11471.899731800848
2
Iteration 8100: Loss = -11471.893891557122
3
Iteration 8200: Loss = -11471.89670103289
4
Iteration 8300: Loss = -11471.894445608708
5
Iteration 8400: Loss = -11471.892679903092
Iteration 8500: Loss = -11471.89415650795
1
Iteration 8600: Loss = -11471.892412487516
Iteration 8700: Loss = -11471.892673516695
1
Iteration 8800: Loss = -11471.892320176845
Iteration 8900: Loss = -11471.8941311748
1
Iteration 9000: Loss = -11471.896087882096
2
Iteration 9100: Loss = -11471.917539483124
3
Iteration 9200: Loss = -11471.906906441205
4
Iteration 9300: Loss = -11471.891670527613
Iteration 9400: Loss = -11471.89808057045
1
Iteration 9500: Loss = -11471.892885912697
2
Iteration 9600: Loss = -11471.8952386377
3
Iteration 9700: Loss = -11471.920456937814
4
Iteration 9800: Loss = -11471.996121404261
5
Iteration 9900: Loss = -11471.89415640606
6
Iteration 10000: Loss = -11471.891791349875
7
Iteration 10100: Loss = -11471.892412978275
8
Iteration 10200: Loss = -11471.891538259477
Iteration 10300: Loss = -11471.892412402225
1
Iteration 10400: Loss = -11471.967183400064
2
Iteration 10500: Loss = -11471.957393721385
3
Iteration 10600: Loss = -11471.892627305255
4
Iteration 10700: Loss = -11471.89429690368
5
Iteration 10800: Loss = -11471.893237941875
6
Iteration 10900: Loss = -11471.893547807198
7
Iteration 11000: Loss = -11471.907386842682
8
Iteration 11100: Loss = -11471.891082267832
Iteration 11200: Loss = -11471.898214961893
1
Iteration 11300: Loss = -11471.895466872505
2
Iteration 11400: Loss = -11471.892060935339
3
Iteration 11500: Loss = -11471.890087344002
Iteration 11600: Loss = -11471.892046580368
1
Iteration 11700: Loss = -11471.898480629037
2
Iteration 11800: Loss = -11471.921627263218
3
Iteration 11900: Loss = -11471.890061966815
Iteration 12000: Loss = -11471.890661255195
1
Iteration 12100: Loss = -11471.896376476447
2
Iteration 12200: Loss = -11471.893575007985
3
Iteration 12300: Loss = -11471.910964082053
4
Iteration 12400: Loss = -11471.904921112362
5
Iteration 12500: Loss = -11471.898877540814
6
Iteration 12600: Loss = -11471.8918113427
7
Iteration 12700: Loss = -11471.890381003284
8
Iteration 12800: Loss = -11471.891174965896
9
Iteration 12900: Loss = -11471.8922172951
10
Iteration 13000: Loss = -11471.89479970241
11
Iteration 13100: Loss = -11471.894361699437
12
Iteration 13200: Loss = -11471.895873812473
13
Iteration 13300: Loss = -11471.891110845409
14
Iteration 13400: Loss = -11471.901424201415
15
Stopping early at iteration 13400 due to no improvement.
pi: tensor([[0.7507, 0.2493],
        [0.2233, 0.7767]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4426, 0.5574], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3995, 0.1116],
         [0.6395, 0.2016]],

        [[0.6416, 0.0977],
         [0.6144, 0.7164]],

        [[0.5392, 0.1119],
         [0.5005, 0.6170]],

        [[0.6206, 0.0924],
         [0.6801, 0.7229]],

        [[0.5420, 0.0983],
         [0.5681, 0.5893]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318355379102
Average Adjusted Rand Index: 0.9839992163297293
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21137.16408553789
Iteration 100: Loss = -12163.877993985228
Iteration 200: Loss = -12157.183716001608
Iteration 300: Loss = -12041.140838893103
Iteration 400: Loss = -11659.862535222795
Iteration 500: Loss = -11574.649512074391
Iteration 600: Loss = -11527.969325079875
Iteration 700: Loss = -11503.55877461871
Iteration 800: Loss = -11502.830845745668
Iteration 900: Loss = -11495.686549042348
Iteration 1000: Loss = -11495.407540998709
Iteration 1100: Loss = -11494.233227047685
Iteration 1200: Loss = -11494.143011672084
Iteration 1300: Loss = -11494.068961022347
Iteration 1400: Loss = -11493.970363716035
Iteration 1500: Loss = -11487.361899833537
Iteration 1600: Loss = -11487.306822085642
Iteration 1700: Loss = -11487.24108200811
Iteration 1800: Loss = -11472.105761866966
Iteration 1900: Loss = -11472.075042953778
Iteration 2000: Loss = -11472.05531098414
Iteration 2100: Loss = -11472.040023465712
Iteration 2200: Loss = -11472.025985253946
Iteration 2300: Loss = -11472.014073919794
Iteration 2400: Loss = -11472.002769443663
Iteration 2500: Loss = -11471.986610559889
Iteration 2600: Loss = -11471.969797951253
Iteration 2700: Loss = -11471.96239633906
Iteration 2800: Loss = -11471.956238060216
Iteration 2900: Loss = -11471.950947837035
Iteration 3000: Loss = -11471.946247110796
Iteration 3100: Loss = -11471.94197693262
Iteration 3200: Loss = -11471.938139805901
Iteration 3300: Loss = -11471.934702905695
Iteration 3400: Loss = -11471.932290046767
Iteration 3500: Loss = -11471.928618380709
Iteration 3600: Loss = -11471.925920157404
Iteration 3700: Loss = -11471.924020924478
Iteration 3800: Loss = -11471.921140695196
Iteration 3900: Loss = -11471.918992777022
Iteration 4000: Loss = -11471.930585504671
1
Iteration 4100: Loss = -11471.91525537215
Iteration 4200: Loss = -11471.914457434748
Iteration 4300: Loss = -11471.912137936099
Iteration 4400: Loss = -11471.911221766659
Iteration 4500: Loss = -11471.909550051665
Iteration 4600: Loss = -11471.908375317875
Iteration 4700: Loss = -11471.907461142267
Iteration 4800: Loss = -11471.906307399753
Iteration 4900: Loss = -11471.905585264481
Iteration 5000: Loss = -11471.917279405734
1
Iteration 5100: Loss = -11471.903981215302
Iteration 5200: Loss = -11471.902817966073
Iteration 5300: Loss = -11471.90398846091
1
Iteration 5400: Loss = -11471.901342402476
Iteration 5500: Loss = -11471.903444206977
1
Iteration 5600: Loss = -11471.900032957788
Iteration 5700: Loss = -11471.89954388482
Iteration 5800: Loss = -11471.898705736983
Iteration 5900: Loss = -11471.89918239687
1
Iteration 6000: Loss = -11471.897189316753
Iteration 6100: Loss = -11471.89983202529
1
Iteration 6200: Loss = -11471.898264237525
2
Iteration 6300: Loss = -11471.89612350319
Iteration 6400: Loss = -11471.895573284219
Iteration 6500: Loss = -11471.895412607528
Iteration 6600: Loss = -11471.894886575326
Iteration 6700: Loss = -11471.894599982217
Iteration 6800: Loss = -11471.900795600437
1
Iteration 6900: Loss = -11471.896604210424
2
Iteration 7000: Loss = -11471.900288753746
3
Iteration 7100: Loss = -11471.89756652133
4
Iteration 7200: Loss = -11471.893382470313
Iteration 7300: Loss = -11471.908674570175
1
Iteration 7400: Loss = -11471.893364520778
Iteration 7500: Loss = -11471.89514746247
1
Iteration 7600: Loss = -11471.893111274287
Iteration 7700: Loss = -11471.894265563076
1
Iteration 7800: Loss = -11471.894725942711
2
Iteration 7900: Loss = -11471.894320503829
3
Iteration 8000: Loss = -11471.89258046175
Iteration 8100: Loss = -11471.896372813224
1
Iteration 8200: Loss = -11471.906936643387
2
Iteration 8300: Loss = -11471.892020017425
Iteration 8400: Loss = -11471.892577259117
1
Iteration 8500: Loss = -11471.894459643048
2
Iteration 8600: Loss = -11471.891517573782
Iteration 8700: Loss = -11471.895290448807
1
Iteration 8800: Loss = -11471.891481072025
Iteration 8900: Loss = -11471.891848215912
1
Iteration 9000: Loss = -11471.892042980626
2
Iteration 9100: Loss = -11471.89659070168
3
Iteration 9200: Loss = -11471.896436072126
4
Iteration 9300: Loss = -11471.894199237466
5
Iteration 9400: Loss = -11471.895395241567
6
Iteration 9500: Loss = -11471.910701801196
7
Iteration 9600: Loss = -11471.900118772934
8
Iteration 9700: Loss = -11471.904222308667
9
Iteration 9800: Loss = -11472.009890598143
10
Iteration 9900: Loss = -11471.898238715452
11
Iteration 10000: Loss = -11471.891047411802
Iteration 10100: Loss = -11471.901660660875
1
Iteration 10200: Loss = -11471.923224009533
2
Iteration 10300: Loss = -11471.938191291874
3
Iteration 10400: Loss = -11471.898830619382
4
Iteration 10500: Loss = -11471.909088033499
5
Iteration 10600: Loss = -11471.937579293786
6
Iteration 10700: Loss = -11471.892066023574
7
Iteration 10800: Loss = -11471.89426873708
8
Iteration 10900: Loss = -11471.893316502206
9
Iteration 11000: Loss = -11471.895103619023
10
Iteration 11100: Loss = -11471.891155210556
11
Iteration 11200: Loss = -11471.89061300032
Iteration 11300: Loss = -11471.891848143361
1
Iteration 11400: Loss = -11471.900515219519
2
Iteration 11500: Loss = -11471.89372489072
3
Iteration 11600: Loss = -11471.89605972079
4
Iteration 11700: Loss = -11471.89324996525
5
Iteration 11800: Loss = -11471.897699969175
6
Iteration 11900: Loss = -11471.891597665794
7
Iteration 12000: Loss = -11472.002883958872
8
Iteration 12100: Loss = -11471.90338931475
9
Iteration 12200: Loss = -11471.898043875412
10
Iteration 12300: Loss = -11471.890285837952
Iteration 12400: Loss = -11471.890078258839
Iteration 12500: Loss = -11471.901860235968
1
Iteration 12600: Loss = -11471.903934468477
2
Iteration 12700: Loss = -11471.891883789911
3
Iteration 12800: Loss = -11471.890701100934
4
Iteration 12900: Loss = -11471.913769897797
5
Iteration 13000: Loss = -11471.895928398673
6
Iteration 13100: Loss = -11471.929607084307
7
Iteration 13200: Loss = -11471.91165298423
8
Iteration 13300: Loss = -11471.948144612845
9
Iteration 13400: Loss = -11471.890012425109
Iteration 13500: Loss = -11471.889959093018
Iteration 13600: Loss = -11471.909391402314
1
Iteration 13700: Loss = -11471.907552616642
2
Iteration 13800: Loss = -11471.993427266303
3
Iteration 13900: Loss = -11471.889972810828
Iteration 14000: Loss = -11471.906745765897
1
Iteration 14100: Loss = -11471.892078458799
2
Iteration 14200: Loss = -11471.89713650797
3
Iteration 14300: Loss = -11471.89180955433
4
Iteration 14400: Loss = -11471.895431535013
5
Iteration 14500: Loss = -11471.89377162022
6
Iteration 14600: Loss = -11471.897326303906
7
Iteration 14700: Loss = -11471.89572151653
8
Iteration 14800: Loss = -11471.906054464598
9
Iteration 14900: Loss = -11471.931524903885
10
Iteration 15000: Loss = -11471.900945825246
11
Iteration 15100: Loss = -11471.89146507872
12
Iteration 15200: Loss = -11471.897027018085
13
Iteration 15300: Loss = -11471.899258108759
14
Iteration 15400: Loss = -11471.899022116506
15
Stopping early at iteration 15400 due to no improvement.
pi: tensor([[0.7478, 0.2522],
        [0.2260, 0.7740]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4411, 0.5589], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3991, 0.1117],
         [0.5908, 0.2020]],

        [[0.6559, 0.0978],
         [0.6934, 0.6486]],

        [[0.5793, 0.1119],
         [0.6916, 0.5437]],

        [[0.5062, 0.0922],
         [0.5378, 0.6276]],

        [[0.5532, 0.0983],
         [0.6037, 0.6125]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840318355379102
Average Adjusted Rand Index: 0.9839992163297293
11481.089739882842
[0.9840318355379102, 0.9840318355379102] [0.9839992163297293, 0.9839992163297293] [11471.901424201415, 11471.899022116506]
-------------------------------------
This iteration is 84
True Objective function: Loss = -11640.254676326182
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21294.509406914247
Iteration 100: Loss = -12442.417013225862
Iteration 200: Loss = -11926.202583898119
Iteration 300: Loss = -11798.303017159564
Iteration 400: Loss = -11771.93266589597
Iteration 500: Loss = -11771.25152261621
Iteration 600: Loss = -11770.936303818035
Iteration 700: Loss = -11770.7548014516
Iteration 800: Loss = -11770.638315767883
Iteration 900: Loss = -11770.55814331725
Iteration 1000: Loss = -11770.500044839402
Iteration 1100: Loss = -11770.456351932979
Iteration 1200: Loss = -11770.422545327712
Iteration 1300: Loss = -11770.395794059308
Iteration 1400: Loss = -11770.374181832276
Iteration 1500: Loss = -11770.356515474077
Iteration 1600: Loss = -11770.341799552987
Iteration 1700: Loss = -11770.329500347092
Iteration 1800: Loss = -11770.318974073809
Iteration 1900: Loss = -11770.310019726412
Iteration 2000: Loss = -11770.302245990733
Iteration 2100: Loss = -11770.295463147082
Iteration 2200: Loss = -11770.289497199119
Iteration 2300: Loss = -11770.285173595075
Iteration 2400: Loss = -11770.279554539968
Iteration 2500: Loss = -11770.275352407069
Iteration 2600: Loss = -11770.27160324067
Iteration 2700: Loss = -11770.268186497708
Iteration 2800: Loss = -11770.265053799427
Iteration 2900: Loss = -11770.262667002327
Iteration 3000: Loss = -11770.259464584411
Iteration 3100: Loss = -11770.25684971542
Iteration 3200: Loss = -11770.256666658506
Iteration 3300: Loss = -11770.251602528011
Iteration 3400: Loss = -11770.252339303552
1
Iteration 3500: Loss = -11770.244868898855
Iteration 3600: Loss = -11770.238721246968
Iteration 3700: Loss = -11770.221271518778
Iteration 3800: Loss = -11769.283597643469
Iteration 3900: Loss = -11767.48225358196
Iteration 4000: Loss = -11764.017797793611
Iteration 4100: Loss = -11763.790986464202
Iteration 4200: Loss = -11763.772455372946
Iteration 4300: Loss = -11763.763504788287
Iteration 4400: Loss = -11763.759516865593
Iteration 4500: Loss = -11763.763378835636
1
Iteration 4600: Loss = -11763.755481942233
Iteration 4700: Loss = -11763.751141957284
Iteration 4800: Loss = -11763.732098257053
Iteration 4900: Loss = -11763.730115476892
Iteration 5000: Loss = -11763.729192476947
Iteration 5100: Loss = -11763.728274611298
Iteration 5200: Loss = -11763.729652721117
1
Iteration 5300: Loss = -11763.725753792665
Iteration 5400: Loss = -11763.72276994311
Iteration 5500: Loss = -11763.721406604736
Iteration 5600: Loss = -11763.720892363071
Iteration 5700: Loss = -11763.722882914217
1
Iteration 5800: Loss = -11763.72125667839
2
Iteration 5900: Loss = -11763.719763464152
Iteration 6000: Loss = -11763.719621260949
Iteration 6100: Loss = -11763.720644876756
1
Iteration 6200: Loss = -11763.719052833238
Iteration 6300: Loss = -11763.719120980573
Iteration 6400: Loss = -11763.718693929535
Iteration 6500: Loss = -11763.71840042879
Iteration 6600: Loss = -11763.719091138893
1
Iteration 6700: Loss = -11763.718181731498
Iteration 6800: Loss = -11763.722597965729
1
Iteration 6900: Loss = -11763.727042757047
2
Iteration 7000: Loss = -11763.724718541893
3
Iteration 7100: Loss = -11763.71759923066
Iteration 7200: Loss = -11763.717724597072
1
Iteration 7300: Loss = -11763.717623987308
Iteration 7400: Loss = -11763.717299765554
Iteration 7500: Loss = -11763.739910320226
1
Iteration 7600: Loss = -11763.717092187406
Iteration 7700: Loss = -11763.717214116603
1
Iteration 7800: Loss = -11763.716903116463
Iteration 7900: Loss = -11763.71745666436
1
Iteration 8000: Loss = -11763.718151445077
2
Iteration 8100: Loss = -11763.721223831595
3
Iteration 8200: Loss = -11763.716278056396
Iteration 8300: Loss = -11763.713860071764
Iteration 8400: Loss = -11763.705810771296
Iteration 8500: Loss = -11763.704880011232
Iteration 8600: Loss = -11763.704876243755
Iteration 8700: Loss = -11763.70572142726
1
Iteration 8800: Loss = -11763.803194131682
2
Iteration 8900: Loss = -11763.704656065162
Iteration 9000: Loss = -11763.705917168738
1
Iteration 9100: Loss = -11763.705995985603
2
Iteration 9200: Loss = -11763.704662482902
Iteration 9300: Loss = -11763.709000978295
1
Iteration 9400: Loss = -11763.707316504351
2
Iteration 9500: Loss = -11763.705352786059
3
Iteration 9600: Loss = -11763.712018086368
4
Iteration 9700: Loss = -11763.705475416367
5
Iteration 9800: Loss = -11763.709712176624
6
Iteration 9900: Loss = -11763.704805889183
7
Iteration 10000: Loss = -11763.704921549093
8
Iteration 10100: Loss = -11763.705014796993
9
Iteration 10200: Loss = -11763.705660644882
10
Iteration 10300: Loss = -11763.70545527048
11
Iteration 10400: Loss = -11763.72130756809
12
Iteration 10500: Loss = -11763.755857426946
13
Iteration 10600: Loss = -11763.706142590363
14
Iteration 10700: Loss = -11763.704286602251
Iteration 10800: Loss = -11763.704776510698
1
Iteration 10900: Loss = -11763.732169551558
2
Iteration 11000: Loss = -11763.704187978004
Iteration 11100: Loss = -11763.70450910617
1
Iteration 11200: Loss = -11763.795623777158
2
Iteration 11300: Loss = -11763.70470323909
3
Iteration 11400: Loss = -11763.70568137319
4
Iteration 11500: Loss = -11763.70852702573
5
Iteration 11600: Loss = -11763.704957617545
6
Iteration 11700: Loss = -11763.744369909515
7
Iteration 11800: Loss = -11763.704127220495
Iteration 11900: Loss = -11763.706107075113
1
Iteration 12000: Loss = -11763.704270882488
2
Iteration 12100: Loss = -11763.716135444312
3
Iteration 12200: Loss = -11763.705410361023
4
Iteration 12300: Loss = -11763.747499600056
5
Iteration 12400: Loss = -11763.708274859013
6
Iteration 12500: Loss = -11763.829146524387
7
Iteration 12600: Loss = -11763.707348829037
8
Iteration 12700: Loss = -11763.704242523654
9
Iteration 12800: Loss = -11763.705787906989
10
Iteration 12900: Loss = -11763.704522309123
11
Iteration 13000: Loss = -11763.737182830844
12
Iteration 13100: Loss = -11763.730820417883
13
Iteration 13200: Loss = -11763.704656414202
14
Iteration 13300: Loss = -11763.704420653667
15
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[0.7747, 0.2253],
        [0.3532, 0.6468]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1022, 0.8978], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4002, 0.0969],
         [0.5743, 0.2030]],

        [[0.6385, 0.0942],
         [0.5053, 0.5652]],

        [[0.6953, 0.1097],
         [0.6259, 0.5858]],

        [[0.7125, 0.0979],
         [0.6955, 0.5887]],

        [[0.7227, 0.1038],
         [0.5620, 0.6113]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6138934921450915
Average Adjusted Rand Index: 0.7992727272727272
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20427.925783838593
Iteration 100: Loss = -12428.849139862328
Iteration 200: Loss = -12114.355048297693
Iteration 300: Loss = -11922.105639233654
Iteration 400: Loss = -11785.225497929738
Iteration 500: Loss = -11776.894181836788
Iteration 600: Loss = -11771.243547095151
Iteration 700: Loss = -11770.959438126469
Iteration 800: Loss = -11770.787684308769
Iteration 900: Loss = -11770.672845330912
Iteration 1000: Loss = -11770.590959767267
Iteration 1100: Loss = -11770.529598005716
Iteration 1200: Loss = -11770.481950235442
Iteration 1300: Loss = -11770.44399024116
Iteration 1400: Loss = -11770.413678077062
Iteration 1500: Loss = -11770.389034684269
Iteration 1600: Loss = -11770.368590932494
Iteration 1700: Loss = -11770.351071510651
Iteration 1800: Loss = -11770.335951981986
Iteration 1900: Loss = -11770.322626839075
Iteration 2000: Loss = -11770.310764453028
Iteration 2100: Loss = -11770.29983042417
Iteration 2200: Loss = -11770.28884168977
Iteration 2300: Loss = -11770.2759716926
Iteration 2400: Loss = -11770.25520426308
Iteration 2500: Loss = -11770.15968584754
Iteration 2600: Loss = -11768.928441162368
Iteration 2700: Loss = -11767.432171742716
Iteration 2800: Loss = -11766.053683647135
Iteration 2900: Loss = -11763.831121835123
Iteration 3000: Loss = -11763.796137984933
Iteration 3100: Loss = -11763.784629233434
Iteration 3200: Loss = -11763.774024362636
Iteration 3300: Loss = -11763.7596117035
Iteration 3400: Loss = -11763.746270325648
Iteration 3500: Loss = -11763.755067314903
1
Iteration 3600: Loss = -11763.740263352875
Iteration 3700: Loss = -11763.738120953178
Iteration 3800: Loss = -11763.736257974639
Iteration 3900: Loss = -11763.734750861882
Iteration 4000: Loss = -11763.732996191638
Iteration 4100: Loss = -11763.731484417242
Iteration 4200: Loss = -11763.729892829484
Iteration 4300: Loss = -11763.72774035993
Iteration 4400: Loss = -11763.723607595182
Iteration 4500: Loss = -11763.718042984732
Iteration 4600: Loss = -11763.716350978766
Iteration 4700: Loss = -11763.715413578251
Iteration 4800: Loss = -11763.734082820012
1
Iteration 4900: Loss = -11763.713955972662
Iteration 5000: Loss = -11763.713394824108
Iteration 5100: Loss = -11763.71279404024
Iteration 5200: Loss = -11763.712260643882
Iteration 5300: Loss = -11763.712537243862
1
Iteration 5400: Loss = -11763.711327432062
Iteration 5500: Loss = -11763.712927955765
1
Iteration 5600: Loss = -11763.710495593326
Iteration 5700: Loss = -11763.716084855678
1
Iteration 5800: Loss = -11763.711240906718
2
Iteration 5900: Loss = -11763.709497296119
Iteration 6000: Loss = -11763.709472541552
Iteration 6100: Loss = -11763.708930972412
Iteration 6200: Loss = -11763.708848497537
Iteration 6300: Loss = -11763.70842132201
Iteration 6400: Loss = -11763.708174696874
Iteration 6500: Loss = -11763.708017805367
Iteration 6600: Loss = -11763.709894335036
1
Iteration 6700: Loss = -11763.707591197512
Iteration 6800: Loss = -11763.71002788994
1
Iteration 6900: Loss = -11763.708018694884
2
Iteration 7000: Loss = -11763.721249784681
3
Iteration 7100: Loss = -11763.706920845172
Iteration 7200: Loss = -11763.70682240518
Iteration 7300: Loss = -11763.70679333835
Iteration 7400: Loss = -11763.706691821915
Iteration 7500: Loss = -11763.706804598918
1
Iteration 7600: Loss = -11763.707333960245
2
Iteration 7700: Loss = -11763.710432253638
3
Iteration 7800: Loss = -11763.708395189398
4
Iteration 7900: Loss = -11763.70601151796
Iteration 8000: Loss = -11763.714489916094
1
Iteration 8100: Loss = -11763.705856727385
Iteration 8200: Loss = -11763.722189771579
1
Iteration 8300: Loss = -11763.70564345335
Iteration 8400: Loss = -11763.705610469367
Iteration 8500: Loss = -11763.705531084415
Iteration 8600: Loss = -11763.70549664823
Iteration 8700: Loss = -11763.705445511667
Iteration 8800: Loss = -11763.705375024729
Iteration 8900: Loss = -11763.705323421389
Iteration 9000: Loss = -11763.705570800745
1
Iteration 9100: Loss = -11763.719277811553
2
Iteration 9200: Loss = -11763.716118380922
3
Iteration 9300: Loss = -11763.708645164434
4
Iteration 9400: Loss = -11763.705322494296
Iteration 9500: Loss = -11763.715641641993
1
Iteration 9600: Loss = -11763.705241950067
Iteration 9700: Loss = -11763.70516659713
Iteration 9800: Loss = -11763.705091174777
Iteration 9900: Loss = -11763.722340823513
1
Iteration 10000: Loss = -11763.715145698037
2
Iteration 10100: Loss = -11763.704867095246
Iteration 10200: Loss = -11763.911549448976
1
Iteration 10300: Loss = -11763.706022324946
2
Iteration 10400: Loss = -11763.705566528644
3
Iteration 10500: Loss = -11763.705187113992
4
Iteration 10600: Loss = -11763.70514655345
5
Iteration 10700: Loss = -11763.71125044197
6
Iteration 10800: Loss = -11763.827216683261
7
Iteration 10900: Loss = -11763.704500631082
Iteration 11000: Loss = -11763.706634286878
1
Iteration 11100: Loss = -11763.70699091828
2
Iteration 11200: Loss = -11763.712060414506
3
Iteration 11300: Loss = -11763.711219683399
4
Iteration 11400: Loss = -11763.737001638796
5
Iteration 11500: Loss = -11763.704288158993
Iteration 11600: Loss = -11763.705432172026
1
Iteration 11700: Loss = -11763.708879289095
2
Iteration 11800: Loss = -11763.7189862617
3
Iteration 11900: Loss = -11763.712902682175
4
Iteration 12000: Loss = -11763.708222759189
5
Iteration 12100: Loss = -11763.704166173937
Iteration 12200: Loss = -11763.711988068075
1
Iteration 12300: Loss = -11763.704138412773
Iteration 12400: Loss = -11763.704864103916
1
Iteration 12500: Loss = -11763.706493094889
2
Iteration 12600: Loss = -11763.704125252967
Iteration 12700: Loss = -11763.724405539493
1
Iteration 12800: Loss = -11763.750002527988
2
Iteration 12900: Loss = -11763.713891075871
3
Iteration 13000: Loss = -11763.70414492734
Iteration 13100: Loss = -11763.71975705062
1
Iteration 13200: Loss = -11763.8145673711
2
Iteration 13300: Loss = -11763.704125929564
Iteration 13400: Loss = -11763.704929856378
1
Iteration 13500: Loss = -11763.812880507241
2
Iteration 13600: Loss = -11763.706409423377
3
Iteration 13700: Loss = -11763.704285091886
4
Iteration 13800: Loss = -11763.704374250001
5
Iteration 13900: Loss = -11763.7056156803
6
Iteration 14000: Loss = -11763.715387543984
7
Iteration 14100: Loss = -11763.70790813604
8
Iteration 14200: Loss = -11763.710715011528
9
Iteration 14300: Loss = -11763.705005580341
10
Iteration 14400: Loss = -11763.708957760935
11
Iteration 14500: Loss = -11763.753118105167
12
Iteration 14600: Loss = -11763.72012517037
13
Iteration 14700: Loss = -11763.70424801738
14
Iteration 14800: Loss = -11763.706656549648
15
Stopping early at iteration 14800 due to no improvement.
pi: tensor([[0.7744, 0.2256],
        [0.3538, 0.6462]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1024, 0.8976], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4000, 0.0971],
         [0.6162, 0.2031]],

        [[0.6489, 0.0943],
         [0.5227, 0.6975]],

        [[0.6854, 0.1094],
         [0.7282, 0.6768]],

        [[0.5219, 0.0975],
         [0.6073, 0.6067]],

        [[0.5237, 0.1038],
         [0.5791, 0.6729]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.6138934921450915
Average Adjusted Rand Index: 0.7992727272727272
11640.254676326182
[0.6138934921450915, 0.6138934921450915] [0.7992727272727272, 0.7992727272727272] [11763.704420653667, 11763.706656549648]
-------------------------------------
This iteration is 85
True Objective function: Loss = -11259.825235112165
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21919.183572681515
Iteration 100: Loss = -11990.494566641457
Iteration 200: Loss = -11968.414997067728
Iteration 300: Loss = -11583.350307753166
Iteration 400: Loss = -11502.630861047472
Iteration 500: Loss = -11420.064238264451
Iteration 600: Loss = -11403.472288522607
Iteration 700: Loss = -11403.32464922816
Iteration 800: Loss = -11403.250360580789
Iteration 900: Loss = -11403.204943590712
Iteration 1000: Loss = -11403.174109898664
Iteration 1100: Loss = -11403.15148881868
Iteration 1200: Loss = -11403.133269109212
Iteration 1300: Loss = -11403.114526862237
Iteration 1400: Loss = -11402.879151886595
Iteration 1500: Loss = -11402.18204697448
Iteration 1600: Loss = -11402.174640877489
Iteration 1700: Loss = -11402.168573884446
Iteration 1800: Loss = -11402.163368917536
Iteration 1900: Loss = -11401.968304029313
Iteration 2000: Loss = -11401.86134019126
Iteration 2100: Loss = -11401.857109958091
Iteration 2200: Loss = -11401.85379097188
Iteration 2300: Loss = -11401.757876135214
Iteration 2400: Loss = -11401.75029259112
Iteration 2500: Loss = -11401.74835475186
Iteration 2600: Loss = -11401.746747898034
Iteration 2700: Loss = -11401.745119596742
Iteration 2800: Loss = -11401.744468490422
Iteration 2900: Loss = -11401.742485444429
Iteration 3000: Loss = -11401.752547764103
1
Iteration 3100: Loss = -11401.74009642856
Iteration 3200: Loss = -11401.738247942976
Iteration 3300: Loss = -11401.737353619648
Iteration 3400: Loss = -11401.736359909608
Iteration 3500: Loss = -11401.736081996207
Iteration 3600: Loss = -11401.734971504733
Iteration 3700: Loss = -11401.739081983029
1
Iteration 3800: Loss = -11401.733797631694
Iteration 3900: Loss = -11401.733303899699
Iteration 4000: Loss = -11401.732742767435
Iteration 4100: Loss = -11401.732284690504
Iteration 4200: Loss = -11401.73714100582
1
Iteration 4300: Loss = -11401.731476836809
Iteration 4400: Loss = -11401.73112891549
Iteration 4500: Loss = -11401.731323925003
1
Iteration 4600: Loss = -11401.731181203237
Iteration 4700: Loss = -11401.735575827144
1
Iteration 4800: Loss = -11401.731403363814
2
Iteration 4900: Loss = -11401.744553528695
3
Iteration 5000: Loss = -11401.7298521023
Iteration 5100: Loss = -11401.729382558475
Iteration 5200: Loss = -11401.72908578581
Iteration 5300: Loss = -11401.728900924543
Iteration 5400: Loss = -11401.742662069366
1
Iteration 5500: Loss = -11401.72852478211
Iteration 5600: Loss = -11401.734363105063
1
Iteration 5700: Loss = -11401.728163034833
Iteration 5800: Loss = -11401.730555517375
1
Iteration 5900: Loss = -11401.727900689184
Iteration 6000: Loss = -11401.740885187564
1
Iteration 6100: Loss = -11401.727748862486
Iteration 6200: Loss = -11401.727567156784
Iteration 6300: Loss = -11401.727525572924
Iteration 6400: Loss = -11401.727379456199
Iteration 6500: Loss = -11401.767225518859
1
Iteration 6600: Loss = -11401.727190302508
Iteration 6700: Loss = -11401.727085594097
Iteration 6800: Loss = -11401.72710277218
Iteration 6900: Loss = -11401.72697413316
Iteration 7000: Loss = -11401.73224726428
1
Iteration 7100: Loss = -11401.729657991003
2
Iteration 7200: Loss = -11401.72686716856
Iteration 7300: Loss = -11401.72672591417
Iteration 7400: Loss = -11401.726831742679
1
Iteration 7500: Loss = -11401.729761460794
2
Iteration 7600: Loss = -11401.726577093641
Iteration 7700: Loss = -11401.726605029879
Iteration 7800: Loss = -11401.727973661835
1
Iteration 7900: Loss = -11401.72690314149
2
Iteration 8000: Loss = -11401.726386725359
Iteration 8100: Loss = -11401.726927534906
1
Iteration 8200: Loss = -11401.72634745877
Iteration 8300: Loss = -11401.726822701212
1
Iteration 8400: Loss = -11401.726474019502
2
Iteration 8500: Loss = -11401.726266104824
Iteration 8600: Loss = -11401.730522454127
1
Iteration 8700: Loss = -11401.72621814569
Iteration 8800: Loss = -11401.726235035363
Iteration 8900: Loss = -11401.730569002151
1
Iteration 9000: Loss = -11401.728945724355
2
Iteration 9100: Loss = -11401.729708142644
3
Iteration 9200: Loss = -11401.72611511876
Iteration 9300: Loss = -11401.728449670434
1
Iteration 9400: Loss = -11401.747757802714
2
Iteration 9500: Loss = -11401.726291113382
3
Iteration 9600: Loss = -11401.726067228046
Iteration 9700: Loss = -11401.738709019162
1
Iteration 9800: Loss = -11401.729129644895
2
Iteration 9900: Loss = -11401.726048391041
Iteration 10000: Loss = -11401.731790121235
1
Iteration 10100: Loss = -11401.729694314567
2
Iteration 10200: Loss = -11401.726299642432
3
Iteration 10300: Loss = -11401.727082677464
4
Iteration 10400: Loss = -11401.727332560245
5
Iteration 10500: Loss = -11401.730402071333
6
Iteration 10600: Loss = -11401.735397979359
7
Iteration 10700: Loss = -11401.726635746527
8
Iteration 10800: Loss = -11401.724988778466
Iteration 10900: Loss = -11401.733679666535
1
Iteration 11000: Loss = -11401.725243232388
2
Iteration 11100: Loss = -11401.729543231177
3
Iteration 11200: Loss = -11401.72483107099
Iteration 11300: Loss = -11401.775986186834
1
Iteration 11400: Loss = -11401.724757671249
Iteration 11500: Loss = -11401.821465127641
1
Iteration 11600: Loss = -11401.724760069808
Iteration 11700: Loss = -11401.734509322123
1
Iteration 11800: Loss = -11401.725952379516
2
Iteration 11900: Loss = -11401.726210532348
3
Iteration 12000: Loss = -11401.72474938131
Iteration 12100: Loss = -11401.725463395242
1
Iteration 12200: Loss = -11401.760512538229
2
Iteration 12300: Loss = -11401.72474556644
Iteration 12400: Loss = -11401.72458901496
Iteration 12500: Loss = -11401.743771382706
1
Iteration 12600: Loss = -11401.72453710205
Iteration 12700: Loss = -11401.724817019218
1
Iteration 12800: Loss = -11401.75077431504
2
Iteration 12900: Loss = -11401.72479980915
3
Iteration 13000: Loss = -11401.724657261635
4
Iteration 13100: Loss = -11401.738726122056
5
Iteration 13200: Loss = -11401.744432033778
6
Iteration 13300: Loss = -11401.72959797058
7
Iteration 13400: Loss = -11401.725184553668
8
Iteration 13500: Loss = -11401.73041841513
9
Iteration 13600: Loss = -11401.833111950407
10
Iteration 13700: Loss = -11401.738115286988
11
Iteration 13800: Loss = -11401.723643739786
Iteration 13900: Loss = -11401.724187085218
1
Iteration 14000: Loss = -11401.884579272659
2
Iteration 14100: Loss = -11401.74936875377
3
Iteration 14200: Loss = -11401.723503961692
Iteration 14300: Loss = -11401.736938077594
1
Iteration 14400: Loss = -11401.823244763982
2
Iteration 14500: Loss = -11401.779456055614
3
Iteration 14600: Loss = -11401.723472097221
Iteration 14700: Loss = -11401.723366414446
Iteration 14800: Loss = -11401.723739069896
1
Iteration 14900: Loss = -11401.725782856029
2
Iteration 15000: Loss = -11401.725506014602
3
Iteration 15100: Loss = -11401.844406336138
4
Iteration 15200: Loss = -11401.723581631264
5
Iteration 15300: Loss = -11401.723297860302
Iteration 15400: Loss = -11401.725110376496
1
Iteration 15500: Loss = -11401.78026046086
2
Iteration 15600: Loss = -11401.730248360376
3
Iteration 15700: Loss = -11401.723299476429
Iteration 15800: Loss = -11401.726024042404
1
Iteration 15900: Loss = -11401.768769013008
2
Iteration 16000: Loss = -11401.723149418456
Iteration 16100: Loss = -11401.725845506524
1
Iteration 16200: Loss = -11401.855301507558
2
Iteration 16300: Loss = -11401.732168614368
3
Iteration 16400: Loss = -11401.72334521616
4
Iteration 16500: Loss = -11401.726797484936
5
Iteration 16600: Loss = -11401.738444296661
6
Iteration 16700: Loss = -11401.732090384905
7
Iteration 16800: Loss = -11401.723498569894
8
Iteration 16900: Loss = -11401.723151105207
Iteration 17000: Loss = -11401.724228767727
1
Iteration 17100: Loss = -11401.737013360946
2
Iteration 17200: Loss = -11401.75324499698
3
Iteration 17300: Loss = -11401.815135752246
4
Iteration 17400: Loss = -11401.723917041581
5
Iteration 17500: Loss = -11401.723706145798
6
Iteration 17600: Loss = -11401.814828033988
7
Iteration 17700: Loss = -11401.72661166211
8
Iteration 17800: Loss = -11401.791374398874
9
Iteration 17900: Loss = -11401.726844066172
10
Iteration 18000: Loss = -11401.75268852379
11
Iteration 18100: Loss = -11401.776813574323
12
Iteration 18200: Loss = -11401.809604354876
13
Iteration 18300: Loss = -11401.731120742124
14
Iteration 18400: Loss = -11401.72368568214
15
Stopping early at iteration 18400 due to no improvement.
pi: tensor([[0.5446, 0.4554],
        [0.3019, 0.6981]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4485, 0.5515], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4027, 0.0969],
         [0.7084, 0.2046]],

        [[0.6697, 0.0977],
         [0.6304, 0.6083]],

        [[0.7027, 0.0922],
         [0.6007, 0.5250]],

        [[0.6635, 0.1105],
         [0.6042, 0.6089]],

        [[0.6411, 0.0941],
         [0.5594, 0.6513]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.03915171288743882
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.5585024435939
Average Adjusted Rand Index: 0.7998303425774878
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20081.527515201327
Iteration 100: Loss = -11994.05826904836
Iteration 200: Loss = -11992.138852452676
Iteration 300: Loss = -11973.376447853701
Iteration 400: Loss = -11283.835147399792
Iteration 500: Loss = -11257.26667973528
Iteration 600: Loss = -11256.56375364261
Iteration 700: Loss = -11256.11128092265
Iteration 800: Loss = -11255.91174044008
Iteration 900: Loss = -11254.454616057077
Iteration 1000: Loss = -11254.34381405765
Iteration 1100: Loss = -11254.199409321282
Iteration 1200: Loss = -11254.159829106586
Iteration 1300: Loss = -11254.129866262094
Iteration 1400: Loss = -11254.106215316555
Iteration 1500: Loss = -11254.087010050689
Iteration 1600: Loss = -11254.071092898505
Iteration 1700: Loss = -11254.057769807432
Iteration 1800: Loss = -11254.046532273525
Iteration 1900: Loss = -11254.037007221365
Iteration 2000: Loss = -11254.028908911834
Iteration 2100: Loss = -11254.043365075378
1
Iteration 2200: Loss = -11254.015721220036
Iteration 2300: Loss = -11254.010313601677
Iteration 2400: Loss = -11254.005584856895
Iteration 2500: Loss = -11254.001339183265
Iteration 2600: Loss = -11253.997701844924
Iteration 2700: Loss = -11253.994252161865
Iteration 2800: Loss = -11253.991228554096
Iteration 2900: Loss = -11253.989255345146
Iteration 3000: Loss = -11253.986029860029
Iteration 3100: Loss = -11253.983785088196
Iteration 3200: Loss = -11253.98832267252
1
Iteration 3300: Loss = -11253.979881836784
Iteration 3400: Loss = -11253.979390147993
Iteration 3500: Loss = -11253.976613924617
Iteration 3600: Loss = -11253.975492365304
Iteration 3700: Loss = -11253.973977841299
Iteration 3800: Loss = -11253.972986692828
Iteration 3900: Loss = -11253.977635776844
1
Iteration 4000: Loss = -11253.97631268944
2
Iteration 4100: Loss = -11253.969154430039
Iteration 4200: Loss = -11253.968724361064
Iteration 4300: Loss = -11253.966227667172
Iteration 4400: Loss = -11253.964852573237
Iteration 4500: Loss = -11253.96494323703
Iteration 4600: Loss = -11253.963293766938
Iteration 4700: Loss = -11253.966307200748
1
Iteration 4800: Loss = -11253.962066645874
Iteration 4900: Loss = -11253.96189369277
Iteration 5000: Loss = -11253.96153853553
Iteration 5100: Loss = -11253.960701168922
Iteration 5200: Loss = -11253.962333639303
1
Iteration 5300: Loss = -11253.960130889582
Iteration 5400: Loss = -11253.95932366679
Iteration 5500: Loss = -11253.958969726767
Iteration 5600: Loss = -11253.97047196717
1
Iteration 5700: Loss = -11253.95841102344
Iteration 5800: Loss = -11253.958148158112
Iteration 5900: Loss = -11253.957715333338
Iteration 6000: Loss = -11253.958332493186
1
Iteration 6100: Loss = -11253.957230171472
Iteration 6200: Loss = -11253.956984758954
Iteration 6300: Loss = -11253.956877091365
Iteration 6400: Loss = -11253.956637392906
Iteration 6500: Loss = -11253.95726052232
1
Iteration 6600: Loss = -11253.956224912876
Iteration 6700: Loss = -11253.956055609711
Iteration 6800: Loss = -11253.955905441115
Iteration 6900: Loss = -11253.956002471157
Iteration 7000: Loss = -11253.980090020477
1
Iteration 7100: Loss = -11253.95617184771
2
Iteration 7200: Loss = -11253.990573119892
3
Iteration 7300: Loss = -11253.95759727835
4
Iteration 7400: Loss = -11253.95535507045
Iteration 7500: Loss = -11254.042845482636
1
Iteration 7600: Loss = -11253.95958587199
2
Iteration 7700: Loss = -11253.962504671063
3
Iteration 7800: Loss = -11253.954653610988
Iteration 7900: Loss = -11253.954650920474
Iteration 8000: Loss = -11253.954734229652
Iteration 8100: Loss = -11253.954444476598
Iteration 8200: Loss = -11253.978505013369
1
Iteration 8300: Loss = -11254.042241326599
2
Iteration 8400: Loss = -11253.9542629278
Iteration 8500: Loss = -11253.95528743596
1
Iteration 8600: Loss = -11254.040516448924
2
Iteration 8700: Loss = -11253.954048436222
Iteration 8800: Loss = -11253.954720884609
1
Iteration 8900: Loss = -11253.961019991784
2
Iteration 9000: Loss = -11253.954197314693
3
Iteration 9100: Loss = -11253.954949879078
4
Iteration 9200: Loss = -11253.953802830343
Iteration 9300: Loss = -11253.953836482298
Iteration 9400: Loss = -11253.955546952277
1
Iteration 9500: Loss = -11253.964472336995
2
Iteration 9600: Loss = -11253.962588763612
3
Iteration 9700: Loss = -11253.953635808302
Iteration 9800: Loss = -11253.976656653018
1
Iteration 9900: Loss = -11253.954407831156
2
Iteration 10000: Loss = -11253.956547509908
3
Iteration 10100: Loss = -11253.984880076607
4
Iteration 10200: Loss = -11253.95349815287
Iteration 10300: Loss = -11253.968534637646
1
Iteration 10400: Loss = -11253.953466228202
Iteration 10500: Loss = -11253.953943380659
1
Iteration 10600: Loss = -11253.954271066155
2
Iteration 10700: Loss = -11253.953363823191
Iteration 10800: Loss = -11253.954287419083
1
Iteration 10900: Loss = -11253.95350788231
2
Iteration 11000: Loss = -11253.953402807774
Iteration 11100: Loss = -11253.959922640377
1
Iteration 11200: Loss = -11253.953313685863
Iteration 11300: Loss = -11253.954672004262
1
Iteration 11400: Loss = -11253.953308374885
Iteration 11500: Loss = -11253.953938344272
1
Iteration 11600: Loss = -11253.989813409631
2
Iteration 11700: Loss = -11253.969113011897
3
Iteration 11800: Loss = -11253.954722253013
4
Iteration 11900: Loss = -11253.955165358631
5
Iteration 12000: Loss = -11254.023605021073
6
Iteration 12100: Loss = -11253.968170994916
7
Iteration 12200: Loss = -11253.956137522206
8
Iteration 12300: Loss = -11253.953291912912
Iteration 12400: Loss = -11253.969791632127
1
Iteration 12500: Loss = -11253.972661585422
2
Iteration 12600: Loss = -11253.956933258929
3
Iteration 12700: Loss = -11253.953312001104
Iteration 12800: Loss = -11253.95477768106
1
Iteration 12900: Loss = -11254.035782937275
2
Iteration 13000: Loss = -11253.963743638076
3
Iteration 13100: Loss = -11253.966372437726
4
Iteration 13200: Loss = -11254.063664709678
5
Iteration 13300: Loss = -11253.954176896148
6
Iteration 13400: Loss = -11253.960840620708
7
Iteration 13500: Loss = -11253.95359610037
8
Iteration 13600: Loss = -11253.953396160117
Iteration 13700: Loss = -11253.966288646534
1
Iteration 13800: Loss = -11253.953329567672
Iteration 13900: Loss = -11253.953177002804
Iteration 14000: Loss = -11253.954428952873
1
Iteration 14100: Loss = -11253.95670308502
2
Iteration 14200: Loss = -11253.968296701178
3
Iteration 14300: Loss = -11254.096152902992
4
Iteration 14400: Loss = -11253.960378999622
5
Iteration 14500: Loss = -11253.953942987051
6
Iteration 14600: Loss = -11253.954088588162
7
Iteration 14700: Loss = -11254.013708722208
8
Iteration 14800: Loss = -11253.95464780037
9
Iteration 14900: Loss = -11253.988399887681
10
Iteration 15000: Loss = -11253.953345322148
11
Iteration 15100: Loss = -11253.954018865037
12
Iteration 15200: Loss = -11253.95342612073
13
Iteration 15300: Loss = -11253.953635660608
14
Iteration 15400: Loss = -11253.953660453146
15
Stopping early at iteration 15400 due to no improvement.
pi: tensor([[0.7586, 0.2414],
        [0.1951, 0.8049]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4489, 0.5511], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4023, 0.0968],
         [0.6338, 0.2023]],

        [[0.6330, 0.0977],
         [0.7175, 0.5147]],

        [[0.6552, 0.0915],
         [0.6770, 0.6219]],

        [[0.6046, 0.0988],
         [0.6902, 0.6170]],

        [[0.5153, 0.0935],
         [0.5123, 0.6704]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11259.825235112165
[0.5585024435939, 1.0] [0.7998303425774878, 1.0] [11401.72368568214, 11253.953660453146]
-------------------------------------
This iteration is 86
True Objective function: Loss = -11540.619767034359
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23658.061667723083
Iteration 100: Loss = -12223.352605628126
Iteration 200: Loss = -12171.655375989258
Iteration 300: Loss = -11778.466503939326
Iteration 400: Loss = -11629.489056755992
Iteration 500: Loss = -11566.987996615777
Iteration 600: Loss = -11540.069735385665
Iteration 700: Loss = -11534.720650250552
Iteration 800: Loss = -11533.616219773403
Iteration 900: Loss = -11533.485500058814
Iteration 1000: Loss = -11533.429418474097
Iteration 1100: Loss = -11533.38924358426
Iteration 1200: Loss = -11533.359062192305
Iteration 1300: Loss = -11533.335491923746
Iteration 1400: Loss = -11533.316391928254
Iteration 1500: Loss = -11533.300547759656
Iteration 1600: Loss = -11533.287355588227
Iteration 1700: Loss = -11533.276192279523
Iteration 1800: Loss = -11533.26635856333
Iteration 1900: Loss = -11533.257357193785
Iteration 2000: Loss = -11533.254454666248
Iteration 2100: Loss = -11533.243338570872
Iteration 2200: Loss = -11533.238418143646
Iteration 2300: Loss = -11533.234133620681
Iteration 2400: Loss = -11533.230404396836
Iteration 2500: Loss = -11533.227095110858
Iteration 2600: Loss = -11533.224180759407
Iteration 2700: Loss = -11533.221566697788
Iteration 2800: Loss = -11533.21921127362
Iteration 2900: Loss = -11533.217038100824
Iteration 3000: Loss = -11533.215088592791
Iteration 3100: Loss = -11533.21331206575
Iteration 3200: Loss = -11533.211608131443
Iteration 3300: Loss = -11533.22013020469
1
Iteration 3400: Loss = -11533.197028039389
Iteration 3500: Loss = -11533.194808464901
Iteration 3600: Loss = -11533.193740820552
Iteration 3700: Loss = -11533.192993948123
Iteration 3800: Loss = -11533.191484325444
Iteration 3900: Loss = -11533.190630908395
Iteration 4000: Loss = -11533.193668291948
1
Iteration 4100: Loss = -11533.188875323534
Iteration 4200: Loss = -11533.188189035654
Iteration 4300: Loss = -11533.188118330885
Iteration 4400: Loss = -11533.189836108817
1
Iteration 4500: Loss = -11533.186366991835
Iteration 4600: Loss = -11533.185783381845
Iteration 4700: Loss = -11533.185997350001
1
Iteration 4800: Loss = -11533.184853868215
Iteration 4900: Loss = -11533.185893295593
1
Iteration 5000: Loss = -11533.189584368214
2
Iteration 5100: Loss = -11533.184779992207
Iteration 5200: Loss = -11533.18333608378
Iteration 5300: Loss = -11533.184791049527
1
Iteration 5400: Loss = -11533.183100149774
Iteration 5500: Loss = -11533.183412793955
1
Iteration 5600: Loss = -11533.182155289162
Iteration 5700: Loss = -11533.182650172002
1
Iteration 5800: Loss = -11533.181707184185
Iteration 5900: Loss = -11533.18272597579
1
Iteration 6000: Loss = -11533.181280870715
Iteration 6100: Loss = -11533.181556092173
1
Iteration 6200: Loss = -11533.182034346328
2
Iteration 6300: Loss = -11533.189450663169
3
Iteration 6400: Loss = -11533.180591949853
Iteration 6500: Loss = -11533.181252328355
1
Iteration 6600: Loss = -11533.18125781823
2
Iteration 6700: Loss = -11533.182057473636
3
Iteration 6800: Loss = -11533.186590355914
4
Iteration 6900: Loss = -11533.179912087171
Iteration 7000: Loss = -11533.180077374182
1
Iteration 7100: Loss = -11533.199559836623
2
Iteration 7200: Loss = -11533.179818581857
Iteration 7300: Loss = -11533.181675369995
1
Iteration 7400: Loss = -11533.179389959752
Iteration 7500: Loss = -11533.185247882626
1
Iteration 7600: Loss = -11533.179244603583
Iteration 7700: Loss = -11533.222544458673
1
Iteration 7800: Loss = -11533.17918995044
Iteration 7900: Loss = -11533.193703067738
1
Iteration 8000: Loss = -11533.178991281018
Iteration 8100: Loss = -11533.179114385475
1
Iteration 8200: Loss = -11533.179253772303
2
Iteration 8300: Loss = -11533.178794880625
Iteration 8400: Loss = -11533.223377415841
1
Iteration 8500: Loss = -11533.178712712612
Iteration 8600: Loss = -11533.282112968043
1
Iteration 8700: Loss = -11533.179226576962
2
Iteration 8800: Loss = -11533.198227774126
3
Iteration 8900: Loss = -11533.191416357202
4
Iteration 9000: Loss = -11533.179962063172
5
Iteration 9100: Loss = -11533.18452139737
6
Iteration 9200: Loss = -11533.17886436339
7
Iteration 9300: Loss = -11533.179936034086
8
Iteration 9400: Loss = -11533.185427839338
9
Iteration 9500: Loss = -11533.178419759357
Iteration 9600: Loss = -11533.201208246945
1
Iteration 9700: Loss = -11533.191126896554
2
Iteration 9800: Loss = -11533.189970081154
3
Iteration 9900: Loss = -11533.184306426749
4
Iteration 10000: Loss = -11533.20452295215
5
Iteration 10100: Loss = -11533.178990020735
6
Iteration 10200: Loss = -11533.19226325229
7
Iteration 10300: Loss = -11533.181857757208
8
Iteration 10400: Loss = -11533.197627306805
9
Iteration 10500: Loss = -11533.186094766868
10
Iteration 10600: Loss = -11533.186775497397
11
Iteration 10700: Loss = -11533.180482037309
12
Iteration 10800: Loss = -11533.186812761876
13
Iteration 10900: Loss = -11533.178613533073
14
Iteration 11000: Loss = -11533.179494127955
15
Stopping early at iteration 11000 due to no improvement.
pi: tensor([[0.7497, 0.2503],
        [0.2595, 0.7405]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5184, 0.4816], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.1047],
         [0.5588, 0.3899]],

        [[0.5801, 0.1068],
         [0.5791, 0.5962]],

        [[0.5847, 0.1054],
         [0.6425, 0.6461]],

        [[0.6779, 0.1016],
         [0.7302, 0.5013]],

        [[0.7072, 0.0942],
         [0.5274, 0.5566]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961683251316
Average Adjusted Rand Index: 0.9761604351641242
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21291.480307231832
Iteration 100: Loss = -12225.759405489915
Iteration 200: Loss = -12212.70956406244
Iteration 300: Loss = -11832.642192965188
Iteration 400: Loss = -11686.793575458172
Iteration 500: Loss = -11660.660257612895
Iteration 600: Loss = -11650.715111959571
Iteration 700: Loss = -11632.017116096014
Iteration 800: Loss = -11612.632213743684
Iteration 900: Loss = -11570.568923743002
Iteration 1000: Loss = -11562.98489100998
Iteration 1100: Loss = -11544.05861399717
Iteration 1200: Loss = -11543.887107469302
Iteration 1300: Loss = -11533.46124612373
Iteration 1400: Loss = -11533.396628553373
Iteration 1500: Loss = -11533.356977602685
Iteration 1600: Loss = -11533.3282145321
Iteration 1700: Loss = -11533.307191674121
Iteration 1800: Loss = -11533.290785116635
Iteration 1900: Loss = -11533.277559519698
Iteration 2000: Loss = -11533.26674117328
Iteration 2100: Loss = -11533.257591853944
Iteration 2200: Loss = -11533.249873119974
Iteration 2300: Loss = -11533.24328539456
Iteration 2400: Loss = -11533.237511990055
Iteration 2500: Loss = -11533.232484617964
Iteration 2600: Loss = -11533.228071608737
Iteration 2700: Loss = -11533.224159473886
Iteration 2800: Loss = -11533.220706261312
Iteration 2900: Loss = -11533.217610110652
Iteration 3000: Loss = -11533.214833032147
Iteration 3100: Loss = -11533.21329805912
Iteration 3200: Loss = -11533.210017518619
Iteration 3300: Loss = -11533.208721177456
Iteration 3400: Loss = -11533.206080216723
Iteration 3500: Loss = -11533.206217284858
1
Iteration 3600: Loss = -11533.20271544763
Iteration 3700: Loss = -11533.204813764434
1
Iteration 3800: Loss = -11533.199541912772
Iteration 3900: Loss = -11533.197318791925
Iteration 4000: Loss = -11533.19562454497
Iteration 4100: Loss = -11533.192873684167
Iteration 4200: Loss = -11533.1917454011
Iteration 4300: Loss = -11533.192414831987
1
Iteration 4400: Loss = -11533.190002484254
Iteration 4500: Loss = -11533.203068544057
1
Iteration 4600: Loss = -11533.188932265055
Iteration 4700: Loss = -11533.187879798475
Iteration 4800: Loss = -11533.187290232152
Iteration 4900: Loss = -11533.189251133881
1
Iteration 5000: Loss = -11533.187073829205
Iteration 5100: Loss = -11533.185774631418
Iteration 5200: Loss = -11533.185307616512
Iteration 5300: Loss = -11533.184993937268
Iteration 5400: Loss = -11533.184995485977
Iteration 5500: Loss = -11533.184069983758
Iteration 5600: Loss = -11533.18539776038
1
Iteration 5700: Loss = -11533.183234181299
Iteration 5800: Loss = -11533.182937997683
Iteration 5900: Loss = -11533.184743645708
1
Iteration 6000: Loss = -11533.182813808475
Iteration 6100: Loss = -11533.182011995217
Iteration 6200: Loss = -11533.181873514186
Iteration 6300: Loss = -11533.181176036767
Iteration 6400: Loss = -11533.182994887904
1
Iteration 6500: Loss = -11533.171097946
Iteration 6600: Loss = -11533.172484274417
1
Iteration 6700: Loss = -11533.174036005552
2
Iteration 6800: Loss = -11533.170957491291
Iteration 6900: Loss = -11533.170149253063
Iteration 7000: Loss = -11533.170236777865
Iteration 7100: Loss = -11533.169836342753
Iteration 7200: Loss = -11533.169706771963
Iteration 7300: Loss = -11533.17056944783
1
Iteration 7400: Loss = -11533.1699085883
2
Iteration 7500: Loss = -11533.17401258371
3
Iteration 7600: Loss = -11533.169229927751
Iteration 7700: Loss = -11533.169154824855
Iteration 7800: Loss = -11533.18775030792
1
Iteration 7900: Loss = -11533.170206703751
2
Iteration 8000: Loss = -11533.169119356811
Iteration 8100: Loss = -11533.176491628523
1
Iteration 8200: Loss = -11533.169712802062
2
Iteration 8300: Loss = -11533.177310277933
3
Iteration 8400: Loss = -11533.170155369982
4
Iteration 8500: Loss = -11533.16855108362
Iteration 8600: Loss = -11533.16863815409
Iteration 8700: Loss = -11533.19055857964
1
Iteration 8800: Loss = -11533.168246093954
Iteration 8900: Loss = -11533.16825758683
Iteration 9000: Loss = -11533.168190306133
Iteration 9100: Loss = -11533.168072460092
Iteration 9200: Loss = -11533.169390429546
1
Iteration 9300: Loss = -11533.168801888462
2
Iteration 9400: Loss = -11533.179740638274
3
Iteration 9500: Loss = -11533.172450964017
4
Iteration 9600: Loss = -11533.168381739626
5
Iteration 9700: Loss = -11533.182666898427
6
Iteration 9800: Loss = -11533.197066098162
7
Iteration 9900: Loss = -11533.269163023833
8
Iteration 10000: Loss = -11533.167619782114
Iteration 10100: Loss = -11533.17078922319
1
Iteration 10200: Loss = -11533.190639410484
2
Iteration 10300: Loss = -11533.196945239135
3
Iteration 10400: Loss = -11533.173027899737
4
Iteration 10500: Loss = -11533.168501751732
5
Iteration 10600: Loss = -11533.226885541057
6
Iteration 10700: Loss = -11533.168589882354
7
Iteration 10800: Loss = -11533.199903210603
8
Iteration 10900: Loss = -11533.174312893709
9
Iteration 11000: Loss = -11533.167921641247
10
Iteration 11100: Loss = -11533.167217972781
Iteration 11200: Loss = -11533.170007489036
1
Iteration 11300: Loss = -11533.181372630263
2
Iteration 11400: Loss = -11533.168215846996
3
Iteration 11500: Loss = -11533.174876549312
4
Iteration 11600: Loss = -11533.169960810259
5
Iteration 11700: Loss = -11533.296034172494
6
Iteration 11800: Loss = -11533.16704843016
Iteration 11900: Loss = -11533.167781622122
1
Iteration 12000: Loss = -11533.16868506356
2
Iteration 12100: Loss = -11533.186685084294
3
Iteration 12200: Loss = -11533.175380461655
4
Iteration 12300: Loss = -11533.167388412743
5
Iteration 12400: Loss = -11533.171220393739
6
Iteration 12500: Loss = -11533.177344751122
7
Iteration 12600: Loss = -11533.168559556518
8
Iteration 12700: Loss = -11533.17147345045
9
Iteration 12800: Loss = -11533.227983865427
10
Iteration 12900: Loss = -11533.206315580579
11
Iteration 13000: Loss = -11533.17140543807
12
Iteration 13100: Loss = -11533.168167947479
13
Iteration 13200: Loss = -11533.176126702258
14
Iteration 13300: Loss = -11533.176153784958
15
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[0.7395, 0.2605],
        [0.2492, 0.7508]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4803, 0.5197], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3907, 0.1049],
         [0.6382, 0.1999]],

        [[0.5812, 0.1067],
         [0.5046, 0.5196]],

        [[0.6022, 0.1056],
         [0.6319, 0.6356]],

        [[0.7296, 0.1019],
         [0.6482, 0.6788]],

        [[0.6423, 0.0941],
         [0.5526, 0.5427]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961683251316
Average Adjusted Rand Index: 0.9761604351641242
11540.619767034359
[0.9760961683251316, 0.9760961683251316] [0.9761604351641242, 0.9761604351641242] [11533.179494127955, 11533.176153784958]
-------------------------------------
This iteration is 87
True Objective function: Loss = -11656.931187852699
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24343.475926123556
Iteration 100: Loss = -12340.42793795843
Iteration 200: Loss = -11676.478898604184
Iteration 300: Loss = -11651.430686851609
Iteration 400: Loss = -11651.071258665745
Iteration 500: Loss = -11650.924451960247
Iteration 600: Loss = -11650.845561320615
Iteration 700: Loss = -11650.796553384145
Iteration 800: Loss = -11650.763565188183
Iteration 900: Loss = -11650.74000539124
Iteration 1000: Loss = -11650.722462114245
Iteration 1100: Loss = -11650.7089827244
Iteration 1200: Loss = -11650.698425394618
Iteration 1300: Loss = -11650.690014023363
Iteration 1400: Loss = -11650.68321161685
Iteration 1500: Loss = -11650.677584723046
Iteration 1600: Loss = -11650.672935441253
Iteration 1700: Loss = -11650.668948709777
Iteration 1800: Loss = -11650.6655638971
Iteration 1900: Loss = -11650.662670683345
Iteration 2000: Loss = -11650.66012941342
Iteration 2100: Loss = -11650.65788538726
Iteration 2200: Loss = -11650.655946770308
Iteration 2300: Loss = -11650.654230855793
Iteration 2400: Loss = -11650.652698027392
Iteration 2500: Loss = -11650.651337377985
Iteration 2600: Loss = -11650.650101705758
Iteration 2700: Loss = -11650.649683385365
Iteration 2800: Loss = -11650.648027108873
Iteration 2900: Loss = -11650.647131723406
Iteration 3000: Loss = -11650.646375369392
Iteration 3100: Loss = -11650.645570946004
Iteration 3200: Loss = -11650.644927538442
Iteration 3300: Loss = -11650.644289963018
Iteration 3400: Loss = -11650.643702527348
Iteration 3500: Loss = -11650.643216989149
Iteration 3600: Loss = -11650.642718875619
Iteration 3700: Loss = -11650.64491011085
1
Iteration 3800: Loss = -11650.641883505696
Iteration 3900: Loss = -11650.64153863723
Iteration 4000: Loss = -11650.641186916773
Iteration 4100: Loss = -11650.64084391117
Iteration 4200: Loss = -11650.640581641044
Iteration 4300: Loss = -11650.64031169566
Iteration 4400: Loss = -11650.640106875911
Iteration 4500: Loss = -11650.640157342867
Iteration 4600: Loss = -11650.639612733023
Iteration 4700: Loss = -11650.639445776464
Iteration 4800: Loss = -11650.6391890249
Iteration 4900: Loss = -11650.639063875173
Iteration 5000: Loss = -11650.643047680398
1
Iteration 5100: Loss = -11650.638939721232
Iteration 5200: Loss = -11650.646771923412
1
Iteration 5300: Loss = -11650.639696532326
2
Iteration 5400: Loss = -11650.638328934208
Iteration 5500: Loss = -11650.638095449767
Iteration 5600: Loss = -11650.64822415241
1
Iteration 5700: Loss = -11650.63785261614
Iteration 5800: Loss = -11650.637740641228
Iteration 5900: Loss = -11650.637605199167
Iteration 6000: Loss = -11650.637468906905
Iteration 6100: Loss = -11650.637834734043
1
Iteration 6200: Loss = -11650.637263304332
Iteration 6300: Loss = -11650.638020011127
1
Iteration 6400: Loss = -11650.637233932894
Iteration 6500: Loss = -11650.637752723527
1
Iteration 6600: Loss = -11650.648908529085
2
Iteration 6700: Loss = -11650.640042921385
3
Iteration 6800: Loss = -11650.636933930797
Iteration 6900: Loss = -11650.638541577733
1
Iteration 7000: Loss = -11650.636756132306
Iteration 7100: Loss = -11650.637331548172
1
Iteration 7200: Loss = -11650.63748713588
2
Iteration 7300: Loss = -11650.645473821118
3
Iteration 7400: Loss = -11650.638053230043
4
Iteration 7500: Loss = -11650.666524954146
5
Iteration 7600: Loss = -11650.636737412611
Iteration 7700: Loss = -11650.636305823944
Iteration 7800: Loss = -11650.63627843476
Iteration 7900: Loss = -11650.636275324205
Iteration 8000: Loss = -11650.636772965303
1
Iteration 8100: Loss = -11650.636214694146
Iteration 8200: Loss = -11650.636123616927
Iteration 8300: Loss = -11650.636110624719
Iteration 8400: Loss = -11650.636071668727
Iteration 8500: Loss = -11650.636122760474
Iteration 8600: Loss = -11650.63905435594
1
Iteration 8700: Loss = -11650.636034339937
Iteration 8800: Loss = -11650.636028147848
Iteration 8900: Loss = -11650.63599113832
Iteration 9000: Loss = -11650.636428226539
1
Iteration 9100: Loss = -11650.642749664024
2
Iteration 9200: Loss = -11650.652602277187
3
Iteration 9300: Loss = -11650.637368923844
4
Iteration 9400: Loss = -11650.64053112129
5
Iteration 9500: Loss = -11650.638851434227
6
Iteration 9600: Loss = -11650.637719110995
7
Iteration 9700: Loss = -11650.638324592035
8
Iteration 9800: Loss = -11650.652420155198
9
Iteration 9900: Loss = -11650.881158808987
10
Iteration 10000: Loss = -11650.634335751593
Iteration 10100: Loss = -11650.634365456028
Iteration 10200: Loss = -11650.638414246256
1
Iteration 10300: Loss = -11650.637074495915
2
Iteration 10400: Loss = -11650.63682765271
3
Iteration 10500: Loss = -11650.634812725048
4
Iteration 10600: Loss = -11650.639819715601
5
Iteration 10700: Loss = -11650.637805491217
6
Iteration 10800: Loss = -11650.747959963248
7
Iteration 10900: Loss = -11650.63449603344
8
Iteration 11000: Loss = -11650.634738432005
9
Iteration 11100: Loss = -11650.638999764575
10
Iteration 11200: Loss = -11650.646770167537
11
Iteration 11300: Loss = -11650.636744024074
12
Iteration 11400: Loss = -11650.63436924072
Iteration 11500: Loss = -11650.640219482786
1
Iteration 11600: Loss = -11650.63522823194
2
Iteration 11700: Loss = -11650.638975012822
3
Iteration 11800: Loss = -11650.635993909475
4
Iteration 11900: Loss = -11650.638553838877
5
Iteration 12000: Loss = -11650.662971353375
6
Iteration 12100: Loss = -11650.647636118527
7
Iteration 12200: Loss = -11650.63729112383
8
Iteration 12300: Loss = -11650.701689929858
9
Iteration 12400: Loss = -11650.639882806941
10
Iteration 12500: Loss = -11650.636155890003
11
Iteration 12600: Loss = -11650.660496927327
12
Iteration 12700: Loss = -11650.636081745086
13
Iteration 12800: Loss = -11650.638568947199
14
Iteration 12900: Loss = -11650.639657774007
15
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[0.7409, 0.2591],
        [0.2986, 0.7014]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4404, 0.5596], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3984, 0.0999],
         [0.6461, 0.1931]],

        [[0.6139, 0.1027],
         [0.5353, 0.5630]],

        [[0.6874, 0.1037],
         [0.7283, 0.6853]],

        [[0.5245, 0.1071],
         [0.6609, 0.7100]],

        [[0.5644, 0.1118],
         [0.5935, 0.5282]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23863.53239267469
Iteration 100: Loss = -12353.600451930457
Iteration 200: Loss = -11853.665563260653
Iteration 300: Loss = -11784.120598496238
Iteration 400: Loss = -11668.579639768162
Iteration 500: Loss = -11651.312248002785
Iteration 600: Loss = -11651.069481979517
Iteration 700: Loss = -11650.961626940145
Iteration 800: Loss = -11650.897383400754
Iteration 900: Loss = -11650.85461515737
Iteration 1000: Loss = -11650.824197631553
Iteration 1100: Loss = -11650.801520151577
Iteration 1200: Loss = -11650.783782053168
Iteration 1300: Loss = -11650.767803863342
Iteration 1400: Loss = -11650.711063147872
Iteration 1500: Loss = -11650.700765039956
Iteration 1600: Loss = -11650.693239013039
Iteration 1700: Loss = -11650.68701902774
Iteration 1800: Loss = -11650.681764519093
Iteration 1900: Loss = -11650.67729214641
Iteration 2000: Loss = -11650.673405691978
Iteration 2100: Loss = -11650.67007470046
Iteration 2200: Loss = -11650.667097327601
Iteration 2300: Loss = -11650.664446727578
Iteration 2400: Loss = -11650.662083833167
Iteration 2500: Loss = -11650.677643444385
1
Iteration 2600: Loss = -11650.65798630144
Iteration 2700: Loss = -11650.65609772859
Iteration 2800: Loss = -11650.654446125005
Iteration 2900: Loss = -11650.652786215927
Iteration 3000: Loss = -11650.65158859356
Iteration 3100: Loss = -11650.65048459087
Iteration 3200: Loss = -11650.649535375012
Iteration 3300: Loss = -11650.648620642005
Iteration 3400: Loss = -11650.647899395097
Iteration 3500: Loss = -11650.647019113148
Iteration 3600: Loss = -11650.660227574837
1
Iteration 3700: Loss = -11650.646505832698
Iteration 3800: Loss = -11650.6503547511
1
Iteration 3900: Loss = -11650.644599253965
Iteration 4000: Loss = -11650.64482017
1
Iteration 4100: Loss = -11650.643653742727
Iteration 4200: Loss = -11650.647654119457
1
Iteration 4300: Loss = -11650.645341872592
2
Iteration 4400: Loss = -11650.642477444468
Iteration 4500: Loss = -11650.645970922636
1
Iteration 4600: Loss = -11650.642060301028
Iteration 4700: Loss = -11650.641517229162
Iteration 4800: Loss = -11650.641209529393
Iteration 4900: Loss = -11650.640814091172
Iteration 5000: Loss = -11650.640479283458
Iteration 5100: Loss = -11650.63892037983
Iteration 5200: Loss = -11650.63829605396
Iteration 5300: Loss = -11650.63921922188
1
Iteration 5400: Loss = -11650.63737241087
Iteration 5500: Loss = -11650.675412978608
1
Iteration 5600: Loss = -11650.63698909109
Iteration 5700: Loss = -11650.636867302419
Iteration 5800: Loss = -11650.642485458256
1
Iteration 5900: Loss = -11650.636608006043
Iteration 6000: Loss = -11650.636485640773
Iteration 6100: Loss = -11650.636354598817
Iteration 6200: Loss = -11650.636241681066
Iteration 6300: Loss = -11650.638548415342
1
Iteration 6400: Loss = -11650.63604344725
Iteration 6500: Loss = -11650.636291573448
1
Iteration 6600: Loss = -11650.635915433819
Iteration 6700: Loss = -11650.636339567243
1
Iteration 6800: Loss = -11650.637297465242
2
Iteration 6900: Loss = -11650.648152254591
3
Iteration 7000: Loss = -11650.641960577686
4
Iteration 7100: Loss = -11650.635716733977
Iteration 7200: Loss = -11650.635579786636
Iteration 7300: Loss = -11650.635534343031
Iteration 7400: Loss = -11650.63649826067
1
Iteration 7500: Loss = -11650.639686118657
2
Iteration 7600: Loss = -11650.640202018363
3
Iteration 7700: Loss = -11650.642194445285
4
Iteration 7800: Loss = -11650.636020421429
5
Iteration 7900: Loss = -11650.635825947418
6
Iteration 8000: Loss = -11650.636038090945
7
Iteration 8100: Loss = -11650.63428549292
Iteration 8200: Loss = -11650.634122442381
Iteration 8300: Loss = -11650.635436840605
1
Iteration 8400: Loss = -11650.634063911086
Iteration 8500: Loss = -11650.633979672715
Iteration 8600: Loss = -11650.637259880603
1
Iteration 8700: Loss = -11650.634479545633
2
Iteration 8800: Loss = -11650.634001048255
Iteration 8900: Loss = -11650.634084244295
Iteration 9000: Loss = -11650.635261216248
1
Iteration 9100: Loss = -11650.6367237161
2
Iteration 9200: Loss = -11650.721876265126
3
Iteration 9300: Loss = -11650.64760140546
4
Iteration 9400: Loss = -11650.672781247093
5
Iteration 9500: Loss = -11650.639281184967
6
Iteration 9600: Loss = -11650.635140712877
7
Iteration 9700: Loss = -11650.635095872554
8
Iteration 9800: Loss = -11650.634135200267
Iteration 9900: Loss = -11650.635619715207
1
Iteration 10000: Loss = -11650.666691224009
2
Iteration 10100: Loss = -11650.634727829965
3
Iteration 10200: Loss = -11650.63678774159
4
Iteration 10300: Loss = -11650.634177315542
Iteration 10400: Loss = -11650.635116134548
1
Iteration 10500: Loss = -11650.633653882318
Iteration 10600: Loss = -11650.634166697888
1
Iteration 10700: Loss = -11650.646614314002
2
Iteration 10800: Loss = -11650.633794529338
3
Iteration 10900: Loss = -11650.64008693579
4
Iteration 11000: Loss = -11650.692120142086
5
Iteration 11100: Loss = -11650.695991649973
6
Iteration 11200: Loss = -11650.687850839318
7
Iteration 11300: Loss = -11650.637366027924
8
Iteration 11400: Loss = -11650.633609627424
Iteration 11500: Loss = -11650.650106950574
1
Iteration 11600: Loss = -11650.638710461662
2
Iteration 11700: Loss = -11650.63367404196
Iteration 11800: Loss = -11650.634400299188
1
Iteration 11900: Loss = -11650.636339133844
2
Iteration 12000: Loss = -11650.645946053148
3
Iteration 12100: Loss = -11650.636483641323
4
Iteration 12200: Loss = -11650.640138586507
5
Iteration 12300: Loss = -11650.636213467844
6
Iteration 12400: Loss = -11650.63514119616
7
Iteration 12500: Loss = -11650.6365738096
8
Iteration 12600: Loss = -11650.636141932213
9
Iteration 12700: Loss = -11650.649669654209
10
Iteration 12800: Loss = -11650.63526044854
11
Iteration 12900: Loss = -11650.636964172301
12
Iteration 13000: Loss = -11650.635676685324
13
Iteration 13100: Loss = -11650.65115770004
14
Iteration 13200: Loss = -11650.644192243773
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[0.7419, 0.2581],
        [0.2956, 0.7044]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4376, 0.5624], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3977, 0.0998],
         [0.6366, 0.1936]],

        [[0.5348, 0.1026],
         [0.6367, 0.6053]],

        [[0.7206, 0.1037],
         [0.6860, 0.6362]],

        [[0.6506, 0.1073],
         [0.6394, 0.6281]],

        [[0.7298, 0.1122],
         [0.6827, 0.5940]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11656.931187852699
[1.0, 1.0] [1.0, 1.0] [11650.639657774007, 11650.644192243773]
-------------------------------------
This iteration is 88
True Objective function: Loss = -11637.32399173985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20944.657958548658
Iteration 100: Loss = -12435.50119985916
Iteration 200: Loss = -12405.562414919339
Iteration 300: Loss = -11788.793112256222
Iteration 400: Loss = -11760.18482628313
Iteration 500: Loss = -11759.30543966278
Iteration 600: Loss = -11758.912907965188
Iteration 700: Loss = -11758.690670971668
Iteration 800: Loss = -11758.547780424857
Iteration 900: Loss = -11758.450084332133
Iteration 1000: Loss = -11758.377738971074
Iteration 1100: Loss = -11758.319364805142
Iteration 1200: Loss = -11758.263916133834
Iteration 1300: Loss = -11758.094704934487
Iteration 1400: Loss = -11757.736715730385
Iteration 1500: Loss = -11728.412892006887
Iteration 1600: Loss = -11655.611289800985
Iteration 1700: Loss = -11643.159019080704
Iteration 1800: Loss = -11633.579042840032
Iteration 1900: Loss = -11633.504702788052
Iteration 2000: Loss = -11633.464955617725
Iteration 2100: Loss = -11633.438617658914
Iteration 2200: Loss = -11633.419381435579
Iteration 2300: Loss = -11633.404388276498
Iteration 2400: Loss = -11633.392137622992
Iteration 2500: Loss = -11633.383908603442
Iteration 2600: Loss = -11633.371758720696
Iteration 2700: Loss = -11633.362151742007
Iteration 2800: Loss = -11633.353802503061
Iteration 2900: Loss = -11633.347998048544
Iteration 3000: Loss = -11633.343516868279
Iteration 3100: Loss = -11633.339718719983
Iteration 3200: Loss = -11633.336482215473
Iteration 3300: Loss = -11633.333396099184
Iteration 3400: Loss = -11633.330828929733
Iteration 3500: Loss = -11633.330725667942
Iteration 3600: Loss = -11633.326350938776
Iteration 3700: Loss = -11633.328348221003
1
Iteration 3800: Loss = -11633.323175656984
Iteration 3900: Loss = -11633.321195600753
Iteration 4000: Loss = -11633.31996164037
Iteration 4100: Loss = -11633.318419039806
Iteration 4200: Loss = -11633.31729980352
Iteration 4300: Loss = -11633.316175437523
Iteration 4400: Loss = -11633.315122885922
Iteration 4500: Loss = -11633.314564417073
Iteration 4600: Loss = -11633.313290115802
Iteration 4700: Loss = -11633.315105345793
1
Iteration 4800: Loss = -11633.31216384377
Iteration 4900: Loss = -11633.315998314309
1
Iteration 5000: Loss = -11633.317887633186
2
Iteration 5100: Loss = -11633.309997290105
Iteration 5200: Loss = -11633.309882212696
Iteration 5300: Loss = -11633.309867485925
Iteration 5400: Loss = -11633.308729300785
Iteration 5500: Loss = -11633.307953397785
Iteration 5600: Loss = -11633.307184959176
Iteration 5700: Loss = -11633.31016344825
1
Iteration 5800: Loss = -11633.305907386655
Iteration 5900: Loss = -11633.306503654743
1
Iteration 6000: Loss = -11633.308585756742
2
Iteration 6100: Loss = -11633.304937827246
Iteration 6200: Loss = -11633.304599069283
Iteration 6300: Loss = -11633.318223322063
1
Iteration 6400: Loss = -11633.303861109523
Iteration 6500: Loss = -11633.304160912103
1
Iteration 6600: Loss = -11633.303396530051
Iteration 6700: Loss = -11633.303196836214
Iteration 6800: Loss = -11633.303754747052
1
Iteration 6900: Loss = -11633.313614075923
2
Iteration 7000: Loss = -11633.302680676903
Iteration 7100: Loss = -11633.306424834173
1
Iteration 7200: Loss = -11633.302382032309
Iteration 7300: Loss = -11633.302283172614
Iteration 7400: Loss = -11633.302038347289
Iteration 7500: Loss = -11633.302258626634
1
Iteration 7600: Loss = -11633.304723809968
2
Iteration 7700: Loss = -11633.311078440373
3
Iteration 7800: Loss = -11633.301812301019
Iteration 7900: Loss = -11633.316033535773
1
Iteration 8000: Loss = -11633.377506748804
2
Iteration 8100: Loss = -11633.309293989874
3
Iteration 8200: Loss = -11633.301575736203
Iteration 8300: Loss = -11633.374776104118
1
Iteration 8400: Loss = -11633.30163515043
Iteration 8500: Loss = -11633.301170550665
Iteration 8600: Loss = -11633.30148945276
1
Iteration 8700: Loss = -11633.300884954377
Iteration 8800: Loss = -11633.300729757115
Iteration 8900: Loss = -11633.301531745441
1
Iteration 9000: Loss = -11633.31129321059
2
Iteration 9100: Loss = -11633.300560069292
Iteration 9200: Loss = -11633.301071240781
1
Iteration 9300: Loss = -11633.30480139711
2
Iteration 9400: Loss = -11633.300465517
Iteration 9500: Loss = -11633.309392041348
1
Iteration 9600: Loss = -11633.302513679353
2
Iteration 9700: Loss = -11633.30376839456
3
Iteration 9800: Loss = -11633.324476556641
4
Iteration 9900: Loss = -11633.317244815345
5
Iteration 10000: Loss = -11633.301162910833
6
Iteration 10100: Loss = -11633.301195256849
7
Iteration 10200: Loss = -11633.301280841435
8
Iteration 10300: Loss = -11633.300485153808
Iteration 10400: Loss = -11633.300876093239
1
Iteration 10500: Loss = -11633.311200163193
2
Iteration 10600: Loss = -11633.301832774947
3
Iteration 10700: Loss = -11633.315418480172
4
Iteration 10800: Loss = -11633.456137695573
5
Iteration 10900: Loss = -11633.301624215488
6
Iteration 11000: Loss = -11633.306057858053
7
Iteration 11100: Loss = -11633.31683465475
8
Iteration 11200: Loss = -11633.30940910861
9
Iteration 11300: Loss = -11633.299937234897
Iteration 11400: Loss = -11633.305084765392
1
Iteration 11500: Loss = -11633.301951671196
2
Iteration 11600: Loss = -11633.300190318409
3
Iteration 11700: Loss = -11633.300073278184
4
Iteration 11800: Loss = -11633.300990792808
5
Iteration 11900: Loss = -11633.303008220279
6
Iteration 12000: Loss = -11633.333624723999
7
Iteration 12100: Loss = -11633.314545524374
8
Iteration 12200: Loss = -11633.379439108405
9
Iteration 12300: Loss = -11633.335793957245
10
Iteration 12400: Loss = -11633.329745898132
11
Iteration 12500: Loss = -11633.320829534492
12
Iteration 12600: Loss = -11633.299798689064
Iteration 12700: Loss = -11633.301311333533
1
Iteration 12800: Loss = -11633.312899656048
2
Iteration 12900: Loss = -11633.32275765677
3
Iteration 13000: Loss = -11633.302603894988
4
Iteration 13100: Loss = -11633.315006039986
5
Iteration 13200: Loss = -11633.327900241704
6
Iteration 13300: Loss = -11633.3022040814
7
Iteration 13400: Loss = -11633.301567856262
8
Iteration 13500: Loss = -11633.300462760153
9
Iteration 13600: Loss = -11633.323117246462
10
Iteration 13700: Loss = -11633.306552481681
11
Iteration 13800: Loss = -11633.30641345016
12
Iteration 13900: Loss = -11633.306493848588
13
Iteration 14000: Loss = -11633.317451579322
14
Iteration 14100: Loss = -11633.356082764583
15
Stopping early at iteration 14100 due to no improvement.
pi: tensor([[0.7725, 0.2275],
        [0.2686, 0.7314]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4368, 0.5632], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3951, 0.1005],
         [0.5771, 0.1980]],

        [[0.7182, 0.1041],
         [0.5916, 0.5248]],

        [[0.6530, 0.1010],
         [0.6860, 0.7071]],

        [[0.5168, 0.0951],
         [0.5871, 0.5884]],

        [[0.5151, 0.1063],
         [0.6353, 0.6244]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919996552039955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19491.97380969483
Iteration 100: Loss = -11634.514421414464
Iteration 200: Loss = -11633.796069547774
Iteration 300: Loss = -11633.587492721126
Iteration 400: Loss = -11633.490261053828
Iteration 500: Loss = -11633.435926891842
Iteration 600: Loss = -11633.402306104643
Iteration 700: Loss = -11633.379848594668
Iteration 800: Loss = -11633.36405889289
Iteration 900: Loss = -11633.352598472376
Iteration 1000: Loss = -11633.343902911554
Iteration 1100: Loss = -11633.33720473609
Iteration 1200: Loss = -11633.331828168348
Iteration 1300: Loss = -11633.327589849208
Iteration 1400: Loss = -11633.324061290807
Iteration 1500: Loss = -11633.321115661005
Iteration 1600: Loss = -11633.318705028938
Iteration 1700: Loss = -11633.316643716393
Iteration 1800: Loss = -11633.31485028028
Iteration 1900: Loss = -11633.313338907154
Iteration 2000: Loss = -11633.312035156094
Iteration 2100: Loss = -11633.310842782521
Iteration 2200: Loss = -11633.309824784634
Iteration 2300: Loss = -11633.308934898381
Iteration 2400: Loss = -11633.3081564816
Iteration 2500: Loss = -11633.309660626135
1
Iteration 2600: Loss = -11633.306950294877
Iteration 2700: Loss = -11633.306359643844
Iteration 2800: Loss = -11633.308528794132
1
Iteration 2900: Loss = -11633.305857390947
Iteration 3000: Loss = -11633.308822391475
1
Iteration 3100: Loss = -11633.304470120409
Iteration 3200: Loss = -11633.304142199026
Iteration 3300: Loss = -11633.303959897425
Iteration 3400: Loss = -11633.303744415936
Iteration 3500: Loss = -11633.30553787793
1
Iteration 3600: Loss = -11633.30301915173
Iteration 3700: Loss = -11633.303403015267
1
Iteration 3800: Loss = -11633.305039954135
2
Iteration 3900: Loss = -11633.30241945329
Iteration 4000: Loss = -11633.304361479937
1
Iteration 4100: Loss = -11633.302968491142
2
Iteration 4200: Loss = -11633.302621578088
3
Iteration 4300: Loss = -11633.307367707985
4
Iteration 4400: Loss = -11633.301803905666
Iteration 4500: Loss = -11633.301496836872
Iteration 4600: Loss = -11633.301476574397
Iteration 4700: Loss = -11633.301436800053
Iteration 4800: Loss = -11633.30135735802
Iteration 4900: Loss = -11633.301096420408
Iteration 5000: Loss = -11633.301430157377
1
Iteration 5100: Loss = -11633.300914762898
Iteration 5200: Loss = -11633.300873714043
Iteration 5300: Loss = -11633.304836122494
1
Iteration 5400: Loss = -11633.301610117633
2
Iteration 5500: Loss = -11633.306732386873
3
Iteration 5600: Loss = -11633.300552418734
Iteration 5700: Loss = -11633.300537306242
Iteration 5800: Loss = -11633.301680000806
1
Iteration 5900: Loss = -11633.30057347663
Iteration 6000: Loss = -11633.30066491662
Iteration 6100: Loss = -11633.300378113707
Iteration 6200: Loss = -11633.300279359277
Iteration 6300: Loss = -11633.300245004097
Iteration 6400: Loss = -11633.310577080869
1
Iteration 6500: Loss = -11633.3001807265
Iteration 6600: Loss = -11633.300238740767
Iteration 6700: Loss = -11633.300119480435
Iteration 6800: Loss = -11633.300106904315
Iteration 6900: Loss = -11633.300096345902
Iteration 7000: Loss = -11633.30133934803
1
Iteration 7100: Loss = -11633.300060562617
Iteration 7200: Loss = -11633.2999758448
Iteration 7300: Loss = -11633.30027117853
1
Iteration 7400: Loss = -11633.300180513956
2
Iteration 7500: Loss = -11633.30077756753
3
Iteration 7600: Loss = -11633.336803147278
4
Iteration 7700: Loss = -11633.299887837118
Iteration 7800: Loss = -11633.300559162522
1
Iteration 7900: Loss = -11633.299871129904
Iteration 8000: Loss = -11633.299876364717
Iteration 8100: Loss = -11633.300298761853
1
Iteration 8200: Loss = -11633.299827842307
Iteration 8300: Loss = -11633.299944820941
1
Iteration 8400: Loss = -11633.300895903341
2
Iteration 8500: Loss = -11633.306248518365
3
Iteration 8600: Loss = -11633.299789436616
Iteration 8700: Loss = -11633.29989116095
1
Iteration 8800: Loss = -11633.299953644659
2
Iteration 8900: Loss = -11633.300897476238
3
Iteration 9000: Loss = -11633.30256280615
4
Iteration 9100: Loss = -11633.299745360202
Iteration 9200: Loss = -11633.304256954447
1
Iteration 9300: Loss = -11633.305260900584
2
Iteration 9400: Loss = -11633.319794684096
3
Iteration 9500: Loss = -11633.303906431114
4
Iteration 9600: Loss = -11633.30041855037
5
Iteration 9700: Loss = -11633.301012617158
6
Iteration 9800: Loss = -11633.42016809433
7
Iteration 9900: Loss = -11633.30782458799
8
Iteration 10000: Loss = -11633.300373514916
9
Iteration 10100: Loss = -11633.303279607811
10
Iteration 10200: Loss = -11633.30163793133
11
Iteration 10300: Loss = -11633.301111795805
12
Iteration 10400: Loss = -11633.302617017556
13
Iteration 10500: Loss = -11633.302280505117
14
Iteration 10600: Loss = -11633.306281045683
15
Stopping early at iteration 10600 due to no improvement.
pi: tensor([[0.7320, 0.2680],
        [0.2277, 0.7723]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5655, 0.4345], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1987, 0.1001],
         [0.6807, 0.3964]],

        [[0.7096, 0.1038],
         [0.5512, 0.6567]],

        [[0.6300, 0.1010],
         [0.6726, 0.5312]],

        [[0.6739, 0.0962],
         [0.7296, 0.6378]],

        [[0.6493, 0.1065],
         [0.6363, 0.6606]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999775871758
Average Adjusted Rand Index: 0.9919996552039955
11637.32399173985
[0.9919999775871758, 0.9919999775871758] [0.9919996552039955, 0.9919996552039955] [11633.356082764583, 11633.306281045683]
-------------------------------------
This iteration is 89
True Objective function: Loss = -11287.490840260527
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20716.206671640597
Iteration 100: Loss = -12057.249981220275
Iteration 200: Loss = -12053.415740415785
Iteration 300: Loss = -12043.08462005795
Iteration 400: Loss = -11428.805385344354
Iteration 500: Loss = -11325.157863332759
Iteration 600: Loss = -11318.188103307983
Iteration 700: Loss = -11307.053389044491
Iteration 800: Loss = -11302.163695336445
Iteration 900: Loss = -11285.601175603228
Iteration 1000: Loss = -11285.536235439064
Iteration 1100: Loss = -11285.491371067099
Iteration 1200: Loss = -11285.458156209497
Iteration 1300: Loss = -11285.432633375354
Iteration 1400: Loss = -11285.412236981238
Iteration 1500: Loss = -11285.395587859617
Iteration 1600: Loss = -11285.381791575976
Iteration 1700: Loss = -11285.370279385203
Iteration 1800: Loss = -11285.360655338665
Iteration 1900: Loss = -11285.35252113008
Iteration 2000: Loss = -11285.345435645282
Iteration 2100: Loss = -11285.339013377934
Iteration 2200: Loss = -11285.332653566938
Iteration 2300: Loss = -11285.323790207789
Iteration 2400: Loss = -11285.310396220952
Iteration 2500: Loss = -11283.382500183363
Iteration 2600: Loss = -11280.155947490472
Iteration 2700: Loss = -11280.144092805936
Iteration 2800: Loss = -11280.136123418955
Iteration 2900: Loss = -11280.133645153408
Iteration 3000: Loss = -11280.13140159872
Iteration 3100: Loss = -11280.128680359565
Iteration 3200: Loss = -11280.12652089083
Iteration 3300: Loss = -11280.12445168359
Iteration 3400: Loss = -11280.122617762994
Iteration 3500: Loss = -11280.121140973191
Iteration 3600: Loss = -11280.119830359625
Iteration 3700: Loss = -11280.118594305486
Iteration 3800: Loss = -11280.117452399287
Iteration 3900: Loss = -11280.117591921617
1
Iteration 4000: Loss = -11280.115457797883
Iteration 4100: Loss = -11280.114559836695
Iteration 4200: Loss = -11280.113701792161
Iteration 4300: Loss = -11280.112857014114
Iteration 4400: Loss = -11280.111867021818
Iteration 4500: Loss = -11280.11036574829
Iteration 4600: Loss = -11280.09463212555
Iteration 4700: Loss = -11280.075747541798
Iteration 4800: Loss = -11280.07530289994
Iteration 4900: Loss = -11280.074583442434
Iteration 5000: Loss = -11280.073898598564
Iteration 5100: Loss = -11280.073139260028
Iteration 5200: Loss = -11280.071155232912
Iteration 5300: Loss = -11280.070294929494
Iteration 5400: Loss = -11280.07362301118
1
Iteration 5500: Loss = -11280.06942044733
Iteration 5600: Loss = -11280.069133738249
Iteration 5700: Loss = -11280.071113067293
1
Iteration 5800: Loss = -11280.068636454078
Iteration 5900: Loss = -11280.073824138419
1
Iteration 6000: Loss = -11280.068134043766
Iteration 6100: Loss = -11280.077755839195
1
Iteration 6200: Loss = -11280.07662133953
2
Iteration 6300: Loss = -11280.067611988157
Iteration 6400: Loss = -11280.067369595688
Iteration 6500: Loss = -11280.067038172474
Iteration 6600: Loss = -11280.092935355995
1
Iteration 6700: Loss = -11280.066659304835
Iteration 6800: Loss = -11280.066649464612
Iteration 6900: Loss = -11280.069514912184
1
Iteration 7000: Loss = -11280.066187508255
Iteration 7100: Loss = -11280.06600656921
Iteration 7200: Loss = -11280.065935769746
Iteration 7300: Loss = -11280.06584251475
Iteration 7400: Loss = -11280.069471033261
1
Iteration 7500: Loss = -11280.065676665808
Iteration 7600: Loss = -11280.071214178435
1
Iteration 7700: Loss = -11280.071433511654
2
Iteration 7800: Loss = -11280.065351837738
Iteration 7900: Loss = -11280.111271577027
1
Iteration 8000: Loss = -11280.067497912934
2
Iteration 8100: Loss = -11280.065180895657
Iteration 8200: Loss = -11280.069041070032
1
Iteration 8300: Loss = -11280.083405853247
2
Iteration 8400: Loss = -11280.066427042631
3
Iteration 8500: Loss = -11280.065084939295
Iteration 8600: Loss = -11280.06517
Iteration 8700: Loss = -11280.093941890074
1
Iteration 8800: Loss = -11280.064778311169
Iteration 8900: Loss = -11280.066289189606
1
Iteration 9000: Loss = -11280.065201347943
2
Iteration 9100: Loss = -11280.06572477253
3
Iteration 9200: Loss = -11280.102964845313
4
Iteration 9300: Loss = -11280.139383754466
5
Iteration 9400: Loss = -11280.122493198876
6
Iteration 9500: Loss = -11280.068449334332
7
Iteration 9600: Loss = -11280.066105300279
8
Iteration 9700: Loss = -11280.068074692532
9
Iteration 9800: Loss = -11280.065870904069
10
Iteration 9900: Loss = -11280.069714320347
11
Iteration 10000: Loss = -11280.094231195115
12
Iteration 10100: Loss = -11280.064240420657
Iteration 10200: Loss = -11280.076806078665
1
Iteration 10300: Loss = -11280.072128365086
2
Iteration 10400: Loss = -11280.064616568894
3
Iteration 10500: Loss = -11280.065121994008
4
Iteration 10600: Loss = -11280.064212777843
Iteration 10700: Loss = -11280.06546907614
1
Iteration 10800: Loss = -11280.121268750754
2
Iteration 10900: Loss = -11280.083354892471
3
Iteration 11000: Loss = -11280.080355064181
4
Iteration 11100: Loss = -11280.068100884944
5
Iteration 11200: Loss = -11280.067238638165
6
Iteration 11300: Loss = -11280.077566928894
7
Iteration 11400: Loss = -11280.06989419258
8
Iteration 11500: Loss = -11280.07616769766
9
Iteration 11600: Loss = -11280.07231642618
10
Iteration 11700: Loss = -11280.070547317106
11
Iteration 11800: Loss = -11280.075442611706
12
Iteration 11900: Loss = -11280.069317563157
13
Iteration 12000: Loss = -11280.106845367849
14
Iteration 12100: Loss = -11280.064413891821
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.7090, 0.2910],
        [0.2452, 0.7548]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4692, 0.5308], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4040, 0.1018],
         [0.7135, 0.2069]],

        [[0.5269, 0.0879],
         [0.5692, 0.7247]],

        [[0.6414, 0.0935],
         [0.6177, 0.6905]],

        [[0.5189, 0.0899],
         [0.5217, 0.6195]],

        [[0.6823, 0.0920],
         [0.5206, 0.6860]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24264.167199365045
Iteration 100: Loss = -12058.034092819606
Iteration 200: Loss = -12046.300265722302
Iteration 300: Loss = -11692.722433200443
Iteration 400: Loss = -11492.8488738838
Iteration 500: Loss = -11460.282992301381
Iteration 600: Loss = -11434.08750854377
Iteration 700: Loss = -11420.907867628774
Iteration 800: Loss = -11420.740535814362
Iteration 900: Loss = -11420.640328605381
Iteration 1000: Loss = -11420.570548608295
Iteration 1100: Loss = -11420.518807590104
Iteration 1200: Loss = -11420.47791624014
Iteration 1300: Loss = -11420.44272342497
Iteration 1400: Loss = -11420.413702294429
Iteration 1500: Loss = -11420.390706513042
Iteration 1600: Loss = -11420.370741142038
Iteration 1700: Loss = -11420.352934659593
Iteration 1800: Loss = -11420.33628442175
Iteration 1900: Loss = -11420.319476414432
Iteration 2000: Loss = -11420.301615361845
Iteration 2100: Loss = -11420.281496003214
Iteration 2200: Loss = -11420.258561906525
Iteration 2300: Loss = -11420.235893048664
Iteration 2400: Loss = -11420.213046819885
Iteration 2500: Loss = -11420.197199705999
Iteration 2600: Loss = -11420.186626248
Iteration 2700: Loss = -11420.179561278037
Iteration 2800: Loss = -11420.174253213087
Iteration 2900: Loss = -11420.17007149115
Iteration 3000: Loss = -11420.166730954908
Iteration 3100: Loss = -11420.16362271485
Iteration 3200: Loss = -11420.161004178126
Iteration 3300: Loss = -11420.180552993026
1
Iteration 3400: Loss = -11420.156729339826
Iteration 3500: Loss = -11420.154864632535
Iteration 3600: Loss = -11420.169385307301
1
Iteration 3700: Loss = -11420.15178927531
Iteration 3800: Loss = -11420.150921200073
Iteration 3900: Loss = -11420.149188499954
Iteration 4000: Loss = -11420.148038321686
Iteration 4100: Loss = -11420.14691775395
Iteration 4200: Loss = -11420.145905187801
Iteration 4300: Loss = -11420.14540022651
Iteration 4400: Loss = -11420.143863720728
Iteration 4500: Loss = -11420.143245141231
Iteration 4600: Loss = -11420.141583481929
Iteration 4700: Loss = -11420.141783309078
1
Iteration 4800: Loss = -11420.134005806283
Iteration 4900: Loss = -11420.124922852217
Iteration 5000: Loss = -11420.10005444101
Iteration 5100: Loss = -11420.0862597719
Iteration 5200: Loss = -11420.08555649075
Iteration 5300: Loss = -11420.08480410479
Iteration 5400: Loss = -11420.084334167796
Iteration 5500: Loss = -11420.083874429298
Iteration 5600: Loss = -11420.083460931726
Iteration 5700: Loss = -11420.085073006287
1
Iteration 5800: Loss = -11420.082742866149
Iteration 5900: Loss = -11420.082441653529
Iteration 6000: Loss = -11420.082167872662
Iteration 6100: Loss = -11420.081920846844
Iteration 6200: Loss = -11420.081659696483
Iteration 6300: Loss = -11420.081640550523
Iteration 6400: Loss = -11420.081124910026
Iteration 6500: Loss = -11420.081205870641
Iteration 6600: Loss = -11420.080663149716
Iteration 6700: Loss = -11420.080453595132
Iteration 6800: Loss = -11420.08022376521
Iteration 6900: Loss = -11420.08001412829
Iteration 7000: Loss = -11420.079608399185
Iteration 7100: Loss = -11420.080903485008
1
Iteration 7200: Loss = -11420.078407016987
Iteration 7300: Loss = -11420.075805538057
Iteration 7400: Loss = -11420.078466025498
1
Iteration 7500: Loss = -11420.074015117627
Iteration 7600: Loss = -11420.071971264417
Iteration 7700: Loss = -11420.071557944697
Iteration 7800: Loss = -11420.024642840724
Iteration 7900: Loss = -11419.997068804938
Iteration 8000: Loss = -11419.996729363878
Iteration 8100: Loss = -11419.995698996601
Iteration 8200: Loss = -11419.98011167625
Iteration 8300: Loss = -11419.98893662862
1
Iteration 8400: Loss = -11419.979767634997
Iteration 8500: Loss = -11419.979977579133
1
Iteration 8600: Loss = -11419.979554685546
Iteration 8700: Loss = -11419.980295793124
1
Iteration 8800: Loss = -11419.979410198073
Iteration 8900: Loss = -11419.978858387474
Iteration 9000: Loss = -11419.979806996707
1
Iteration 9100: Loss = -11420.027305833795
2
Iteration 9200: Loss = -11419.961106173625
Iteration 9300: Loss = -11419.959196119285
Iteration 9400: Loss = -11412.640327432073
Iteration 9500: Loss = -11398.783151768872
Iteration 9600: Loss = -11398.716266886258
Iteration 9700: Loss = -11398.703429400666
Iteration 9800: Loss = -11398.707753373816
1
Iteration 9900: Loss = -11398.703780895974
2
Iteration 10000: Loss = -11398.701926361704
Iteration 10100: Loss = -11398.71153586824
1
Iteration 10200: Loss = -11398.701637254882
Iteration 10300: Loss = -11398.702249466989
1
Iteration 10400: Loss = -11398.701490336036
Iteration 10500: Loss = -11398.701155076633
Iteration 10600: Loss = -11398.712055255337
1
Iteration 10700: Loss = -11398.705446589074
2
Iteration 10800: Loss = -11398.719904585727
3
Iteration 10900: Loss = -11398.718344047762
4
Iteration 11000: Loss = -11398.698909791887
Iteration 11100: Loss = -11398.759371680546
1
Iteration 11200: Loss = -11398.698972636525
Iteration 11300: Loss = -11398.728737002231
1
Iteration 11400: Loss = -11398.6985226654
Iteration 11500: Loss = -11398.700203114668
1
Iteration 11600: Loss = -11398.69848639176
Iteration 11700: Loss = -11398.7024480167
1
Iteration 11800: Loss = -11398.698437023713
Iteration 11900: Loss = -11398.705659771336
1
Iteration 12000: Loss = -11398.698444588994
Iteration 12100: Loss = -11398.698621930323
1
Iteration 12200: Loss = -11398.719357978069
2
Iteration 12300: Loss = -11398.698406319365
Iteration 12400: Loss = -11398.703810655954
1
Iteration 12500: Loss = -11398.698379337304
Iteration 12600: Loss = -11398.698505178405
1
Iteration 12700: Loss = -11398.712875814163
2
Iteration 12800: Loss = -11398.698366185634
Iteration 12900: Loss = -11398.706969220717
1
Iteration 13000: Loss = -11398.619779355378
Iteration 13100: Loss = -11398.254614653553
Iteration 13200: Loss = -11398.253203978293
Iteration 13300: Loss = -11398.261762159145
1
Iteration 13400: Loss = -11398.252878754343
Iteration 13500: Loss = -11398.252959484693
Iteration 13600: Loss = -11398.25258114547
Iteration 13700: Loss = -11398.27288969885
1
Iteration 13800: Loss = -11398.252786920391
2
Iteration 13900: Loss = -11398.252384166537
Iteration 14000: Loss = -11398.256668147216
1
Iteration 14100: Loss = -11398.252331212572
Iteration 14200: Loss = -11398.252816907587
1
Iteration 14300: Loss = -11398.2544408069
2
Iteration 14400: Loss = -11398.281023542366
3
Iteration 14500: Loss = -11398.252061234112
Iteration 14600: Loss = -11398.253261499085
1
Iteration 14700: Loss = -11398.496597680274
2
Iteration 14800: Loss = -11398.268681559923
3
Iteration 14900: Loss = -11398.307276585689
4
Iteration 15000: Loss = -11398.251906012505
Iteration 15100: Loss = -11398.259367723027
1
Iteration 15200: Loss = -11398.25190062966
Iteration 15300: Loss = -11398.25268816273
1
Iteration 15400: Loss = -11398.263603960051
2
Iteration 15500: Loss = -11398.252132517595
3
Iteration 15600: Loss = -11398.25232548285
4
Iteration 15700: Loss = -11398.258156702364
5
Iteration 15800: Loss = -11398.253665198596
6
Iteration 15900: Loss = -11398.256829893717
7
Iteration 16000: Loss = -11398.25586082757
8
Iteration 16100: Loss = -11398.252002556148
9
Iteration 16200: Loss = -11398.273552900684
10
Iteration 16300: Loss = -11398.254933946337
11
Iteration 16400: Loss = -11398.257844892145
12
Iteration 16500: Loss = -11398.261514524818
13
Iteration 16600: Loss = -11398.251957501347
Iteration 16700: Loss = -11398.264380017734
1
Iteration 16800: Loss = -11398.251902164871
Iteration 16900: Loss = -11398.26157979872
1
Iteration 17000: Loss = -11398.252169763375
2
Iteration 17100: Loss = -11398.252788607037
3
Iteration 17200: Loss = -11398.257713664876
4
Iteration 17300: Loss = -11398.270469708888
5
Iteration 17400: Loss = -11398.251892702561
Iteration 17500: Loss = -11398.257145008753
1
Iteration 17600: Loss = -11398.252951537133
2
Iteration 17700: Loss = -11398.252077479336
3
Iteration 17800: Loss = -11398.265289411176
4
Iteration 17900: Loss = -11398.251910078165
Iteration 18000: Loss = -11398.25188017689
Iteration 18100: Loss = -11398.253297559047
1
Iteration 18200: Loss = -11398.251864258944
Iteration 18300: Loss = -11398.296654594655
1
Iteration 18400: Loss = -11398.251870740552
Iteration 18500: Loss = -11398.252286201468
1
Iteration 18600: Loss = -11398.251951121865
Iteration 18700: Loss = -11398.43539492089
1
Iteration 18800: Loss = -11398.252514864746
2
Iteration 18900: Loss = -11398.432527904648
3
Iteration 19000: Loss = -11398.251896654167
Iteration 19100: Loss = -11398.272262735278
1
Iteration 19200: Loss = -11398.251921833762
Iteration 19300: Loss = -11398.252422250522
1
Iteration 19400: Loss = -11398.259122594001
2
Iteration 19500: Loss = -11398.255337180839
3
Iteration 19600: Loss = -11398.25191600453
Iteration 19700: Loss = -11398.251977582811
Iteration 19800: Loss = -11398.530296694484
1
Iteration 19900: Loss = -11398.25280147176
2
pi: tensor([[0.6489, 0.3511],
        [0.3345, 0.6655]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2017, 0.7983], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4016, 0.1011],
         [0.5884, 0.2175]],

        [[0.6433, 0.0884],
         [0.5408, 0.5319]],

        [[0.6684, 0.0929],
         [0.5280, 0.5746]],

        [[0.6399, 0.0899],
         [0.6064, 0.5930]],

        [[0.5233, 0.0915],
         [0.6626, 0.5172]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 65
Adjusted Rand Index: 0.08320034701004249
time is 1
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.5465897973872775
Average Adjusted Rand Index: 0.8166400694020085
11287.490840260527
[1.0, 0.5465897973872775] [1.0, 0.8166400694020085] [11280.064413891821, 11398.45379496974]
-------------------------------------
This iteration is 90
True Objective function: Loss = -11474.915665471859
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23015.29532097065
Iteration 100: Loss = -12084.205509422325
Iteration 200: Loss = -12075.124596939455
Iteration 300: Loss = -11700.545842244986
Iteration 400: Loss = -11523.667947561566
Iteration 500: Loss = -11496.932753215013
Iteration 600: Loss = -11470.35446209758
Iteration 700: Loss = -11452.98216389113
Iteration 800: Loss = -11452.687307023774
Iteration 900: Loss = -11452.557224658618
Iteration 1000: Loss = -11452.469732817268
Iteration 1100: Loss = -11452.406931545685
Iteration 1200: Loss = -11452.360144919754
Iteration 1300: Loss = -11452.308872553538
Iteration 1400: Loss = -11452.241738300274
Iteration 1500: Loss = -11452.219705977353
Iteration 1600: Loss = -11452.200637346014
Iteration 1700: Loss = -11452.183485867221
Iteration 1800: Loss = -11452.170689103443
Iteration 1900: Loss = -11452.160620141434
Iteration 2000: Loss = -11452.152153276304
Iteration 2100: Loss = -11452.144773444781
Iteration 2200: Loss = -11452.138388819625
Iteration 2300: Loss = -11452.132805130515
Iteration 2400: Loss = -11452.127896468228
Iteration 2500: Loss = -11452.124086037047
Iteration 2600: Loss = -11452.119598755417
Iteration 2700: Loss = -11452.115937483251
Iteration 2800: Loss = -11452.112592215437
Iteration 2900: Loss = -11452.11805367048
1
Iteration 3000: Loss = -11452.10590612107
Iteration 3100: Loss = -11452.103393294232
Iteration 3200: Loss = -11452.10116694222
Iteration 3300: Loss = -11452.099204691258
Iteration 3400: Loss = -11452.103232546506
1
Iteration 3500: Loss = -11452.095874622155
Iteration 3600: Loss = -11452.09456831622
Iteration 3700: Loss = -11452.093108707368
Iteration 3800: Loss = -11452.09187877306
Iteration 3900: Loss = -11452.093245270875
1
Iteration 4000: Loss = -11452.08974512376
Iteration 4100: Loss = -11452.090466828204
1
Iteration 4200: Loss = -11452.087929279483
Iteration 4300: Loss = -11452.092397535622
1
Iteration 4400: Loss = -11452.086368773093
Iteration 4500: Loss = -11452.100488366172
1
Iteration 4600: Loss = -11452.084993604089
Iteration 4700: Loss = -11452.087576357044
1
Iteration 4800: Loss = -11452.085193306213
2
Iteration 4900: Loss = -11452.083271822356
Iteration 5000: Loss = -11452.083984063795
1
Iteration 5100: Loss = -11452.082585333157
Iteration 5200: Loss = -11452.082256632399
Iteration 5300: Loss = -11452.08148817224
Iteration 5400: Loss = -11452.08334124924
1
Iteration 5500: Loss = -11452.080792938272
Iteration 5600: Loss = -11452.08199400217
1
Iteration 5700: Loss = -11452.080054899367
Iteration 5800: Loss = -11452.08066362633
1
Iteration 5900: Loss = -11452.079456690015
Iteration 6000: Loss = -11452.080477019981
1
Iteration 6100: Loss = -11452.078882236163
Iteration 6200: Loss = -11452.08198135191
1
Iteration 6300: Loss = -11452.078483371486
Iteration 6400: Loss = -11452.07917618553
1
Iteration 6500: Loss = -11452.087713337405
2
Iteration 6600: Loss = -11452.078168813705
Iteration 6700: Loss = -11452.077643064002
Iteration 6800: Loss = -11452.07747468282
Iteration 6900: Loss = -11452.090877887973
1
Iteration 7000: Loss = -11452.07716124455
Iteration 7100: Loss = -11452.079843356907
1
Iteration 7200: Loss = -11452.076940001038
Iteration 7300: Loss = -11452.07708147808
1
Iteration 7400: Loss = -11452.076999798157
Iteration 7500: Loss = -11452.076639947656
Iteration 7600: Loss = -11452.08066469086
1
Iteration 7700: Loss = -11452.076442772115
Iteration 7800: Loss = -11452.07814535582
1
Iteration 7900: Loss = -11452.07738400049
2
Iteration 8000: Loss = -11452.076262527324
Iteration 8100: Loss = -11452.076041448283
Iteration 8200: Loss = -11452.07599222394
Iteration 8300: Loss = -11452.208667228806
1
Iteration 8400: Loss = -11452.0829602276
2
Iteration 8500: Loss = -11452.075767246622
Iteration 8600: Loss = -11452.209170938037
1
Iteration 8700: Loss = -11452.07563033147
Iteration 8800: Loss = -11452.075573897022
Iteration 8900: Loss = -11452.075785399253
1
Iteration 9000: Loss = -11452.075408816467
Iteration 9100: Loss = -11452.08343956619
1
Iteration 9200: Loss = -11452.0753394823
Iteration 9300: Loss = -11452.119952768251
1
Iteration 9400: Loss = -11452.102497326703
2
Iteration 9500: Loss = -11452.088198198026
3
Iteration 9600: Loss = -11452.144138618121
4
Iteration 9700: Loss = -11452.075352612697
Iteration 9800: Loss = -11452.213696358185
1
Iteration 9900: Loss = -11452.094619988391
2
Iteration 10000: Loss = -11452.075951182574
3
Iteration 10100: Loss = -11452.08126902038
4
Iteration 10200: Loss = -11452.079588315946
5
Iteration 10300: Loss = -11452.108209070433
6
Iteration 10400: Loss = -11452.07622910262
7
Iteration 10500: Loss = -11452.080403262347
8
Iteration 10600: Loss = -11452.109521130073
9
Iteration 10700: Loss = -11452.07566617667
10
Iteration 10800: Loss = -11452.07490186598
Iteration 10900: Loss = -11452.075281379035
1
Iteration 11000: Loss = -11452.07513270072
2
Iteration 11100: Loss = -11452.07970469356
3
Iteration 11200: Loss = -11452.075678886378
4
Iteration 11300: Loss = -11452.08140473317
5
Iteration 11400: Loss = -11452.081887017544
6
Iteration 11500: Loss = -11452.074731542303
Iteration 11600: Loss = -11452.075885117096
1
Iteration 11700: Loss = -11452.084847103817
2
Iteration 11800: Loss = -11452.086318045629
3
Iteration 11900: Loss = -11452.080778816751
4
Iteration 12000: Loss = -11452.075430469102
5
Iteration 12100: Loss = -11452.075010311384
6
Iteration 12200: Loss = -11452.082884186811
7
Iteration 12300: Loss = -11452.076983088751
8
Iteration 12400: Loss = -11452.074750382353
Iteration 12500: Loss = -11452.078749424702
1
Iteration 12600: Loss = -11452.081014749005
2
Iteration 12700: Loss = -11452.092457403625
3
Iteration 12800: Loss = -11452.075049016481
4
Iteration 12900: Loss = -11452.075395180029
5
Iteration 13000: Loss = -11452.077254820428
6
Iteration 13100: Loss = -11452.08172353752
7
Iteration 13200: Loss = -11452.075966049391
8
Iteration 13300: Loss = -11452.074908075427
9
Iteration 13400: Loss = -11452.075502697115
10
Iteration 13500: Loss = -11452.109160740216
11
Iteration 13600: Loss = -11452.081045962523
12
Iteration 13700: Loss = -11452.106161019274
13
Iteration 13800: Loss = -11452.075679469519
14
Iteration 13900: Loss = -11452.074802586918
Iteration 14000: Loss = -11452.075557683786
1
Iteration 14100: Loss = -11452.131169150764
2
Iteration 14200: Loss = -11452.085242670742
3
Iteration 14300: Loss = -11452.074616926317
Iteration 14400: Loss = -11452.081145772923
1
Iteration 14500: Loss = -11452.0746345758
Iteration 14600: Loss = -11452.07741000478
1
Iteration 14700: Loss = -11452.078296336931
2
Iteration 14800: Loss = -11452.084969787946
3
Iteration 14900: Loss = -11452.084199790599
4
Iteration 15000: Loss = -11452.173750565174
5
Iteration 15100: Loss = -11452.083555044976
6
Iteration 15200: Loss = -11452.077912048811
7
Iteration 15300: Loss = -11452.075038785035
8
Iteration 15400: Loss = -11452.07626088963
9
Iteration 15500: Loss = -11452.07554796378
10
Iteration 15600: Loss = -11452.07477768293
11
Iteration 15700: Loss = -11452.075511304763
12
Iteration 15800: Loss = -11452.123136771463
13
Iteration 15900: Loss = -11452.074720246093
Iteration 16000: Loss = -11452.075837808348
1
Iteration 16100: Loss = -11452.107725193953
2
Iteration 16200: Loss = -11452.074866287416
3
Iteration 16300: Loss = -11452.078746513329
4
Iteration 16400: Loss = -11452.108618594282
5
Iteration 16500: Loss = -11452.153168932196
6
Iteration 16600: Loss = -11452.07936612194
7
Iteration 16700: Loss = -11452.075265780717
8
Iteration 16800: Loss = -11452.097729254156
9
Iteration 16900: Loss = -11452.075846968328
10
Iteration 17000: Loss = -11452.098297563281
11
Iteration 17100: Loss = -11452.07572724929
12
Iteration 17200: Loss = -11452.074805256985
Iteration 17300: Loss = -11452.077853850975
1
Iteration 17400: Loss = -11452.074786253075
Iteration 17500: Loss = -11452.076375527735
1
Iteration 17600: Loss = -11452.084640943074
2
Iteration 17700: Loss = -11452.07589960062
3
Iteration 17800: Loss = -11452.074703473174
Iteration 17900: Loss = -11452.074680361546
Iteration 18000: Loss = -11452.075449053811
1
Iteration 18100: Loss = -11452.075108691915
2
Iteration 18200: Loss = -11452.076019724533
3
Iteration 18300: Loss = -11452.080069864305
4
Iteration 18400: Loss = -11452.155604254425
5
Iteration 18500: Loss = -11452.1168458502
6
Iteration 18600: Loss = -11452.07652733548
7
Iteration 18700: Loss = -11452.074704175144
Iteration 18800: Loss = -11452.07515361676
1
Iteration 18900: Loss = -11452.076140444762
2
Iteration 19000: Loss = -11452.076301130164
3
Iteration 19100: Loss = -11452.113588816574
4
Iteration 19200: Loss = -11452.074775175985
Iteration 19300: Loss = -11452.084279172848
1
Iteration 19400: Loss = -11452.079414792066
2
Iteration 19500: Loss = -11452.074577501697
Iteration 19600: Loss = -11452.078886626525
1
Iteration 19700: Loss = -11452.07454352661
Iteration 19800: Loss = -11452.074700210029
1
Iteration 19900: Loss = -11452.074685121383
2
pi: tensor([[0.7054, 0.2946],
        [0.2341, 0.7659]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4991, 0.5009], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3924, 0.0939],
         [0.7018, 0.1994]],

        [[0.6699, 0.1019],
         [0.5074, 0.5043]],

        [[0.6561, 0.1010],
         [0.7186, 0.5652]],

        [[0.6131, 0.0974],
         [0.6713, 0.6355]],

        [[0.6298, 0.1179],
         [0.5767, 0.7117]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9764835585716855
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22775.558179652002
Iteration 100: Loss = -12083.867863349105
Iteration 200: Loss = -11855.15664373973
Iteration 300: Loss = -11582.743826113827
Iteration 400: Loss = -11452.753184071596
Iteration 500: Loss = -11452.448901483009
Iteration 600: Loss = -11452.332102651575
Iteration 700: Loss = -11452.267082971199
Iteration 800: Loss = -11452.22540412019
Iteration 900: Loss = -11452.196830961891
Iteration 1000: Loss = -11452.176149175823
Iteration 1100: Loss = -11452.16056565306
Iteration 1200: Loss = -11452.148418333669
Iteration 1300: Loss = -11452.138831354256
Iteration 1400: Loss = -11452.131099006407
Iteration 1500: Loss = -11452.124790817736
Iteration 1600: Loss = -11452.119561705178
Iteration 1700: Loss = -11452.115141430295
Iteration 1800: Loss = -11452.11139701963
Iteration 1900: Loss = -11452.10813812934
Iteration 2000: Loss = -11452.105328918482
Iteration 2100: Loss = -11452.102879535189
Iteration 2200: Loss = -11452.100712789188
Iteration 2300: Loss = -11452.09883485731
Iteration 2400: Loss = -11452.097182591404
Iteration 2500: Loss = -11452.09568645078
Iteration 2600: Loss = -11452.094335837897
Iteration 2700: Loss = -11452.093095618751
Iteration 2800: Loss = -11452.092035904941
Iteration 2900: Loss = -11452.092767357091
1
Iteration 3000: Loss = -11452.090162918408
Iteration 3100: Loss = -11452.089341273737
Iteration 3200: Loss = -11452.088618698086
Iteration 3300: Loss = -11452.087907988525
Iteration 3400: Loss = -11452.087307954025
Iteration 3500: Loss = -11452.086706830552
Iteration 3600: Loss = -11452.08619321347
Iteration 3700: Loss = -11452.085635663228
Iteration 3800: Loss = -11452.0870931325
1
Iteration 3900: Loss = -11452.084504080967
Iteration 4000: Loss = -11452.086844213834
1
Iteration 4100: Loss = -11452.083391669392
Iteration 4200: Loss = -11452.102392677825
1
Iteration 4300: Loss = -11452.082689319768
Iteration 4400: Loss = -11452.100346023179
1
Iteration 4500: Loss = -11452.08628734454
2
Iteration 4600: Loss = -11452.087391781655
3
Iteration 4700: Loss = -11452.081907461454
Iteration 4800: Loss = -11452.081654462934
Iteration 4900: Loss = -11452.081562394636
Iteration 5000: Loss = -11452.081182857239
Iteration 5100: Loss = -11452.08094703034
Iteration 5200: Loss = -11452.080823439006
Iteration 5300: Loss = -11452.081419529532
1
Iteration 5400: Loss = -11452.080642462692
Iteration 5500: Loss = -11452.080983746444
1
Iteration 5600: Loss = -11452.080327970814
Iteration 5700: Loss = -11452.079749453545
Iteration 5800: Loss = -11452.076802308782
Iteration 5900: Loss = -11452.076404416946
Iteration 6000: Loss = -11452.076272584089
Iteration 6100: Loss = -11452.077002135524
1
Iteration 6200: Loss = -11452.077293436558
2
Iteration 6300: Loss = -11452.079181279047
3
Iteration 6400: Loss = -11452.08079925321
4
Iteration 6500: Loss = -11452.0759667514
Iteration 6600: Loss = -11452.07579633787
Iteration 6700: Loss = -11452.075694417963
Iteration 6800: Loss = -11452.075658374504
Iteration 6900: Loss = -11452.07782366822
1
Iteration 7000: Loss = -11452.075544254925
Iteration 7100: Loss = -11452.07545021922
Iteration 7200: Loss = -11452.075684203199
1
Iteration 7300: Loss = -11452.075835549342
2
Iteration 7400: Loss = -11452.07683790486
3
Iteration 7500: Loss = -11452.078207567563
4
Iteration 7600: Loss = -11452.075339368543
Iteration 7700: Loss = -11452.075274895926
Iteration 7800: Loss = -11452.076612452354
1
Iteration 7900: Loss = -11452.075390584385
2
Iteration 8000: Loss = -11452.07856559749
3
Iteration 8100: Loss = -11452.075893413834
4
Iteration 8200: Loss = -11452.07593078487
5
Iteration 8300: Loss = -11452.077680223823
6
Iteration 8400: Loss = -11452.076032357278
7
Iteration 8500: Loss = -11452.079901391728
8
Iteration 8600: Loss = -11452.1234937546
9
Iteration 8700: Loss = -11452.07495509462
Iteration 8800: Loss = -11452.078757428866
1
Iteration 8900: Loss = -11452.099170485282
2
Iteration 9000: Loss = -11452.076102177149
3
Iteration 9100: Loss = -11452.07526837062
4
Iteration 9200: Loss = -11452.075209778095
5
Iteration 9300: Loss = -11452.080990169748
6
Iteration 9400: Loss = -11452.07932146264
7
Iteration 9500: Loss = -11452.07513727052
8
Iteration 9600: Loss = -11452.074934116124
Iteration 9700: Loss = -11452.075743096751
1
Iteration 9800: Loss = -11452.078274616488
2
Iteration 9900: Loss = -11452.07477751892
Iteration 10000: Loss = -11452.077248542002
1
Iteration 10100: Loss = -11452.109094594643
2
Iteration 10200: Loss = -11452.077233835505
3
Iteration 10300: Loss = -11452.07506493454
4
Iteration 10400: Loss = -11452.07655872699
5
Iteration 10500: Loss = -11452.11853516269
6
Iteration 10600: Loss = -11452.074734108022
Iteration 10700: Loss = -11452.075954999567
1
Iteration 10800: Loss = -11452.10114987151
2
Iteration 10900: Loss = -11452.079843189233
3
Iteration 11000: Loss = -11452.0779789174
4
Iteration 11100: Loss = -11452.100171604348
5
Iteration 11200: Loss = -11452.095396306018
6
Iteration 11300: Loss = -11452.096735061012
7
Iteration 11400: Loss = -11452.077974469576
8
Iteration 11500: Loss = -11452.080718453471
9
Iteration 11600: Loss = -11452.080864172398
10
Iteration 11700: Loss = -11452.07474724498
Iteration 11800: Loss = -11452.07532889736
1
Iteration 11900: Loss = -11452.076242750962
2
Iteration 12000: Loss = -11452.0817993897
3
Iteration 12100: Loss = -11452.074721112704
Iteration 12200: Loss = -11452.076799386183
1
Iteration 12300: Loss = -11452.075035211887
2
Iteration 12400: Loss = -11452.083133287992
3
Iteration 12500: Loss = -11452.079764501113
4
Iteration 12600: Loss = -11452.094641364021
5
Iteration 12700: Loss = -11452.079848738635
6
Iteration 12800: Loss = -11452.074717687494
Iteration 12900: Loss = -11452.0750645067
1
Iteration 13000: Loss = -11452.077511502983
2
Iteration 13100: Loss = -11452.079473105003
3
Iteration 13200: Loss = -11452.103204421717
4
Iteration 13300: Loss = -11452.074686788545
Iteration 13400: Loss = -11452.075960770637
1
Iteration 13500: Loss = -11452.074999377675
2
Iteration 13600: Loss = -11452.076179094329
3
Iteration 13700: Loss = -11452.074659098991
Iteration 13800: Loss = -11452.075569844095
1
Iteration 13900: Loss = -11452.085073369944
2
Iteration 14000: Loss = -11452.085758931216
3
Iteration 14100: Loss = -11452.115798477324
4
Iteration 14200: Loss = -11452.142034886723
5
Iteration 14300: Loss = -11452.07695177728
6
Iteration 14400: Loss = -11452.074766494845
7
Iteration 14500: Loss = -11452.09164190718
8
Iteration 14600: Loss = -11452.090201477855
9
Iteration 14700: Loss = -11452.188091910699
10
Iteration 14800: Loss = -11452.080265071698
11
Iteration 14900: Loss = -11452.077520246577
12
Iteration 15000: Loss = -11452.084720462939
13
Iteration 15100: Loss = -11452.083585081675
14
Iteration 15200: Loss = -11452.075397116994
15
Stopping early at iteration 15200 due to no improvement.
pi: tensor([[0.7659, 0.2341],
        [0.2946, 0.7054]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5014, 0.4986], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1993, 0.0941],
         [0.5202, 0.3925]],

        [[0.5964, 0.1019],
         [0.5917, 0.5407]],

        [[0.5781, 0.1010],
         [0.5698, 0.6811]],

        [[0.7273, 0.0974],
         [0.5277, 0.6959]],

        [[0.6940, 0.1180],
         [0.5790, 0.7162]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
Global Adjusted Rand Index: 0.9760953012611165
Average Adjusted Rand Index: 0.9764835585716855
11474.915665471859
[0.9760953012611165, 0.9760953012611165] [0.9764835585716855, 0.9764835585716855] [11452.075205148769, 11452.075397116994]
-------------------------------------
This iteration is 91
True Objective function: Loss = -11422.332986040668
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22076.66185741505
Iteration 100: Loss = -12154.815447540215
Iteration 200: Loss = -12009.457156280549
Iteration 300: Loss = -11681.670430413697
Iteration 400: Loss = -11561.886404979567
Iteration 500: Loss = -11559.780927444275
Iteration 600: Loss = -11543.741623160542
Iteration 700: Loss = -11537.324751627959
Iteration 800: Loss = -11537.120724161328
Iteration 900: Loss = -11537.006191031933
Iteration 1000: Loss = -11536.92291782001
Iteration 1100: Loss = -11536.842917444292
Iteration 1200: Loss = -11536.559214250898
Iteration 1300: Loss = -11536.490150646887
Iteration 1400: Loss = -11536.44147453919
Iteration 1500: Loss = -11528.975418870828
Iteration 1600: Loss = -11528.955673029453
Iteration 1700: Loss = -11528.940195058058
Iteration 1800: Loss = -11528.927258482978
Iteration 1900: Loss = -11528.916140170979
Iteration 2000: Loss = -11528.906206654323
Iteration 2100: Loss = -11528.897514882801
Iteration 2200: Loss = -11528.889703858087
Iteration 2300: Loss = -11528.882072382663
Iteration 2400: Loss = -11528.87306869957
Iteration 2500: Loss = -11528.857566186804
Iteration 2600: Loss = -11528.839199644146
Iteration 2700: Loss = -11528.83196612361
Iteration 2800: Loss = -11528.82670126528
Iteration 2900: Loss = -11528.811173976426
Iteration 3000: Loss = -11528.791184608192
Iteration 3100: Loss = -11528.768852293606
Iteration 3200: Loss = -11528.759867753186
Iteration 3300: Loss = -11528.750215593023
Iteration 3400: Loss = -11521.77516480325
Iteration 3500: Loss = -11521.553437619561
Iteration 3600: Loss = -11517.451030031818
Iteration 3700: Loss = -11507.348326325771
Iteration 3800: Loss = -11504.887841726068
Iteration 3900: Loss = -11493.499400786442
Iteration 4000: Loss = -11483.317071574638
Iteration 4100: Loss = -11456.759130373875
Iteration 4200: Loss = -11453.117935830443
Iteration 4300: Loss = -11444.656040570522
Iteration 4400: Loss = -11444.641031837575
Iteration 4500: Loss = -11444.551081459107
Iteration 4600: Loss = -11422.062923048938
Iteration 4700: Loss = -11422.056392752915
Iteration 4800: Loss = -11422.048229473692
Iteration 4900: Loss = -11415.068831155815
Iteration 5000: Loss = -11415.065114331708
Iteration 5100: Loss = -11415.072666315284
1
Iteration 5200: Loss = -11415.06155884545
Iteration 5300: Loss = -11415.060790544605
Iteration 5400: Loss = -11415.0596145112
Iteration 5500: Loss = -11415.058110625818
Iteration 5600: Loss = -11415.010824122493
Iteration 5700: Loss = -11415.000724372776
Iteration 5800: Loss = -11414.999893934026
Iteration 5900: Loss = -11414.998809690413
Iteration 6000: Loss = -11414.995382192372
Iteration 6100: Loss = -11414.985021919123
Iteration 6200: Loss = -11414.98456411365
Iteration 6300: Loss = -11414.983964336976
Iteration 6400: Loss = -11414.983423434966
Iteration 6500: Loss = -11414.982996812314
Iteration 6600: Loss = -11414.982554863422
Iteration 6700: Loss = -11414.982315170031
Iteration 6800: Loss = -11414.990145361504
1
Iteration 6900: Loss = -11414.980583954371
Iteration 7000: Loss = -11414.979355625193
Iteration 7100: Loss = -11414.977993498624
Iteration 7200: Loss = -11414.972987594978
Iteration 7300: Loss = -11415.011373001487
1
Iteration 7400: Loss = -11414.979129508361
2
Iteration 7500: Loss = -11414.971744834822
Iteration 7600: Loss = -11414.97190632827
1
Iteration 7700: Loss = -11414.987146326004
2
Iteration 7800: Loss = -11414.971664388639
Iteration 7900: Loss = -11414.971350034684
Iteration 8000: Loss = -11414.97114585424
Iteration 8100: Loss = -11414.975116590693
1
Iteration 8200: Loss = -11414.971027685177
Iteration 8300: Loss = -11414.974457429797
1
Iteration 8400: Loss = -11414.970808679778
Iteration 8500: Loss = -11414.999755450706
1
Iteration 8600: Loss = -11414.97069553937
Iteration 8700: Loss = -11415.045863571297
1
Iteration 8800: Loss = -11414.970511400617
Iteration 8900: Loss = -11414.97045712658
Iteration 9000: Loss = -11414.970343554423
Iteration 9100: Loss = -11414.970029842605
Iteration 9200: Loss = -11414.97160244297
1
Iteration 9300: Loss = -11414.969882639765
Iteration 9400: Loss = -11414.970308935508
1
Iteration 9500: Loss = -11415.029911195468
2
Iteration 9600: Loss = -11414.969842645585
Iteration 9700: Loss = -11414.977524220472
1
Iteration 9800: Loss = -11414.973814963465
2
Iteration 9900: Loss = -11415.049228641737
3
Iteration 10000: Loss = -11415.040127306293
4
Iteration 10100: Loss = -11414.99047167
5
Iteration 10200: Loss = -11414.98341346704
6
Iteration 10300: Loss = -11414.972704109026
7
Iteration 10400: Loss = -11414.96995099252
8
Iteration 10500: Loss = -11414.992741221398
9
Iteration 10600: Loss = -11414.972012681948
10
Iteration 10700: Loss = -11414.97846372793
11
Iteration 10800: Loss = -11414.969502865471
Iteration 10900: Loss = -11414.976102295104
1
Iteration 11000: Loss = -11415.003472263585
2
Iteration 11100: Loss = -11414.976479984309
3
Iteration 11200: Loss = -11414.992333869879
4
Iteration 11300: Loss = -11414.973964712914
5
Iteration 11400: Loss = -11414.975217719777
6
Iteration 11500: Loss = -11415.076963427495
7
Iteration 11600: Loss = -11414.972701432185
8
Iteration 11700: Loss = -11414.979727203412
9
Iteration 11800: Loss = -11414.968931660593
Iteration 11900: Loss = -11414.976737352697
1
Iteration 12000: Loss = -11414.980528256585
2
Iteration 12100: Loss = -11414.968919348892
Iteration 12200: Loss = -11414.96908795765
1
Iteration 12300: Loss = -11414.989254297814
2
Iteration 12400: Loss = -11414.97469743669
3
Iteration 12500: Loss = -11414.973325226618
4
Iteration 12600: Loss = -11415.01158747077
5
Iteration 12700: Loss = -11414.982796830614
6
Iteration 12800: Loss = -11414.974294796648
7
Iteration 12900: Loss = -11414.975293007888
8
Iteration 13000: Loss = -11414.968322876153
Iteration 13100: Loss = -11414.970089809936
1
Iteration 13200: Loss = -11414.972355671716
2
Iteration 13300: Loss = -11414.972516798814
3
Iteration 13400: Loss = -11414.993756365471
4
Iteration 13500: Loss = -11414.968421209236
Iteration 13600: Loss = -11414.970767245391
1
Iteration 13700: Loss = -11414.970713496648
2
Iteration 13800: Loss = -11414.985685009358
3
Iteration 13900: Loss = -11414.976521721592
4
Iteration 14000: Loss = -11414.968519016767
Iteration 14100: Loss = -11414.969645724505
1
Iteration 14200: Loss = -11414.98944069273
2
Iteration 14300: Loss = -11414.96814134363
Iteration 14400: Loss = -11414.9738480938
1
Iteration 14500: Loss = -11415.013120609867
2
Iteration 14600: Loss = -11414.96882835253
3
Iteration 14700: Loss = -11414.968952942374
4
Iteration 14800: Loss = -11414.984097734981
5
Iteration 14900: Loss = -11414.969694285564
6
Iteration 15000: Loss = -11414.979623752675
7
Iteration 15100: Loss = -11414.96813659925
Iteration 15200: Loss = -11414.971438779705
1
Iteration 15300: Loss = -11415.024151454198
2
Iteration 15400: Loss = -11414.980285435708
3
Iteration 15500: Loss = -11414.968154843857
Iteration 15600: Loss = -11414.975428106973
1
Iteration 15700: Loss = -11414.968115201553
Iteration 15800: Loss = -11414.968786950547
1
Iteration 15900: Loss = -11414.972799117748
2
Iteration 16000: Loss = -11414.972005154243
3
Iteration 16100: Loss = -11414.968963483565
4
Iteration 16200: Loss = -11414.970448136264
5
Iteration 16300: Loss = -11414.971156607966
6
Iteration 16400: Loss = -11415.084197526812
7
Iteration 16500: Loss = -11414.970497111459
8
Iteration 16600: Loss = -11414.979387282117
9
Iteration 16700: Loss = -11414.96828316642
10
Iteration 16800: Loss = -11414.968898190156
11
Iteration 16900: Loss = -11414.974968578628
12
Iteration 17000: Loss = -11415.007029682822
13
Iteration 17100: Loss = -11415.031356846406
14
Iteration 17200: Loss = -11415.058026423143
15
Stopping early at iteration 17200 due to no improvement.
pi: tensor([[0.7707, 0.2293],
        [0.2124, 0.7876]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6000, 0.4000], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.1023],
         [0.7238, 0.4025]],

        [[0.6780, 0.1128],
         [0.6954, 0.5271]],

        [[0.6494, 0.0960],
         [0.6636, 0.7240]],

        [[0.6519, 0.0987],
         [0.5743, 0.5252]],

        [[0.7120, 0.0928],
         [0.6693, 0.6671]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999034306759
Average Adjusted Rand Index: 0.9919996552039955
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21417.073304626178
Iteration 100: Loss = -12158.290622896955
Iteration 200: Loss = -12003.32507516542
Iteration 300: Loss = -11632.224461575204
Iteration 400: Loss = -11549.564512018886
Iteration 500: Loss = -11533.082456715996
Iteration 600: Loss = -11532.309384993727
Iteration 700: Loss = -11531.029415237059
Iteration 800: Loss = -11513.566784444749
Iteration 900: Loss = -11513.325334388028
Iteration 1000: Loss = -11513.193412029294
Iteration 1100: Loss = -11513.09934187539
Iteration 1200: Loss = -11513.029898941842
Iteration 1300: Loss = -11512.97671993576
Iteration 1400: Loss = -11512.934785904445
Iteration 1500: Loss = -11512.900855112892
Iteration 1600: Loss = -11512.873005268491
Iteration 1700: Loss = -11512.849784938728
Iteration 1800: Loss = -11512.830083343975
Iteration 1900: Loss = -11512.813200649156
Iteration 2000: Loss = -11512.798628823683
Iteration 2100: Loss = -11512.785926886641
Iteration 2200: Loss = -11512.774731889105
Iteration 2300: Loss = -11512.764695368802
Iteration 2400: Loss = -11512.755631163614
Iteration 2500: Loss = -11512.747267843293
Iteration 2600: Loss = -11512.738927817303
Iteration 2700: Loss = -11512.729217941838
Iteration 2800: Loss = -11512.712407696057
Iteration 2900: Loss = -11512.703019122693
Iteration 3000: Loss = -11512.695094151588
Iteration 3100: Loss = -11512.684183009533
Iteration 3200: Loss = -11512.65156788748
Iteration 3300: Loss = -11512.307331651424
Iteration 3400: Loss = -11511.948931473955
Iteration 3500: Loss = -11511.641757764583
Iteration 3600: Loss = -11511.413585464898
Iteration 3700: Loss = -11511.308168119976
Iteration 3800: Loss = -11511.274633338273
Iteration 3900: Loss = -11511.260346515031
Iteration 4000: Loss = -11511.231026333558
Iteration 4100: Loss = -11510.977186534717
Iteration 4200: Loss = -11510.430120171148
Iteration 4300: Loss = -11510.425278989891
Iteration 4400: Loss = -11510.422053264554
Iteration 4500: Loss = -11510.419421762526
Iteration 4600: Loss = -11510.422057973821
1
Iteration 4700: Loss = -11510.415119036537
Iteration 4800: Loss = -11510.414416897243
Iteration 4900: Loss = -11510.412809787826
Iteration 5000: Loss = -11510.414105526124
1
Iteration 5100: Loss = -11510.41080095484
Iteration 5200: Loss = -11510.423358693799
1
Iteration 5300: Loss = -11510.408823059697
Iteration 5400: Loss = -11510.407853227543
Iteration 5500: Loss = -11510.40329168383
Iteration 5600: Loss = -11508.21819930781
Iteration 5700: Loss = -11508.170925368
Iteration 5800: Loss = -11508.064272359461
Iteration 5900: Loss = -11508.062511468348
Iteration 6000: Loss = -11508.061730906547
Iteration 6100: Loss = -11508.06153385331
Iteration 6200: Loss = -11508.060132699255
Iteration 6300: Loss = -11508.059312309997
Iteration 6400: Loss = -11508.058253161185
Iteration 6500: Loss = -11508.053445377622
Iteration 6600: Loss = -11508.034738135777
Iteration 6700: Loss = -11508.032852401237
Iteration 6800: Loss = -11508.03191731457
Iteration 6900: Loss = -11508.033657319767
1
Iteration 7000: Loss = -11508.031115517992
Iteration 7100: Loss = -11508.0306129491
Iteration 7200: Loss = -11508.030231232135
Iteration 7300: Loss = -11508.030272762939
Iteration 7400: Loss = -11508.03200072519
1
Iteration 7500: Loss = -11508.029991962469
Iteration 7600: Loss = -11508.0292218055
Iteration 7700: Loss = -11508.029222089124
Iteration 7800: Loss = -11508.02956460924
1
Iteration 7900: Loss = -11508.029418121947
2
Iteration 8000: Loss = -11508.030903638555
3
Iteration 8100: Loss = -11508.03349448648
4
Iteration 8200: Loss = -11508.028631774629
Iteration 8300: Loss = -11508.02871199028
Iteration 8400: Loss = -11508.034191696106
1
Iteration 8500: Loss = -11508.063656446586
2
Iteration 8600: Loss = -11508.032307744126
3
Iteration 8700: Loss = -11508.027747327727
Iteration 8800: Loss = -11508.027592382501
Iteration 8900: Loss = -11508.02862773367
1
Iteration 9000: Loss = -11508.206643764079
2
Iteration 9100: Loss = -11508.027297239405
Iteration 9200: Loss = -11508.291663119195
1
Iteration 9300: Loss = -11508.03698473337
2
Iteration 9400: Loss = -11508.028740864847
3
Iteration 9500: Loss = -11508.027037387901
Iteration 9600: Loss = -11508.148482077977
1
Iteration 9700: Loss = -11508.026899402777
Iteration 9800: Loss = -11508.033568129045
1
Iteration 9900: Loss = -11508.026805799827
Iteration 10000: Loss = -11508.065305296732
1
Iteration 10100: Loss = -11508.026750646028
Iteration 10200: Loss = -11508.20008935954
1
Iteration 10300: Loss = -11508.026631764411
Iteration 10400: Loss = -11508.042859980527
1
Iteration 10500: Loss = -11508.027015589334
2
Iteration 10600: Loss = -11508.029113808814
3
Iteration 10700: Loss = -11508.106805280571
4
Iteration 10800: Loss = -11508.026302368478
Iteration 10900: Loss = -11508.058175492552
1
Iteration 11000: Loss = -11508.027491831055
2
Iteration 11100: Loss = -11508.04243773242
3
Iteration 11200: Loss = -11508.026303085626
Iteration 11300: Loss = -11508.061103418293
1
Iteration 11400: Loss = -11508.026454520781
2
Iteration 11500: Loss = -11508.034971159028
3
Iteration 11600: Loss = -11508.026881556352
4
Iteration 11700: Loss = -11508.027220717418
5
Iteration 11800: Loss = -11508.123848933463
6
Iteration 11900: Loss = -11508.026052796467
Iteration 12000: Loss = -11508.026623354466
1
Iteration 12100: Loss = -11508.100544342808
2
Iteration 12200: Loss = -11508.02601613519
Iteration 12300: Loss = -11508.034149010222
1
Iteration 12400: Loss = -11508.029756976008
2
Iteration 12500: Loss = -11508.026402209127
3
Iteration 12600: Loss = -11508.027332522994
4
Iteration 12700: Loss = -11508.03165041305
5
Iteration 12800: Loss = -11508.049234670456
6
Iteration 12900: Loss = -11508.03059056585
7
Iteration 13000: Loss = -11508.047657177078
8
Iteration 13100: Loss = -11508.025981338562
Iteration 13200: Loss = -11508.034976117087
1
Iteration 13300: Loss = -11508.03848915524
2
Iteration 13400: Loss = -11508.121415937412
3
Iteration 13500: Loss = -11508.030709931667
4
Iteration 13600: Loss = -11508.025945781448
Iteration 13700: Loss = -11508.029649153194
1
Iteration 13800: Loss = -11508.040477902297
2
Iteration 13900: Loss = -11508.12994828875
3
Iteration 14000: Loss = -11508.026144585756
4
Iteration 14100: Loss = -11508.02781040988
5
Iteration 14200: Loss = -11508.032971561346
6
Iteration 14300: Loss = -11508.029362642848
7
Iteration 14400: Loss = -11508.03067450303
8
Iteration 14500: Loss = -11508.027538234157
9
Iteration 14600: Loss = -11508.026876593514
10
Iteration 14700: Loss = -11508.026293314757
11
Iteration 14800: Loss = -11508.046280665585
12
Iteration 14900: Loss = -11508.02605560221
13
Iteration 15000: Loss = -11508.026966549753
14
Iteration 15100: Loss = -11508.030564447692
15
Stopping early at iteration 15100 due to no improvement.
pi: tensor([[0.6975, 0.3025],
        [0.2180, 0.7820]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9149, 0.0851], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.1004],
         [0.5565, 0.4094]],

        [[0.5490, 0.1129],
         [0.6503, 0.6536]],

        [[0.6097, 0.0968],
         [0.6469, 0.5041]],

        [[0.7228, 0.0991],
         [0.5012, 0.7031]],

        [[0.6639, 0.0935],
         [0.7102, 0.7260]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.03042433947157726
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.645662540325096
Average Adjusted Rand Index: 0.7859147873096799
11422.332986040668
[0.9919999034306759, 0.645662540325096] [0.9919996552039955, 0.7859147873096799] [11415.058026423143, 11508.030564447692]
-------------------------------------
This iteration is 92
True Objective function: Loss = -11393.105488025833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23758.687867975084
Iteration 100: Loss = -12149.666351676959
Iteration 200: Loss = -12097.274749155285
Iteration 300: Loss = -11695.202989718226
Iteration 400: Loss = -11551.248626589202
Iteration 500: Loss = -11546.523985922417
Iteration 600: Loss = -11545.791275494532
Iteration 700: Loss = -11545.436074634641
Iteration 800: Loss = -11545.353564177327
Iteration 900: Loss = -11545.308036137918
Iteration 1000: Loss = -11545.276509205723
Iteration 1100: Loss = -11545.253505804343
Iteration 1200: Loss = -11545.236031337432
Iteration 1300: Loss = -11545.222384274846
Iteration 1400: Loss = -11545.211382694497
Iteration 1500: Loss = -11545.202340058337
Iteration 1600: Loss = -11545.194760859094
Iteration 1700: Loss = -11545.18801281953
Iteration 1800: Loss = -11545.183625526553
Iteration 1900: Loss = -11545.170837620413
Iteration 2000: Loss = -11545.166334051451
Iteration 2100: Loss = -11545.162803828762
Iteration 2200: Loss = -11545.159636254435
Iteration 2300: Loss = -11545.15677768531
Iteration 2400: Loss = -11545.154301712071
Iteration 2500: Loss = -11545.152030112598
Iteration 2600: Loss = -11545.150038481193
Iteration 2700: Loss = -11545.14826056485
Iteration 2800: Loss = -11545.146755832531
Iteration 2900: Loss = -11545.145289755177
Iteration 3000: Loss = -11545.14397191508
Iteration 3100: Loss = -11545.142730169442
Iteration 3200: Loss = -11545.141634915588
Iteration 3300: Loss = -11545.140921927754
Iteration 3400: Loss = -11545.139581740072
Iteration 3500: Loss = -11545.139707665721
1
Iteration 3600: Loss = -11545.138090379112
Iteration 3700: Loss = -11545.137397352544
Iteration 3800: Loss = -11545.13692574736
Iteration 3900: Loss = -11545.136240780614
Iteration 4000: Loss = -11545.135729948433
Iteration 4100: Loss = -11545.137785734561
1
Iteration 4200: Loss = -11545.13475282543
Iteration 4300: Loss = -11545.134689735643
Iteration 4400: Loss = -11545.141510122441
1
Iteration 4500: Loss = -11545.13366794368
Iteration 4600: Loss = -11545.133436703894
Iteration 4700: Loss = -11545.133494769025
Iteration 4800: Loss = -11545.13380718866
1
Iteration 4900: Loss = -11545.132656981761
Iteration 5000: Loss = -11545.135280292207
1
Iteration 5100: Loss = -11545.132202525261
Iteration 5200: Loss = -11545.133439335224
1
Iteration 5300: Loss = -11545.131673992586
Iteration 5400: Loss = -11545.132166945252
1
Iteration 5500: Loss = -11545.136727631583
2
Iteration 5600: Loss = -11545.13163265147
Iteration 5700: Loss = -11545.135455369245
1
Iteration 5800: Loss = -11545.133745221181
2
Iteration 5900: Loss = -11545.130973492187
Iteration 6000: Loss = -11545.13364162374
1
Iteration 6100: Loss = -11545.131510574636
2
Iteration 6200: Loss = -11545.130973030848
Iteration 6300: Loss = -11545.130278520708
Iteration 6400: Loss = -11545.13039785673
1
Iteration 6500: Loss = -11545.130128281473
Iteration 6600: Loss = -11545.131449426402
1
Iteration 6700: Loss = -11545.130388795875
2
Iteration 6800: Loss = -11545.129922961321
Iteration 6900: Loss = -11545.129802318392
Iteration 7000: Loss = -11545.129879670258
Iteration 7100: Loss = -11545.130580561761
1
Iteration 7200: Loss = -11545.12977710362
Iteration 7300: Loss = -11545.129464141211
Iteration 7400: Loss = -11545.129415695548
Iteration 7500: Loss = -11545.13011466768
1
Iteration 7600: Loss = -11545.132101323043
2
Iteration 7700: Loss = -11545.129432655427
Iteration 7800: Loss = -11545.130957418265
1
Iteration 7900: Loss = -11545.132050977583
2
Iteration 8000: Loss = -11545.170233653693
3
Iteration 8100: Loss = -11545.129123684872
Iteration 8200: Loss = -11545.129103593148
Iteration 8300: Loss = -11545.206801153587
1
Iteration 8400: Loss = -11545.129011897896
Iteration 8500: Loss = -11545.137858988126
1
Iteration 8600: Loss = -11545.12892556434
Iteration 8700: Loss = -11545.12895002582
Iteration 8800: Loss = -11545.179957119533
1
Iteration 8900: Loss = -11545.128883564063
Iteration 9000: Loss = -11545.128826539076
Iteration 9100: Loss = -11545.128977494458
1
Iteration 9200: Loss = -11545.12880896341
Iteration 9300: Loss = -11545.128803849724
Iteration 9400: Loss = -11545.128792783198
Iteration 9500: Loss = -11545.12884761737
Iteration 9600: Loss = -11545.128759313342
Iteration 9700: Loss = -11545.128720891644
Iteration 9800: Loss = -11545.129513852511
1
Iteration 9900: Loss = -11545.128760363714
Iteration 10000: Loss = -11545.143901316416
1
Iteration 10100: Loss = -11545.152058844602
2
Iteration 10200: Loss = -11545.129112684737
3
Iteration 10300: Loss = -11545.133508867499
4
Iteration 10400: Loss = -11545.128205162862
Iteration 10500: Loss = -11545.128786289652
1
Iteration 10600: Loss = -11545.146866310562
2
Iteration 10700: Loss = -11545.128549103863
3
Iteration 10800: Loss = -11545.129268910958
4
Iteration 10900: Loss = -11545.243055674795
5
Iteration 11000: Loss = -11545.138712726246
6
Iteration 11100: Loss = -11545.128273661212
Iteration 11200: Loss = -11545.136098697407
1
Iteration 11300: Loss = -11545.137080807095
2
Iteration 11400: Loss = -11545.167960703313
3
Iteration 11500: Loss = -11545.128200989964
Iteration 11600: Loss = -11545.129728904945
1
Iteration 11700: Loss = -11545.150853345756
2
Iteration 11800: Loss = -11545.128063904433
Iteration 11900: Loss = -11545.128882499757
1
Iteration 12000: Loss = -11545.132697007446
2
Iteration 12100: Loss = -11545.141331520934
3
Iteration 12200: Loss = -11545.12962215043
4
Iteration 12300: Loss = -11545.129348078099
5
Iteration 12400: Loss = -11545.18317017039
6
Iteration 12500: Loss = -11545.136519372807
7
Iteration 12600: Loss = -11545.130884209131
8
Iteration 12700: Loss = -11545.150912363115
9
Iteration 12800: Loss = -11545.135954105643
10
Iteration 12900: Loss = -11545.129736503082
11
Iteration 13000: Loss = -11545.127989250863
Iteration 13100: Loss = -11545.130131144815
1
Iteration 13200: Loss = -11545.17378174309
2
Iteration 13300: Loss = -11545.128186913968
3
Iteration 13400: Loss = -11545.128954941647
4
Iteration 13500: Loss = -11545.129488149227
5
Iteration 13600: Loss = -11545.12997523733
6
Iteration 13700: Loss = -11545.128658990914
7
Iteration 13800: Loss = -11545.131271032964
8
Iteration 13900: Loss = -11545.12888832226
9
Iteration 14000: Loss = -11545.133828897291
10
Iteration 14100: Loss = -11545.128685396727
11
Iteration 14200: Loss = -11545.129198804472
12
Iteration 14300: Loss = -11545.129009061893
13
Iteration 14400: Loss = -11545.132209259495
14
Iteration 14500: Loss = -11545.13127021058
15
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[0.7696, 0.2304],
        [0.3833, 0.6167]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5200, 0.4800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2147, 0.1043],
         [0.5559, 0.3911]],

        [[0.7094, 0.0913],
         [0.5860, 0.6341]],

        [[0.6618, 0.1001],
         [0.6511, 0.5906]],

        [[0.5878, 0.0961],
         [0.5527, 0.6255]],

        [[0.5525, 0.0998],
         [0.7293, 0.6205]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 30
Adjusted Rand Index: 0.1537171964665678
Global Adjusted Rand Index: 0.5059600990430038
Average Adjusted Rand Index: 0.8149040625243016
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21931.22116125897
Iteration 100: Loss = -12140.801058270157
Iteration 200: Loss = -11457.325707014725
Iteration 300: Loss = -11388.784699157988
Iteration 400: Loss = -11387.585354321056
Iteration 500: Loss = -11387.09321110832
Iteration 600: Loss = -11386.75172517621
Iteration 700: Loss = -11386.263476834132
Iteration 800: Loss = -11386.152424149313
Iteration 900: Loss = -11386.077915520073
Iteration 1000: Loss = -11386.023681338578
Iteration 1100: Loss = -11385.9826376548
Iteration 1200: Loss = -11385.950637374908
Iteration 1300: Loss = -11385.925503107783
Iteration 1400: Loss = -11385.904539409803
Iteration 1500: Loss = -11385.887507508729
Iteration 1600: Loss = -11385.875260546583
Iteration 1700: Loss = -11385.861313313382
Iteration 1800: Loss = -11385.851065585566
Iteration 1900: Loss = -11385.842260466392
Iteration 2000: Loss = -11385.834578159267
Iteration 2100: Loss = -11385.827912416882
Iteration 2200: Loss = -11385.822053874737
Iteration 2300: Loss = -11385.816831771192
Iteration 2400: Loss = -11385.812291933871
Iteration 2500: Loss = -11385.808189663087
Iteration 2600: Loss = -11385.804557651316
Iteration 2700: Loss = -11385.801707094713
Iteration 2800: Loss = -11385.798348602559
Iteration 2900: Loss = -11385.79571046224
Iteration 3000: Loss = -11385.793336974251
Iteration 3100: Loss = -11385.79265812814
Iteration 3200: Loss = -11385.78914229396
Iteration 3300: Loss = -11385.787352961212
Iteration 3400: Loss = -11385.812448954139
1
Iteration 3500: Loss = -11385.784171128395
Iteration 3600: Loss = -11385.78275213523
Iteration 3700: Loss = -11385.798294500624
1
Iteration 3800: Loss = -11385.780296858753
Iteration 3900: Loss = -11385.779186212016
Iteration 4000: Loss = -11385.778172981562
Iteration 4100: Loss = -11385.777287599949
Iteration 4200: Loss = -11385.776371757294
Iteration 4300: Loss = -11385.7756044966
Iteration 4400: Loss = -11385.781175790042
1
Iteration 4500: Loss = -11385.774134858115
Iteration 4600: Loss = -11385.773471076593
Iteration 4700: Loss = -11385.777700696899
1
Iteration 4800: Loss = -11385.772344876525
Iteration 4900: Loss = -11385.777165515588
1
Iteration 5000: Loss = -11385.771330157517
Iteration 5100: Loss = -11385.771162852821
Iteration 5200: Loss = -11385.770478012342
Iteration 5300: Loss = -11385.775747006612
1
Iteration 5400: Loss = -11385.76966424452
Iteration 5500: Loss = -11385.770503462745
1
Iteration 5600: Loss = -11385.770001529168
2
Iteration 5700: Loss = -11385.769958717183
3
Iteration 5800: Loss = -11385.76743513029
Iteration 5900: Loss = -11385.767352165723
Iteration 6000: Loss = -11385.766332743808
Iteration 6100: Loss = -11385.766250937782
Iteration 6200: Loss = -11385.76589952518
Iteration 6300: Loss = -11385.766386133067
1
Iteration 6400: Loss = -11385.765445337567
Iteration 6500: Loss = -11385.770743389732
1
Iteration 6600: Loss = -11385.766647528731
2
Iteration 6700: Loss = -11385.764935078949
Iteration 6800: Loss = -11385.764764011456
Iteration 6900: Loss = -11385.76462186539
Iteration 7000: Loss = -11385.764468870198
Iteration 7100: Loss = -11385.768500907332
1
Iteration 7200: Loss = -11385.764201079366
Iteration 7300: Loss = -11385.76413298701
Iteration 7400: Loss = -11385.764477888995
1
Iteration 7500: Loss = -11385.763894521366
Iteration 7600: Loss = -11385.764032072055
1
Iteration 7700: Loss = -11385.770061326348
2
Iteration 7800: Loss = -11385.764160768764
3
Iteration 7900: Loss = -11385.763584958926
Iteration 8000: Loss = -11385.780018414676
1
Iteration 8100: Loss = -11385.763453238367
Iteration 8200: Loss = -11385.76331867714
Iteration 8300: Loss = -11385.765149246812
1
Iteration 8400: Loss = -11385.763209405564
Iteration 8500: Loss = -11385.766818402326
1
Iteration 8600: Loss = -11385.763059509622
Iteration 8700: Loss = -11385.763401761487
1
Iteration 8800: Loss = -11385.762981301332
Iteration 8900: Loss = -11385.764251305865
1
Iteration 9000: Loss = -11385.762815243563
Iteration 9100: Loss = -11385.777396773223
1
Iteration 9200: Loss = -11385.764198839712
2
Iteration 9300: Loss = -11385.77456333654
3
Iteration 9400: Loss = -11385.765433052879
4
Iteration 9500: Loss = -11385.763782425307
5
Iteration 9600: Loss = -11385.848670661078
6
Iteration 9700: Loss = -11385.762676285143
Iteration 9800: Loss = -11385.778734373796
1
Iteration 9900: Loss = -11385.765128672025
2
Iteration 10000: Loss = -11385.773410374022
3
Iteration 10100: Loss = -11385.762567115575
Iteration 10200: Loss = -11385.763987309907
1
Iteration 10300: Loss = -11385.767725001899
2
Iteration 10400: Loss = -11385.764848748524
3
Iteration 10500: Loss = -11385.766050153483
4
Iteration 10600: Loss = -11385.762626210631
Iteration 10700: Loss = -11385.775131456223
1
Iteration 10800: Loss = -11385.768081566112
2
Iteration 10900: Loss = -11385.765594686549
3
Iteration 11000: Loss = -11385.828280473786
4
Iteration 11100: Loss = -11385.764712560673
5
Iteration 11200: Loss = -11385.764307391497
6
Iteration 11300: Loss = -11385.804797135763
7
Iteration 11400: Loss = -11385.779095112222
8
Iteration 11500: Loss = -11385.775787266142
9
Iteration 11600: Loss = -11385.776363138813
10
Iteration 11700: Loss = -11385.764426470216
11
Iteration 11800: Loss = -11385.773494598048
12
Iteration 11900: Loss = -11385.78691284149
13
Iteration 12000: Loss = -11385.791758175767
14
Iteration 12100: Loss = -11385.785969730689
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.7728, 0.2272],
        [0.2108, 0.7892]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4735, 0.5265], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3976, 0.1054],
         [0.5701, 0.2005]],

        [[0.6271, 0.0910],
         [0.6475, 0.5868]],

        [[0.6561, 0.1000],
         [0.5318, 0.5285]],

        [[0.5032, 0.0961],
         [0.5251, 0.5738]],

        [[0.6080, 0.0984],
         [0.6032, 0.6616]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320660777474
Average Adjusted Rand Index: 0.9841606232309879
11393.105488025833
[0.5059600990430038, 0.9840320660777474] [0.8149040625243016, 0.9841606232309879] [11545.13127021058, 11385.785969730689]
-------------------------------------
This iteration is 93
True Objective function: Loss = -11674.000811123846
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21896.58744884756
Iteration 100: Loss = -12568.113931101165
Iteration 200: Loss = -12199.356245781466
Iteration 300: Loss = -11692.038660448674
Iteration 400: Loss = -11671.923022099232
Iteration 500: Loss = -11670.500868920826
Iteration 600: Loss = -11670.282439059143
Iteration 700: Loss = -11670.15780250858
Iteration 800: Loss = -11670.077468601705
Iteration 900: Loss = -11670.021596077495
Iteration 1000: Loss = -11669.980405979482
Iteration 1100: Loss = -11669.948358853391
Iteration 1200: Loss = -11669.922611687443
Iteration 1300: Loss = -11669.902696089308
Iteration 1400: Loss = -11669.886943033054
Iteration 1500: Loss = -11669.874077018721
Iteration 1600: Loss = -11669.86344616523
Iteration 1700: Loss = -11669.854439982682
Iteration 1800: Loss = -11669.846858131641
Iteration 1900: Loss = -11669.840271059988
Iteration 2000: Loss = -11669.834524239252
Iteration 2100: Loss = -11669.82950677063
Iteration 2200: Loss = -11669.825060446385
Iteration 2300: Loss = -11669.821124299388
Iteration 2400: Loss = -11669.817533254638
Iteration 2500: Loss = -11669.814462462471
Iteration 2600: Loss = -11669.81144103569
Iteration 2700: Loss = -11669.808821549608
Iteration 2800: Loss = -11669.813841741934
1
Iteration 2900: Loss = -11669.804311004675
Iteration 3000: Loss = -11669.802339472624
Iteration 3100: Loss = -11669.80055798303
Iteration 3200: Loss = -11669.798905363736
Iteration 3300: Loss = -11669.797439560765
Iteration 3400: Loss = -11669.796104058752
Iteration 3500: Loss = -11669.796496598457
1
Iteration 3600: Loss = -11669.793869862073
Iteration 3700: Loss = -11669.79287327846
Iteration 3800: Loss = -11669.81279558464
1
Iteration 3900: Loss = -11669.791143265744
Iteration 4000: Loss = -11669.796929455377
1
Iteration 4100: Loss = -11669.789740992781
Iteration 4200: Loss = -11669.789039776288
Iteration 4300: Loss = -11669.788648184805
Iteration 4400: Loss = -11669.787932830748
Iteration 4500: Loss = -11669.787366090266
Iteration 4600: Loss = -11669.786957036713
Iteration 4700: Loss = -11669.786461902111
Iteration 4800: Loss = -11669.786045649902
Iteration 4900: Loss = -11669.785662739187
Iteration 5000: Loss = -11669.785276690449
Iteration 5100: Loss = -11669.784955321702
Iteration 5200: Loss = -11669.78510965906
1
Iteration 5300: Loss = -11669.784302099099
Iteration 5400: Loss = -11669.784030420324
Iteration 5500: Loss = -11669.787841981462
1
Iteration 5600: Loss = -11669.78351411361
Iteration 5700: Loss = -11669.78328194625
Iteration 5800: Loss = -11669.7948436042
1
Iteration 5900: Loss = -11669.782847424645
Iteration 6000: Loss = -11669.78268347499
Iteration 6100: Loss = -11669.782469553384
Iteration 6200: Loss = -11669.782367342525
Iteration 6300: Loss = -11669.782134073188
Iteration 6400: Loss = -11669.781991854019
Iteration 6500: Loss = -11669.782182870478
1
Iteration 6600: Loss = -11669.781730599587
Iteration 6700: Loss = -11669.781649901284
Iteration 6800: Loss = -11669.781510807563
Iteration 6900: Loss = -11669.78143009187
Iteration 7000: Loss = -11669.781293367007
Iteration 7100: Loss = -11669.943188410327
1
Iteration 7200: Loss = -11669.78121990393
Iteration 7300: Loss = -11669.784223105496
1
Iteration 7400: Loss = -11669.858911410887
2
Iteration 7500: Loss = -11669.780823858833
Iteration 7600: Loss = -11669.78139893926
1
Iteration 7700: Loss = -11669.780691912296
Iteration 7800: Loss = -11669.783688948117
1
Iteration 7900: Loss = -11669.79451848928
2
Iteration 8000: Loss = -11669.78054286975
Iteration 8100: Loss = -11669.781307426898
1
Iteration 8200: Loss = -11669.781201752116
2
Iteration 8300: Loss = -11669.780489748788
Iteration 8400: Loss = -11669.780421885724
Iteration 8500: Loss = -11669.796723488276
1
Iteration 8600: Loss = -11669.78357980699
2
Iteration 8700: Loss = -11669.782405503374
3
Iteration 8800: Loss = -11669.780781698544
4
Iteration 8900: Loss = -11669.785865245163
5
Iteration 9000: Loss = -11669.780088497117
Iteration 9100: Loss = -11669.780457798011
1
Iteration 9200: Loss = -11669.790010894512
2
Iteration 9300: Loss = -11669.792951732838
3
Iteration 9400: Loss = -11669.781056441892
4
Iteration 9500: Loss = -11669.782790317395
5
Iteration 9600: Loss = -11669.78902334134
6
Iteration 9700: Loss = -11669.782539107757
7
Iteration 9800: Loss = -11669.790587002444
8
Iteration 9900: Loss = -11669.780848286153
9
Iteration 10000: Loss = -11669.780164567499
Iteration 10100: Loss = -11669.780081801127
Iteration 10200: Loss = -11669.782323614
1
Iteration 10300: Loss = -11669.78885096326
2
Iteration 10400: Loss = -11669.833222856261
3
Iteration 10500: Loss = -11669.799487298129
4
Iteration 10600: Loss = -11669.784072273489
5
Iteration 10700: Loss = -11669.80702856442
6
Iteration 10800: Loss = -11669.788222028948
7
Iteration 10900: Loss = -11669.779715018656
Iteration 11000: Loss = -11669.780158209076
1
Iteration 11100: Loss = -11669.8573514406
2
Iteration 11200: Loss = -11669.781688723986
3
Iteration 11300: Loss = -11669.782794838908
4
Iteration 11400: Loss = -11669.781803354876
5
Iteration 11500: Loss = -11669.780823619969
6
Iteration 11600: Loss = -11669.78378070132
7
Iteration 11700: Loss = -11669.77963819383
Iteration 11800: Loss = -11669.78027569857
1
Iteration 11900: Loss = -11669.794788929052
2
Iteration 12000: Loss = -11669.791974769743
3
Iteration 12100: Loss = -11669.781343513305
4
Iteration 12200: Loss = -11669.784449976492
5
Iteration 12300: Loss = -11669.793660000496
6
Iteration 12400: Loss = -11669.819124203506
7
Iteration 12500: Loss = -11669.781221055726
8
Iteration 12600: Loss = -11669.788040129828
9
Iteration 12700: Loss = -11669.856596085174
10
Iteration 12800: Loss = -11669.857776204557
11
Iteration 12900: Loss = -11669.834588103366
12
Iteration 13000: Loss = -11669.81250459339
13
Iteration 13100: Loss = -11669.784866759195
14
Iteration 13200: Loss = -11669.78027899518
15
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[0.7502, 0.2498],
        [0.2310, 0.7690]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4991, 0.5009], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2040, 0.1060],
         [0.6584, 0.4081]],

        [[0.6224, 0.1025],
         [0.5410, 0.7183]],

        [[0.5763, 0.0974],
         [0.5094, 0.6261]],

        [[0.5699, 0.1008],
         [0.6185, 0.7045]],

        [[0.6425, 0.0943],
         [0.5404, 0.6402]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999904673683
Average Adjusted Rand Index: 0.9919995611635631
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22690.975115028952
Iteration 100: Loss = -12573.310946874053
Iteration 200: Loss = -12560.695334244107
Iteration 300: Loss = -12134.333399919607
Iteration 400: Loss = -11756.616149706218
Iteration 500: Loss = -11696.826394040658
Iteration 600: Loss = -11672.75797650663
Iteration 700: Loss = -11672.140972466213
Iteration 800: Loss = -11671.79145409129
Iteration 900: Loss = -11671.568980412228
Iteration 1000: Loss = -11671.4144254685
Iteration 1100: Loss = -11671.253017120298
Iteration 1200: Loss = -11670.223854357973
Iteration 1300: Loss = -11670.158264451524
Iteration 1400: Loss = -11670.107034088243
Iteration 1500: Loss = -11670.06575676242
Iteration 1600: Loss = -11670.031784385434
Iteration 1700: Loss = -11670.00344547964
Iteration 1800: Loss = -11669.979564450292
Iteration 1900: Loss = -11669.959179490113
Iteration 2000: Loss = -11669.941671512064
Iteration 2100: Loss = -11669.926486444607
Iteration 2200: Loss = -11669.91316463304
Iteration 2300: Loss = -11669.901511343094
Iteration 2400: Loss = -11669.891201546816
Iteration 2500: Loss = -11669.882065563852
Iteration 2600: Loss = -11669.874282720504
Iteration 2700: Loss = -11669.866591255039
Iteration 2800: Loss = -11669.859986086401
Iteration 2900: Loss = -11669.854043103645
Iteration 3000: Loss = -11669.84866163904
Iteration 3100: Loss = -11669.84377532175
Iteration 3200: Loss = -11669.839272000494
Iteration 3300: Loss = -11669.842458554887
1
Iteration 3400: Loss = -11669.831531350912
Iteration 3500: Loss = -11669.828137303128
Iteration 3600: Loss = -11669.827939751916
Iteration 3700: Loss = -11669.82217894125
Iteration 3800: Loss = -11669.82491969294
1
Iteration 3900: Loss = -11669.81759340983
Iteration 4000: Loss = -11669.814853915435
Iteration 4100: Loss = -11669.82062933361
1
Iteration 4200: Loss = -11669.810816712225
Iteration 4300: Loss = -11669.823601518887
1
Iteration 4400: Loss = -11669.807312547846
Iteration 4500: Loss = -11669.805702122554
Iteration 4600: Loss = -11669.804986840572
Iteration 4700: Loss = -11669.802660919046
Iteration 4800: Loss = -11669.816503980492
1
Iteration 4900: Loss = -11669.799847913044
Iteration 5000: Loss = -11669.798558168979
Iteration 5100: Loss = -11669.806712380934
1
Iteration 5200: Loss = -11669.796327465729
Iteration 5300: Loss = -11669.798727772579
1
Iteration 5400: Loss = -11669.794433804545
Iteration 5500: Loss = -11669.793505921827
Iteration 5600: Loss = -11669.792661324534
Iteration 5700: Loss = -11669.791748144471
Iteration 5800: Loss = -11669.795516810107
1
Iteration 5900: Loss = -11669.790070950738
Iteration 6000: Loss = -11669.789310598122
Iteration 6100: Loss = -11669.789699346731
1
Iteration 6200: Loss = -11669.788133986955
Iteration 6300: Loss = -11669.78764779984
Iteration 6400: Loss = -11669.787208399592
Iteration 6500: Loss = -11669.786718088168
Iteration 6600: Loss = -11669.787391548613
1
Iteration 6700: Loss = -11669.785916566041
Iteration 6800: Loss = -11669.785589955482
Iteration 6900: Loss = -11669.789014942282
1
Iteration 7000: Loss = -11669.784910867847
Iteration 7100: Loss = -11669.784634146617
Iteration 7200: Loss = -11669.810817616593
1
Iteration 7300: Loss = -11669.798785127763
2
Iteration 7400: Loss = -11669.783855917118
Iteration 7500: Loss = -11669.785314972978
1
Iteration 7600: Loss = -11669.78347255855
Iteration 7700: Loss = -11669.783422279228
Iteration 7800: Loss = -11669.785789510945
1
Iteration 7900: Loss = -11669.78283321994
Iteration 8000: Loss = -11669.787524871115
1
Iteration 8100: Loss = -11669.782531261933
Iteration 8200: Loss = -11669.782756498473
1
Iteration 8300: Loss = -11669.862474822583
2
Iteration 8400: Loss = -11669.782025375476
Iteration 8500: Loss = -11669.782051036638
Iteration 8600: Loss = -11669.87565174495
1
Iteration 8700: Loss = -11669.782232845104
2
Iteration 8800: Loss = -11669.781610823768
Iteration 8900: Loss = -11669.787753508934
1
Iteration 9000: Loss = -11669.78551059493
2
Iteration 9100: Loss = -11669.790356052159
3
Iteration 9200: Loss = -11669.781772110378
4
Iteration 9300: Loss = -11669.782266288857
5
Iteration 9400: Loss = -11669.78774528898
6
Iteration 9500: Loss = -11669.78729026585
7
Iteration 9600: Loss = -11669.781960463048
8
Iteration 9700: Loss = -11669.781992800805
9
Iteration 9800: Loss = -11669.821813684084
10
Iteration 9900: Loss = -11669.782897671754
11
Iteration 10000: Loss = -11669.782187221606
12
Iteration 10100: Loss = -11669.783771259097
13
Iteration 10200: Loss = -11669.78275712542
14
Iteration 10300: Loss = -11669.807231495142
15
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[0.7525, 0.2475],
        [0.2329, 0.7671]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5023, 0.4977], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2033, 0.1063],
         [0.7032, 0.4080]],

        [[0.6529, 0.1027],
         [0.5352, 0.6283]],

        [[0.7206, 0.0977],
         [0.7120, 0.6287]],

        [[0.7197, 0.1009],
         [0.6369, 0.6670]],

        [[0.5685, 0.0944],
         [0.6179, 0.6546]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999904673683
Average Adjusted Rand Index: 0.9919995611635631
11674.000811123846
[0.9919999904673683, 0.9919999904673683] [0.9919995611635631, 0.9919995611635631] [11669.78027899518, 11669.807231495142]
-------------------------------------
This iteration is 94
True Objective function: Loss = -11430.153031730853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22342.131984838787
Iteration 100: Loss = -12179.043209794068
Iteration 200: Loss = -12156.532797194248
Iteration 300: Loss = -11845.941838945773
Iteration 400: Loss = -11598.750630195233
Iteration 500: Loss = -11564.631693685827
Iteration 600: Loss = -11563.309495267233
Iteration 700: Loss = -11549.18000688475
Iteration 800: Loss = -11483.463963780807
Iteration 900: Loss = -11446.27104296591
Iteration 1000: Loss = -11445.660843295218
Iteration 1100: Loss = -11440.794778453843
Iteration 1200: Loss = -11423.386107991875
Iteration 1300: Loss = -11423.293576307624
Iteration 1400: Loss = -11423.233544775303
Iteration 1500: Loss = -11423.183160624769
Iteration 1600: Loss = -11422.734803751742
Iteration 1700: Loss = -11418.6638886619
Iteration 1800: Loss = -11418.640669649012
Iteration 1900: Loss = -11418.621985454276
Iteration 2000: Loss = -11418.606272994277
Iteration 2100: Loss = -11418.592838520002
Iteration 2200: Loss = -11418.581459409948
Iteration 2300: Loss = -11418.571993652731
Iteration 2400: Loss = -11418.564132985606
Iteration 2500: Loss = -11418.557335165302
Iteration 2600: Loss = -11418.551389153341
Iteration 2700: Loss = -11418.545997957786
Iteration 2800: Loss = -11418.541281338948
Iteration 2900: Loss = -11418.538101521288
Iteration 3000: Loss = -11418.534018337497
Iteration 3100: Loss = -11418.52989956268
Iteration 3200: Loss = -11418.526853531319
Iteration 3300: Loss = -11418.528194606595
1
Iteration 3400: Loss = -11418.52149346826
Iteration 3500: Loss = -11418.51916789682
Iteration 3600: Loss = -11418.517293589603
Iteration 3700: Loss = -11418.515138905377
Iteration 3800: Loss = -11418.513314313608
Iteration 3900: Loss = -11418.512328694658
Iteration 4000: Loss = -11418.510208649888
Iteration 4100: Loss = -11418.510090699892
Iteration 4200: Loss = -11418.507848706957
Iteration 4300: Loss = -11418.506340728018
Iteration 4400: Loss = -11418.50523662575
Iteration 4500: Loss = -11418.505052014436
Iteration 4600: Loss = -11418.503249490166
Iteration 4700: Loss = -11418.502352496938
Iteration 4800: Loss = -11418.50235801385
Iteration 4900: Loss = -11418.500720797601
Iteration 5000: Loss = -11418.49993255408
Iteration 5100: Loss = -11418.501766589998
1
Iteration 5200: Loss = -11418.498270485261
Iteration 5300: Loss = -11418.490341687426
Iteration 5400: Loss = -11418.425895885955
Iteration 5500: Loss = -11418.424879558812
Iteration 5600: Loss = -11418.424256080814
Iteration 5700: Loss = -11418.423687242892
Iteration 5800: Loss = -11418.42411037125
1
Iteration 5900: Loss = -11418.422655876178
Iteration 6000: Loss = -11418.422235575865
Iteration 6100: Loss = -11418.440389670446
1
Iteration 6200: Loss = -11418.421539433704
Iteration 6300: Loss = -11418.421180932355
Iteration 6400: Loss = -11418.42374792506
1
Iteration 6500: Loss = -11418.42063158785
Iteration 6600: Loss = -11418.423332882867
1
Iteration 6700: Loss = -11418.420762071091
2
Iteration 6800: Loss = -11418.420802992483
3
Iteration 6900: Loss = -11418.41965009004
Iteration 7000: Loss = -11418.424758799976
1
Iteration 7100: Loss = -11418.419283071062
Iteration 7200: Loss = -11418.419275399501
Iteration 7300: Loss = -11418.41901588039
Iteration 7400: Loss = -11418.418755533354
Iteration 7500: Loss = -11418.418645873382
Iteration 7600: Loss = -11418.418499634055
Iteration 7700: Loss = -11418.426095719286
1
Iteration 7800: Loss = -11418.418159290402
Iteration 7900: Loss = -11418.418581652713
1
Iteration 8000: Loss = -11418.41795106694
Iteration 8100: Loss = -11418.417924277777
Iteration 8200: Loss = -11418.417722849346
Iteration 8300: Loss = -11418.417711814955
Iteration 8400: Loss = -11418.432694192721
1
Iteration 8500: Loss = -11418.41743637621
Iteration 8600: Loss = -11418.431855917686
1
Iteration 8700: Loss = -11418.417234996528
Iteration 8800: Loss = -11418.42427465513
1
Iteration 8900: Loss = -11418.42241867775
2
Iteration 9000: Loss = -11418.475891016362
3
Iteration 9100: Loss = -11418.41698781109
Iteration 9200: Loss = -11418.421972245156
1
Iteration 9300: Loss = -11418.416890051134
Iteration 9400: Loss = -11418.418500687065
1
Iteration 9500: Loss = -11418.416747810877
Iteration 9600: Loss = -11418.41713674647
1
Iteration 9700: Loss = -11418.416684401553
Iteration 9800: Loss = -11418.419023578992
1
Iteration 9900: Loss = -11418.450321586452
2
Iteration 10000: Loss = -11418.41699219059
3
Iteration 10100: Loss = -11418.41710202489
4
Iteration 10200: Loss = -11418.417477827315
5
Iteration 10300: Loss = -11418.420248771241
6
Iteration 10400: Loss = -11418.42023302336
7
Iteration 10500: Loss = -11418.416429715715
Iteration 10600: Loss = -11418.464869609845
1
Iteration 10700: Loss = -11418.54432639841
2
Iteration 10800: Loss = -11418.417468516607
3
Iteration 10900: Loss = -11418.417229548131
4
Iteration 11000: Loss = -11418.416481120918
Iteration 11100: Loss = -11418.416425577308
Iteration 11200: Loss = -11418.419471284587
1
Iteration 11300: Loss = -11418.41888826771
2
Iteration 11400: Loss = -11418.416228310909
Iteration 11500: Loss = -11418.45035728323
1
Iteration 11600: Loss = -11418.417889782906
2
Iteration 11700: Loss = -11418.420459535017
3
Iteration 11800: Loss = -11418.426292266693
4
Iteration 11900: Loss = -11418.416733615624
5
Iteration 12000: Loss = -11418.416252624373
Iteration 12100: Loss = -11418.416116070515
Iteration 12200: Loss = -11418.457161535507
1
Iteration 12300: Loss = -11418.421771601626
2
Iteration 12400: Loss = -11418.417349445685
3
Iteration 12500: Loss = -11418.41763526247
4
Iteration 12600: Loss = -11418.416036888173
Iteration 12700: Loss = -11418.430739442301
1
Iteration 12800: Loss = -11418.415985539703
Iteration 12900: Loss = -11418.422534164061
1
Iteration 13000: Loss = -11418.579388851154
2
Iteration 13100: Loss = -11418.417543970052
3
Iteration 13200: Loss = -11418.442420327905
4
Iteration 13300: Loss = -11418.416777623908
5
Iteration 13400: Loss = -11418.418259578235
6
Iteration 13500: Loss = -11418.523769258814
7
Iteration 13600: Loss = -11418.41685048246
8
Iteration 13700: Loss = -11418.4171103238
9
Iteration 13800: Loss = -11418.42426442841
10
Iteration 13900: Loss = -11418.425161121953
11
Iteration 14000: Loss = -11418.425922048646
12
Iteration 14100: Loss = -11418.41613553758
13
Iteration 14200: Loss = -11418.466436059884
14
Iteration 14300: Loss = -11418.41614445075
15
Stopping early at iteration 14300 due to no improvement.
pi: tensor([[0.7407, 0.2593],
        [0.2508, 0.7492]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5095, 0.4905], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.0972],
         [0.6225, 0.3864]],

        [[0.6326, 0.1018],
         [0.6741, 0.6667]],

        [[0.6928, 0.0926],
         [0.6494, 0.6034]],

        [[0.5324, 0.0879],
         [0.6387, 0.6810]],

        [[0.6168, 0.1031],
         [0.6083, 0.6120]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 95%|█████████▌| 95/100 [25:33:44<1:15:24, 904.90s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 96%|█████████▌| 96/100 [25:49:01<1:00:34, 908.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 97%|█████████▋| 97/100 [26:01:57<43:26, 868.73s/it]  /home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 98%|█████████▊| 98/100 [26:19:21<30:42, 921.40s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 99%|█████████▉| 99/100 [26:31:13<14:18, 858.62s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
100%|██████████| 100/100 [26:46:27<00:00, 875.16s/it]100%|██████████| 100/100 [26:46:27<00:00, 963.88s/it]
tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320020521122
Average Adjusted Rand Index: 0.9839998119331363
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20274.9274618156
Iteration 100: Loss = -12182.116553255697
Iteration 200: Loss = -12149.446503170053
Iteration 300: Loss = -11802.055035418573
Iteration 400: Loss = -11495.250990950764
Iteration 500: Loss = -11449.076850889072
Iteration 600: Loss = -11420.602807968966
Iteration 700: Loss = -11420.142240147703
Iteration 800: Loss = -11419.571591863758
Iteration 900: Loss = -11419.031005894536
Iteration 1000: Loss = -11418.925584715773
Iteration 1100: Loss = -11418.849945813703
Iteration 1200: Loss = -11418.793225333267
Iteration 1300: Loss = -11418.749158592404
Iteration 1400: Loss = -11418.71391868955
Iteration 1500: Loss = -11418.684798478298
Iteration 1600: Loss = -11418.6592287872
Iteration 1700: Loss = -11418.630073252005
Iteration 1800: Loss = -11418.560748344291
Iteration 1900: Loss = -11418.543472254207
Iteration 2000: Loss = -11418.53070591265
Iteration 2100: Loss = -11418.519952370323
Iteration 2200: Loss = -11418.510720249371
Iteration 2300: Loss = -11418.504983134653
Iteration 2400: Loss = -11418.49549877716
Iteration 2500: Loss = -11418.489251590661
Iteration 2600: Loss = -11418.483551290727
Iteration 2700: Loss = -11418.47848347328
Iteration 2800: Loss = -11418.47395916394
Iteration 2900: Loss = -11418.469827308
Iteration 3000: Loss = -11418.46611491568
Iteration 3100: Loss = -11418.4626847743
Iteration 3200: Loss = -11418.459358770378
Iteration 3300: Loss = -11418.456874956779
Iteration 3400: Loss = -11418.469763975823
1
Iteration 3500: Loss = -11418.45052338675
Iteration 3600: Loss = -11418.44878305057
Iteration 3700: Loss = -11418.445210482412
Iteration 3800: Loss = -11418.443123679996
Iteration 3900: Loss = -11418.442874714436
Iteration 4000: Loss = -11418.439717222796
Iteration 4100: Loss = -11418.442981598315
1
Iteration 4200: Loss = -11418.443646748396
2
Iteration 4300: Loss = -11418.435565322747
Iteration 4400: Loss = -11418.436192147125
1
Iteration 4500: Loss = -11418.433433490323
Iteration 4600: Loss = -11418.432100348009
Iteration 4700: Loss = -11418.43237223529
1
Iteration 4800: Loss = -11418.430079044296
Iteration 4900: Loss = -11418.429135530378
Iteration 5000: Loss = -11418.43268901009
1
Iteration 5100: Loss = -11418.427329450507
Iteration 5200: Loss = -11418.426527728945
Iteration 5300: Loss = -11418.425810468165
Iteration 5400: Loss = -11418.425132058794
Iteration 5500: Loss = -11418.424878681555
Iteration 5600: Loss = -11418.424028745134
Iteration 5700: Loss = -11418.423602879377
Iteration 5800: Loss = -11418.42306907531
Iteration 5900: Loss = -11418.422650850478
Iteration 6000: Loss = -11418.425107639325
1
Iteration 6100: Loss = -11418.421889003494
Iteration 6200: Loss = -11418.423528822832
1
Iteration 6300: Loss = -11418.421228736073
Iteration 6400: Loss = -11418.42357438753
1
Iteration 6500: Loss = -11418.420632851143
Iteration 6600: Loss = -11418.420409048673
Iteration 6700: Loss = -11418.420919849512
1
Iteration 6800: Loss = -11418.419879208324
Iteration 6900: Loss = -11418.419810963474
Iteration 7000: Loss = -11418.420983764281
1
Iteration 7100: Loss = -11418.419262329546
Iteration 7200: Loss = -11418.419078687028
Iteration 7300: Loss = -11418.418896573805
Iteration 7400: Loss = -11418.41900186868
1
Iteration 7500: Loss = -11418.418543224634
Iteration 7600: Loss = -11418.419932787448
1
Iteration 7700: Loss = -11418.418690232229
2
Iteration 7800: Loss = -11418.430464287987
3
Iteration 7900: Loss = -11418.417982735264
Iteration 8000: Loss = -11418.41830217445
1
Iteration 8100: Loss = -11418.417770035738
Iteration 8200: Loss = -11418.418671433428
1
Iteration 8300: Loss = -11418.417548047513
Iteration 8400: Loss = -11418.41765351886
1
Iteration 8500: Loss = -11418.417360711323
Iteration 8600: Loss = -11418.41733719218
Iteration 8700: Loss = -11418.41726387844
Iteration 8800: Loss = -11418.421618817676
1
Iteration 8900: Loss = -11418.41756727358
2
Iteration 9000: Loss = -11418.417465823892
3
Iteration 9100: Loss = -11418.420688937293
4
Iteration 9200: Loss = -11418.41738930372
5
Iteration 9300: Loss = -11418.420859682645
6
Iteration 9400: Loss = -11418.417186993624
Iteration 9500: Loss = -11418.416930745501
Iteration 9600: Loss = -11418.418200322903
1
Iteration 9700: Loss = -11418.41747953316
2
Iteration 9800: Loss = -11418.417079948096
3
Iteration 9900: Loss = -11418.416604848915
Iteration 10000: Loss = -11418.434516245967
1
Iteration 10100: Loss = -11418.601832233613
2
Iteration 10200: Loss = -11418.41641775321
Iteration 10300: Loss = -11418.430961581174
1
Iteration 10400: Loss = -11418.416342676164
Iteration 10500: Loss = -11418.416831690764
1
Iteration 10600: Loss = -11418.508605771907
2
Iteration 10700: Loss = -11418.41670104309
3
Iteration 10800: Loss = -11418.447876621762
4
Iteration 10900: Loss = -11418.416800961344
5
Iteration 11000: Loss = -11418.421148086567
6
Iteration 11100: Loss = -11418.417060784677
7
Iteration 11200: Loss = -11418.41702615283
8
Iteration 11300: Loss = -11418.416175695706
Iteration 11400: Loss = -11418.41640696774
1
Iteration 11500: Loss = -11418.416274752462
Iteration 11600: Loss = -11418.423889564463
1
Iteration 11700: Loss = -11418.421171193804
2
Iteration 11800: Loss = -11418.488148280629
3
Iteration 11900: Loss = -11418.425400391867
4
Iteration 12000: Loss = -11418.416285079677
Iteration 12100: Loss = -11418.453018660863
1
Iteration 12200: Loss = -11418.42195067655
2
Iteration 12300: Loss = -11418.457294633441
3
Iteration 12400: Loss = -11418.417633084615
4
Iteration 12500: Loss = -11418.417852213643
5
Iteration 12600: Loss = -11418.436577514049
6
Iteration 12700: Loss = -11418.419013588064
7
Iteration 12800: Loss = -11418.424487829934
8
Iteration 12900: Loss = -11418.457736025719
9
Iteration 13000: Loss = -11418.416760970515
10
Iteration 13100: Loss = -11418.419495784254
11
Iteration 13200: Loss = -11418.416493878463
12
Iteration 13300: Loss = -11418.417604271674
13
Iteration 13400: Loss = -11418.420532089349
14
Iteration 13500: Loss = -11418.56664374999
15
Stopping early at iteration 13500 due to no improvement.
pi: tensor([[0.7532, 0.2468],
        [0.2599, 0.7401]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4887, 0.5113], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3839, 0.0976],
         [0.5729, 0.2009]],

        [[0.7039, 0.1014],
         [0.6681, 0.5832]],

        [[0.6954, 0.0919],
         [0.6176, 0.6663]],

        [[0.7291, 0.0879],
         [0.6936, 0.7022]],

        [[0.7208, 0.1026],
         [0.5995, 0.6452]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320020521122
Average Adjusted Rand Index: 0.9839998119331363
11430.153031730853
[0.9840320020521122, 0.9840320020521122] [0.9839998119331363, 0.9839998119331363] [11418.41614445075, 11418.56664374999]
-------------------------------------
This iteration is 95
True Objective function: Loss = -11457.701232198497
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21087.257577913864
Iteration 100: Loss = -12322.613725468305
Iteration 200: Loss = -12278.162384581923
Iteration 300: Loss = -11877.404907843622
Iteration 400: Loss = -11822.75616451671
Iteration 500: Loss = -11782.602095989068
Iteration 600: Loss = -11780.76950356384
Iteration 700: Loss = -11780.59082283197
Iteration 800: Loss = -11780.47439102704
Iteration 900: Loss = -11779.97629541314
Iteration 1000: Loss = -11777.103194337422
Iteration 1100: Loss = -11776.804199248945
Iteration 1200: Loss = -11774.96126082081
Iteration 1300: Loss = -11774.910736281125
Iteration 1400: Loss = -11774.755547545077
Iteration 1500: Loss = -11774.640350329393
Iteration 1600: Loss = -11774.472809324609
Iteration 1700: Loss = -11773.65759933367
Iteration 1800: Loss = -11773.315183017174
Iteration 1900: Loss = -11769.659476598194
Iteration 2000: Loss = -11762.701005450577
Iteration 2100: Loss = -11762.608874662095
Iteration 2200: Loss = -11762.24567968495
Iteration 2300: Loss = -11762.072369303089
Iteration 2400: Loss = -11753.608595558602
Iteration 2500: Loss = -11753.26197531984
Iteration 2600: Loss = -11753.186610677181
Iteration 2700: Loss = -11753.175524248889
Iteration 2800: Loss = -11753.14976998637
Iteration 2900: Loss = -11753.143811386624
Iteration 3000: Loss = -11753.146917724698
1
Iteration 3100: Loss = -11753.138183263143
Iteration 3200: Loss = -11753.135823916658
Iteration 3300: Loss = -11753.13483724911
Iteration 3400: Loss = -11753.1296377206
Iteration 3500: Loss = -11753.111066791786
Iteration 3600: Loss = -11753.113093960716
1
Iteration 3700: Loss = -11753.103568230053
Iteration 3800: Loss = -11753.103179989082
Iteration 3900: Loss = -11753.101517952968
Iteration 4000: Loss = -11753.101593421807
Iteration 4100: Loss = -11753.099223827556
Iteration 4200: Loss = -11752.608903353508
Iteration 4300: Loss = -11752.606335105611
Iteration 4400: Loss = -11752.61485941854
1
Iteration 4500: Loss = -11752.605131019229
Iteration 4600: Loss = -11752.604586671587
Iteration 4700: Loss = -11752.607594932339
1
Iteration 4800: Loss = -11752.603690534725
Iteration 4900: Loss = -11752.603204339595
Iteration 5000: Loss = -11752.602779962608
Iteration 5100: Loss = -11752.60233979837
Iteration 5200: Loss = -11752.601684035213
Iteration 5300: Loss = -11752.603709545076
1
Iteration 5400: Loss = -11752.599217544883
Iteration 5500: Loss = -11752.600807277951
1
Iteration 5600: Loss = -11752.598107868263
Iteration 5700: Loss = -11752.598141042417
Iteration 5800: Loss = -11752.599853848493
1
Iteration 5900: Loss = -11752.597610627869
Iteration 6000: Loss = -11752.597566215809
Iteration 6100: Loss = -11752.598032428781
1
Iteration 6200: Loss = -11752.596842283461
Iteration 6300: Loss = -11752.596827615605
Iteration 6400: Loss = -11752.596452439873
Iteration 6500: Loss = -11752.596116909828
Iteration 6600: Loss = -11752.595663687733
Iteration 6700: Loss = -11752.595629620531
Iteration 6800: Loss = -11752.5952386705
Iteration 6900: Loss = -11752.59504995296
Iteration 7000: Loss = -11752.594906415503
Iteration 7100: Loss = -11752.594470217944
Iteration 7200: Loss = -11752.604983002873
1
Iteration 7300: Loss = -11752.589833261196
Iteration 7400: Loss = -11752.58972495594
Iteration 7500: Loss = -11752.589625468097
Iteration 7600: Loss = -11752.593316030761
1
Iteration 7700: Loss = -11752.593049126752
2
Iteration 7800: Loss = -11752.59071299248
3
Iteration 7900: Loss = -11752.592140704257
4
Iteration 8000: Loss = -11752.589379767398
Iteration 8100: Loss = -11752.589241065574
Iteration 8200: Loss = -11752.58937344306
1
Iteration 8300: Loss = -11752.59077422951
2
Iteration 8400: Loss = -11752.594204451647
3
Iteration 8500: Loss = -11752.589068741
Iteration 8600: Loss = -11752.590705747485
1
Iteration 8700: Loss = -11752.588913835538
Iteration 8800: Loss = -11752.579512579354
Iteration 8900: Loss = -11752.578712465147
Iteration 9000: Loss = -11752.57848971431
Iteration 9100: Loss = -11752.57845651098
Iteration 9200: Loss = -11752.578525032912
Iteration 9300: Loss = -11752.578406172734
Iteration 9400: Loss = -11752.579674177978
1
Iteration 9500: Loss = -11752.578360561725
Iteration 9600: Loss = -11752.612995152469
1
Iteration 9700: Loss = -11752.590298394556
2
Iteration 9800: Loss = -11752.579554670545
3
Iteration 9900: Loss = -11752.581299108379
4
Iteration 10000: Loss = -11752.620618204162
5
Iteration 10100: Loss = -11752.583734119735
6
Iteration 10200: Loss = -11752.578576723286
7
Iteration 10300: Loss = -11752.587212124827
8
Iteration 10400: Loss = -11752.581009515428
9
Iteration 10500: Loss = -11752.57949083457
10
Iteration 10600: Loss = -11752.621566298158
11
Iteration 10700: Loss = -11752.578098884916
Iteration 10800: Loss = -11752.578269917574
1
Iteration 10900: Loss = -11752.584019339638
2
Iteration 11000: Loss = -11752.578265600992
3
Iteration 11100: Loss = -11752.586755478436
4
Iteration 11200: Loss = -11752.586703565616
5
Iteration 11300: Loss = -11752.579802241791
6
Iteration 11400: Loss = -11752.617056818974
7
Iteration 11500: Loss = -11752.5780445212
Iteration 11600: Loss = -11752.578468619002
1
Iteration 11700: Loss = -11752.578164523793
2
Iteration 11800: Loss = -11752.588996903689
3
Iteration 11900: Loss = -11752.730157784023
4
Iteration 12000: Loss = -11752.586875035711
5
Iteration 12100: Loss = -11752.57807740807
Iteration 12200: Loss = -11752.579711721995
1
Iteration 12300: Loss = -11752.647774108294
2
Iteration 12400: Loss = -11752.587383831069
3
Iteration 12500: Loss = -11752.579409135358
4
Iteration 12600: Loss = -11752.59053746799
5
Iteration 12700: Loss = -11752.580549549435
6
Iteration 12800: Loss = -11752.582844339006
7
Iteration 12900: Loss = -11752.578311118144
8
Iteration 13000: Loss = -11752.578850263902
9
Iteration 13100: Loss = -11752.58325496779
10
Iteration 13200: Loss = -11752.577990575375
Iteration 13300: Loss = -11752.579902791536
1
Iteration 13400: Loss = -11752.579152950822
2
Iteration 13500: Loss = -11752.577925322354
Iteration 13600: Loss = -11752.5780804574
1
Iteration 13700: Loss = -11752.613625869317
2
Iteration 13800: Loss = -11752.577962461954
Iteration 13900: Loss = -11752.57817328778
1
Iteration 14000: Loss = -11752.577892631723
Iteration 14100: Loss = -11752.578181505954
1
Iteration 14200: Loss = -11752.577900946097
Iteration 14300: Loss = -11752.688360215485
1
Iteration 14400: Loss = -11752.577883620403
Iteration 14500: Loss = -11752.596648192806
1
Iteration 14600: Loss = -11752.578360029189
2
Iteration 14700: Loss = -11752.582644497874
3
Iteration 14800: Loss = -11752.585328220794
4
Iteration 14900: Loss = -11752.57847237576
5
Iteration 15000: Loss = -11752.580055324372
6
Iteration 15100: Loss = -11752.577933298657
Iteration 15200: Loss = -11752.578249387934
1
Iteration 15300: Loss = -11752.612993514444
2
Iteration 15400: Loss = -11752.577887754509
Iteration 15500: Loss = -11752.578865120073
1
Iteration 15600: Loss = -11752.578105627283
2
Iteration 15700: Loss = -11752.579129664662
3
Iteration 15800: Loss = -11752.578474238622
4
Iteration 15900: Loss = -11752.57851922788
5
Iteration 16000: Loss = -11752.578295081858
6
Iteration 16100: Loss = -11752.606759671407
7
Iteration 16200: Loss = -11752.579625906503
8
Iteration 16300: Loss = -11752.578649628775
9
Iteration 16400: Loss = -11752.579888938875
10
Iteration 16500: Loss = -11752.583164781523
11
Iteration 16600: Loss = -11752.584183898884
12
Iteration 16700: Loss = -11752.578139735944
13
Iteration 16800: Loss = -11752.578559543785
14
Iteration 16900: Loss = -11752.585227230562
15
Stopping early at iteration 16900 due to no improvement.
pi: tensor([[0.7078, 0.2922],
        [0.3154, 0.6846]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8215, 0.1785], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2230, 0.1064],
         [0.6486, 0.4035]],

        [[0.5406, 0.1007],
         [0.6575, 0.5692]],

        [[0.5542, 0.1000],
         [0.5490, 0.5978]],

        [[0.5555, 0.0968],
         [0.5979, 0.5072]],

        [[0.6611, 0.0942],
         [0.7252, 0.7282]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 37
Adjusted Rand Index: 0.059651835367741005
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 30
Adjusted Rand Index: 0.1537171964665678
time is 2
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.217506077756761
Average Adjusted Rand Index: 0.6426738063668618
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22032.771201937732
Iteration 100: Loss = -12324.496026107856
Iteration 200: Loss = -12316.814212877096
Iteration 300: Loss = -11845.73657238534
Iteration 400: Loss = -11612.129095607728
Iteration 500: Loss = -11571.144225730179
Iteration 600: Loss = -11506.196211738976
Iteration 700: Loss = -11478.077055932768
Iteration 800: Loss = -11453.881603831409
Iteration 900: Loss = -11453.591285022852
Iteration 1000: Loss = -11453.43772250443
Iteration 1100: Loss = -11453.336738440483
Iteration 1200: Loss = -11453.264766941576
Iteration 1300: Loss = -11453.210829278003
Iteration 1400: Loss = -11453.168953478931
Iteration 1500: Loss = -11453.135587509278
Iteration 1600: Loss = -11453.108511114071
Iteration 1700: Loss = -11453.086062090993
Iteration 1800: Loss = -11453.06727704304
Iteration 1900: Loss = -11453.051290348589
Iteration 2000: Loss = -11453.037620514002
Iteration 2100: Loss = -11453.025780772472
Iteration 2200: Loss = -11453.015462029689
Iteration 2300: Loss = -11453.006383190863
Iteration 2400: Loss = -11452.998276303282
Iteration 2500: Loss = -11452.99100829233
Iteration 2600: Loss = -11452.98442782883
Iteration 2700: Loss = -11452.978157620944
Iteration 2800: Loss = -11452.97198038898
Iteration 2900: Loss = -11452.964448066357
Iteration 3000: Loss = -11452.95895196736
Iteration 3100: Loss = -11452.955084919258
Iteration 3200: Loss = -11452.951432855669
Iteration 3300: Loss = -11452.948255447036
Iteration 3400: Loss = -11452.945381881507
Iteration 3500: Loss = -11452.942622842887
Iteration 3600: Loss = -11452.94012894157
Iteration 3700: Loss = -11452.937729955942
Iteration 3800: Loss = -11452.936033716667
Iteration 3900: Loss = -11452.93329676084
Iteration 4000: Loss = -11452.931322643994
Iteration 4100: Loss = -11452.929829872484
Iteration 4200: Loss = -11452.928176042216
Iteration 4300: Loss = -11452.929781134882
1
Iteration 4400: Loss = -11452.925621844368
Iteration 4500: Loss = -11452.924370684832
Iteration 4600: Loss = -11452.923326466762
Iteration 4700: Loss = -11452.922222076326
Iteration 4800: Loss = -11452.931312362196
1
Iteration 4900: Loss = -11452.920428884274
Iteration 5000: Loss = -11452.919863824134
Iteration 5100: Loss = -11452.918828000998
Iteration 5200: Loss = -11452.918111040068
Iteration 5300: Loss = -11452.917601905328
Iteration 5400: Loss = -11452.91678151723
Iteration 5500: Loss = -11452.928760899005
1
Iteration 5600: Loss = -11452.915654403763
Iteration 5700: Loss = -11452.919532238218
1
Iteration 5800: Loss = -11452.91594838606
2
Iteration 5900: Loss = -11452.914188295417
Iteration 6000: Loss = -11452.913803346833
Iteration 6100: Loss = -11452.913355636765
Iteration 6200: Loss = -11452.91363812603
1
Iteration 6300: Loss = -11452.912622214968
Iteration 6400: Loss = -11452.921596241416
1
Iteration 6500: Loss = -11452.911933483087
Iteration 6600: Loss = -11452.911660623226
Iteration 6700: Loss = -11452.911340290571
Iteration 6800: Loss = -11452.91108812626
Iteration 6900: Loss = -11452.916289838566
1
Iteration 7000: Loss = -11452.910601271218
Iteration 7100: Loss = -11452.923781558238
1
Iteration 7200: Loss = -11452.910176048726
Iteration 7300: Loss = -11452.910096781514
Iteration 7400: Loss = -11452.912760439016
1
Iteration 7500: Loss = -11452.909565745096
Iteration 7600: Loss = -11452.920209289585
1
Iteration 7700: Loss = -11452.910334557786
2
Iteration 7800: Loss = -11452.913885189562
3
Iteration 7900: Loss = -11452.908642678394
Iteration 8000: Loss = -11452.909863740375
1
Iteration 8100: Loss = -11452.928640280204
2
Iteration 8200: Loss = -11452.909887726995
3
Iteration 8300: Loss = -11452.908087631135
Iteration 8400: Loss = -11452.916547663404
1
Iteration 8500: Loss = -11452.914236908005
2
Iteration 8600: Loss = -11452.907802887446
Iteration 8700: Loss = -11452.907741282497
Iteration 8800: Loss = -11452.90754308308
Iteration 8900: Loss = -11452.938425335135
1
Iteration 9000: Loss = -11452.91804275687
2
Iteration 9100: Loss = -11452.90725537587
Iteration 9200: Loss = -11452.909099527256
1
Iteration 9300: Loss = -11452.907112587447
Iteration 9400: Loss = -11452.92129980096
1
Iteration 9500: Loss = -11452.90778695719
2
Iteration 9600: Loss = -11452.911020683887
3
Iteration 9700: Loss = -11452.911310923868
4
Iteration 9800: Loss = -11452.907095007306
Iteration 9900: Loss = -11452.90743899732
1
Iteration 10000: Loss = -11452.911588369445
2
Iteration 10100: Loss = -11452.983804762398
3
Iteration 10200: Loss = -11452.908281248549
4
Iteration 10300: Loss = -11452.90737902994
5
Iteration 10400: Loss = -11452.916034691576
6
Iteration 10500: Loss = -11452.91016205469
7
Iteration 10600: Loss = -11452.912743707693
8
Iteration 10700: Loss = -11452.907319773703
9
Iteration 10800: Loss = -11452.908717959956
10
Iteration 10900: Loss = -11452.907419272862
11
Iteration 11000: Loss = -11452.912375594882
12
Iteration 11100: Loss = -11453.025753261474
13
Iteration 11200: Loss = -11452.916916537557
14
Iteration 11300: Loss = -11452.90902065593
15
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[0.7903, 0.2097],
        [0.2155, 0.7845]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4580, 0.5420], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4125, 0.1014],
         [0.5132, 0.2013]],

        [[0.5299, 0.0993],
         [0.6532, 0.5684]],

        [[0.6925, 0.0998],
         [0.6140, 0.7121]],

        [[0.5500, 0.0969],
         [0.6979, 0.6747]],

        [[0.7044, 0.0945],
         [0.5600, 0.6831]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11457.701232198497
[0.217506077756761, 1.0] [0.6426738063668618, 1.0] [11752.585227230562, 11452.90902065593]
-------------------------------------
This iteration is 96
True Objective function: Loss = -11356.908058210178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21572.143858029853
Iteration 100: Loss = -12064.301081411546
Iteration 200: Loss = -11405.97325930343
Iteration 300: Loss = -11354.75649964168
Iteration 400: Loss = -11354.258895575633
Iteration 500: Loss = -11354.114424934345
Iteration 600: Loss = -11354.04408922066
Iteration 700: Loss = -11353.994431435132
Iteration 800: Loss = -11353.962699926975
Iteration 900: Loss = -11347.833586250803
Iteration 1000: Loss = -11347.815911312817
Iteration 1100: Loss = -11347.805787654675
Iteration 1200: Loss = -11347.792936118252
Iteration 1300: Loss = -11347.784864141813
Iteration 1400: Loss = -11347.778231217362
Iteration 1500: Loss = -11347.772715770016
Iteration 1600: Loss = -11347.768043767903
Iteration 1700: Loss = -11347.764078961201
Iteration 1800: Loss = -11347.760760503385
Iteration 1900: Loss = -11347.757801512118
Iteration 2000: Loss = -11347.757646235255
Iteration 2100: Loss = -11347.75296520321
Iteration 2200: Loss = -11347.750896286545
Iteration 2300: Loss = -11347.748900226485
Iteration 2400: Loss = -11347.746865824782
Iteration 2500: Loss = -11347.74411896274
Iteration 2600: Loss = -11347.739220050014
Iteration 2700: Loss = -11347.735157378851
Iteration 2800: Loss = -11347.733885718724
Iteration 2900: Loss = -11347.734138521599
1
Iteration 3000: Loss = -11347.731936462258
Iteration 3100: Loss = -11347.731158290271
Iteration 3200: Loss = -11347.733248167178
1
Iteration 3300: Loss = -11347.736605153817
2
Iteration 3400: Loss = -11347.729242135612
Iteration 3500: Loss = -11347.730440851781
1
Iteration 3600: Loss = -11347.728207624245
Iteration 3700: Loss = -11347.72775817993
Iteration 3800: Loss = -11347.727503624763
Iteration 3900: Loss = -11347.726927785616
Iteration 4000: Loss = -11347.72654350082
Iteration 4100: Loss = -11347.726481195812
Iteration 4200: Loss = -11347.7256651961
Iteration 4300: Loss = -11347.728120381893
1
Iteration 4400: Loss = -11347.724402469164
Iteration 4500: Loss = -11347.723784642
Iteration 4600: Loss = -11347.723339356755
Iteration 4700: Loss = -11347.72446883783
1
Iteration 4800: Loss = -11347.722067924604
Iteration 4900: Loss = -11347.725140424283
1
Iteration 5000: Loss = -11347.721537782463
Iteration 5100: Loss = -11347.721637157618
Iteration 5200: Loss = -11347.721987133265
1
Iteration 5300: Loss = -11347.726843923276
2
Iteration 5400: Loss = -11347.732607690206
3
Iteration 5500: Loss = -11347.720706477601
Iteration 5600: Loss = -11347.7143918498
Iteration 5700: Loss = -11347.720847222914
1
Iteration 5800: Loss = -11347.714798226734
2
Iteration 5900: Loss = -11347.720239942126
3
Iteration 6000: Loss = -11347.713973500186
Iteration 6100: Loss = -11347.716592611372
1
Iteration 6200: Loss = -11347.713808805538
Iteration 6300: Loss = -11347.718597977262
1
Iteration 6400: Loss = -11347.719355623145
2
Iteration 6500: Loss = -11347.716030120568
3
Iteration 6600: Loss = -11347.713605206625
Iteration 6700: Loss = -11347.719536734632
1
Iteration 6800: Loss = -11347.713480408136
Iteration 6900: Loss = -11347.716493546559
1
Iteration 7000: Loss = -11347.713772297691
2
Iteration 7100: Loss = -11347.713467746513
Iteration 7200: Loss = -11347.713283598363
Iteration 7300: Loss = -11347.713229452247
Iteration 7400: Loss = -11347.725360796263
1
Iteration 7500: Loss = -11347.713137459672
Iteration 7600: Loss = -11347.713541620173
1
Iteration 7700: Loss = -11347.722142639577
2
Iteration 7800: Loss = -11347.713027633376
Iteration 7900: Loss = -11347.71936767834
1
Iteration 8000: Loss = -11347.7129846962
Iteration 8100: Loss = -11347.712952349515
Iteration 8200: Loss = -11347.713690307635
1
Iteration 8300: Loss = -11347.717329339055
2
Iteration 8400: Loss = -11347.71321740542
3
Iteration 8500: Loss = -11347.712967389325
Iteration 8600: Loss = -11347.713194057356
1
Iteration 8700: Loss = -11347.715195625966
2
Iteration 8800: Loss = -11347.711606115618
Iteration 8900: Loss = -11347.711409999523
Iteration 9000: Loss = -11347.712618112819
1
Iteration 9100: Loss = -11347.711353863153
Iteration 9200: Loss = -11347.711184146885
Iteration 9300: Loss = -11347.722350261643
1
Iteration 9400: Loss = -11347.708491872441
Iteration 9500: Loss = -11347.710357017491
1
Iteration 9600: Loss = -11347.708471171085
Iteration 9700: Loss = -11347.708498971482
Iteration 9800: Loss = -11347.708685687476
1
Iteration 9900: Loss = -11347.710355393749
2
Iteration 10000: Loss = -11347.712755023036
3
Iteration 10100: Loss = -11347.709374909926
4
Iteration 10200: Loss = -11347.708664026304
5
Iteration 10300: Loss = -11347.74375906519
6
Iteration 10400: Loss = -11347.817294202623
7
Iteration 10500: Loss = -11347.712968395417
8
Iteration 10600: Loss = -11347.709047542388
9
Iteration 10700: Loss = -11347.728317699386
10
Iteration 10800: Loss = -11347.708956375825
11
Iteration 10900: Loss = -11347.708750167372
12
Iteration 11000: Loss = -11347.712949198683
13
Iteration 11100: Loss = -11347.74706697019
14
Iteration 11200: Loss = -11347.723934379672
15
Stopping early at iteration 11200 due to no improvement.
pi: tensor([[0.7562, 0.2438],
        [0.2273, 0.7727]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4298, 0.5702], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3998, 0.0944],
         [0.6767, 0.1990]],

        [[0.6639, 0.0967],
         [0.6334, 0.7195]],

        [[0.7042, 0.1050],
         [0.6542, 0.6773]],

        [[0.5783, 0.0949],
         [0.6547, 0.6289]],

        [[0.6478, 0.1007],
         [0.6375, 0.5866]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9760959038379515
Average Adjusted Rand Index: 0.9761607826731258
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23135.609726121176
Iteration 100: Loss = -11357.511229492828
Iteration 200: Loss = -11349.268155890535
Iteration 300: Loss = -11348.530093702777
Iteration 400: Loss = -11348.233529013602
Iteration 500: Loss = -11348.078858277377
Iteration 600: Loss = -11347.986341235206
Iteration 700: Loss = -11347.925977820474
Iteration 800: Loss = -11347.884098653565
Iteration 900: Loss = -11347.85353611136
Iteration 1000: Loss = -11347.830488161011
Iteration 1100: Loss = -11347.812588903595
Iteration 1200: Loss = -11347.798179378555
Iteration 1300: Loss = -11347.786287478695
Iteration 1400: Loss = -11347.77648741385
Iteration 1500: Loss = -11347.768534385825
Iteration 1600: Loss = -11347.761983612518
Iteration 1700: Loss = -11347.756502074153
Iteration 1800: Loss = -11347.751800227185
Iteration 1900: Loss = -11347.747817605987
Iteration 2000: Loss = -11347.744341065076
Iteration 2100: Loss = -11347.741299681267
Iteration 2200: Loss = -11347.738595221024
Iteration 2300: Loss = -11347.736220195638
Iteration 2400: Loss = -11347.734172720915
Iteration 2500: Loss = -11347.732288750227
Iteration 2600: Loss = -11347.730602581214
Iteration 2700: Loss = -11347.7291130933
Iteration 2800: Loss = -11347.727797428186
Iteration 2900: Loss = -11347.726547429034
Iteration 3000: Loss = -11347.72544453147
Iteration 3100: Loss = -11347.724419010086
Iteration 3200: Loss = -11347.723485429286
Iteration 3300: Loss = -11347.72285061931
Iteration 3400: Loss = -11347.721879713377
Iteration 3500: Loss = -11347.721161137439
Iteration 3600: Loss = -11347.720465922783
Iteration 3700: Loss = -11347.71990344633
Iteration 3800: Loss = -11347.719549276062
Iteration 3900: Loss = -11347.718776962038
Iteration 4000: Loss = -11347.718642563643
Iteration 4100: Loss = -11347.717830738557
Iteration 4200: Loss = -11347.718154970713
1
Iteration 4300: Loss = -11347.71704007093
Iteration 4400: Loss = -11347.716713609554
Iteration 4500: Loss = -11347.716368536672
Iteration 4600: Loss = -11347.716097200788
Iteration 4700: Loss = -11347.722122470863
1
Iteration 4800: Loss = -11347.715593960265
Iteration 4900: Loss = -11347.715266979381
Iteration 5000: Loss = -11347.724472062953
1
Iteration 5100: Loss = -11347.71745100346
2
Iteration 5200: Loss = -11347.717820956579
3
Iteration 5300: Loss = -11347.714440044583
Iteration 5400: Loss = -11347.714621155264
1
Iteration 5500: Loss = -11347.714064714168
Iteration 5600: Loss = -11347.71393311601
Iteration 5700: Loss = -11347.714795074124
1
Iteration 5800: Loss = -11347.713546905095
Iteration 5900: Loss = -11347.713049060014
Iteration 6000: Loss = -11347.709497413447
Iteration 6100: Loss = -11347.70697777738
Iteration 6200: Loss = -11347.706916849147
Iteration 6300: Loss = -11347.723516863016
1
Iteration 6400: Loss = -11347.706675101119
Iteration 6500: Loss = -11347.706579998548
Iteration 6600: Loss = -11347.706548313668
Iteration 6700: Loss = -11347.706389647521
Iteration 6800: Loss = -11347.706384889874
Iteration 6900: Loss = -11347.711299691839
1
Iteration 7000: Loss = -11347.706518492763
2
Iteration 7100: Loss = -11347.70615224785
Iteration 7200: Loss = -11347.707438042064
1
Iteration 7300: Loss = -11347.70603408135
Iteration 7400: Loss = -11347.706970564754
1
Iteration 7500: Loss = -11347.70626331101
2
Iteration 7600: Loss = -11347.706936559669
3
Iteration 7700: Loss = -11347.705810675028
Iteration 7800: Loss = -11347.705956120413
1
Iteration 7900: Loss = -11347.705859602092
Iteration 8000: Loss = -11347.705732539545
Iteration 8100: Loss = -11347.706170685567
1
Iteration 8200: Loss = -11347.705668412687
Iteration 8300: Loss = -11347.705622354568
Iteration 8400: Loss = -11347.706540876192
1
Iteration 8500: Loss = -11347.709428907057
2
Iteration 8600: Loss = -11347.705910472527
3
Iteration 8700: Loss = -11347.705659439218
Iteration 8800: Loss = -11347.705489095682
Iteration 8900: Loss = -11347.706527942984
1
Iteration 9000: Loss = -11347.710635790589
2
Iteration 9100: Loss = -11347.705890408692
3
Iteration 9200: Loss = -11347.70552205627
Iteration 9300: Loss = -11347.720498194667
1
Iteration 9400: Loss = -11347.705428324547
Iteration 9500: Loss = -11347.705678489672
1
Iteration 9600: Loss = -11347.714377981867
2
Iteration 9700: Loss = -11347.714387167038
3
Iteration 9800: Loss = -11347.705747330456
4
Iteration 9900: Loss = -11347.705282619241
Iteration 10000: Loss = -11347.705417131638
1
Iteration 10100: Loss = -11347.705443858948
2
Iteration 10200: Loss = -11347.705362269695
Iteration 10300: Loss = -11347.707144840066
1
Iteration 10400: Loss = -11347.70551891367
2
Iteration 10500: Loss = -11347.716272689988
3
Iteration 10600: Loss = -11347.716788115464
4
Iteration 10700: Loss = -11347.717469771598
5
Iteration 10800: Loss = -11347.775859237152
6
Iteration 10900: Loss = -11347.714686887075
7
Iteration 11000: Loss = -11347.706009155927
8
Iteration 11100: Loss = -11347.71472946115
9
Iteration 11200: Loss = -11347.705221472002
Iteration 11300: Loss = -11347.712969642573
1
Iteration 11400: Loss = -11347.72006622336
2
Iteration 11500: Loss = -11347.707645407088
3
Iteration 11600: Loss = -11347.70758825138
4
Iteration 11700: Loss = -11347.711656894113
5
Iteration 11800: Loss = -11347.70602030417
6
Iteration 11900: Loss = -11347.768537184167
7
Iteration 12000: Loss = -11347.707235917702
8
Iteration 12100: Loss = -11347.705700313174
9
Iteration 12200: Loss = -11347.707723979403
10
Iteration 12300: Loss = -11347.709878457905
11
Iteration 12400: Loss = -11347.722931521832
12
Iteration 12500: Loss = -11347.715035243182
13
Iteration 12600: Loss = -11347.715088532357
14
Iteration 12700: Loss = -11347.828907089273
15
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.7723, 0.2277],
        [0.2504, 0.7496]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5702, 0.4298], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.0943],
         [0.5385, 0.4001]],

        [[0.6059, 0.0978],
         [0.6751, 0.6475]],

        [[0.5334, 0.1053],
         [0.5945, 0.5724]],

        [[0.6512, 0.0953],
         [0.5541, 0.6239]],

        [[0.5997, 0.1011],
         [0.5018, 0.6356]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9760959038379515
Average Adjusted Rand Index: 0.9761607826731258
11356.908058210178
[0.9760959038379515, 0.9760959038379515] [0.9761607826731258, 0.9761607826731258] [11347.723934379672, 11347.828907089273]
-------------------------------------
This iteration is 97
True Objective function: Loss = -11615.644283716018
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20197.519834358052
Iteration 100: Loss = -12344.838910719673
Iteration 200: Loss = -12039.460453645803
Iteration 300: Loss = -11706.845234294291
Iteration 400: Loss = -11629.836751529901
Iteration 500: Loss = -11620.982525087487
Iteration 600: Loss = -11611.552591642685
Iteration 700: Loss = -11606.10599374272
Iteration 800: Loss = -11606.003694485644
Iteration 900: Loss = -11605.909870605285
Iteration 1000: Loss = -11604.431931209654
Iteration 1100: Loss = -11604.396217835685
Iteration 1200: Loss = -11604.36898257073
Iteration 1300: Loss = -11604.347615567876
Iteration 1400: Loss = -11604.330471912965
Iteration 1500: Loss = -11604.316463889445
Iteration 1600: Loss = -11604.304610627049
Iteration 1700: Loss = -11604.293962937929
Iteration 1800: Loss = -11604.27802971335
Iteration 1900: Loss = -11604.246520894734
Iteration 2000: Loss = -11604.23998069748
Iteration 2100: Loss = -11604.234316339182
Iteration 2200: Loss = -11604.22910565005
Iteration 2300: Loss = -11604.223974408105
Iteration 2400: Loss = -11604.218898911397
Iteration 2500: Loss = -11604.21528097293
Iteration 2600: Loss = -11604.21296318856
Iteration 2700: Loss = -11604.209934627159
Iteration 2800: Loss = -11604.207633016373
Iteration 2900: Loss = -11604.205578651785
Iteration 3000: Loss = -11604.203708665409
Iteration 3100: Loss = -11604.201981619817
Iteration 3200: Loss = -11604.200378130578
Iteration 3300: Loss = -11604.19907042851
Iteration 3400: Loss = -11604.197568614703
Iteration 3500: Loss = -11604.19614149262
Iteration 3600: Loss = -11604.20645117946
1
Iteration 3700: Loss = -11604.189710128145
Iteration 3800: Loss = -11604.18771410619
Iteration 3900: Loss = -11604.197666321683
1
Iteration 4000: Loss = -11604.185720387692
Iteration 4100: Loss = -11604.193234121634
1
Iteration 4200: Loss = -11604.184087531386
Iteration 4300: Loss = -11604.183702141112
Iteration 4400: Loss = -11604.188519296134
1
Iteration 4500: Loss = -11604.182166674978
Iteration 4600: Loss = -11604.1817432317
Iteration 4700: Loss = -11604.1861667176
1
Iteration 4800: Loss = -11604.179876576203
Iteration 4900: Loss = -11604.18464586394
1
Iteration 5000: Loss = -11604.178176191068
Iteration 5100: Loss = -11604.17730902923
Iteration 5200: Loss = -11604.17634072236
Iteration 5300: Loss = -11604.175892526851
Iteration 5400: Loss = -11604.175565111682
Iteration 5500: Loss = -11604.175197618566
Iteration 5600: Loss = -11604.179428654435
1
Iteration 5700: Loss = -11604.174668297585
Iteration 5800: Loss = -11604.181160398397
1
Iteration 5900: Loss = -11604.176282616269
2
Iteration 6000: Loss = -11604.175019569482
3
Iteration 6100: Loss = -11604.173774671632
Iteration 6200: Loss = -11604.174065650663
1
Iteration 6300: Loss = -11604.173390855294
Iteration 6400: Loss = -11604.1732998816
Iteration 6500: Loss = -11604.173088118589
Iteration 6600: Loss = -11604.180510659302
1
Iteration 6700: Loss = -11604.1728164542
Iteration 6800: Loss = -11604.172678972196
Iteration 6900: Loss = -11604.173601057295
1
Iteration 7000: Loss = -11604.172476350084
Iteration 7100: Loss = -11604.172361625402
Iteration 7200: Loss = -11604.17230141575
Iteration 7300: Loss = -11604.172162903898
Iteration 7400: Loss = -11604.207466147805
1
Iteration 7500: Loss = -11604.171978482158
Iteration 7600: Loss = -11604.179333623642
1
Iteration 7700: Loss = -11604.171741953867
Iteration 7800: Loss = -11604.174911643524
1
Iteration 7900: Loss = -11604.170016581209
Iteration 8000: Loss = -11604.140464595961
Iteration 8100: Loss = -11604.146142676136
1
Iteration 8200: Loss = -11604.140175691191
Iteration 8300: Loss = -11604.142295132984
1
Iteration 8400: Loss = -11604.140077631799
Iteration 8500: Loss = -11604.141057433975
1
Iteration 8600: Loss = -11604.150592752183
2
Iteration 8700: Loss = -11604.1436975939
3
Iteration 8800: Loss = -11604.141782801962
4
Iteration 8900: Loss = -11604.139977295134
Iteration 9000: Loss = -11604.142697789595
1
Iteration 9100: Loss = -11604.14673024889
2
Iteration 9200: Loss = -11604.157463453212
3
Iteration 9300: Loss = -11604.147594004431
4
Iteration 9400: Loss = -11604.152529526516
5
Iteration 9500: Loss = -11604.235006113578
6
Iteration 9600: Loss = -11604.142145979065
7
Iteration 9700: Loss = -11604.13636444438
Iteration 9800: Loss = -11604.13496067215
Iteration 9900: Loss = -11604.146888412855
1
Iteration 10000: Loss = -11604.134639304706
Iteration 10100: Loss = -11604.141457617998
1
Iteration 10200: Loss = -11604.188390676185
2
Iteration 10300: Loss = -11604.134503272966
Iteration 10400: Loss = -11604.134568191314
Iteration 10500: Loss = -11604.136686658885
1
Iteration 10600: Loss = -11604.13409607288
Iteration 10700: Loss = -11604.214169218745
1
Iteration 10800: Loss = -11604.137839266215
2
Iteration 10900: Loss = -11604.135667483128
3
Iteration 11000: Loss = -11604.13395075516
Iteration 11100: Loss = -11604.136178065311
1
Iteration 11200: Loss = -11604.134888420469
2
Iteration 11300: Loss = -11604.142210991733
3
Iteration 11400: Loss = -11604.139706872096
4
Iteration 11500: Loss = -11604.13544386716
5
Iteration 11600: Loss = -11604.145805770644
6
Iteration 11700: Loss = -11604.14411724536
7
Iteration 11800: Loss = -11604.135975887812
8
Iteration 11900: Loss = -11604.134478818705
9
Iteration 12000: Loss = -11604.143442102406
10
Iteration 12100: Loss = -11604.150154360226
11
Iteration 12200: Loss = -11604.133939758769
Iteration 12300: Loss = -11604.1357009924
1
Iteration 12400: Loss = -11604.166021457439
2
Iteration 12500: Loss = -11604.148427677157
3
Iteration 12600: Loss = -11604.133719257874
Iteration 12700: Loss = -11604.135573469393
1
Iteration 12800: Loss = -11604.17276013351
2
Iteration 12900: Loss = -11604.14838635091
3
Iteration 13000: Loss = -11604.135090488231
4
Iteration 13100: Loss = -11604.138618471114
5
Iteration 13200: Loss = -11604.133934496145
6
Iteration 13300: Loss = -11604.123022887865
Iteration 13400: Loss = -11604.11856881922
Iteration 13500: Loss = -11604.127875231887
1
Iteration 13600: Loss = -11604.117426238912
Iteration 13700: Loss = -11604.119083005171
1
Iteration 13800: Loss = -11604.283128209194
2
Iteration 13900: Loss = -11604.121248951595
3
Iteration 14000: Loss = -11604.120475153637
4
Iteration 14100: Loss = -11604.131080024737
5
Iteration 14200: Loss = -11604.117864219581
6
Iteration 14300: Loss = -11604.121525769258
7
Iteration 14400: Loss = -11604.125215605074
8
Iteration 14500: Loss = -11604.12371568993
9
Iteration 14600: Loss = -11604.121778900297
10
Iteration 14700: Loss = -11604.12146343321
11
Iteration 14800: Loss = -11604.13136876547
12
Iteration 14900: Loss = -11604.121065831636
13
Iteration 15000: Loss = -11604.121359369945
14
Iteration 15100: Loss = -11604.117421891078
Iteration 15200: Loss = -11604.177658097986
1
Iteration 15300: Loss = -11604.120964791655
2
Iteration 15400: Loss = -11604.122808628557
3
Iteration 15500: Loss = -11604.138649556879
4
Iteration 15600: Loss = -11604.126707573809
5
Iteration 15700: Loss = -11604.171344568605
6
Iteration 15800: Loss = -11604.117656499242
7
Iteration 15900: Loss = -11604.125288219038
8
Iteration 16000: Loss = -11604.223038036169
9
Iteration 16100: Loss = -11604.117322440981
Iteration 16200: Loss = -11604.12065695202
1
Iteration 16300: Loss = -11604.12411303681
2
Iteration 16400: Loss = -11604.13634307905
3
Iteration 16500: Loss = -11604.119999362163
4
Iteration 16600: Loss = -11604.142520347292
5
Iteration 16700: Loss = -11604.119074588529
6
Iteration 16800: Loss = -11604.119164423944
7
Iteration 16900: Loss = -11604.157664727049
8
Iteration 17000: Loss = -11604.184312241112
9
Iteration 17100: Loss = -11604.118972715572
10
Iteration 17200: Loss = -11604.1391838573
11
Iteration 17300: Loss = -11604.118199435217
12
Iteration 17400: Loss = -11604.124483521144
13
Iteration 17500: Loss = -11604.117287867628
Iteration 17600: Loss = -11604.132281754202
1
Iteration 17700: Loss = -11604.123087990516
2
Iteration 17800: Loss = -11604.120397367642
3
Iteration 17900: Loss = -11604.119708754404
4
Iteration 18000: Loss = -11604.14005506914
5
Iteration 18100: Loss = -11604.2220214007
6
Iteration 18200: Loss = -11604.127177280583
7
Iteration 18300: Loss = -11604.119613067678
8
Iteration 18400: Loss = -11604.11760936304
9
Iteration 18500: Loss = -11604.127434933087
10
Iteration 18600: Loss = -11604.117982605809
11
Iteration 18700: Loss = -11604.116917655025
Iteration 18800: Loss = -11604.145238378345
1
Iteration 18900: Loss = -11604.124572316781
2
Iteration 19000: Loss = -11604.171115074125
3
Iteration 19100: Loss = -11604.118551153626
4
Iteration 19200: Loss = -11604.132744069067
5
Iteration 19300: Loss = -11604.244542945074
6
Iteration 19400: Loss = -11604.117795803297
7
Iteration 19500: Loss = -11604.119925076408
8
Iteration 19600: Loss = -11604.117294202299
9
Iteration 19700: Loss = -11604.14072570963
10
Iteration 19800: Loss = -11604.162672201544
11
Iteration 19900: Loss = -11604.122573742316
12
pi: tensor([[0.7569, 0.2431],
        [0.2457, 0.7543]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4689, 0.5311], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1871, 0.1031],
         [0.6203, 0.3913]],

        [[0.5313, 0.1030],
         [0.6911, 0.6692]],

        [[0.6853, 0.1140],
         [0.6218, 0.5430]],

        [[0.6019, 0.1028],
         [0.6060, 0.6028]],

        [[0.6249, 0.0972],
         [0.5380, 0.7093]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961758944611
Average Adjusted Rand Index: 0.9764825449692042
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21754.357253058355
Iteration 100: Loss = -12366.85726411663
Iteration 200: Loss = -12343.399641977332
Iteration 300: Loss = -12090.984951530358
Iteration 400: Loss = -11954.112097562806
Iteration 500: Loss = -11938.371364418459
Iteration 600: Loss = -11937.710357215148
Iteration 700: Loss = -11937.027011846478
Iteration 800: Loss = -11922.304225029753
Iteration 900: Loss = -11915.474939419628
Iteration 1000: Loss = -11912.591505901288
Iteration 1100: Loss = -11912.307872731844
Iteration 1200: Loss = -11910.818322704083
Iteration 1300: Loss = -11910.622895607048
Iteration 1400: Loss = -11910.495228404136
Iteration 1500: Loss = -11910.305357033121
Iteration 1600: Loss = -11907.267214260839
Iteration 1700: Loss = -11906.876117497306
Iteration 1800: Loss = -11905.34300355675
Iteration 1900: Loss = -11904.58975361165
Iteration 2000: Loss = -11904.147851253152
Iteration 2100: Loss = -11902.721941379597
Iteration 2200: Loss = -11902.349602369675
Iteration 2300: Loss = -11902.227396117329
Iteration 2400: Loss = -11902.1775506337
Iteration 2500: Loss = -11902.156466277822
Iteration 2600: Loss = -11900.900119757314
Iteration 2700: Loss = -11900.659990427814
Iteration 2800: Loss = -11900.657912627612
Iteration 2900: Loss = -11900.645483174416
Iteration 3000: Loss = -11900.641498535319
Iteration 3100: Loss = -11900.644720528835
1
Iteration 3200: Loss = -11900.635219812966
Iteration 3300: Loss = -11900.633963202015
Iteration 3400: Loss = -11900.630265207272
Iteration 3500: Loss = -11900.628236143795
Iteration 3600: Loss = -11900.636087371835
1
Iteration 3700: Loss = -11900.62547048248
Iteration 3800: Loss = -11900.62348458384
Iteration 3900: Loss = -11900.621656279349
Iteration 4000: Loss = -11900.620638324952
Iteration 4100: Loss = -11900.620540852979
Iteration 4200: Loss = -11900.61805824628
Iteration 4300: Loss = -11900.617014975063
Iteration 4400: Loss = -11900.616132712305
Iteration 4500: Loss = -11900.6152302991
Iteration 4600: Loss = -11900.61445958207
Iteration 4700: Loss = -11900.613686506047
Iteration 4800: Loss = -11900.612964969183
Iteration 4900: Loss = -11900.61240799763
Iteration 5000: Loss = -11900.611754080424
Iteration 5100: Loss = -11900.6111458301
Iteration 5200: Loss = -11900.610912125325
Iteration 5300: Loss = -11900.61354927329
1
Iteration 5400: Loss = -11900.609514230426
Iteration 5500: Loss = -11900.609650763019
1
Iteration 5600: Loss = -11900.608575943648
Iteration 5700: Loss = -11900.60822553517
Iteration 5800: Loss = -11900.608789531027
1
Iteration 5900: Loss = -11900.611650889869
2
Iteration 6000: Loss = -11900.608199262302
Iteration 6100: Loss = -11900.607509896356
Iteration 6200: Loss = -11900.606533795255
Iteration 6300: Loss = -11900.606615240002
Iteration 6400: Loss = -11900.608796026798
1
Iteration 6500: Loss = -11900.605697508365
Iteration 6600: Loss = -11900.605729716166
Iteration 6700: Loss = -11900.60533462846
Iteration 6800: Loss = -11900.605482973382
1
Iteration 6900: Loss = -11900.604688459774
Iteration 7000: Loss = -11900.604159143715
Iteration 7100: Loss = -11900.60562906249
1
Iteration 7200: Loss = -11900.603685938482
Iteration 7300: Loss = -11900.603643363655
Iteration 7400: Loss = -11900.606569875312
1
Iteration 7500: Loss = -11900.605147686927
2
Iteration 7600: Loss = -11900.61781158543
3
Iteration 7700: Loss = -11900.603007315263
Iteration 7800: Loss = -11900.604468788566
1
Iteration 7900: Loss = -11900.602824575699
Iteration 8000: Loss = -11900.604592214106
1
Iteration 8100: Loss = -11900.602607897195
Iteration 8200: Loss = -11900.602769959249
1
Iteration 8300: Loss = -11900.602425710298
Iteration 8400: Loss = -11900.603535129845
1
Iteration 8500: Loss = -11900.602285642577
Iteration 8600: Loss = -11900.642710961105
1
Iteration 8700: Loss = -11900.601999251923
Iteration 8800: Loss = -11900.601306930183
Iteration 8900: Loss = -11900.601822252676
1
Iteration 9000: Loss = -11900.60109475682
Iteration 9100: Loss = -11900.601025573193
Iteration 9200: Loss = -11900.601001797371
Iteration 9300: Loss = -11900.601050358915
Iteration 9400: Loss = -11900.613972036212
1
Iteration 9500: Loss = -11900.608502233066
2
Iteration 9600: Loss = -11900.61783285815
3
Iteration 9700: Loss = -11900.601288572616
4
Iteration 9800: Loss = -11900.600782542879
Iteration 9900: Loss = -11900.605293010909
1
Iteration 10000: Loss = -11900.60078601586
Iteration 10100: Loss = -11900.600710538642
Iteration 10200: Loss = -11900.607345358743
1
Iteration 10300: Loss = -11900.600620999989
Iteration 10400: Loss = -11900.600718062551
Iteration 10500: Loss = -11900.600595181724
Iteration 10600: Loss = -11900.60051443721
Iteration 10700: Loss = -11900.604192759301
1
Iteration 10800: Loss = -11900.60303800177
2
Iteration 10900: Loss = -11900.611446999925
3
Iteration 11000: Loss = -11900.619853813852
4
Iteration 11100: Loss = -11900.602399345948
5
Iteration 11200: Loss = -11900.601437419218
6
Iteration 11300: Loss = -11900.649799771492
7
Iteration 11400: Loss = -11900.605991425882
8
Iteration 11500: Loss = -11900.60069792547
9
Iteration 11600: Loss = -11900.616494167394
10
Iteration 11700: Loss = -11900.600742666586
11
Iteration 11800: Loss = -11900.608575169748
12
Iteration 11900: Loss = -11900.636160078298
13
Iteration 12000: Loss = -11900.601969834666
14
Iteration 12100: Loss = -11900.60125223771
15
Stopping early at iteration 12100 due to no improvement.
pi: tensor([[0.5803, 0.4197],
        [0.4676, 0.5324]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7625, 0.2375], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2313, 0.0953],
         [0.6493, 0.3664]],

        [[0.5839, 0.1021],
         [0.5007, 0.5875]],

        [[0.6856, 0.1138],
         [0.6357, 0.6639]],

        [[0.6188, 0.0986],
         [0.6340, 0.5283]],

        [[0.5848, 0.1001],
         [0.7079, 0.5609]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 23
Adjusted Rand Index: 0.28565910821000057
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 27
Adjusted Rand Index: 0.20478084311186626
Global Adjusted Rand Index: 0.14886031259986143
Average Adjusted Rand Index: 0.6742480608055323
11615.644283716018
[0.9760961758944611, 0.14886031259986143] [0.9764825449692042, 0.6742480608055323] [11604.117075879269, 11900.60125223771]
-------------------------------------
This iteration is 98
True Objective function: Loss = -11605.259620710178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21501.32423500079
Iteration 100: Loss = -12400.670886890972
Iteration 200: Loss = -12245.816720291548
Iteration 300: Loss = -11717.663841652986
Iteration 400: Loss = -11643.860938868229
Iteration 500: Loss = -11630.330962597607
Iteration 600: Loss = -11625.578831090696
Iteration 700: Loss = -11616.698429748092
Iteration 800: Loss = -11615.27381821066
Iteration 900: Loss = -11610.290372044437
Iteration 1000: Loss = -11610.191841306603
Iteration 1100: Loss = -11610.117048454827
Iteration 1200: Loss = -11610.053816483349
Iteration 1300: Loss = -11609.999364877754
Iteration 1400: Loss = -11599.588231773814
Iteration 1500: Loss = -11599.486860008117
Iteration 1600: Loss = -11599.461730171724
Iteration 1700: Loss = -11599.44158512792
Iteration 1800: Loss = -11599.424959587717
Iteration 1900: Loss = -11599.410954602617
Iteration 2000: Loss = -11599.399047766767
Iteration 2100: Loss = -11599.38883844624
Iteration 2200: Loss = -11599.379958181453
Iteration 2300: Loss = -11599.372239958337
Iteration 2400: Loss = -11599.365371410488
Iteration 2500: Loss = -11599.359331174972
Iteration 2600: Loss = -11599.353905464188
Iteration 2700: Loss = -11599.349111358193
Iteration 2800: Loss = -11599.344752057175
Iteration 2900: Loss = -11599.34081937995
Iteration 3000: Loss = -11599.340386804075
Iteration 3100: Loss = -11599.334094400452
Iteration 3200: Loss = -11599.331193109392
Iteration 3300: Loss = -11599.3285370451
Iteration 3400: Loss = -11599.326081402467
Iteration 3500: Loss = -11599.323766656316
Iteration 3600: Loss = -11599.321631939312
Iteration 3700: Loss = -11599.31950446931
Iteration 3800: Loss = -11599.317479605703
Iteration 3900: Loss = -11599.31615397995
Iteration 4000: Loss = -11599.313941232413
Iteration 4100: Loss = -11599.312792864863
Iteration 4200: Loss = -11599.311288303685
Iteration 4300: Loss = -11599.309970380296
Iteration 4400: Loss = -11599.316673028923
1
Iteration 4500: Loss = -11599.307786799667
Iteration 4600: Loss = -11599.307122485223
Iteration 4700: Loss = -11599.305727760486
Iteration 4800: Loss = -11599.304581298326
Iteration 4900: Loss = -11599.301982651872
Iteration 5000: Loss = -11599.2923562709
Iteration 5100: Loss = -11599.30156679241
1
Iteration 5200: Loss = -11599.290790812305
Iteration 5300: Loss = -11599.290137945343
Iteration 5400: Loss = -11599.289572716454
Iteration 5500: Loss = -11599.289013788177
Iteration 5600: Loss = -11599.28899639849
Iteration 5700: Loss = -11599.28801421443
Iteration 5800: Loss = -11599.287497193753
Iteration 5900: Loss = -11599.290983952189
1
Iteration 6000: Loss = -11599.286433755493
Iteration 6100: Loss = -11599.286304677424
Iteration 6200: Loss = -11599.285260674134
Iteration 6300: Loss = -11599.304320022607
1
Iteration 6400: Loss = -11599.277083331532
Iteration 6500: Loss = -11599.273918360994
Iteration 6600: Loss = -11599.27356103858
Iteration 6700: Loss = -11599.27425935999
1
Iteration 6800: Loss = -11599.27377693588
2
Iteration 6900: Loss = -11599.273234958351
Iteration 7000: Loss = -11599.273676228997
1
Iteration 7100: Loss = -11599.274068949315
2
Iteration 7200: Loss = -11599.272194232622
Iteration 7300: Loss = -11599.275089607208
1
Iteration 7400: Loss = -11599.275171892312
2
Iteration 7500: Loss = -11599.342950653909
3
Iteration 7600: Loss = -11599.272041308192
Iteration 7700: Loss = -11599.271349490205
Iteration 7800: Loss = -11599.2735948694
1
Iteration 7900: Loss = -11599.278033015176
2
Iteration 8000: Loss = -11599.26850442275
Iteration 8100: Loss = -11599.289964164402
1
Iteration 8200: Loss = -11599.268226943144
Iteration 8300: Loss = -11599.298702258677
1
Iteration 8400: Loss = -11599.26406461624
Iteration 8500: Loss = -11599.286245950572
1
Iteration 8600: Loss = -11599.26452734984
2
Iteration 8700: Loss = -11599.263691946899
Iteration 8800: Loss = -11599.264316172255
1
Iteration 8900: Loss = -11599.263899266938
2
Iteration 9000: Loss = -11599.262201515
Iteration 9100: Loss = -11599.273162842455
1
Iteration 9200: Loss = -11599.261675364573
Iteration 9300: Loss = -11599.34561830866
1
Iteration 9400: Loss = -11599.262746473834
2
Iteration 9500: Loss = -11599.265480047574
3
Iteration 9600: Loss = -11599.264175379329
4
Iteration 9700: Loss = -11599.276511097734
5
Iteration 9800: Loss = -11599.26143852897
Iteration 9900: Loss = -11599.291998869883
1
Iteration 10000: Loss = -11599.324938539683
2
Iteration 10100: Loss = -11599.272838306038
3
Iteration 10200: Loss = -11599.265855883034
4
Iteration 10300: Loss = -11599.271079134176
5
Iteration 10400: Loss = -11599.324355411805
6
Iteration 10500: Loss = -11599.326823398913
7
Iteration 10600: Loss = -11599.275590590889
8
Iteration 10700: Loss = -11599.263285081739
9
Iteration 10800: Loss = -11599.262058526569
10
Iteration 10900: Loss = -11599.28874317463
11
Iteration 11000: Loss = -11599.281500945985
12
Iteration 11100: Loss = -11599.26222433351
13
Iteration 11200: Loss = -11599.311453508966
14
Iteration 11300: Loss = -11599.264358661987
15
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[0.7550, 0.2450],
        [0.2244, 0.7756]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5704, 0.4296], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3936, 0.1048],
         [0.6571, 0.1965]],

        [[0.5878, 0.0942],
         [0.5496, 0.6567]],

        [[0.7112, 0.1029],
         [0.6897, 0.6475]],

        [[0.6486, 0.1039],
         [0.5717, 0.6632]],

        [[0.5100, 0.0957],
         [0.5864, 0.6648]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9919999944811108
Average Adjusted Rand Index: 0.9919998119331364
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23619.716719296084
Iteration 100: Loss = -12207.715674453359
Iteration 200: Loss = -11633.34550808152
Iteration 300: Loss = -11601.621281249858
Iteration 400: Loss = -11601.170426596016
Iteration 500: Loss = -11599.7232679965
Iteration 600: Loss = -11599.474782573268
Iteration 700: Loss = -11599.433514376264
Iteration 800: Loss = -11599.404149341222
Iteration 900: Loss = -11599.38146877768
Iteration 1000: Loss = -11599.366080613732
Iteration 1100: Loss = -11599.354313595388
Iteration 1200: Loss = -11599.344945800844
Iteration 1300: Loss = -11599.337114315891
Iteration 1400: Loss = -11599.329305836762
Iteration 1500: Loss = -11599.322445367856
Iteration 1600: Loss = -11599.318050412976
Iteration 1700: Loss = -11599.314226908005
Iteration 1800: Loss = -11599.310756511204
Iteration 1900: Loss = -11599.30664174465
Iteration 2000: Loss = -11599.303404628627
Iteration 2100: Loss = -11599.300793212995
Iteration 2200: Loss = -11599.29863410174
Iteration 2300: Loss = -11599.296824724193
Iteration 2400: Loss = -11599.295270122626
Iteration 2500: Loss = -11599.293245150286
Iteration 2600: Loss = -11599.281128377857
Iteration 2700: Loss = -11599.2806236874
Iteration 2800: Loss = -11599.27900798372
Iteration 2900: Loss = -11599.277971334941
Iteration 3000: Loss = -11599.277136291785
Iteration 3100: Loss = -11599.276488582489
Iteration 3200: Loss = -11599.275863355995
Iteration 3300: Loss = -11599.275234580798
Iteration 3400: Loss = -11599.275072350754
Iteration 3500: Loss = -11599.274270199501
Iteration 3600: Loss = -11599.273819682461
Iteration 3700: Loss = -11599.275279552301
1
Iteration 3800: Loss = -11599.273052769133
Iteration 3900: Loss = -11599.276787899342
1
Iteration 4000: Loss = -11599.274687018158
2
Iteration 4100: Loss = -11599.272108269317
Iteration 4200: Loss = -11599.281121396187
1
Iteration 4300: Loss = -11599.271571458268
Iteration 4400: Loss = -11599.272447432015
1
Iteration 4500: Loss = -11599.271113571485
Iteration 4600: Loss = -11599.271453053969
1
Iteration 4700: Loss = -11599.275927853509
2
Iteration 4800: Loss = -11599.272215546236
3
Iteration 4900: Loss = -11599.270513885642
Iteration 5000: Loss = -11599.270255912474
Iteration 5100: Loss = -11599.270067522428
Iteration 5200: Loss = -11599.270082968493
Iteration 5300: Loss = -11599.269950091248
Iteration 5400: Loss = -11599.269707957426
Iteration 5500: Loss = -11599.270436093197
1
Iteration 5600: Loss = -11599.272580360499
2
Iteration 5700: Loss = -11599.271424555553
3
Iteration 5800: Loss = -11599.269315464995
Iteration 5900: Loss = -11599.270930891074
1
Iteration 6000: Loss = -11599.269430456723
2
Iteration 6100: Loss = -11599.274660950832
3
Iteration 6200: Loss = -11599.26900037182
Iteration 6300: Loss = -11599.269013763625
Iteration 6400: Loss = -11599.268903953367
Iteration 6500: Loss = -11599.269271241137
1
Iteration 6600: Loss = -11599.268909744633
Iteration 6700: Loss = -11599.26868275156
Iteration 6800: Loss = -11599.269174522897
1
Iteration 6900: Loss = -11599.26905269448
2
Iteration 7000: Loss = -11599.268560586546
Iteration 7100: Loss = -11599.274963574937
1
Iteration 7200: Loss = -11599.268685232199
2
Iteration 7300: Loss = -11599.273998645796
3
Iteration 7400: Loss = -11599.270181482427
4
Iteration 7500: Loss = -11599.27256646807
5
Iteration 7600: Loss = -11599.268839439854
6
Iteration 7700: Loss = -11599.268503001807
Iteration 7800: Loss = -11599.268352115947
Iteration 7900: Loss = -11599.268255976114
Iteration 8000: Loss = -11599.287993955964
1
Iteration 8100: Loss = -11599.268412510972
2
Iteration 8200: Loss = -11599.268607171933
3
Iteration 8300: Loss = -11599.268240142957
Iteration 8400: Loss = -11599.268245275365
Iteration 8500: Loss = -11599.279306122826
1
Iteration 8600: Loss = -11599.268117985097
Iteration 8700: Loss = -11599.268121046465
Iteration 8800: Loss = -11599.269129439128
1
Iteration 8900: Loss = -11599.299444771535
2
Iteration 9000: Loss = -11599.267658044528
Iteration 9100: Loss = -11599.33985797094
1
Iteration 9200: Loss = -11599.26737590068
Iteration 9300: Loss = -11599.278516324726
1
Iteration 9400: Loss = -11599.271794831118
2
Iteration 9500: Loss = -11599.268620443303
3
Iteration 9600: Loss = -11599.292186316887
4
Iteration 9700: Loss = -11599.269531140953
5
Iteration 9800: Loss = -11599.269107443259
6
Iteration 9900: Loss = -11599.27026512765
7
Iteration 10000: Loss = -11599.275643863888
8
Iteration 10100: Loss = -11599.269097905113
9
Iteration 10200: Loss = -11599.273902566547
10
Iteration 10300: Loss = -11599.272754234624
11
Iteration 10400: Loss = -11599.269366382578
12
Iteration 10500: Loss = -11599.271866367479
13
Iteration 10600: Loss = -11599.271131098696
14
Iteration 10700: Loss = -11599.284177786685
15
Stopping early at iteration 10700 due to no improvement.
pi: tensor([[0.7756, 0.2244],
        [0.2479, 0.7521]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4315, 0.5685], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.1053],
         [0.6059, 0.3946]],

        [[0.5773, 0.0942],
         [0.5901, 0.5640]],

        [[0.6720, 0.1034],
         [0.5450, 0.7243]],

        [[0.6854, 0.1033],
         [0.6184, 0.6037]],

        [[0.6159, 0.0958],
         [0.5582, 0.7245]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9919999944811108
Average Adjusted Rand Index: 0.9919998119331364
11605.259620710178
[0.9919999944811108, 0.9919999944811108] [0.9919998119331364, 0.9919998119331364] [11599.264358661987, 11599.284177786685]
-------------------------------------
This iteration is 99
True Objective function: Loss = -11648.28492923985
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23377.180229851034
Iteration 100: Loss = -12420.272036256734
Iteration 200: Loss = -12393.61549138974
Iteration 300: Loss = -12117.900141315307
Iteration 400: Loss = -11709.191309602285
Iteration 500: Loss = -11677.404327869277
Iteration 600: Loss = -11652.996810610632
Iteration 700: Loss = -11652.360897340146
Iteration 800: Loss = -11652.015424299356
Iteration 900: Loss = -11651.797387098688
Iteration 1000: Loss = -11651.64835019424
Iteration 1100: Loss = -11651.540316605611
Iteration 1200: Loss = -11651.458742158422
Iteration 1300: Loss = -11651.395310078027
Iteration 1400: Loss = -11651.344787159676
Iteration 1500: Loss = -11651.30364500836
Iteration 1600: Loss = -11651.269449666408
Iteration 1700: Loss = -11651.240424443733
Iteration 1800: Loss = -11651.214693807713
Iteration 1900: Loss = -11651.187506902388
Iteration 2000: Loss = -11646.02222619936
Iteration 2100: Loss = -11645.919806979753
Iteration 2200: Loss = -11645.903537452661
Iteration 2300: Loss = -11645.889334106638
Iteration 2400: Loss = -11645.875453365697
Iteration 2500: Loss = -11645.855577017257
Iteration 2600: Loss = -11645.097173220744
Iteration 2700: Loss = -11645.03395700402
Iteration 2800: Loss = -11645.025318869979
Iteration 2900: Loss = -11645.01739131188
Iteration 3000: Loss = -11645.009107656895
Iteration 3100: Loss = -11644.997040630218
Iteration 3200: Loss = -11644.940780494439
Iteration 3300: Loss = -11644.921395396284
Iteration 3400: Loss = -11644.916577351158
Iteration 3500: Loss = -11644.911742914568
Iteration 3600: Loss = -11644.907941687768
Iteration 3700: Loss = -11644.904211126574
Iteration 3800: Loss = -11644.899697195864
Iteration 3900: Loss = -11644.910472023
1
Iteration 4000: Loss = -11644.893012011518
Iteration 4100: Loss = -11644.891411549363
Iteration 4200: Loss = -11644.894422353327
1
Iteration 4300: Loss = -11644.886844716872
Iteration 4400: Loss = -11644.885303569763
Iteration 4500: Loss = -11644.883461824884
Iteration 4600: Loss = -11644.882295660353
Iteration 4700: Loss = -11644.88730971158
1
Iteration 4800: Loss = -11644.879094092124
Iteration 4900: Loss = -11644.884993402567
1
Iteration 5000: Loss = -11644.87611600454
Iteration 5100: Loss = -11644.881627994542
1
Iteration 5200: Loss = -11644.871698062985
Iteration 5300: Loss = -11644.866600474346
Iteration 5400: Loss = -11644.86185945776
Iteration 5500: Loss = -11644.869670880169
1
Iteration 5600: Loss = -11644.858306249396
Iteration 5700: Loss = -11644.858917199572
1
Iteration 5800: Loss = -11644.856879798539
Iteration 5900: Loss = -11644.856634735805
Iteration 6000: Loss = -11644.855652105505
Iteration 6100: Loss = -11644.856601215422
1
Iteration 6200: Loss = -11644.854832197609
Iteration 6300: Loss = -11644.854096293808
Iteration 6400: Loss = -11644.853754089985
Iteration 6500: Loss = -11644.85652204695
1
Iteration 6600: Loss = -11644.85320447653
Iteration 6700: Loss = -11644.852564566012
Iteration 6800: Loss = -11644.852103331095
Iteration 6900: Loss = -11644.85246879833
1
Iteration 7000: Loss = -11644.85270629442
2
Iteration 7100: Loss = -11644.855917993023
3
Iteration 7200: Loss = -11644.851230067969
Iteration 7300: Loss = -11644.851204972863
Iteration 7400: Loss = -11644.850407915537
Iteration 7500: Loss = -11644.854492620942
1
Iteration 7600: Loss = -11644.84998518474
Iteration 7700: Loss = -11644.849772992713
Iteration 7800: Loss = -11644.850202906415
1
Iteration 7900: Loss = -11644.859278998105
2
Iteration 8000: Loss = -11644.849247500066
Iteration 8100: Loss = -11644.84926533923
Iteration 8200: Loss = -11644.860423968335
1
Iteration 8300: Loss = -11644.848760416495
Iteration 8400: Loss = -11644.855167284535
1
Iteration 8500: Loss = -11644.848462418764
Iteration 8600: Loss = -11644.897232239291
1
Iteration 8700: Loss = -11644.848265232447
Iteration 8800: Loss = -11644.859517231154
1
Iteration 8900: Loss = -11644.89712587477
2
Iteration 9000: Loss = -11644.847980322344
Iteration 9100: Loss = -11644.848668698838
1
Iteration 9200: Loss = -11644.849795257645
2
Iteration 9300: Loss = -11644.847698065721
Iteration 9400: Loss = -11644.847625399867
Iteration 9500: Loss = -11644.850583841688
1
Iteration 9600: Loss = -11644.848288385823
2
Iteration 9700: Loss = -11644.848316674903
3
Iteration 9800: Loss = -11644.901730253481
4
Iteration 9900: Loss = -11644.847336434825
Iteration 10000: Loss = -11644.848396649075
1
Iteration 10100: Loss = -11644.884854412585
2
Iteration 10200: Loss = -11644.866633770653
3
Iteration 10300: Loss = -11644.851206352532
4
Iteration 10400: Loss = -11644.847028723681
Iteration 10500: Loss = -11644.84830367608
1
Iteration 10600: Loss = -11644.84876685155
2
Iteration 10700: Loss = -11644.847855308006
3
Iteration 10800: Loss = -11644.858729947622
4
Iteration 10900: Loss = -11644.846806825686
Iteration 11000: Loss = -11644.853300854997
1
Iteration 11100: Loss = -11644.853138600643
2
Iteration 11200: Loss = -11644.84705852861
3
Iteration 11300: Loss = -11644.846913658932
4
Iteration 11400: Loss = -11644.849714307244
5
Iteration 11500: Loss = -11644.847930745382
6
Iteration 11600: Loss = -11644.890981358856
7
Iteration 11700: Loss = -11644.85281533929
8
Iteration 11800: Loss = -11644.848770059234
9
Iteration 11900: Loss = -11644.855690221175
10
Iteration 12000: Loss = -11644.869818200365
11
Iteration 12100: Loss = -11644.846854495821
Iteration 12200: Loss = -11644.847662258973
1
Iteration 12300: Loss = -11644.849686355861
2
Iteration 12400: Loss = -11644.894792978546
3
Iteration 12500: Loss = -11644.84482750843
Iteration 12600: Loss = -11644.844784477016
Iteration 12700: Loss = -11644.85285243082
1
Iteration 12800: Loss = -11644.945992599696
2
Iteration 12900: Loss = -11644.84897673751
3
Iteration 13000: Loss = -11644.845485080248
4
Iteration 13100: Loss = -11644.845210073287
5
Iteration 13200: Loss = -11644.851982482942
6
Iteration 13300: Loss = -11644.86080688838
7
Iteration 13400: Loss = -11644.849466178282
8
Iteration 13500: Loss = -11644.866121400322
9
Iteration 13600: Loss = -11644.959414407755
10
Iteration 13700: Loss = -11644.855180337254
11
Iteration 13800: Loss = -11644.847478529142
12
Iteration 13900: Loss = -11644.84669365838
13
Iteration 14000: Loss = -11644.845211599955
14
Iteration 14100: Loss = -11644.84470439059
Iteration 14200: Loss = -11644.845587091633
1
Iteration 14300: Loss = -11644.847534816201
2
Iteration 14400: Loss = -11644.910519532614
3
Iteration 14500: Loss = -11644.843193020157
Iteration 14600: Loss = -11644.853201691636
1
Iteration 14700: Loss = -11644.847282700275
2
Iteration 14800: Loss = -11644.842079331755
Iteration 14900: Loss = -11644.844053764666
1
Iteration 15000: Loss = -11644.854665785424
2
Iteration 15100: Loss = -11644.883833301046
3
Iteration 15200: Loss = -11644.85642427772
4
Iteration 15300: Loss = -11644.978085195864
5
Iteration 15400: Loss = -11644.851869505821
6
Iteration 15500: Loss = -11644.842616439271
7
Iteration 15600: Loss = -11644.845776375172
8
Iteration 15700: Loss = -11644.964729191233
9
Iteration 15800: Loss = -11644.852933554468
10
Iteration 15900: Loss = -11644.843035406839
11
Iteration 16000: Loss = -11644.848398023607
12
Iteration 16100: Loss = -11644.844265874075
13
Iteration 16200: Loss = -11644.842987059486
14
Iteration 16300: Loss = -11644.858477743277
15
Stopping early at iteration 16300 due to no improvement.
pi: tensor([[0.7604, 0.2396],
        [0.2588, 0.7412]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4859, 0.5141], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1969, 0.1078],
         [0.5885, 0.3991]],

        [[0.5346, 0.0989],
         [0.6156, 0.5204]],

        [[0.6995, 0.1075],
         [0.6592, 0.6055]],

        [[0.6765, 0.1026],
         [0.6692, 0.7185]],

        [[0.5317, 0.1010],
         [0.6993, 0.6311]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21466.152685787365
Iteration 100: Loss = -12417.999185316796
Iteration 200: Loss = -12393.671744799702
Iteration 300: Loss = -12028.165144421999
Iteration 400: Loss = -11667.421140515899
Iteration 500: Loss = -11654.051874519715
Iteration 600: Loss = -11653.274760229106
Iteration 700: Loss = -11646.62122521416
Iteration 800: Loss = -11646.343162698364
Iteration 900: Loss = -11646.097489411182
Iteration 1000: Loss = -11645.339098185023
Iteration 1100: Loss = -11645.268878182751
Iteration 1200: Loss = -11645.216112702654
Iteration 1300: Loss = -11645.174835564056
Iteration 1400: Loss = -11645.141698484344
Iteration 1500: Loss = -11645.114414558122
Iteration 1600: Loss = -11645.091739260148
Iteration 1700: Loss = -11645.072983637658
Iteration 1800: Loss = -11645.057352649623
Iteration 1900: Loss = -11645.044072579454
Iteration 2000: Loss = -11645.032624334079
Iteration 2100: Loss = -11645.02263351806
Iteration 2200: Loss = -11645.013747816462
Iteration 2300: Loss = -11645.009782667337
Iteration 2400: Loss = -11644.998308717022
Iteration 2500: Loss = -11644.991732095397
Iteration 2600: Loss = -11644.986080098428
Iteration 2700: Loss = -11644.980990436752
Iteration 2800: Loss = -11644.976340060899
Iteration 2900: Loss = -11644.971344783713
Iteration 3000: Loss = -11644.958183055413
Iteration 3100: Loss = -11644.896830622845
Iteration 3200: Loss = -11644.893669836547
Iteration 3300: Loss = -11644.893061238252
Iteration 3400: Loss = -11644.888471289456
Iteration 3500: Loss = -11644.886275463672
Iteration 3600: Loss = -11644.884242554906
Iteration 3700: Loss = -11644.882402538065
Iteration 3800: Loss = -11644.880656390853
Iteration 3900: Loss = -11644.879049268497
Iteration 4000: Loss = -11644.877715443796
Iteration 4100: Loss = -11644.876237581286
Iteration 4200: Loss = -11644.874945762127
Iteration 4300: Loss = -11644.873907990563
Iteration 4400: Loss = -11644.873252201622
Iteration 4500: Loss = -11644.87484638895
1
Iteration 4600: Loss = -11644.870549159763
Iteration 4700: Loss = -11644.872121825882
1
Iteration 4800: Loss = -11644.868327833952
Iteration 4900: Loss = -11644.866456531458
Iteration 5000: Loss = -11644.86519893203
Iteration 5100: Loss = -11644.869902312397
1
Iteration 5200: Loss = -11644.862132316426
Iteration 5300: Loss = -11644.86168233536
Iteration 5400: Loss = -11644.868008128768
1
Iteration 5500: Loss = -11644.86033020558
Iteration 5600: Loss = -11644.859667347546
Iteration 5700: Loss = -11644.860196519427
1
Iteration 5800: Loss = -11644.86242340754
2
Iteration 5900: Loss = -11644.863246234396
3
Iteration 6000: Loss = -11644.857758234566
Iteration 6100: Loss = -11644.915942097754
1
Iteration 6200: Loss = -11644.871958408185
2
Iteration 6300: Loss = -11644.914933055028
3
Iteration 6400: Loss = -11644.857151700633
Iteration 6500: Loss = -11644.855779809346
Iteration 6600: Loss = -11644.856429770518
1
Iteration 6700: Loss = -11644.850205244027
Iteration 6800: Loss = -11644.850623757337
1
Iteration 6900: Loss = -11644.849611803586
Iteration 7000: Loss = -11644.852479081119
1
Iteration 7100: Loss = -11644.851732864068
2
Iteration 7200: Loss = -11644.849030713105
Iteration 7300: Loss = -11644.848871233233
Iteration 7400: Loss = -11644.85671549647
1
Iteration 7500: Loss = -11644.84964182991
2
Iteration 7600: Loss = -11644.850894877358
3
Iteration 7700: Loss = -11644.85280173175
4
Iteration 7800: Loss = -11644.848133755775
Iteration 7900: Loss = -11644.84962904223
1
Iteration 8000: Loss = -11644.847888699966
Iteration 8100: Loss = -11644.847842280467
Iteration 8200: Loss = -11644.847668437897
Iteration 8300: Loss = -11644.847620825418
Iteration 8400: Loss = -11644.849604749712
1
Iteration 8500: Loss = -11644.857261743105
2
Iteration 8600: Loss = -11644.91583594373
3
Iteration 8700: Loss = -11644.847275838592
Iteration 8800: Loss = -11644.847149754378
Iteration 8900: Loss = -11644.974465385541
1
Iteration 9000: Loss = -11644.847019296607
Iteration 9100: Loss = -11644.866261755598
1
Iteration 9200: Loss = -11644.859899361365
2
Iteration 9300: Loss = -11644.854421138389
3
Iteration 9400: Loss = -11644.866387889444
4
Iteration 9500: Loss = -11644.846812491802
Iteration 9600: Loss = -11644.852298728169
1
Iteration 9700: Loss = -11644.850985826233
2
Iteration 9800: Loss = -11644.863738070306
3
Iteration 9900: Loss = -11644.847945246082
4
Iteration 10000: Loss = -11644.84964516884
5
Iteration 10100: Loss = -11644.846541501527
Iteration 10200: Loss = -11644.84752318026
1
Iteration 10300: Loss = -11644.926827176147
2
Iteration 10400: Loss = -11644.846374069428
Iteration 10500: Loss = -11644.846865119689
1
Iteration 10600: Loss = -11644.84721506231
2
Iteration 10700: Loss = -11644.846591425872
3
Iteration 10800: Loss = -11644.846770785922
4
Iteration 10900: Loss = -11644.86182865273
5
Iteration 11000: Loss = -11644.855302768685
6
Iteration 11100: Loss = -11644.847965505134
7
Iteration 11200: Loss = -11644.852822229128
8
Iteration 11300: Loss = -11644.89153549656
9
Iteration 11400: Loss = -11644.850307891107
10
Iteration 11500: Loss = -11644.84668179964
11
Iteration 11600: Loss = -11644.846904839223
12
Iteration 11700: Loss = -11644.847139343525
13
Iteration 11800: Loss = -11644.849218594241
14
Iteration 11900: Loss = -11644.89098157419
15
Stopping early at iteration 11900 due to no improvement.
pi: tensor([[0.7411, 0.2589],
        [0.2394, 0.7606]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5144, 0.4856], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4002, 0.1076],
         [0.5885, 0.1966]],

        [[0.6692, 0.0977],
         [0.5085, 0.6092]],

        [[0.5099, 0.1078],
         [0.7310, 0.6305]],

        [[0.6589, 0.1029],
         [0.6105, 0.7270]],

        [[0.6419, 0.1011],
         [0.5860, 0.5799]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 1.0
Average Adjusted Rand Index: 1.0
11648.28492923985
[1.0, 1.0] [1.0, 1.0] [11644.858477743277, 11644.89098157419]
